<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zouyee</title>
  
  <subtitle>life is short, enjoy it</subtitle>
  <link href="https://zoues.com/atom.xml" rel="self"/>
  
  <link href="https://zoues.com/"/>
  <updated>2025-01-25T05:38:26.527Z</updated>
  <id>https://zoues.com/</id>
  
  <author>
    <name>zouyee</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>揭秘JDK 21 faketime演练CPU拉满的迷局</title>
    <link href="https://zoues.com/posts/819cc60d/"/>
    <id>https://zoues.com/posts/819cc60d/</id>
    <published>2025-01-25T05:00:43.000Z</published>
    <updated>2025-01-25T05:38:26.527Z</updated>
    
    <content type="html"><![CDATA[<h1 id="揭秘JDK-21-faketime演练CPU拉满的迷局"><a href="#揭秘JDK-21-faketime演练CPU拉满的迷局" class="headerlink" title="揭秘JDK 21 faketime演练CPU拉满的迷局"></a>揭秘JDK 21 faketime演练CPU拉满的迷局</h1><h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>业务团队需要进行跨年演练，主要通过faketime来模拟未来特定时间点，我们在演练过程中出现CPU上升异常，最终拉满CPU配额，触发CPU throttle现象。</p><p><img src="https://pic1.imgdb.cn/item/6794787cd0e0a243d4f7ba2f.png"></p><p>上述CPU拉满现象在JDK 21版本(容器镜像为AlmaLinux 9)可以稳定复现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 同事提供的复现代码如下所示</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        java.util.Date now = new java.util.Date();</span><br><span class="line">        System.out.println(&quot;time:&quot; + now);</span><br><span class="line"></span><br><span class="line">        for (int k = 0; k &lt; 10; k++) &#123;</span><br><span class="line">            Thread t = new Thread(() -&gt; &#123;</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.sleep(1000);</span><br><span class="line">                &#125; catch (InterruptedException ignore) &#123;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            t.setDaemon(false);</span><br><span class="line">            t.start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过Tomcat环境中的自定义脚本<code>/opt/tomcat/bin/extraenv.sh</code>，用于设置额外的环境变量或运行自定义初始化逻辑，faketime（0.9.10）相关配置如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;&gt;/opt/tomcat/bin/extraenv.sh &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">export LD_PRELOAD=/usr/lib64/faketime/libfaketime.so.1</span><br><span class="line">export FAKETIME=&quot;$&#123;FAKETIME&#125;&quot;</span><br><span class="line">export DONT_FAKE_MONOTONIC=1</span><br><span class="line">EOF</span><br><span class="line">fi</span><br><span class="line">supervisorctl restart tomcat</span><br></pre></td></tr></table></figure><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><h3 id="LD-PRELOAD-简介"><a href="#LD-PRELOAD-简介" class="headerlink" title="LD_PRELOAD 简介"></a>LD_PRELOAD 简介</h3><p>当启动一个动态链接的程序时，它并没有所有需要的函数代码。那么将会发生什么？</p><ol><li>程序被加载到内存中</li><li>动态链接器会找出该程序运行所需的其他库（<code>.so</code> 文件）</li><li>它也会将这些库加载到内存中</li><li>然后将所有东西连接起来</li></ol><p><code>LD_PRELOAD</code>是一个系统环境变量，用于在运行程序时强制加载自定义的共享库。它是 Linux 环境中的动态链接功能的一部分，允许在程序运行时动态地影响共享库的加载行为。</p><p>当一个程序运行时，它会加载所需的共享库（如标准 C 库 <code>libc</code>）。<code>LD_PRELOAD</code> 允许在程序加载其正常依赖的共享库之前，优先加载用户指定的共享库。因此，用户可以用自定义的实现替换某些函数或修改程序的行为，而无需更改程序本身的代码。下述为主要的一些使用场景：</p><p>   a. 函数拦截与重写</p><ul><li><p>通过创建一个共享库，重写某些系统调用或函数的实现（例如 <code>malloc()</code>、<code>open()</code>）。</p></li><li><p>在目标程序运行时，强制优先使用自定义实现，从而修改程序行为。</p><p> b. 性能调试</p></li><li><p>拦截内存分配函数（如 <code>malloc</code> 和 <code>free</code>）以检测内存泄漏。</p></li><li><p>记录函数调用和参数，帮助调试程序运行。</p><p> c. 兼容性修复</p></li><li><p>如果运行的程序依赖某些过时的库或函数，可以通过 <code>LD_PRELOAD</code> 动态提供兼容实现，而不需要更改系统环境。</p><p> d. 时间伪造</p></li><li><p>像 <code>faketime</code> 这样的工具使用 <code>LD_PRELOAD</code> 注入库，从而伪造程序感知到的系统时间。</p><p> e. 安全性测试</p></li><li><p>拦截和检查文件访问、网络调用等敏感操作，模拟安全攻击场景或保护特定资源。</p></li></ul><h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/PATH/DEST.so &lt;<span class="built_in">command</span>&gt;</span><br></pre></td></tr></table></figure><p>   下述为三个主要场景示例：</p><p>   a. 拦截并修改系统调用<br> 假设我们有一个自定义库 <code>myopen.so</code>，重写了 <code>open()</code> 函数以记录文件访问：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">open</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pathname, <span class="type">int</span> flags, ...)</span> &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;File opened: %s\n&quot;</span>, pathname);</span><br><span class="line">    <span class="keyword">return</span> open(pathname, flags);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译为共享库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -shared -fPIC -o myopen.so myopen.c -ldl</span><br></pre></td></tr></table></figure><p>使用 <code>LD_PRELOAD</code> 加载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=./myopen.so <span class="built_in">ls</span></span><br></pre></td></tr></table></figure><p>输出将记录所有文件访问。</p><p>   b. 模拟内存分配问题</p><p>​使用 <code>libmalloc.so</code> 来检测内存分配函数（如 <code>malloc</code> 和 <code>free</code> 的调用次数）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libmalloc.so my_program</span><br></pre></td></tr></table></figure><p>   c. 修复兼容性问题</p><p>​如果程序需要一个旧版本的 <code>libc.so</code>，可以使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/path/to/old_libc.so my_program</span><br></pre></td></tr></table></figure><p>在使用<code>LD_PRELOAD</code> ，需要注意以下几点：</p><p>​a. 优先级</p><ul><li><code>LD_PRELOAD</code> 的库会优先加载并覆盖系统默认的库，但仅对动态链接的程序生效。静态链接的程序不会被影响。</li></ul><p>​    b. 权限限制</p><ul><li>出于安全原因，<code>LD_PRELOAD</code> 通常被忽略在 <code>setuid</code> 或 <code>setgid</code> 程序中，以防止恶意注入。</li></ul><p>​    c. 影响范围</p><ul><li>仅影响当前终端会话或明确使用 <code>LD_PRELOAD</code> 的进程，不会改变全局环境。</li></ul><p>​    d. 调试复杂性</p><ul><li>在覆盖核心系统函数时，使用 <code>LD_PRELOAD</code> 可能引入意外行为。</li></ul><h3 id="faketime简介"><a href="#faketime简介" class="headerlink" title="faketime简介"></a>faketime简介</h3><p><code>libfaketime</code> 是一个与 <code>faketime</code> 工具配套的共享库，通过 <code>LD_PRELOAD</code> 环境变量，在程序载入时优先加载 <code>libfaketime.so.1</code> 共享库。它通过特定虚假时间的逻辑代替 <code>libc</code> 原生提供的时间函数逻辑，实现了为程序指定绝对日期或相对日期的能力，而不影响操作系统的全局时间。</p><ol><li>faketime 设置方式</li></ol><p>libfaketime 接受五种提供 faketime 的方式，包括：</p><p>​a. 通过设置环境变量 <code>FAKETIME</code></p><p>​b. 通过由环境变量 <code>FAKETIME_TIMESTAMP_FILE</code> 声明的文件</p><p>​c. 通过家目录下的 <code>.faketimerc</code> 文件</p><p>​d. 通过系统级别的 <code>/etc/faketimerc</code> 文件</p><p>​e. 通过设置环境变量<code>FAKETIME_UPDATE_TIMESTAMP_FILE</code> 并执行命令 <code>date -s &quot;&quot;</code></p><p>使用 b&#x2F;d&#x2F;d 方式，可以按照语法修改文件内容。由：</p><ul><li><p>环境变量 <code>FAKETIME_CACHE_DURATION</code> 确定的缓存时间，实现 faketime 的热更新</p></li><li><p>或环境变量 <code>FAKETIME_NO_CACHE=1</code> 放弃缓存，实现 faketime 从文件的实时获取</p></li><li><p>或未配置环境变量，使用默认缓存时间 10 秒</p></li></ul><ol start="2"><li>工作原理</li></ol><p><code>libfaketime</code> 拦截常见的时间系统调用，例如：</p><ul><li><code>time()</code></li><li><code>gettimeofday()</code></li><li><code>clock_gettime()</code></li></ul><p>通过注入自己的实现，它会返回伪造的时间值，而不是实际的系统时间。这种方式只对通过动态链接加载的程序有效，静态链接的程序不受影响。faketime有以下几点特性：</p><p>​a. 固定时间<br> 让程序始终感知为某个特定时间点，例如模拟 2025 年 1 月 1 日。</p><p>​b. 时间偏移<br> 以当前时间为基准，增加或减少一定的时间偏移量，例如提前一天或推迟一小时。</p><p>​c. 加速或减速时间<br> 模拟时间流逝的速度变化，例如每秒模拟为两秒（时间加速），或延缓时间流逝速度（时间减速）。</p><p>​d. 动态时间调整<br> 支持通过文件或环境变量动态调整伪造时间的规则，灵活应对测试需求。</p><ol start="3"><li><p>安装与使用</p><p><code>libfaketime</code> 通常可以通过系统的包管理器安装：</p></li></ol><ul><li><p>在 Debian系：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libfaketime</span><br></pre></td></tr></table></figure></li><li><p>在 RHEL系：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install libfaketime</span><br></pre></td></tr></table></figure></li></ul><p>​通过设置 <code>LD_PRELOAD</code>，将 <code>libfaketime</code> 注入目标程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;&lt;伪造时间&gt;&quot;</span> &lt;命令&gt;</span><br></pre></td></tr></table></figure><p>​主要有以下几种场景：</p><p>​a. 固定时间点<br>​ 让程序感知到固定时间，例如 2025 年 1 月 1 日：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;2025-01-01 00:00:00&quot;</span> <span class="built_in">date</span></span><br></pre></td></tr></table></figure><p>​输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wed Jan  1 00:00:00 UTC 2025</span><br></pre></td></tr></table></figure><p>​b. 时间偏移<br>​将时间向前或向后偏移 1 天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;+1d&quot;</span> <span class="built_in">date</span></span><br></pre></td></tr></table></figure><p>​或：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;-1d&quot;</span> <span class="built_in">date</span></span><br></pre></td></tr></table></figure><p>​c. 时间加速<br>     模拟时间加速，让程序运行时间每秒等效于两秒：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;@2x&quot;</span> <span class="built_in">sleep</span> 5</span><br></pre></td></tr></table></figure><p>本应需要 5 秒的任务，现在可能只需 2.5 秒完成。</p><p>​d. 动态调整时间<br>     使用文件配置时间伪造规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib/libfaketime.so.1 FAKETIME=<span class="string">&quot;@&quot;</span> ./my_program</span><br></pre></td></tr></table></figure><p>​通过配置文件（如 <code>/etc/faketimerc</code>）动态调整时间规则。</p><p>​使用faketime需要注意以下几点：</p><p>​a. 仅影响动态链接程序<br>​ 静态链接的程序直接使用系统调用，不会被 <code>libfaketime</code> 拦截，比如<em><strong>Golang</strong></em></p><p>​b. 对多线程程序的影响<br>     多线程程序可能对时间操作敏感，使用 <code>libfaketime</code> 时需要注意可能引发的并发问题。</p><p>​c. 环境隔离<br>     使用 <code>LD_PRELOAD</code> 仅影响当前终端或进程，不会对全局环境产生影响。</p><p>​d. 安全问题<br>     出于安全考虑，某些敏感程序可能会忽略 <code>LD_PRELOAD</code>（如 <code>setuid</code> 程序）。</p><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><blockquote><p> 根据<a href="https://github.com/wolfcw/libfaketime/issues/425">issue</a>的说明，需要针对特定JDK版本，移除变量DONT_FAKE_MONOTONIC</p></blockquote><ol><li>定位JVM时钟</li></ol><p><code>java -Xlog:all=info</code> 是 Java 的一个命令行选项，用于控制 JVM 的日志输出行为, 在排查过程中使用该配置，打印关键信息（连半路出家都算不上）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LD_PRELOAD=/usr/lib64/faketime/libfaketime.so.1 FAKETIME=&quot;@2024-12-22 19:44:32&quot; java -Xlog:all=info Main.java</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[0.006s][info][os       ] Use of pthread_condattr_setclock is supported</span><br><span class="line">[0.006s][info][os       ] Relative timed-wait using pthread_cond_timedwait is associated with CLOCK_MONOTONIC</span><br><span class="line">[0.006s][info][os,thread] Lookup of __pthread_get_minstack succeeded</span><br></pre></td></tr></table></figure><p>可以发现JDK 21底层JVM判断可以使用CLOCK_MONOTONIC时钟是通过pthread_condattr_setclock判断，当 JVM 使用条件变量（如 <code>pthread_cond_timedwait</code>）实现超时等待时，可以选择使用单调时钟（<code>CLOCK_MONOTONIC</code>）来避免系统时间调整对超时行为的影响。</p><p>​2. 排查阻塞点</p><p>通过下述信息可以发现，Thread.Sleep行为调用链以及阻塞点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Thread 2 (Thread 0x7fb00eafa700 (LWP 15984)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb0806e7350, mutex=0x7fb0806e7328, abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be25f in PlatformEvent::park_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0872d8d59 in JavaThread::sleep_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0873989ed in JVM_Sleep () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#6  0x0000000000000000 in ?? ()</span><br></pre></td></tr></table></figure><p>下述为详细DEBUG过程</p><ol><li><p>执行gdb命令，attach java进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gdb -p 15959</span><br></pre></td></tr></table></figure></li><li><p>堆栈回溯</p></li></ol>   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(gdb) bt</span><br><span class="line">#0  0x00007fb088a08017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb089039e6f in CallJavaMainInNewThread () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#2  0x00007fb089036ffd in ContinueInNewThread () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#3  0x00007fb089037a4d in JLI_Launch () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#4  0x000055fa5ded6b0f in main ()</span><br></pre></td></tr></table></figure><ol start="3"><li><p>列出当前所有线程信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">(gdb) info threads</span><br><span class="line">  Id   Target Id         Frame </span><br><span class="line">  25   Thread 0x7fb089023700 (LWP 15960) &quot;java&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  24   Thread 0x7fb08553c700 (LWP 15962) &quot;GC Thread#0&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  23   Thread 0x7fb08543b700 (LWP 15963) &quot;G1 Main Marker&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  22   Thread 0x7fb08533a700 (LWP 15964) &quot;G1 Conc#0&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  21   Thread 0x7fb084a31700 (LWP 15965) &quot;G1 Refine#0&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  20   Thread 0x7fb084930700 (LWP 15966) &quot;G1 Service&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  19   Thread 0x7fb084753700 (LWP 15967) &quot;VM Periodic Tas&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  18   Thread 0x7fb084652700 (LWP 15968) &quot;VM Thread&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  17   Thread 0x7fb0843c8700 (LWP 15969) &quot;Reference Handl&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  16   Thread 0x7fb0842c7700 (LWP 15970) &quot;Finalizer&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  15   Thread 0x7fb0841c6700 (LWP 15971) &quot;Signal Dispatch&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  14   Thread 0x7fb0648af700 (LWP 15972) &quot;Service Thread&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  13   Thread 0x7fb0647ae700 (LWP 15973) &quot;Monitor Deflati&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  12   Thread 0x7fb0646ad700 (LWP 15974) &quot;C2 CompilerThre&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  11   Thread 0x7fb0645ac700 (LWP 15975) &quot;C1 CompilerThre&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  10   Thread 0x7fb0643db700 (LWP 15976) &quot;Common-Cleaner&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  9    Thread 0x7fb0642da700 (LWP 15977) &quot;Notification Th&quot; 0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  8    Thread 0x7fb0641d9700 (LWP 15978) &quot;C2 CompilerThre&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  7    Thread 0x7fb00efff700 (LWP 15979) &quot;C2 CompilerThre&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  6    Thread 0x7fb00eefe700 (LWP 15980) &quot;GC Thread#1&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  5    Thread 0x7fb00edfd700 (LWP 15981) &quot;GC Thread#2&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  4    Thread 0x7fb00ecfc700 (LWP 15982) &quot;GC Thread#3&quot; 0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">  3    Thread 0x7fb00ebfb700 (LWP 15983) &quot;Thread-0&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  2    Thread 0x7fb00eafa700 (LWP 15984) &quot;Thread-1&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">* 1    Thread 0x7fb089024740 (LWP 15959) &quot;java&quot; 0x00007fb088a08017 in pthread_join () from /lib64/libpthread.so.0</span><br></pre></td></tr></table></figure><p>其中关键信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3    Thread 0x7fb00ebfb700 (LWP 15983) &quot;Thread-0&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  2    Thread 0x7fb00eafa700 (LWP 15984) &quot;Thread-1&quot; 0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>显示所有线程的堆栈回溯</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br></pre></td><td class="code"><pre><span class="line">(gdb) thread apply all bt</span><br><span class="line"></span><br><span class="line">Thread 25 (Thread 0x7fb089023700 (LWP 15960)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0879361ea in Threads::destroy_vm() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087355fa2 in jni_DestroyJavaVM () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb089035942 in JavaMain () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#6  0x00007fb089039289 in ThreadJavaMain () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 24 (Thread 0x7fb08553c700 (LWP 15962)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879e4d5b in WorkerThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 23 (Thread 0x7fb08543b700 (LWP 15963)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb08718e80a in G1ConcurrentMarkThread::wait_for_next_cycle() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb08718fa29 in G1ConcurrentMarkThread::run_service() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0870453fb in ConcurrentGCThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#9  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 22 (Thread 0x7fb08533a700 (LWP 15964)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879e4d5b in WorkerThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line">Thread 21 (Thread 0x7fb084a31700 (LWP 15965)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb087195d7c in G1PrimaryConcurrentRefineThread::wait_for_completed_buffers() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0871960f8 in G1ConcurrentRefineThread::run_service() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0870453fb in ConcurrentGCThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#9  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 20 (Thread 0x7fb084930700 (LWP 15966)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb08010b1c0, mutex=0x7fb08010b198, abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0871eac11 in G1ServiceThread::wait_for_task() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0871eafa0 in G1ServiceThread::run_service() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0870453fb in ConcurrentGCThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 19 (Thread 0x7fb084753700 (LWP 15967)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087eefcd0 &lt;mutex_init()::PeriodicTask_lock_storage+48&gt;, mutex=0x7fb087eefca8 &lt;mutex_init()::PeriodicTask_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb08767ed4d in WatcherThread::sleep() const () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb08767ee51 in WatcherThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#9  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 18 (Thread 0x7fb084652700 (LWP 15968)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087eeead0 &lt;mutex_init()::VMOperation_lock_storage+48&gt;, mutex=0x7fb087eeeaa8 &lt;mutex_init()::VMOperation_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879be7f5 in VMThread::wait_for_operation() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879bf318 in VMThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line">#6  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#9  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 17 (Thread 0x7fb0843c8700 (LWP 15969)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb08739937a in JVM_WaitForReferencePendingList () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#5  0x0000000000000004 in ?? ()</span><br><span class="line">#6  0x00007fb0843c7948 in ?? ()</span><br><span class="line">#7  0x0000000000000000 in ?? ()</span><br><span class="line"></span><br><span class="line">Thread 16 (Thread 0x7fb0842c7700 (LWP 15970)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be0ab in PlatformEvent::park() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb08768e3f5 in ObjectMonitor::wait(long, bool, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0878da904 in ObjectSynchronizer::wait(Handle, long, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb08739ad7f in JVM_MonitorWait () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#6  0x00007fb0842c67a8 in ?? ()</span><br><span class="line">#7  0x00007fb06fdf3697 in ?? ()</span><br><span class="line">#8  0x0000000000000000 in ?? ()</span><br><span class="line"></span><br><span class="line">Thread 15 (Thread 0x7fb0841c6700 (LWP 15971)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb08781b93f in os::signal_wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0876a59e5 in signal_thread_entry(JavaThread*, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 14 (Thread 0x7fb0648af700 (LWP 15972)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb08776ca82 in ServiceThread::service_thread_entry(JavaThread*, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 13 (Thread 0x7fb0647ae700 (LWP 15973)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087ef0cd0 &lt;mutex_init()::MonitorDeflation_lock_storage+48&gt;, mutex=0x7fb087ef0ca8 &lt;mutex_init()::MonitorDeflation_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087659353 in MonitorDeflationThread::monitor_deflation_thread_entry(JavaThread*, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#9  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 12 (Thread 0x7fb0646ad700 (LWP 15974)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087ef0250 &lt;mutex_init()::MethodCompileQueue_lock_storage+48&gt;, mutex=0x7fb087ef0228 &lt;mutex_init()::MethodCompileQueue_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087026e76 in CompileQueue::get(CompilerThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb08702ae79 in CompileBroker::compiler_thread_loop() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 11 (Thread 0x7fb0645ac700 (LWP 15975)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087ef0250 &lt;mutex_init()::MethodCompileQueue_lock_storage+48&gt;, mutex=0x7fb087ef0228 &lt;mutex_init()::MethodCompileQueue_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087026e76 in CompileQueue::get(CompilerThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb08702ae79 in CompileBroker::compiler_thread_loop() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 10 (Thread 0x7fb0643db700 (LWP 15976)):</span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb08017cab8, mutex=0x7fb08017ca90, abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be547 in Parker::park(bool, long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0879603c4 in Unsafe_Park () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#5  0x00007fb0643da578 in ?? ()</span><br><span class="line">#6  0x00007fb0643da5e8 in ?? ()</span><br><span class="line">#7  0x00007fb06fdef180 in ?? ()</span><br><span class="line">#8  0x00000000868003f8 in ?? ()</span><br><span class="line">#9  0x00007fb0643da580 in ?? ()</span><br><span class="line">#10 0x0000000000000000 in ?? ()</span><br><span class="line"></span><br><span class="line">Thread 9 (Thread 0x7fb0642da700 (LWP 15977)):</span><br><span class="line">#0  0x00007fb088a0aa35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb0876be95b in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#2  0x00007fb087666369 in Monitor::wait_without_safepoint_check(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb08767f2f2 in NotificationThread::notification_thread_entry(JavaThread*, JavaThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 8 (Thread 0x7fb0641d9700 (LWP 15978)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087ef0250 &lt;mutex_init()::MethodCompileQueue_lock_storage+48&gt;, mutex=0x7fb087ef0228 &lt;mutex_init()::MethodCompileQueue_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087026e76 in CompileQueue::get(CompilerThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb08702ae79 in CompileBroker::compiler_thread_loop() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 7 (Thread 0x7fb00efff700 (LWP 15979)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb087ef0250 &lt;mutex_init()::MethodCompileQueue_lock_storage+48&gt;, mutex=0x7fb087ef0228 &lt;mutex_init()::MethodCompileQueue_lock_storage+8&gt;, </span><br><span class="line">    abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be937 in PlatformMonitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0876663ec in Monitor::wait(unsigned long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb087026e76 in CompileQueue::get(CompilerThread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb08702ae79 in CompileBroker::compiler_thread_loop() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0872d5cd8 in JavaThread::thread_main_inner() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line">#7  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#8  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#9  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#10 0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 6 (Thread 0x7fb00eefe700 (LWP 15980)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879e4d5b in WorkerThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 5 (Thread 0x7fb00edfd700 (LWP 15981)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879e4d5b in WorkerThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 4 (Thread 0x7fb00ecfc700 (LWP 15982)):</span><br><span class="line">#0  0x00007fb088a0cb3b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088a0cbcf in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0</span><br><span class="line">#2  0x00007fb088a0cc6b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0</span><br><span class="line">#3  0x00007fb08776afa2 in PosixSemaphore::wait() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0879e4d5b in WorkerThread::run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb0879283b8 in Thread::call_run() () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#6  0x00007fb0876b312a in thread_native_entry(Thread*) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#7  0x00007fb088a06ea5 in start_thread () from /lib64/libpthread.so.0</span><br><span class="line">#8  0x00007fb08852bb0d in clone () from /lib64/libc.so.6</span><br><span class="line"></span><br><span class="line">Thread 3 (Thread 0x7fb00ebfb700 (LWP 15983)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb08070b250, mutex=0x7fb08070b228, abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be25f in PlatformEvent::park_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0872d8d59 in JavaThread::sleep_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0873989ed in JVM_Sleep () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#6  0x0000000000000000 in ?? ()</span><br><span class="line">---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---</span><br><span class="line"></span><br><span class="line">Thread 2 (Thread 0x7fb00eafa700 (LWP 15984)):</span><br><span class="line">#0  0x00007fb088a0ade2 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb088c22499 in pthread_cond_timedwait_common (cond=0x7fb0806e7350, mutex=0x7fb0806e7328, abstime=&lt;optimized out&gt;, compat=FT_COMPAT_GLIBC_2_3_2) at libfaketime.c:2840</span><br><span class="line">#2  0x00007fb0876be25f in PlatformEvent::park_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#3  0x00007fb0872d8d59 in JavaThread::sleep_nanos(long) () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#4  0x00007fb0873989ed in JVM_Sleep () from /usr/java/jdk21/lib/server/libjvm.so</span><br><span class="line">#5  0x00007fb06fdf39c0 in ?? ()</span><br><span class="line">#6  0x0000000000000000 in ?? ()</span><br><span class="line"></span><br><span class="line">Thread 1 (Thread 0x7fb089024740 (LWP 15959)):</span><br><span class="line">#0  0x00007fb088a08017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007fb089039e6f in CallJavaMainInNewThread () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#2  0x00007fb089036ffd in ContinueInNewThread () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#3  0x00007fb089037a4d in JLI_Launch () from /usr/java/jdk21/bin/../lib/libjli.so</span><br><span class="line">#4  0x000055fa5ded6b0f in main ()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="3"><li>根因溯源</li></ol><p>其中Thread.Sleep在jdk21+9版本下的调用路径：</p><p>a. Thread.Sleep <a href="https://github.com/openjdk/jdk21u/blob/e45287d1ad9dcadf8a23d3271f1b675b8dade0ac/src/java.base/share/classes/java/lang/Thread.java#L498">https://github.com/openjdk/jdk21u/blob/e45287d1ad9dcadf8a23d3271f1b675b8dade0ac/src/java.base/share/classes/java/lang/Thread.java#L498</a></p><p>b. Thread.Sleep-&gt; sleep0</p><p>c. sleep2 对应JNI实现 JVM_Sleep</p><p>d. JVM_Sleep中使用jvm实现sleep_nanos <a href="https://github.com/openjdk/jdk21u/blob/e45287d1ad9dcadf8a23d3271f1b675b8dade0ac/src/hotspot/share/runtime/javaThread.cpp#L1990C18-L1990C29">https://github.com/openjdk/jdk21u/blob/e45287d1ad9dcadf8a23d3271f1b675b8dade0ac/src/hotspot/share/runtime/javaThread.cpp#L1990C18-L1990C29</a></p><p>e. 其中sleep_nanos内部调用slp-&gt;park_nanos(nanos_remaining)</p><p>f. park_nanos调用了glibc的pthread_cond_timedwait,但被faketime劫持</p><ul><li>faketime中pthread_cond_timedwait,因clock_id不是MONOTONIC，因此计算用了realtime</li><li>然后传递至glibc的pthread_cond_timedwait，其调用clock_gettime来做超时判断，这里同样返回的是faketime</li><li>导致一直等待，jvm的thread无法unpark</li></ul></li></ol><p>其中sleep_nanos 的实现如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">bool JavaThread::sleep_nanos(jlong nanos) &#123;</span><br><span class="line">  assert(this == Thread::current(),  &quot;thread consistency check&quot;);</span><br><span class="line">  assert(nanos &gt;= 0, &quot;nanos are in range&quot;);</span><br><span class="line"></span><br><span class="line">  ParkEvent * const slp = this-&gt;_SleepEvent;</span><br><span class="line">  // Because there can be races with thread interruption sending an unpark()</span><br><span class="line">  // to the event, we explicitly reset it here to avoid an immediate return.</span><br><span class="line">  // The actual interrupt state will be checked before we park().</span><br><span class="line">  slp-&gt;reset();</span><br><span class="line">  // Thread interruption establishes a happens-before ordering in the</span><br><span class="line">  // Java Memory Model, so we need to ensure we synchronize with the</span><br><span class="line">  // interrupt state.</span><br><span class="line">  OrderAccess::fence();</span><br><span class="line"></span><br><span class="line">  jlong prevtime = os::javaTimeNanos();</span><br><span class="line"></span><br><span class="line">  jlong nanos_remaining = nanos;</span><br><span class="line"></span><br><span class="line">  for (;;) &#123;</span><br><span class="line">    // interruption has precedence over timing out</span><br><span class="line">    if (this-&gt;is_interrupted(true)) &#123;</span><br><span class="line">      return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (nanos_remaining &lt;= 0) &#123;</span><br><span class="line">      return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">      ThreadBlockInVM tbivm(this);</span><br><span class="line">      OSThreadWaitState osts(this-&gt;osthread(), false /* not Object.wait() */);</span><br><span class="line">      slp-&gt;park_nanos(nanos_remaining);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Update elapsed time tracking</span><br><span class="line">    jlong newtime = os::javaTimeNanos();</span><br><span class="line">    if (newtime - prevtime &lt; 0) &#123;</span><br><span class="line">      // time moving backwards, should only happen if no monotonic clock</span><br><span class="line">      // not a guarantee() because JVM should not abort on kernel/glibc bugs</span><br><span class="line">      assert(false,</span><br><span class="line">             &quot;unexpected time moving backwards detected in JavaThread::sleep()&quot;);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      nanos_remaining -= (newtime - prevtime);</span><br><span class="line">    &#125;</span><br><span class="line">    prevtime = newtime;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中faktime针对pthread_cond_timedwait在0.9.8与0.9.10均实现了劫持，但关于<code>CLOCK_MONOTONIC</code>行为应该是存在差异的，其中0.9.10根据Glibc版本做了自适应调整</p><p><img src="https://pic1.imgdb.cn/item/6794787dd0e0a243d4f7ba30.png"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><p><a href="https://www.ffutop.com/posts/2024-08-06-faketime/">https://www.ffutop.com/posts/2024-08-06-faketime/</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/2240075">https://cloud.tencent.com/developer/article/2240075</a></p></li><li><p><a href="https://github.com/wolfcw/libfaketime/issues/469">https://github.com/wolfcw/libfaketime/issues/469</a></p></li><li><p><a href="https://github.com/wolfcw/libfaketime/commit/0e61d3d1917f530b5e3f66db2ed5dd6acd20e798#diff-9c808c04a6c7f0cf8c7c99d7d03f9e6cfd420f7733dd38d1519d86b4da92a7c3R3571">https://github.com/wolfcw/libfaketime/commit/0e61d3d1917f530b5e3f66db2ed5dd6acd20e798#diff-9c808c04a6c7f0cf8c7c99d7d03f9e6cfd420f7733dd38d1519d86b4da92a7c3R3571</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;揭秘JDK-21-faketime演练CPU拉满的迷局&quot;&gt;&lt;a href=&quot;#揭秘JDK-21-faketime演练CPU拉满的迷局&quot; class=&quot;headerlink&quot; title=&quot;揭秘JDK 21 faketime演练CPU拉满的迷局&quot;&gt;&lt;/a&gt;揭秘JDK</summary>
      
    
    
    
    
    <category term="misc" scheme="https://zoues.com/tags/misc/"/>
    
  </entry>
  
  <entry>
    <title>Databricks Serverless服务启动优化大揭秘</title>
    <link href="https://zoues.com/posts/99d0d9bd/"/>
    <id>https://zoues.com/posts/99d0d9bd/</id>
    <published>2025-01-17T01:10:43.000Z</published>
    <updated>2025-01-17T01:18:07.072Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>  </p><ul><li>Databricks Serverless计算每天启动数百万台虚拟机，其中绝大部分是数据和 AI 产品，针对如此规模的场景，如何高效地运行基础设施将是一个巨大的挑战。  </li><li>本文介绍了系统性层面的一系列优化，旨在将虚拟机启动耗时减少7倍，最终每天能够节省数千万分钟的计算时间</li><li>这些优化使 Databricks 能够以最低的成本为Serverless用户提供更低的延迟和高性能的产品体验。</li></ul><p>Databricks Serverless计算基础设施横跨三大云服务商以管理数百万台虚拟机，针对如此规模的场景，如何高效地运营基础设施是一项巨大的挑战。通过该文，我们分享最近所做的一些工作，让用户体验到真正的Serverless产品：不单单是提供计算资源，同时包括底层系统（例如完整的 Apache Spark 集群或大型语言模型服务等）均能够在几秒钟内为大规模的数据和 AI 工作负载做好准备。</p><p>在我们之前，没有任一Serverless平台能够在几秒钟内运行如此多样化的数据和 AI 工作负载，其耗时关键在于设置 VM 环境。关于如何实现最佳性能，这不仅涉及各种软件包安装，还需要彻底预热运行时环境。我们以 Databricks Runtime（DBR）为例，它需要预热 JVM 的 JIT 编译器，以便从一开始就为客户提供最佳性能。</p><p>在这篇博客中，我们介绍了当前开发的一系列系统级相关优化，其旨在将预加载了 Databricks 软件的虚拟机（简称 Databricks VM）的启动时间从分钟缩短到秒级——自我们Serverless平台推出以来，启动效率提高了 7 倍，当前几乎支持所有 Databricks 产品。这些优化涵盖了整个软件栈，从操作系统和容器运行时到托管应用程序，帮助我们每天节省数千万分钟的计算时间，并为 Databricks Serverless客户提供最优性价比的服务。</p><h3 id="Databricks虚机启动流程简介"><a href="#Databricks虚机启动流程简介" class="headerlink" title="Databricks虚机启动流程简介"></a>Databricks虚机启动流程简介</h3><p><img src="https://www.databricks.com/sites/default/files/inline-images/booting-databricks-vm.png?v=1732579199" alt="The boot sequence of a Databricks VM on Serverless Platform"></p><p>上图描述了虚拟机启动的三个重要阶段：</p><ol><li><p>操作系统启动<br>Databricks 虚拟机的启动从通用操作系统启动开始：启动内核，启动系统服务，启动容器运行时，最后连接到集群管理器，</p><blockquote><p>注：该管理器负责管理所有虚拟机</p></blockquote></li><li><p>拉取容器镜像<br>为了简化运行时资源管理与部署，Databricks将应用程序打包为容器镜像。虚拟机连接到集群管理器后，它将拉取容器基础组件列表（有点类似Kubelet进程），开始从容器镜像仓库下载镜像。这些镜像不仅包括最新的 Databricks Runtime，还包括用于日志处理、虚拟机健康监控、指标上报等平台管理类的基础工具。</p></li><li><p>容器内设置<br>最后，虚拟机启动工作负载容器，初始化环境并启动服务。以 Databricks Runtime 为例——其初始化过程涉及加载数千个 Java 库，并通过执行一系列精心选择的查询来预热 JVM。我们运行预热查询，强制 JVM 将字节码即时编译（JIT）为常见代码路径的本地机器指令，这确保用户从第一次查询开始就能享受最佳的运行时性能。运行大量的预热查询可以确保系统为所有类型的查询和数据处理需求提供低延迟体验。但是这种查询可能导致初始化过程需要额外消耗几分钟。</p></li></ol><p>针对上述每个阶段的耗时点，我们通过以下方案来降低延迟：</p><p><strong>专用Serverless操作系统</strong><br>对于 Databricks Serverless计算，我们负责管理整个软件栈，因此可以通过构建一个专门的Serverless操作系统，以满足运行短暂虚拟机的需求。具体来说，我们只需包含运行容器所需的基本软件，并调整其启动顺序，使其比通用操作系统更早启动关键服务。我们调整操作系统配置，以优先考虑缓冲 I&#x2F;O 写入需求，减少启动过程中的磁盘瓶颈。</p><p>通过去除不必要的操作系统组件（例如，禁用 USB 子系统）来加速虚拟机启动效率，同时使启动过程更适合云环境。在虚拟机中，操作系统从远程磁盘启动，磁盘内容在启动过程中被拉取到物理主机，云提供商通过预测哪些块扇区更可能被访问以优化该过程。云供应商针对较小的操作系统镜像能够更有效地缓存磁盘内容。</p><p>此外，我们定制了Serverless操作系统，以减少启动过程中的 I&#x2F;O 竞争，因为启动期间通常涉及大量的文件写入。例如，调整系统设置，在内核将文件写入磁盘之前，将更多文件写入缓冲区。同时我们还修改了容器运行时，以减少镜像拉取和创建容器时的阻塞性同步写入问题。我们主要针对短暂、无状态的虚拟机设计了上述优化，在这些虚拟机中，电源中断和系统崩溃导致的数据丢失问题无影响。</p><p><strong>延迟容器文件系统</strong><br>在 Databricks 虚拟机连接到集群管理器后，需要先下载几个GB的容器镜像，然后才能初始化 Databricks Runtime 和其他应用，例如日志处理、指标上报等工具。即使将全部网络带宽和&#x2F;或磁盘吞吐量用满，下载过程仍可能需要几分钟。另一方面，根据前期的分析，下载容器镜像占据了 76% 的容器启动时间，但启动容器时，只需要 6.4% 的数据即可开始正常工作。</p><p><img src="https://www.databricks.com/sites/default/files/inline-images/booting-databricks-vms-compute-blog-img-2.png?v=1732018112" alt="A lazy container filesystem based on overlaybd"></p><p>如上图所示，我们使用了懒加载容器文件系统。在构建容器镜像时，我们增加了一个额外的步骤，将基于 gzip 的镜像格式转换为适合懒加载的基于块设备的格式。这使得容器镜像在生产环境中可以表示为一个具有 4MB 扇区的可寻址块设备。</p><p>在拉取容器镜像时，我们定制的容器运行时仅需检索设置容器根目录所需的元数据，包括目录结构、文件名和权限，并相应地创建一个虚拟块设备，然后将虚拟块设备挂载到容器中，使应用程序可以立即运行。当应用程序第一次读取文件时，针对虚拟块设备的 I&#x2F;O 请求将触发镜像获取进程，该进程从远程容器镜像仓库中检索实际的块内容。检索到的块内容会被本地缓存，以防止重复的网络往返请求到容器注册表，减少变动网络延迟对未来读取的影响。</p><p>懒加载容器文件系统消除了在启动应用程序之前下载整个容器镜像的需求，将镜像拉取延迟从几分钟减少到仅几秒钟。通过将镜像下载过程分布在更长的时间内，来缓解带宽的压力，避免了限速。</p><p><strong>检查点&#x2F;恢复预初始化容器</strong><br>最后，我们通过执行一个长时间的容器内设置序列来初始化容器，然后再将虚拟机标记为ready以提供服务。对于 Databricks Runtime，我们预加载所有必要的 Java 类，并预热 Spark JVM 进程。虽然这种方法为用户的初始查询提供了最佳性能，但它显著增加了启动时间。此外，同样的设置过程会在每个 Databricks 启动的虚拟机上重复执行。</p><p>我们通过缓存完全预热的状态的容器镜像来解决耗时过多的启动过程。具体来说，我们对预初始化容器进行进程树检查点，并将其作为模板来启动未来相同工作负载类型的实例。在这种设置中，容器被直接“恢复”到一致的初始化状态，完全跳过了重复且昂贵的设置过程。</p><p><img src="https://www.databricks.com/sites/default/files/inline-images/booting-databricks-vms-compute-blog-img-3.png?v=1732018112" alt="Checkpointing a Databricks Runtime (DBR) container.  The red rectangles represent the state being saved during checkpointing"></p><p>我们在定制的容器运行时中实现并集成了检查点&#x2F;恢复功能。上图展示了其工作原理。在chekpoint过程中，容器运行时首先冻结容器的整个进程树，以确保状态一致性。然后，它将进程状态转储到磁盘，包括加载的库、打开的文件描述符、整个堆状态（包括 JIT 编译的本地代码）以及堆栈内存。此外，它还保存容器文件系统的可写层，以保留在容器初始化过程中创建&#x2F;修改的文件。这使得我们可以在以后恢复内存中的进程状态和磁盘上的文件系统状态。我们将检查点打包成一个 OCI&#x2F;Docker 兼容的镜像，然后像标准容器镜像一样使用容器镜像仓库存储与分发。</p><p>虽然从概念上来看较为简单，但它也面临着一些挑战：</p><ol><li><p><strong>Databricks Runtime 必须兼容检查点&#x2F;恢复</strong><br>最初Databricks Runtime 并不兼容检查点&#x2F;恢复，因为（1）Databricks Runtime 可能会访问非通用信息（如主机名、IP 地址、甚至是pod 名称）以支持各种场景，而我们可能会在许多不同的虚拟机上恢复相同的检查点（2）Databricks Runtime 无法处理时间变化场景，因为恢复可能发生在检查点创建后的几天或几周。为了解决这个问题，我们在 Databricks Runtime 中引入了一个检查点&#x2F;恢复兼容模式。该模式延迟绑定主机特定信息，直到恢复后才执行。它还添加了恢复前和恢复后的钩子，以在检查点&#x2F;恢复过程中启用自定义逻辑。例如，Databricks Runtime 可以利用这些钩子通过暂停和恢复心跳来管理时间变化，重新建立外部网络连接等。</p></li><li><p><strong>Databricks Runtime 兼容检查点&#x2F;恢复</strong><br>检查点捕获的是容器的最终进程状态，因此它由许多因素决定，例如 Databricks Runtime 版本、应用配置、堆大小、CPU 的指令集架构（ISA）等。比如将一个在 64GB 虚拟机上创建的检查点恢复到 32GB 虚拟机上可能会导致内存溢出（OOM）问题，而将一个在英特尔 CPU 上创建的检查点恢复到 AMD CPU 上可能会由于 JVM 的 JIT 编译器是基于 ISA 生成的本地代码，这可能导致非法指令。这对于设计能够跟上 Databricks Runtime 开发和计算基础设施快速发展的检查点提出了巨大的挑战。我们按需创建检查点，创建的检查点随后会通过容器镜像仓库上传并分发，以便具有匹配签名的工作负载可以直接从这些检查点恢复。这种方法不仅简化了检查点生成pipeline的设计，还确保了所有创建的检查点在生产环境中真实可用。</p></li><li><p><strong>恢复的唯一性</strong><br>从相同的检查点启动多个容器可能会破坏唯一性原则。例如，随机数生成器（RNG）将共享相同的种子，恢复后开始输出相同的随机数序列。我们跟踪初始化过程中创建的 RNG 对象，并利用恢复后钩子重新为 RNG 对象设定种子，以保障它们的唯一性。</p></li></ol><p>通过生产评估，我们发现这一系列优化将 Databricks Runtime 的初始化和预热时间从几分钟缩减到10 秒钟左右。该功能还允许进行更深层次的 JVM 预热，而无需担心时间问题。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Databricks致力于通过持续创新为客户实现最大化价值，同时为客户提供最佳的性价比。这篇博客描述了一系列深层次的系统优化，成功将 Databricks 虚拟机的启动时间缩短了 7 倍。这不仅为Serverless客户提供了更好的性能体验，还使我们能够以最低的价格为客户提供这种级别的用户体验。同时，我们在计算暖池的规模时，将考虑到虚拟机启动时间的缩短，以进一步降低Serverless成本。最后，我们要感谢开源社区，正是从中受益，我们才能将这些优化付诸实践。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><a href="https://www.databricks.com/blog/booting-databricks-vms-7x-faster-serverless-compute">https://www.databricks.com/blog/booting-databricks-vms-7x-faster-serverless-compute</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Databricks Serverless计算每天启动数百万台虚拟机，其中绝大部分是数据和 AI 产品，针对如此规模的场景，如何高效地运行基础设施将是一个巨大的挑战。  &lt;/li&gt;
&lt;li&gt;本文介绍了系统</summary>
      
    
    
    
    
    <category term="serverless" scheme="https://zoues.com/tags/serverless/"/>
    
  </entry>
  
  <entry>
    <title>It&#39;s time to embark on the dockerless journey</title>
    <link href="https://zoues.com/posts/58bd45d9/"/>
    <id>https://zoues.com/posts/58bd45d9/</id>
    <published>2024-12-13T02:23:48.000Z</published>
    <updated>2024-12-13T02:28:25.157Z</updated>
    
    <content type="html"><![CDATA[<h1 id="It’s-time-to-embark-on-the-dockerless-journey"><a href="#It’s-time-to-embark-on-the-dockerless-journey" class="headerlink" title="It’s time to embark on the dockerless journey"></a>It’s time to embark on the dockerless journey</h1><h2 id="Background-Introduction"><a href="#Background-Introduction" class="headerlink" title="Background Introduction"></a>Background Introduction</h2><p>Docker, as one of the earliest and most widely used container runtimes, has become deeply ingrained in user workflows due to its intuitive interface and extensive features. In Kubernetes’ early days, it used the built-in <strong>dockershim</strong> component to integrate with Docker. However, as container technologies evolved, Kubernetes introduced the <strong>Container Runtime Interface (CRI)</strong> to standardize and expand container runtime support. This innovation enhanced Kubernetes’ compatibility with multiple container runtimes, offering users greater flexibility and choice.</p><p>The CRI enables Kubernetes system components to interact seamlessly with various container runtimes. This shift expanded Kubernetes’ scope beyond Docker and reduced its dependency on Docker and dockershim. Today, Kubernetes users can confidently adopt other robust runtimes like <strong>containerd</strong> and <strong>CRI-O</strong>, simplifying the Kubernetes architecture and improving operational efficiency.</p><p>Starting with Kubernetes version 1.24, official support for Docker (via dockershim) was removed. Similarly, Amazon EKS declared <strong>containerd</strong> as its sole supported runtime. Following this direction, our cluster’s container runtime was transitioned to <strong>containerd</strong>, replacing Docker CLI with <strong>nerdctl</strong>.</p><p>To better understand this shift, it’s important to introduce key container ecosystem components such as <code>libcontainer</code>, <code>runc</code>, <code>containerd</code>, <code>CRI</code>, and <code>OCI</code>. These components play critical roles in creating an efficient and flexible container runtime environment. By delving into these concepts, we can enhance operational expertise and lay a solid foundation for future application deployment and maintenance. For detailed information on <strong>nerdctl</strong>, please refer to the corresponding section.</p><hr><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><h3 id="OCI-Standards-Open-Container-Initiative"><a href="#OCI-Standards-Open-Container-Initiative" class="headerlink" title="OCI Standards (Open Container Initiative)"></a>OCI Standards (Open Container Initiative)</h3><p>The <strong>Open Container Initiative (OCI)</strong>, spearheaded by organizations like Docker and CoreOS (later acquired by Red Hat), establishes unified industry standards for container formats and runtimes. The OCI defines two primary specifications:</p><ul><li><strong>Image Specification (image-spec):</strong> Specifies the format and structure of container images.</li><li><strong>Runtime Specification (runtime-spec):</strong> Governs the execution of container images. The most widely used low-level runtime today is <strong>runC</strong>, which includes <code>libcontainer</code> and supports operations like namespace and cgroup calls.</li></ul><h4 id="Main-High-Level-Container-Runtimes"><a href="#Main-High-Level-Container-Runtimes" class="headerlink" title="Main High-Level Container Runtimes"></a>Main High-Level Container Runtimes</h4><p>High-level container runtimes offer advanced functionality such as image management, unpacking, and API integration. The three most popular ones are:</p><ol><li><strong>containerd</strong>: A CNCF project originally contributed by Docker.</li><li><strong>Podman</strong>: A Red Hat project, fully supported in RHEL 8.</li><li><strong>CRI-O</strong>: Another CNCF project designed to integrate seamlessly with Kubernetes.</li></ol><h4 id="Container-Relationships"><a href="#Container-Relationships" class="headerlink" title="Container Relationships"></a>Container Relationships</h4><p>The diagram below illustrates the relationships within the container ecosystem, highlighting the interaction between various components.</p><p><img src="https://pic.imgdb.cn/item/673ab8f0d29ded1a8c3fc841.webp" alt="OCI Container Relationships"></p><p>Using Docker based on containerd as a container runtime can be categorized as a “high-high-level” runtime.</p><hr><h3 id="Containerd"><a href="#Containerd" class="headerlink" title="Containerd"></a>Containerd</h3><p>Launched as an independent project in early 2016 and donated to CNCF in March 2017, <strong>containerd</strong> is a lightweight, robust, and portable container runtime that supports Linux and Windows. It manages complete container lifecycles, including image management, container execution, and networking (via CNI plugins).</p><h4 id="Key-Components-of-Containerd"><a href="#Key-Components-of-Containerd" class="headerlink" title="Key Components of Containerd"></a>Key Components of Containerd</h4><ol><li><strong>Runtime:</strong> Manages container lifecycle tasks, including creation, execution, and monitoring.</li><li><strong>Storage:</strong> Provides image storage and management capabilities.</li><li><strong>gRPC:</strong> Facilitates communication with upper-layer applications, offering container management APIs.</li><li><strong>Metrics:</strong> Offers performance and resource utilization metrics, primarily based on cgroups.</li><li><strong>Metadata:</strong> Stores essential information about images and containers.</li><li><strong>Tasks:</strong> Organizes containers as tasks, encompassing runtime states and process data.</li><li><strong>Events:</strong> Emits state change notifications to help applications monitor and react to container events.</li></ol><p><img src="https://pic.imgdb.cn/item/673ab904d29ded1a8c3fdb48.webp" alt="Containerd Architecture"></p><h4 id="Comparison-Between-Docker-and-Containerd"><a href="#Comparison-Between-Docker-and-Containerd" class="headerlink" title="Comparison Between Docker and Containerd"></a>Comparison Between Docker and Containerd</h4><table><thead><tr><th><strong>Feature</strong></th><th><strong>Containerd</strong></th><th><strong>Docker</strong></th></tr></thead><tbody><tr><td><strong>Architecture</strong></td><td>Focused on container lifecycle management.</td><td>Comprehensive container platform.</td></tr><tr><td><strong>Image Management</strong></td><td>Basic image transfer and storage features.</td><td>Advanced image building and versioning.</td></tr><tr><td><strong>Ecosystem</strong></td><td>Modular, integrates into larger ecosystems.</td><td>Integrated tools and services.</td></tr><tr><td><strong>Security</strong></td><td>Optimized for core functionality.</td><td>Enhanced with image signing and keys.</td></tr><tr><td><strong>Extensibility</strong></td><td>Lightweight, supports diverse scenarios.</td><td>Plugin mechanism for customization.</td></tr><tr><td><strong>Deployment</strong></td><td>Requires tools like Kubernetes.</td><td>All-in-one solution for containers.</td></tr><tr><td><strong>Community</strong></td><td>Backed by CNCF and cloud-native ecosystems.</td><td>Docker-led, vibrant developer base.</td></tr></tbody></table><hr><h3 id="CRI-Container-Runtime-Interface"><a href="#CRI-Container-Runtime-Interface" class="headerlink" title="CRI (Container Runtime Interface)"></a>CRI (Container Runtime Interface)</h3><p>The CRI, introduced by Kubernetes, standardizes interactions between Kubernetes and container runtimes. Initially, Kubernetes relied on Docker’s API but later adopted the CRI to reduce dependencies and support emerging runtimes.</p><p>CRI defines two gRPC services:</p><ol><li><strong>ImageService:</strong> Manages operations like pulling, listing, and deleting container images.</li><li><strong>RuntimeService:</strong> Handles Pod and container lifecycle management.</li></ol><h4 id="Evolution-of-CRI-and-Dockershim"><a href="#Evolution-of-CRI-and-Dockershim" class="headerlink" title="Evolution of CRI and Dockershim"></a>Evolution of CRI and Dockershim</h4><p>In early Kubernetes versions, dockershim acted as an adapter, enabling Docker to conform to CRI. Over time, this approach proved suboptimal due to the complexity of Docker’s architecture. By Kubernetes 1.20, dockershim was deprecated, and by 1.24, it was fully removed.</p><p><img src="https://pic.imgdb.cn/item/673ab91fd29ded1a8c3ff970.png" alt="Dockershim Interaction"></p><p>Switching to <strong>containerd</strong> offers significant efficiency improvements by simplifying the runtime stack. Kubernetes now interacts directly with CRI-compatible runtimes like containerd, without relying on dockershim. This evolution aligns with Kubernetes’ focus on modularity and scalability.</p><h2 id="Command-Line-Overview"><a href="#Command-Line-Overview" class="headerlink" title="Command Line Overview"></a>Command Line Overview</h2><p>Docker, <code>ctr</code>, <code>nerdctl</code>, and <code>crictl</code> are container management tools, each designed for specific purposes and use cases. Here’s a comparison:</p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a><strong>Docker</strong></h3><ul><li><strong>Overview</strong>: Docker is a popular container management tool providing a comprehensive ecosystem for building, running, and managing containers.</li><li><strong>Features</strong>: Supports image building, distribution (integrated with Docker Hub), and container lifecycle management (start, stop, delete, etc.).</li><li><strong>Interface</strong>: Offers a powerful and user-friendly CLI and API, making it easy for developers and operators to adopt.</li><li><strong>Key Characteristics</strong>:<ul><li>Rich ecosystem (e.g., Docker Compose).</li><li>Supports advanced orchestration (e.g., Swarm).</li><li>Includes an additional daemon (<code>dockerd</code>) to collaborate with container runtimes.</li></ul></li></ul><h3 id="ctr"><a href="#ctr" class="headerlink" title="ctr"></a><strong>ctr</strong></h3><ul><li><strong>Overview</strong>: <code>ctr</code> is the native CLI provided by <code>containerd</code> for direct interaction with it.</li><li><strong>Features</strong>: Enables basic operations like pulling images, running containers, and managing image storage but is simplified for lower-level operations and debugging.</li><li><strong>Interface</strong>: Unlike Docker, <code>ctr</code> targets <code>containerd</code> users without offering advanced APIs or ecosystem support.</li><li><strong>Key Characteristics</strong>:<ul><li>Suitable for developers and system integrators needing direct access to <code>containerd</code>.</li><li>Lacks advanced features like image building (focused solely on runtime management).</li></ul></li></ul><h3 id="nerdctl"><a href="#nerdctl" class="headerlink" title="nerdctl"></a><strong>nerdctl</strong></h3><ul><li><strong>Overview</strong>: <code>nerdctl</code> is a CLI tool based on <code>containerd</code> that supports Docker-style commands.</li><li><strong>Features</strong>: Provides a Docker-like user experience for running containers, building images, and supporting CNI plugins and volumes.</li><li><strong>Interface</strong>: Closely mirrors Docker CLI, making migration from Docker to <code>nerdctl</code> straightforward.</li><li><strong>Key Characteristics</strong>:<ul><li>Operates without the Docker daemon, relying solely on <code>containerd</code>.</li><li>Integrates well with Kubernetes (when using <code>containerd</code> as the runtime) and supports Docker Compose-like functionality.</li></ul></li></ul><h3 id="crictl"><a href="#crictl" class="headerlink" title="crictl"></a><strong>crictl</strong></h3><ul><li><strong>Overview</strong>: <code>crictl</code> is a CLI tool for the Container Runtime Interface (CRI), designed specifically for Kubernetes container management.</li><li><strong>Features</strong>: Primarily interacts with CRI-compatible runtimes (e.g., <code>containerd</code> and <code>CRI-O</code>) for managing Pods, images, and containers.</li><li><strong>Interface</strong>: Focuses on Kubernetes cluster debugging and management, with commands tailored to Kubernetes scenarios.</li><li><strong>Key Characteristics</strong>:<ul><li>Highly integrated with Kubernetes, often used to debug or manage Pod and container states.</li><li>Cannot build images, focusing exclusively on container and Pod lifecycle management.</li></ul></li></ul><hr><h3 id="Horizontal-Comparison-for-Specific-Use-Cases"><a href="#Horizontal-Comparison-for-Specific-Use-Cases" class="headerlink" title="Horizontal Comparison for Specific Use Cases"></a><strong>Horizontal Comparison for Specific Use Cases</strong></h3><h4 id="1-Image-Management"><a href="#1-Image-Management" class="headerlink" title="1. Image Management"></a>1. <strong>Image Management</strong></h4><table><thead><tr><th><strong>Feature</strong></th><th><strong>Docker CLI</strong></th><th><strong>ctr</strong></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>List local images</td><td><code>docker images</code></td><td><code>ctr i ls</code></td><td><code>crictl images</code></td><td><code>nerdctl images</code></td></tr><tr><td>Pull image</td><td><code>docker pull</code></td><td><code>ctr i pull</code></td><td><code>crictl pull</code></td><td><code>nerdctl pull</code></td></tr><tr><td>Push image</td><td><code>docker push</code></td><td><code>ctr i push</code></td><td>N&#x2F;A</td><td><code>nerdctl push</code></td></tr><tr><td>Remove local image</td><td><code>docker rmi</code></td><td><code>ctr i rm</code></td><td><code>crictl rmi</code></td><td><code>nerdctl rmi</code></td></tr><tr><td>View image details</td><td><code>docker inspect</code></td><td>N&#x2F;A</td><td><code>crictl inspecti</code></td><td><code>nerdctl inspect</code></td></tr><tr><td>Tag image</td><td><code>docker tag</code></td><td><code>ctr i tag</code></td><td>N&#x2F;A</td><td><code>nerdctl tag</code></td></tr><tr><td>Export image</td><td><code>docker export</code></td><td><code>ctr i export</code></td><td>N&#x2F;A</td><td><code>nerdctl save</code></td></tr><tr><td>Import image</td><td><code>docker import</code></td><td><code>ctr i import</code></td><td>N&#x2F;A</td><td><code>nerdctl load</code></td></tr><tr><td>Build image</td><td><code>docker build</code></td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>nerdctl build</code></td></tr></tbody></table><h4 id="2-Container-Management"><a href="#2-Container-Management" class="headerlink" title="2. Container Management"></a>2. <strong>Container Management</strong></h4><table><thead><tr><th><strong>Feature</strong></th><th><strong>Docker CLI</strong></th><th><strong>ctr</strong></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>List containers</td><td><code>docker ps</code></td><td><code>ctr c ls</code></td><td><code>crictl ps</code></td><td><code>nerdctl ps</code></td></tr><tr><td>Create container</td><td><code>docker create</code></td><td><code>ctr c create</code></td><td><code>crictl create</code></td><td><code>nerdctl create</code></td></tr><tr><td>Start container</td><td><code>docker start</code></td><td><code>ctr t start</code></td><td><code>crictl start</code></td><td><code>nerdctl start</code></td></tr><tr><td>Stop container</td><td><code>docker stop</code></td><td>N&#x2F;A</td><td><code>crictl stop</code></td><td><code>nerdctl stop</code></td></tr><tr><td>Remove container</td><td><code>docker rm</code></td><td><code>ctr c rm</code></td><td><code>crictl rm</code></td><td><code>nerdctl rm</code></td></tr><tr><td>Inspect container</td><td><code>docker inspect</code></td><td><code>ctr c info</code></td><td><code>crictl inspect</code></td><td><code>nerdctl inspect</code></td></tr><tr><td>Attach container</td><td><code>docker attach</code></td><td><code>ctr t attach</code></td><td><code>crictl attach</code></td><td><code>nerdctl attach</code></td></tr><tr><td>Execute in container</td><td><code>docker exec</code></td><td><code>ctr t exec</code></td><td><code>crictl exec</code></td><td><code>nerdctl exec</code></td></tr><tr><td>View logs</td><td><code>docker logs</code></td><td>N&#x2F;A</td><td><code>crictl logs</code></td><td><code>nerdctl logs</code></td></tr><tr><td>Monitor stats</td><td><code>docker stats</code></td><td><code>ctr t metrics</code></td><td><code>crictl stats</code></td><td><code>nerdctl stats</code></td></tr></tbody></table><h4 id="3-Pod-Management"><a href="#3-Pod-Management" class="headerlink" title="3. Pod Management"></a>3. <strong>Pod Management</strong></h4><table><thead><tr><th><strong>Feature</strong></th><th><strong>Docker CLI</strong></th><th><strong>ctr</strong></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>List Pods</td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>crictl pods</code></td><td>N&#x2F;A</td></tr><tr><td>Inspect Pods</td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>crictl inspectp</code></td><td>N&#x2F;A</td></tr><tr><td>Run Pod</td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>crictl runp</code></td><td>N&#x2F;A</td></tr><tr><td>Stop Pod</td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>crictl stopp</code></td><td>N&#x2F;A</td></tr><tr><td>Remove Pod</td><td>N&#x2F;A</td><td>N&#x2F;A</td><td><code>crictl rmp</code></td><td>N&#x2F;A</td></tr></tbody></table><hr><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><ul><li><strong>Docker</strong>: User-friendly and feature-rich, ideal for standalone container management and development environments.</li><li><strong>ctr</strong>: Lightweight, low-level tool for direct interaction with <code>containerd</code>, mainly for debugging and backend management.</li><li><strong>nerdctl</strong>: Docker CLI-compatible, great for users wanting Docker-like commands while directly utilizing <code>containerd</code>.</li><li><strong>crictl</strong>: A Kubernetes-focused tool for managing Pods and containers via CRI; not suitable for general development.</li></ul><p><code>nerdctl</code> is a lightweight alternative to Docker based on <code>containerd</code>, while <code>crictl</code> specializes in Kubernetes CRI-based Pod management.</p><h2 id="Guide-to-Using-Containerd"><a href="#Guide-to-Using-Containerd" class="headerlink" title="Guide to Using Containerd"></a>Guide to Using Containerd</h2><p><strong>nerdctl</strong> is a CLI tool compatible with the Docker CLI style, and it directly supports Docker Compose syntax. This significantly enhances efficiency when using containerd for local development, testing, or single-node container deployment.</p><hr><h3 id="1-Installation"><a href="#1-Installation" class="headerlink" title="1. Installation"></a>1. Installation</h3><p>You can download the appropriate package from the <a href="https://github.com/containerd/nerdctl/releases">GitHub Release</a> page and extract it to a directory in your <code>PATH</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If containerd is not installed, use the nerdctl-full-&lt;VERSION&gt;-linux-amd64.tar.gz package.</span></span><br><span class="line">➜  ~ wget https://github.com/containerd/nerdctl/releases/download/v1.7.5/nerdctl-1.7.5-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">➜  ~ <span class="built_in">mkdir</span> -p /usr/local/containerd/bin/ &amp;&amp; tar -zxvf nerdctl-1.7.5-linux-amd64.tar.gz nerdctl &amp;&amp; <span class="built_in">mv</span> nerdctl /usr/local/containerd/bin/</span><br><span class="line">➜  ~ <span class="built_in">ln</span> -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl</span><br><span class="line">➜  ~ nerdctl version</span><br></pre></td></tr></table></figure><p>Sample output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">WARN[0000] unable to determine buildctl version: exec: &quot;buildctl&quot;: executable file not found in $PATH </span><br><span class="line">Client:</span><br><span class="line"> Version:    v1.7.5</span><br><span class="line"> OS/Arch:    linux/amd64</span><br><span class="line"> Git commit: cffed372371dcbea3dc9a646ce5a913fc1c09513</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> containerd:</span><br><span class="line">  Version: v2.0.0-beta.2-33-g96bf529cb</span><br><span class="line">  GitCommit: 96bf529cbf55940ddb96bb8adc8be51b11922ebb</span><br><span class="line"> runc:</span><br><span class="line">  Version: 1.1.4</span><br><span class="line">  GitCommit: v1.1.4-0-g5fd4c4d</span><br></pre></td></tr></table></figure><p>After installation, you can start exploring the <code>nerdctl</code> CLI.</p><hr><h3 id="2-Command-Overview"><a href="#2-Command-Overview" class="headerlink" title="2. Command Overview"></a>2. Command Overview</h3><h4 id="Container-Management"><a href="#Container-Management" class="headerlink" title="Container Management"></a><strong>Container Management</strong></h4><p><strong>Run</strong><br>The <code>nerdctl run</code> command works similarly to <code>docker run</code>. For example:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpine</span><br></pre></td></tr></table></figure><p>Available options are consistent with Docker, such as <code>-i</code>, <code>-t</code>, <code>--cpus</code>, and <code>--memory</code>. Use <code>nerdctl run --help</code> for more options.</p><p><strong>Exec</strong><br>Execute commands within a container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl <span class="built_in">exec</span> -it &lt;container_id&gt; <span class="built_in">date</span></span><br></pre></td></tr></table></figure><p><strong>List Containers (ps)</strong><br>List all containers (active containers by default):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl ps</span><br></pre></td></tr></table></figure><p>For Kubernetes containers, use the <code>-n k8s.io</code> namespace flag:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io ps</span><br></pre></td></tr></table></figure><p><strong>Inspect</strong><br>Display detailed container information:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io inspect &lt;container_id&gt;</span><br></pre></td></tr></table></figure><p><strong>Logs</strong><br>Fetch container logs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io logs &lt;container_id&gt;</span><br></pre></td></tr></table></figure><p>Supports options like <code>-f</code>, <code>-t</code>, <code>-n</code>, <code>--since</code>, and <code>--until</code>.</p><p><strong>Stop and Remove</strong><br>Stop and delete containers:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io stop &lt;container_id&gt;</span><br><span class="line">➜  ~ nerdctl -n k8s.io <span class="built_in">rm</span> &lt;container_id&gt;</span><br></pre></td></tr></table></figure><p>Use <code>-f</code> or <code>--force</code> for forced removal.</p><hr><h4 id="Image-Management"><a href="#Image-Management" class="headerlink" title="Image Management"></a><strong>Image Management</strong></h4><p><strong>List Images</strong><br>View available images:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io images</span><br></pre></td></tr></table></figure><p><strong>Pull Images</strong><br>Pull an image from a registry:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io pull &lt;image_name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure><p><strong>Other Commands</strong><br><code>nerdctl</code> supports other image-related commands like <code>build</code>, <code>tag</code>, <code>push</code>, and <code>inspect</code>. Use <code>nerdctl image --help</code> for a complete list.</p><hr><p>This guide highlights key <code>nerdctl</code> commands, demonstrating its compatibility with Docker-like workflows while leveraging containerd’s efficiency.</p><h2 id="CRICTL-Usage-Guide"><a href="#CRICTL-Usage-Guide" class="headerlink" title="CRICTL Usage Guide"></a>CRICTL Usage Guide</h2><p><code>crictl</code> traces its roots to the Container Runtime Interface (CRI) introduced in Kubernetes version 1.5. CRI standardizes the interface between Kubernetes and container runtimes, enabling Kubernetes to interact seamlessly with various runtimes like Docker, containerd, and CRI-O.</p><p>As Kubernetes evolved, the demand for more features and better performance from container runtimes grew. CRI addressed these needs by providing a standardized interface for runtime implementation, simplifying the development of custom runtimes.</p><p>In this context, <code>crictl</code> was developed as a command-line tool to interact with CRI-compliant container runtimes. It facilitates managing containers and images, querying container states and metadata, and viewing logs. By streamlining runtime management and debugging, <code>crictl</code> promotes CRI adoption.</p><p>Below are the primary uses and features of <code>crictl</code>:</p><ol><li><strong>Container Management</strong>: Start, stop, delete, and query container states.</li><li><strong>Image Management</strong>: Pull, push, delete, and inspect container images.</li><li><strong>Log Viewing</strong>: View stdout and stderr logs of containers.</li><li><strong>Container Information</strong>: Query detailed container information such as ID, name, status, and IP.</li><li><strong>Metadata Management</strong>: Annotate and tag containers.</li></ol><p><code>crictl</code> is a robust tool for managing containers and images within a runtime, debugging containerized applications, and integrating with orchestration systems like Kubernetes.</p><hr><h3 id="1-Installation-1"><a href="#1-Installation-1" class="headerlink" title="1. Installation"></a>1. Installation</h3><p>First, install <code>crictl</code> by downloading the appropriate binary from the <a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools release page</a>. Extract the binary to a PATH location:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ VERSION=<span class="string">&quot;v1.22.0&quot;</span></span><br><span class="line">➜  ~ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/<span class="variable">$VERSION</span>/crictl-<span class="variable">$VERSION</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="comment"># Use a mirror for faster downloads if needed:</span></span><br><span class="line"><span class="comment"># wget https://download.fastgit.org/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz</span></span><br><span class="line">➜  ~ tar zxvf crictl-<span class="variable">$VERSION</span>-linux-amd64.tar.gz -C /usr/local/bin</span><br><span class="line">➜  ~ <span class="built_in">rm</span> -f crictl-<span class="variable">$VERSION</span>-linux-amd64.tar.gz</span><br><span class="line">➜  ~ crictl -v</span><br><span class="line">crictl version v1.22.0</span><br></pre></td></tr></table></figure><p>If the version command runs successfully, the <code>crictl</code> tool is installed.</p><hr><h3 id="2-Command-Overview-1"><a href="#2-Command-Overview-1" class="headerlink" title="2. Command Overview"></a>2. Command Overview</h3><p>After installation, configure the default settings in <code>/etc/crictl.yaml</code> by specifying the runtime and image endpoint addresses:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">runtime-endpoint:</span> <span class="string">unix:///var/run/containerd/containerd.sock</span></span><br><span class="line"><span class="attr">image-endpoint:</span> <span class="string">unix:///var/run/containerd/containerd.sock</span></span><br><span class="line"><span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">pull-image-on-create:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">disable-pull-on-run:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>Once configured, you can start using <code>crictl</code>.</p><h4 id="Pod-Management"><a href="#Pod-Management" class="headerlink" title="Pod Management"></a>Pod Management</h4><p>Use the <code>crictl pods</code> command to list the running pods on a node:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                       NAMESPACE           ATTEMPT             RUNTIME</span><br><span class="line">1f617ebf0524c       8 weeks ago         Ready               node-exporter-ggjwf        node-exporter       0                   (default)</span><br></pre></td></tr></table></figure><p>Filter pods by name or label:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl pods --label app=node-exporter</span><br><span class="line">POD ID              CREATED             STATE               NAME                  NAMESPACE           ATTEMPT             RUNTIME</span><br><span class="line">1f617ebf0524c       8 weeks ago         Ready               node-exporter-ggjwf   node-exporter       0                   (default)</span><br></pre></td></tr></table></figure><h4 id="Image-Management-1"><a href="#Image-Management-1" class="headerlink" title="Image Management"></a>Image Management</h4><p>List all images with <code>crictl images</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl images</span><br><span class="line">IMAGE                                      TAG                 IMAGE ID            SIZE</span><br><span class="line">hub.cloud.ctripcorp.com/prom/node-exporter v1.4.0              d3e443c987ef4       11.5MB</span><br></pre></td></tr></table></figure><p>View detailed image information with the <code>-v</code> flag:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl images -v</span><br><span class="line">ID: sha256:d3e443c987ef405e1be101647873d86b5729c9c47bb1dd1ab59ccb24bc9e322c</span><br><span class="line">RepoTags: docker.io/prom/node-exporter:v1.4.0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="Container-Management-1"><a href="#Container-Management-1" class="headerlink" title="Container Management"></a>Container Management</h4><p>List running containers using <code>crictl ps</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl ps</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID</span><br><span class="line">e7f156d31942f       d3e443c987ef4       2 days ago          Running             main                2                   bf1704937991a</span><br></pre></td></tr></table></figure><p>Filter containers by status with the <code>-s</code> option:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl ps -s Exited</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID</span><br><span class="line">22a0adacf702f       d3e443c987ef4       2 days ago          Exited              main                1                   bf1704937991a</span><br></pre></td></tr></table></figure><p>Execute commands within a container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl <span class="built_in">exec</span> -it e7f156d31942f <span class="built_in">whoami</span></span><br><span class="line">root</span><br></pre></td></tr></table></figure><p>View container logs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl logs e7f156d31942f</span><br><span class="line">ts=2024-11-15T08:37:07.980Z <span class="built_in">caller</span>=node_exporter.go:182 level=info msg=<span class="string">&quot;Starting node_exporter&quot;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Follow logs in real time using <code>-f</code> or limit output with <code>--tail N</code>.</p><p>Monitor container resource usage with <code>crictl stats</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl stats e7f156d31942f</span><br><span class="line">CONTAINER           NAME                CPU %               MEM                 DISK                INODES</span><br><span class="line">e7f156d31942f       main                0.00                37.13MB             0B                  18</span><br></pre></td></tr></table></figure><p>For more features, refer to <a href="https://github.com/kubernetes-sigs/cri-tools">kubernetes-sigs&#x2F;cri-tools</a>.</p><hr><h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ol><li><a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/">Kubernetes Container Runtimes</a></li><li><a href="https://zhuanlan.zhihu.com/p/666200234">Zhihu Article</a></li><li><a href="https://www.qikqiak.com/k8strain2/containerd/runtime/">Qikqiak Guide</a></li><li><a href="https://github.com/kubernetes/cri-api/tree/master/pkg/apis">CRI API Reference</a></li><li><a href="http://www.opennaru.com/kubernetes/containerd/">OpenNaru Article</a></li><li><a href="https://www.slideshare.net/AkihiroSuda/container-plumbing-days-2023-why-was-nerdctl-made">Nerdctl Presentation</a></li><li><a href="https://www.alibabacloud.com/blog/a-discussion-on-container-runtime---starting-with-dockershim-being-deleted-by-kubernetes_600118">Alibaba Cloud Blog</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;It’s-time-to-embark-on-the-dockerless-journey&quot;&gt;&lt;a href=&quot;#It’s-time-to-embark-on-the-dockerless-journey&quot; class=&quot;headerlink&quot; title=&quot;It</summary>
      
    
    
    
    
    <category term="Containerd" scheme="https://zoues.com/tags/Containerd/"/>
    
    <category term="runtime" scheme="https://zoues.com/tags/runtime/"/>
    
  </entry>
  
  <entry>
    <title>Ziglang适配OpenMP 循环指令以实现共享内存并行计算</title>
    <link href="https://zoues.com/posts/d1d12995/"/>
    <id>https://zoues.com/posts/d1d12995/</id>
    <published>2024-12-04T00:23:48.000Z</published>
    <updated>2024-12-04T00:47:26.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ziglang适配OpenMP循环指令以实现共享内存并行计算"><a href="#Ziglang适配OpenMP循环指令以实现共享内存并行计算" class="headerlink" title="Ziglang适配OpenMP循环指令以实现共享内存并行计算"></a>Ziglang适配OpenMP循环指令以实现共享内存并行计算</h1><p>Original： <a href="https://arxiv.org/html/2408.09902v1">https://arxiv.org/html/2408.09902v1</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Zig 编程语言以性能和安全性为核心设计目标，近年来逐渐受到欢迎。由于 Zig 基于 LLVM 构建，因此能够利用该生态系统的诸多优势，包括访问丰富的支持后端，这使得 Zig 在高性能工作负载方向具备显著潜力。然而，Zig 尚未在高性能计算（HPC）领域赢得广泛关注，其中一个原因是其缺乏基于预编译指令（pragma）实现共享内存并行计算的能力。</p><p>本文描述了如何通过优化 Zig 编译器来支持 OpenMP 循环指令，并使用 NASA 的并行基准测试套件（NPB）来测试其性能表现。 Zig 与 OpenMP 的集成不仅在扩展性上可与 Fortran 和 C 的 NPB 参考实现相媲美，同时在某些场景下，Zig 的性能相较Fortran来说，提升幅度多大1.25倍。</p><p><em><strong>Index Terms:</strong></em>  Zig, OpenMP, LLVM, High Performance Computing, NAS Parallel Benchmark suite</p><blockquote><p>目前Zig 社区在推动移除LLVM、LCD以及Clang代码库依赖，详见<a href="https://github.com/ziglang/zig/issues/16270">https://github.com/ziglang/zig/issues/16270</a></p></blockquote><h3 id="I-介绍"><a href="#I-介绍" class="headerlink" title="I 介绍"></a>I 介绍</h3><p>随着高性能计算（HPC）领域迈入百亿亿次计算（Exascale）时代，面临的一个关键问题是如何选择用于超级计算机日益复杂场景的编程语言。目前 Fortran 和 C 等传统语言仍然占据着HPC领域的绝大多数份额。</p><p>Zig 是一种系统编程语言，由 Andrew Kelly 于 2016 年创建，设计目标是追求快速和安全。近年来，围绕这一语言逐渐形成充满活力的生态。目前Zig 在 HPC 领域已有一些边缘应用，例如 Cerebras 的 CSL 编程技术（用于其 Wafer Scale Engine，WSE）的开发是基于 Zig ，但总体而言，Zig 在 HPC 中尚未被广泛采用。其未被广泛采用的原因之一是语言本身缺乏对常见 HPC 编程技术的支持。尽管 Zig 的 C 互操作性特性十分出色，使得使用 MPI 相对简单，但其缺少对于 HPC 中普遍存在的基于编译指令（pragma）的共享内存并行编程的支持，因此需要对Zig编译器进行修改。</p><p>本文探索了一种通过为Zig 编译器添加OpenMP 循环指令的支持，实现基于编译指令的共享内存并行特性。通过调用 LLVM 的 OpenMP 运行时库，我们描述了支持 OpenMP 循环指令所需的修改，并比较了 NASA 的 NAS 并行基准测试套件（NPB）中内核在 C、Fortran 和 Zig 之间的性能表现。本文的结构如下：第二部分描述了该研究的背景；第三部分探讨了为支持 OpenMP 循环指令对 Zig 的增强；第四部分重点介绍了评估方法，包括将 Zig 与 C 和 Fortran 集成的探索；第五部分分析了 Zig 与 OpenMP 在这些基准中的性能表现；最后，第六部分总结了本文的研究，并讨论了未来工作。</p><p>本文的主要贡献包括：</p><ol><li><p>描述如何与 Zig 编译器中集成 OpenMP 循环指令。</p></li><li><p>首次探索 Zig 与 Fortran 代码的集成方式，为将 Zig 应用于更大规模的传统代码库提供了可能。</p></li><li><p>对 Zig、Fortran 和 C 在三项 HPC 基准测试中的进行性能对比，其中涵盖线程化时的加速比以及运行时性能。最终证明 Zig 表现良好，是 HPC 领域的可行选择。</p></li></ol><h3 id="II-背景与相关工作"><a href="#II-背景与相关工作" class="headerlink" title="II 背景与相关工作"></a>II 背景与相关工作</h3><p>LLVM 是一套工具和库，部分用于生成和操作LLVM-IR<sup>[1]</sup>。LLVM 上构建了丰富的后端，可以从这种内部表示生成机器代码，并支持多种硬件，包括 CPU、GPU 和 FPGA。除了在主代码库中提供针对 C 和 Fortran 的前端工具（如 Clang 和 Flang）外，LLVM 还被许多流行的编程语言使用，如 Swift<sup>[2]</sup>、Rust<sup>[3]</sup>和 Zig<sup>[4]</sup>。</p><p>LLVM 还提供了其自身的 OpenMP 库。OpenMP 是一种通过多线程实现的基于编译指令的共享内存并行编程技术，也是 HPC 中最受欢迎的编程技术之一。OpenMP 的核心功能通过指令实现，因此编译器需要修改以支持这些编译指令。OpenMP 标准<sup>[5]</sup>规定了 C、C++ 和 Fortran 程序员如何使用该技术，其中编译器指令在 C 和 C++ 中表示为预编译指令（pragma），而在 Fortran 中则表示为特殊注释。定义指令开始的这串标记被称为标志符。</p><h4 id="II-A-Zig"><a href="#II-A-Zig" class="headerlink" title="II-A Zig"></a>II-A Zig</h4><p>Zig 是由 Andrew Kelly 于 2016 年创建的系统编程语言，旨在成为 C 的一种更优化、更安全且更易读的替代方案<sup>[4]</sup>。Zig 的设计专注于减少程序执行时间，同时在安全性和编程体验上相比 C 提供更高的水平。Zig 使用 LLVM 编译器基础设施<sup>[6]</sup>进行代码生成，从而能够利用其优化功能<sup>[7]</sup>。这种对 LLVM 的利用使得 Zig 支持大量 CPU 架构和操作系统，其目标是支持所有由 LLVM 支持的目标平台<sup>[8]</sup>。</p><p>Zig 提供了一些安全特性来改善软件开发体验，主要包括比 C 更强的类型系统和改进的静态分析功能，以及在调试模式下编译器启用的可选运行时安全检查。静态分析功能可以帮助程序员防止常见的错误，例如解引用空指针或与整数和浮点数类型转换相关的截断和舍入错误。例如，在 C 中，对于 <code>int *ptr = 0</code>，解引用并读取 <code>ptr</code> 是合法的，但在运行时可能导致段错误。示例1 中的两个代码示例展示了 Zig 中如何防止这一问题。这两个示例均无法编译。第一个示例尝试将整数文字赋值给指针，这种隐式转换被 Zig 的类型系统所禁止。示例 1 中的第二个示例使用内置的 <code>@intToPtr</code> 函数执行显式的整数到指针转换，这种也会失败，因为在 Zig 中，只有可空指针可以被赋值为零。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var ptr: *i32 = 0;</span><br><span class="line">_ = ptr.*;</span><br><span class="line">// —————————————</span><br><span class="line">var ptr: *i32 = @intToPtr(*i32, 0);</span><br><span class="line">_ = ptr.*;</span><br></pre></td></tr></table></figure><blockquote><p>示例 1：在 Zig 中如何表示解引用和读取 <code>ptr</code> 的示例（基于 C 中的 <code>int *ptr = 0</code>）。然而，由于 Zig 提供的安全性，这些示例均无法编译</p></blockquote><p>在编译时无法识别的错误可能会通过运行时的安全检查标记为未定义行为。Zig 为代码编译提供了两种模式：生产模式和调试模式。在调试模式下，额外的代码会被插入到可执行文件中，例如检查是否发生了数组越界或整数溢出。如果发生此类情况，会触发运行时错误。而在生产模式中，出于性能原因，不提供此类安全检查，因此未定义行为不会被捕获到。Zig建议程序员在开发代码时使用调试模式，在代码成熟后切换到生产模式。</p><p>Zig 的设计目标之一是与现有的 C 代码库实现互操作<sup>[9]</sup>。这使得开发者能够利用 C 的库和框架，例如 MPI<sup>[10]</sup>，并在项目中逐步用 Zig 替代 C。为实现这种互操作，Zig 提供了一种方法，既可以调用 C 函数，也可以让 C 调用 Zig 函数。示例2展示了从 Zig 调用 C 标准库函数 <code>puts</code> 的示例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">extern fn puts(s: [*:0]const u8) c_int;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">// calling puts</span><br><span class="line">  _ = puts(&quot;hello world&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>示例 2：从 Zig 调用 C 函数的示例  </p></blockquote><p>示例3 展示了函数 <code>add</code> 被导出以供 C 使用的示例。该函数可以通过编译为目标文件（object file）或静态库(so)方式，链接到 C 程序中进行访问。此外，Zig 可以自动生成一个包含所有导出函数签名的 C 头文件，供 C 程序调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pub export fn add(a: c_int, b: c_int) c_int&#123;</span><br><span class="line"> return a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>示例3：导出 Zig 函数供 C 使用的示例</p></blockquote><p>Zig 编译器还提供了将 C 源代码转换为 Zig 的工具，这可以加速将整个项目或部分项目迁移到 Zig 的过程。此机制也被编译器用于自动解析 C 头文件，并导入其中的函数、结构体和常量。示例4展示了一个来自示例2的修改代码，其中函数 <code>puts</code> 的显式声明被替换为通过自动翻译 C 标准库 <code>stdio.h</code> 头文件所实现的导入。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// Importing the C stdio.h header</span><br><span class="line">const stdio = @cImport(@cInclude(&quot;stdio.h&quot;));</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line"> // calling the puts function from the stdio.h header</span><br><span class="line"> _ = stdio.puts(&quot;hello world&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>示例4：通过 Zig 导入 C 头文件并调用其中函数的示例</p></blockquote><h3 id="III-Zig适配OpenMP"><a href="#III-Zig适配OpenMP" class="headerlink" title="III Zig适配OpenMP"></a>III Zig适配OpenMP</h3><p>LLVM 提供了OpenMP 运行时库，而本文工作的目标是调用该库提供的函数，在 Zig 中实现基于 pragma 的共享内存并发编程。本节探讨了将带有 OpenMP pragma 注解的 Zig 源代码与 LLVM 的运行时库连接的方法。所有修改均基于 Zig 0.10.1版本。</p><h4 id="III-A-词法分析与语法解析"><a href="#III-A-词法分析与语法解析" class="headerlink" title="III-A 词法分析与语法解析"></a>III-A 词法分析与语法解析</h4><p>如第 II 节所述，OpenMP 依赖于 pragma 来指定程序如何并行，但 Zig 本身并不支持 pragma语法。因此，必须将其作为一种新类型的语句添加到Zig中。我们决定将 pragma 实现为特殊的注释，这与 Fortran 中的支持方式类似。  </p><p>Zig 编译管道的第一步是词法分析（tokenisation），主要决策点是选择将整个 pragma 解析为单个标记，还是将其每个字段解析为独立的标记，分别对应图 1 中的选项 A 和 B。最终决定通过标识符（sentinel），然后将 pragma 的其余部分作为常规代码进行标记化，假装标识符不存在。这种方法之所以可行，是因为 pragma 完全由 Zig 本身使用的标记组成。  </p><p><img src="https://arxiv.org/html/2409.20148v1/extracted/5889514/images/pragma_tokens.png" alt="Refer to caption"></p><blockquote><p>图 1：解析方式选择的示意图，A) 将整个 pragma 解析为单个标记，或 B) 将 pragma 分解为多个标记</p></blockquote><p>Zig 的词法分析器支持对关键字进行标记化。因此，最初计划利用此机制来解析 OpenMP 的指令和子句（例如 <code>parallel</code> 或 <code>default</code>）作为关键字。然而，这种方法行不通，因为在 Zig 中关键字不能用作标识符，添加这些关键字会破坏与现有代码的兼容性。因此，解决方案是将 OpenMP 的关键字存储为标识符，并在解析时将其与常规标识符区分开。</p><p>标记化完成后，下一步是解析，这一步从标记生成抽象语法树（AST）。Pragma 应像其他语句一样被处理，Zig 解析器的核心是 <code>eatToken</code> 方法。该方法接受一个枚举值，表示标记的类型（称为标记标签）。如果下一个标记与标签匹配，则返回并推进解析器到下一个标记，否则返回 null。然而，由于 OpenMP 关键字未分配唯一的标签，该函数无法按正常工作。因此，添加了一组新标签来表示不同的 OpenMP 关键字，并使用字符串到关键字标记的哈希映射来识别字符串是否为关键字。我们修改了 <code>eatToken</code> 函数，使其能够接受新增关键词，并在解析 OpenMP 关键字标签时相应地解析标识符标签。</p><p>每个 OpenMP 指令都有一个 AST 节点标签，子句作为节点数据存储。子句数据存储在 <code>extra_data</code> 数组中，该数组是Zig 编译器用于注释 AST 节点的杂项数据的 32 位整数数组。所有子句数据必须能够以这种形式表示，其中所有子句都存储在单个数据结构中，其整数表示子句不同。</p><h4 id="III-A1-处理列表子句"><a href="#III-A1-处理列表子句" class="headerlink" title="III-A1 处理列表子句"></a>III-A1 处理列表子句</h4><p><code>private</code>、<code>firstprivate</code> 和 <code>shared</code> 子句被定义为标识符的列表。在获取每个标识符的 AST 节点索引后，这些索引被连续存储在 <code>extra_data</code> 数组中，子句结构的开始和结束索引则存储在子句中。图 2 展示了 <code>private</code> 子句的一个示例，其中指令节点包含一个索引到 <code>extra_data</code> 数组，表示子句结构的起始位置。</p><p><img src="https://arxiv.org/html/2409.20148v1/extracted/5889514/images/private_clause.png" alt="Refer to caption"></p><blockquote><p>图 2：将私有变量存储在 <code>extra_data</code> 数组中的示例</p></blockquote><h4 id="III-A2-处理压缩子句"><a href="#III-A2-处理压缩子句" class="headerlink" title="III-A2 处理压缩子句"></a>III-A2 处理压缩子句</h4><p>非列表子句的存储大小是静态已知的，因此可以将它们存储在单一结构中。通过将该结构标记为压缩结构，可以将其视为一个 32 位整数并存储在 <code>extra_data</code> 数组中。这种方法允许通过读取数组的单个索引提取所有数据，无需进一步的间接访问。例如，循环调度信息存储为一个 3 位的枚举值（表示调度类型）以及一个 29 位的整数（表示区块大小），其支持多达 536,870,912 次迭代。由于区块大小必须大于 0<sup>[5]</sup>，值 0 表示未指定区块大小。</p><p>有些子句可以用少于 32 位表示，并将它们组合到一个压缩结构中。例如，<code>default</code> 子句使用 2 位的枚举表示，而 <code>nowait</code> 子句用一个布尔值表示，占用压缩结构中的 1 位。<code>collapse</code> 子句则占用 4 位，因为用户通常不会希望折叠超过 16 层的循环。</p><h4 id="III-B-代码生成"><a href="#III-B-代码生成" class="headerlink" title="III-B 代码生成"></a>III-B 代码生成</h4><p>在将 OpenMP 的 pragma 进行词法分析和语法解析后，下一步是代码生成。支持 OpenMP 的典型编译器会在指令的位置插入对 OpenMP 运行时的调用。这里的替换需要在编译期改造AST语法树时完成。</p><p>我们最初尝试直接修改 AST 并注入所需的 OpenMP 调用。然而，在 Zig 中，AST 节点与原始源代码之间存在严格的关联性，因此无法随意添加新的节点。基于此，我们尝试了一种变通方案：在解析目标源代码之前，向其开头预置一个函数和结构定义模板，以便在代码生成期间复制这些模板来完成 OpenMP 函数和结构实例化。然而，由于当前 Zig编译器的设计，此方法不可行，因为在编译过程中很难找到这些模板的位置并将其传播到 AST中。</p><p>因此，我们采用了基于预处理器的方法，这种方法的优点在于可以轻松生成新代码，而无需手动确保每个标记和 AST 节点引用源文件都在固定位置。当然，这种预处理方法也存在一些挑战，主要是因为 Zig并未涵盖该场景的步骤。首先，所有未使用的函数参数和非全局范围的变量必须显式丢弃，这意味着只有已知会使用的变量才应生成。第二个挑战是，在预处理阶段缺乏语义上下文（例如变量类型及其用途），这一点在 <strong>III-B3</strong> 中有更详细讨论。</p><p>将预处理器纳入 Zig 编译器的一个核心部分，具有以下几个优点。首先，这使预处理器可以复用 Zig 编译器中内置的解析基础设施。其次，通过在文件加载后立即执行预处理器，可以在无需修改的情况下继续使用编译器的缓存机制。</p><p>我们的预处理器在多个环节运行，通过每次处理不同的 OpenMP 构造来替换相关代码。其总体算法的伪代码在<strong>清单 5</strong>中进行了描述。例如，所有并行区域在工作共享循环之前被替换。因此，只要嵌套的构造属于不同类型，就无需在预处理器中进行特殊处理。伪代码中的 <code>&lt;&lt;adjust source offset&gt;&gt;</code> 是因为节点以源代码列表的偏移量表示，因此在每次替换代码后必须调整修改的位置偏移量。此外，伪代码展示了为每个替换节点通过 <code>create-payload</code> 创建一个负载（payload）。此负载包含进行替换所需的信息，例如每个指令需要在源代码中执行替换的位置，以及该指令的具体信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">FUNCTION preprocess (source, step)</span><br><span class="line"></span><br><span class="line">ast := parse-source-into-ast(source)</span><br><span class="line">replacements := empty-list</span><br><span class="line"></span><br><span class="line">FOREACH node IN ast DO</span><br><span class="line">IF node IS OpenMP-node AND</span><br><span class="line">&lt;&lt;node matches current step&gt;&gt; THEN</span><br><span class="line">append(replacements, create-payload(node))</span><br><span class="line">END</span><br><span class="line">END</span><br><span class="line"></span><br><span class="line">IF step = parallel THEN</span><br><span class="line">FOREACH replacement IN replacements</span><br><span class="line">&lt;&lt;perform parallel region replacement&gt;&gt;</span><br><span class="line">&lt;&lt;adjust source offset&gt;&gt;</span><br><span class="line">END</span><br><span class="line">ELSE IF step == while THEN</span><br><span class="line">FOREACH replacement IN replacements</span><br><span class="line">&lt;&lt;perform worksharing loop replacement&gt;&gt;</span><br><span class="line">&lt;&lt;adjust source offset&gt;&gt;</span><br><span class="line">END</span><br><span class="line">END</span><br><span class="line"></span><br><span class="line">IF &lt;&lt;is last step&gt;&gt; THEN</span><br><span class="line">RETURN source</span><br><span class="line">ELSE</span><br><span class="line">RETURN preprocess(source, step)</span><br><span class="line"></span><br><span class="line">END</span><br></pre></td></tr></table></figure><blockquote><p> 示例5：替换 OpenMP Pragma 和子句的预处理器算法的伪代码</p></blockquote><h4 id="III-B1-处理并行区域"><a href="#III-B1-处理并行区域" class="headerlink" title="III-B1 处理并行区域"></a>III-B1 处理并行区域</h4><p>大多数编译器通过函数分解的方式表示 OpenMP 并行区域，其中生成一个包含并行区域内容的函数<sup>[11]</sup>。访问这些区域的变量，例如默认共享的变量或通过 <code>shared</code>、<code>firstprivate</code> 或 <code>reduction</code> 子句显式捕获的变量，会作为参数传递给该函数。然后，该函数的指针被传递给 OpenMP 运行时库的函数，该函数会在每个线程上调用它。例如，LLVM 的 OpenMP API 使用 <code>__kmpc_fork_call</code> 实现此功能。</p><p>我们选择采用上述方法，传递给分解函数的变量作为参数传递给 OpenMP 运行时库函数 <code>__kmpc_fork_call</code>，后者将它们转发给分解函数的回调。<code>__kmpc_fork_call</code> 是变参函数，接受可变数量的参数。我们的设计是将参数分为三组，每组表示为 <code>?*anyopaque</code> 指针，这是 Zig 中等同于 C 的 <code>void *</code> 的类型。这三组参数指向包含 <code>firstprivate</code>、<code>shared</code> 和 <code>reduction</code> 子句变量的结构体。</p><p>在分解函数中，一旦 <code>?*anyopaque</code> 指针被还原为其原始类型，就会为这些结构体的每个成员变量创建变量并初始化值。例如：</p><ul><li>对于 <code>firstprivate</code> 子句，值为并行区域外作用域中的变量值；</li><li>对于 <code>shared</code> 子句，需要通过指针访问变量，并将共享变量的访问重写为指针访问；</li><li>对于 <code>private</code> 变量，只需在分解函数中简单定义。</li></ul><p><strong>Reduction 操作</strong>更为复杂，通过使用 Zig 的标准原子类型创建一个值来实现。一个 reduction 结构体被创建，包含指向这些原子值的指针，并以与其他变量相同的方式传递给分解函数回调。分解函数为每个 reduction 变量创建一个单独的变量，并使用 reduction 变量中持有的初始值进行初始化。初始化必须符合 OpenMP 标准<sup>[5]</sup>。为了保证线程安全，基于原子类型定义了原子读-修改-写操作。</p><p>然而，这种方法受限于 Zig 支持的原子操作。目前 Zig 仅支持加法、减法、最小值、最大值、二进制与（AND）、或（OR）、非与（NAND）、异或（XOR）和比较交换（CAS）。例如，乘法和逻辑与或不支持。我们使用 CAS 循环算法<sup>[12]</sup>实现了这些缺失的 reduction 操作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">atom := &lt;&lt;value to be updated&gt;&gt;</span><br><span class="line">operand := &lt;&lt;value to update it with&gt;&gt;</span><br><span class="line"></span><br><span class="line">old := atomic-load(atom)</span><br><span class="line">new := old * operand</span><br><span class="line"></span><br><span class="line">WHILE TRUE DO</span><br><span class="line"> exchange-success, actual-value :=</span><br><span class="line"> compare-and-swap(&amp;atom, old, new)</span><br><span class="line"> IF exchange-success THEN</span><br><span class="line"> BREAK</span><br><span class="line"> ELSE</span><br><span class="line"> old = actual-value</span><br><span class="line"> new = old * operand</span><br><span class="line"> END</span><br><span class="line">END</span><br></pre></td></tr></table></figure><blockquote><p> 清单 6 展示了我们使用 CAS 算法实现乘法 reduction 的伪代码。</p></blockquote><h4 id="III-B2-处理工作共享循环"><a href="#III-B2-处理工作共享循环" class="headerlink" title="III-B2 处理工作共享循环"></a>III-B2 处理工作共享循环</h4><p>与并行区域不同，工作共享循环不需要分解函数。Clang 的 OpenMP API 提供了两种实现工作共享循环的策略：</p><ol><li><strong>静态调度</strong>：通过 <code>__kmpc_for_static_*</code> 函数实现；</li><li><strong>动态、分布式和运行时调度</strong>：通过 <code>__kmpc_dispatch_*</code> 函数实现。</li></ol><p>这两种策略都要求明确循环的上界、下界、增量和比较操作符：</p><ul><li>比较操作符直接从 Zig <code>while</code> 循环的条件中获取；</li><li>下界由循环计数器变量的初始值决定；</li><li>上界来自比较操作符右侧的值；</li><li>增量来自继续表达式中增量操作符右侧的值。</li></ul><p>静态调度的 <code>__kmpc_for_static_*</code> 包括：</p><ul><li><code>__kmpc_for_static_init</code>：执行循环迭代；</li><li><code>__kmpc_for_static_fini</code>：每个线程完成后调用以最终化循环。</li></ul><p>对于动态循环，<code>__kmpc_dispatch_next</code> 用于处理下一个批次的迭代，而 <code>__kmpc_dispatch_init</code> 接收调度类型（如 <code>kmp_sch_dynamic_chunked</code>、<code>kmp_sch_guided_chunked</code>、<code>kmp_sch_runtime</code>）。</p><h4 id="III-B3-变量重写"><a href="#III-B3-变量重写" class="headerlink" title="III-B3 变量重写"></a>III-B3 变量重写</h4><p>预处理器尽量利用已有的变量名和表达式，例如，在分解函数中解包 <code>private</code> 和 <code>firstprivate</code> 变量时复用相同的变量名。但也有不尽如人意之处，例如，<code>shared</code> 变量必须重写为指针访问，而工作共享循环的 reduction 临时变量可能不能与其对应的 <code>shared</code> 变量同名。</p><p>由于预处理时缺乏语义上下文，这种替换更具挑战性。得益于 Zig 的简单语法<sup>[13]</sup>和无变量遮蔽机制，其只需利用 AST即可实现变量重写。</p><h4 id="III-C-封装-OpenMP-运行时功能"><a href="#III-C-封装-OpenMP-运行时功能" class="headerlink" title="III-C 封装 OpenMP 运行时功能"></a>III-C 封装 OpenMP 运行时功能</h4><p>OpenMP 标准定义了一组运行时函数，这些函数必须由符合 OpenMP 实现的运行时库提供。这些函数旨在让用户直接调用，通过 <code>omp_</code> 前缀标识，例如 <code>omp_get_thread_num</code> 和 <code>omp_get_num_threads</code>。为了使 Zig 程序员能够使用这些函数，我们在标准库中添加了一个 <code>omp</code> 命名空间，并通过 Zig 编译器的 <code>translate-c</code> 功能将所有函数声明从 C 转换为 Zig。这些转换后的函数声明随后被重新导出，同时移除了 <code>omp_</code> 前缀。示例 7 展示了如何通过此方法在 Zig 中获取线程 ID：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const omp = @import(&quot;std&quot;).omp;</span><br><span class="line">const thread_id = omp.get_thread_num();</span><br></pre></td></tr></table></figure><blockquote><p>示例 7：使用 OpenMP 库封装器在 Zig 中获取线程 ID。</p></blockquote><p>除了为 Zig 程序员提供的标准 OpenMP API，我们还需要一个内部 OpenMP API 供预处理器使用，以实现将 OpenMP 编译指令映射到 LLVM OpenMP 运行时库。这些函数和常量并非由 OpenMP 标准定义，而是由运行时实现（例如 LLVM OpenMP 的 <code>libomp</code>）提供。这些函数声明与标准 OpenMP 函数采用相同方式转换，但被放置在 <code>.omp.internal</code> 命名空间中。与标准 API 不同，这些函数在导出时没有移除前缀，它们非直接供程序员使用。</p><p>此外，<code>.omp.internal</code> 命名空间还包含一些开发的辅助工具，这些工具被预处理器用来支持 OpenMP 功能的实现。例如，<code>__kmpc_dispatch_*</code> 和 <code>__kmpc_for_static_*</code> 函数族的通用封装器，以及 III-B1 节中描述的 CAS 循环 reduction 算法。</p><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p>本文中的所有基准测试均在 Cray-EX ARCHER2 超级计算机的单节点上进行。每个节点由两个 64 核 AMD EPYC 7742 处理器组成，每个核包含32KB 的 L1 数据缓存、32KB 的 L1 指令缓存和 512KB 的 L2 缓存，以及由四个核共享的 16.4MB 的 L3 缓存。基准测试中使用的 OpenMP 运行时为基于 LLVM 13.0.0 的 libomp，它被 AMD 优化的 C 和 Fortran 编译器（AOCC）使用。C 和 Fortran 基准测试的参考实现分别通过 AOCC 的 Clang 和 Flang 编译器进行编译，使用相同的 OpenMP 运行时。与 Zig 版本基准测试集成的 Fortran 代码由 GNU gfortran 编译器（版本 7.5.0）编译，所有 C 代码均由 AMD Clang（版本 13.0.0）编译。AOCC 和 gfortran 的版本均为基准测试时 ARCHER2 平台上可用的最新版本。</p><p>每项基准测试针对每种线程数运行 5 次，并报告 5 次运行的平均值。执行时间使用参考实现中的内部计时器测量。</p><p>由于目前尚无现成的 Zig 高性能计算 (HPC) 基准测试，本文决定使用其他成熟 HPC 语言（如 Fortran 和 C）创建的基准测试，并将其转换为 Zig。为将 C 代码转换为 Zig，本文使用了 Zig 提供的 <code>translate-c</code> 子命令。然而，该工具实际使用中存在一些限制。首先，它会忽略所有 C 的编译指令（pragma），导致所有 OpenMP 相关信息丢失。其次，<code>translate-c</code> 会在转换代码为 Zig 之前执行 C 的预处理器，这意味着所有通过 <code>#include</code> 引入的头文件也会被转换并出现在生成的 Zig 代码中。例如，示例 8 展示了一段定义返回预处理器定义常量的 C 代码，而清单 9 显示了转换后的 Zig 代码，尽管 <code>CONSTANT</code> 被定义了，但并未直接使用，取而代之的是其展开后的值。因此，尽管该工具有一定帮助，但这些限制意味着部分代码仍需手动移植并进行严格验证。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#define CONSTANT (37 + 5)</span><br><span class="line"></span><br><span class="line">int foo(void) &#123; return CONSTANT; &#125;</span><br></pre></td></tr></table></figure><blockquote><p>示例 8: 用 translate-c 子命令转换的 C 程序</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pub export fn foo() c_int &#123;</span><br><span class="line">    return @as(c_int, 37) + @as(c_int, 5);</span><br><span class="line">&#125;</span><br><span class="line">pub const CONSTANT = @as(c_int, 37) + @as(c_int, 5);</span><br></pre></td></tr></table></figure><blockquote><p>示例 9: 使用 translate-c 子命令从 C 代码转换的 Zig 程序</p></blockquote><p>Zig 编译器没有提供 Fortran 等效的 translate-c，因此所有 Fortran 代码都需要手动移植。然而，Zig 和 Fortran 之间有几个主要区别，最显著的是 Fortran 中的数组是从 1 开始索引的，且 DO 循环的上界是包含在内的，而 Zig 中则不是。因此，在这样的移植过程中，所有数组索引和循环下界都需要调整，这增加了复杂性。</p><p>尽管以前从未这样做过，但从 Zig 调用 Fortran 过程的过程类似于调用 C 函数，所有参数类型都更改为指针。此外，为了符合 LLVM 的名称修饰方案，必须在函数名的末尾添加一个下划线。同样，也可以从 Fortran 调用 Zig 函数，但必须再次注意名称修饰方案。例如，只有 GNU gfortran 提供可预测的全局变量条目名称。</p><h3 id="V-结果与评估"><a href="#V-结果与评估" class="headerlink" title="V 结果与评估"></a>V 结果与评估</h3><p>在这项工作中，我们利用了 NAS 并行基准测试 (NPB) 套件中的内核，该套件包括基于计算流体动力学 (CFD) 应用中常见算法模式的各种内核。</p><h4 id="V-A-共轭梯度-CG"><a href="#V-A-共轭梯度-CG" class="headerlink" title="V-A 共轭梯度 (CG)"></a>V-A 共轭梯度 (CG)</h4><p>共轭梯度 (CG) 是我们选择的第一个基准测试，它利用了我们支持的大量 OpenMP 特性。我们将 Fortran 中的 conj_grad 子程序移植到 Zig 中，该子程序占据了大约 95% 的运行时间。这个子程序包括并行和工作共享指令、private、shared 和 firstprivate 变量共享子句、nowait 子句，以及在并行区域和工作共享循环上的归约操作。此外，这个内核还代表了在高性能计算 (HPC) 中形成大量工作负载的迭代算法。</p><p><img src="https://arxiv.org/html/2409.20148v1/extracted/5889514/images/cg-speedup.png" alt="Refer to caption"></p><blockquote><p>图 3：CG 基准测试（C 类）在不同线程数下的加速比（包括我们在 Zig 中的方法和 Fortran 参考实现）</p></blockquote><p>图3展示了在不同线程数下进行强缩放时，在C类问题规模下CG 内核的加速比。两种语言在 64 个线程以内通常遵循阿姆达尔定律，但在 96 和 128 个线程上运行时表现显著优于预期。这似乎是算法的固有特性，因为我们的 Zig 移植版和参考实现都遵循几乎相同的加速曲线，表明在这个基准测试中，Fortran 中的 OpenMP 和 Zig 中的 OpenMP 之间的性能非常相似。</p><blockquote><p>表 I：在强缩放时，Zig 和 Fortran NPB CG 基准测试（C 类）在不同线程数下的运行时间</p></blockquote><table><thead><tr><th><strong>Number of threads</strong></th><th><strong>Zig runtime (s)</strong></th><th><strong>Fortran runtime (s)</strong></th></tr></thead><tbody><tr><td>1</td><td>149.40</td><td>170.17</td></tr><tr><td>2</td><td>82.34</td><td>83.35</td></tr><tr><td>16</td><td>21.85</td><td>21.80</td></tr><tr><td>32</td><td>11.26</td><td>11.28</td></tr><tr><td>64</td><td>5.83</td><td>5.98</td></tr><tr><td>96</td><td>2.80</td><td>2.98</td></tr><tr><td>128</td><td>1.81</td><td>2.07</td></tr></tbody></table><p>表I显示了 Zig 和 Fortran 中 CG 基准测试的运行时间，可以看出，Zig 版本在单核上比 Fortran 代码快 1.15 倍，随后在所有其他线程数下性能大致相等，尽管 Zig 往往比 Fortran 略快。</p><h4 id="V-B-极易并行-EP"><a href="#V-B-极易并行-EP" class="headerlink" title="V-B 极易并行 (EP)"></a>V-B 极易并行 (EP)</h4><p>极易并行 (EP) 内核仅关注计算性能，不需要线程之间的同步，并具有高效的内存访问模式。除了计时和验证例程外，我们将整个代码从 Fortran 移植到 Zig。这个内核利用了 private 和 firstprivate 变量共享子句，以及并行区域归约。此外，还使用了 threadprivate 和 atomic 指令。</p><p><img src="https://arxiv.org/html/2409.20148v1/extracted/5889514/images/ep-speedup.png" alt="Refer to caption"></p><blockquote><p>图 4：EP 基准测试（C 类）在不同线程数下的加速比（包括我们在 Zig 中的方法和 Fortran 参考实现）</p></blockquote><p>图4显示C类问题规模时进行强缩放时，Zig 移植版和 Fortran 参考实现版本的 EP 基准测试的加速比。可以看出，对于 Zig 移植版和参考实现，因为该算法不需要线程之间的通信，所以加速比与线程数成正比。例外情况出现在 128 个线程时，Fortran 参考实现的加速比超过了 128 倍，意味着该基准测试受益于超线性缩放，而在 Zig 移植版中未观察到这种情况。这可能是由于 Fortran 版本在更多线程数下更好地利用了缓存，因为每个线程的问题规模减少了。</p><blockquote><p>表 II：在强缩放时，Zig 和 Fortran NPB EP 基准测试（C 类）在不同线程数下的运行时间</p></blockquote><table><thead><tr><th><strong>Number of threads</strong></th><th><strong>Zig runtime (s)</strong></th><th><strong>Fortran runtime (s)</strong></th></tr></thead><tbody><tr><td>1</td><td>147.66</td><td>185.26</td></tr><tr><td>2</td><td>76.17</td><td>94.90</td></tr><tr><td>16</td><td>9.84</td><td>11.83</td></tr><tr><td>32</td><td>4.72</td><td>5.92</td></tr><tr><td>64</td><td>2.29</td><td>2.84</td></tr><tr><td>96</td><td>1.57</td><td>1.97</td></tr><tr><td>128</td><td>1.36</td><td>1.42</td></tr></tbody></table><p>表II显示了在强缩放时，Zig 移植版和 Fortran版本实现的 EP 基准测试的运行时间，可以看出，Zig 版本平均比Fortan版本快 1.2 倍。这与 CG 基准测试类似，基于 Fortran 在这些科学工作负载中的流行程度，我们对此结论感到惊讶。尽管 Fortran 版本在 128 核时表现变好，但其执行速度仍然比 Zig 版本的基准测试慢。</p><h4 id="V-C-整数排序-IS"><a href="#V-C-整数排序-IS" class="headerlink" title="V-C 整数排序 (IS)"></a>V-C 整数排序 (IS)</h4><p>整数排序 (IS) 内核包含间接内存访问，旨在对内存子系统施加压力。该内核利用了 private 和 firstprivate 共享指令，并使用了 static,1 调度。IS 基准测试与本文考虑的其他基准测试的主要区别在于，它是用 C 语言编写的，我们将占总运行时间约 70% 的 rank 函数移植到了 Zig。</p><p><img src="https://arxiv.org/html/2409.20148v1/extracted/5889514/images/is-speedup.png" alt="Refer to caption"></p><p>图 5：IS 基准测试（C 类）在不同线程数下的加速比（包括我们在 Zig 中的方法和 C 参考实现）</p><p>图5显示了在C类问题规模下进行强缩放时，Zig 移植版与 C 参考实现的 IS 基准测试在不同线程数下的加速比。可以看出，这两个版本的基准测试在整个线程数范围内遵循非常相似的缩放模式，然而 Zig 版本在初始阶段缩放得更好，因此在较多线程数时能够提供更大的加速比。</p><p>表 III：在强缩放时，Zig 和 Fortran NPB IS 基准测试（C 类）在不同线程数下的运行时间</p><table><thead><tr><th><strong>Number of threads</strong></th><th><strong>Zig runtime (s)</strong></th><th><strong>Fortran runtime (s)</strong></th></tr></thead><tbody><tr><td>1</td><td>11.87</td><td>9.29</td></tr><tr><td>2</td><td>6.12</td><td>4.76</td></tr><tr><td>16</td><td>1.05</td><td>0.93</td></tr><tr><td>32</td><td>0.55</td><td>0.54</td></tr><tr><td>64</td><td>0.33</td><td>0.31</td></tr><tr><td>96</td><td>0.29</td><td>0.28</td></tr><tr><td>64</td><td>0.27</td><td>0.24</td></tr></tbody></table><p>表III显示了在强缩放时，IS 基准测试的运行时间。可以看出，与 Fortran 基准测试相比，对于用 C 实现的基准测试，C 版本在单线程上表现最佳。虽然在并发1场景下运行时差异明显，在更多线程数时，两种语言的性能非常接近。</p><h3 id="VI-结论与进一步工作"><a href="#VI-结论与进一步工作" class="headerlink" title="VI 结论与进一步工作"></a>VI 结论与进一步工作</h3><p>在本文中，我们探讨了通过添加 OpenMP 的循环共享结构来增强 Zig。Zig 最初设计为一种系统编程语言，并利用了 LLVM 生态系统，该语言的一个主要特点是提供性能和安全性，这使其成为高性能计算（HPC）未来的一个非常有趣的潜在编程语言。虽然在 Zig 中调用 C 函数的能力意味着与 MPI 的集成相对简单，但支持基于 pragma 的 OpenMP 方法需要对编译器进行额外工作，但这对于该语言被 HPC 社区采纳却至关重要。</p><p>在描述了我们通过在编译器中支持 OpenMP 循环指令来为 Zig 添加基于 pragma 的共享内存并行性的方法之后，我们进行了使用 NASA 的 NPB 基准测试套件的性能对比。我们证明了该方法提供了类似于 C 和 Fortran 编译器的线程缩放，通过观察发现 Zig 基准测试的运行时间低于其 Fortran 对应版本。鉴于 Fortran 在科学计算中的应用范围，这些结果令我们感到惊讶。</p><p>我们认为推动 Zig 在 HPC 中落地的关键性条件将是为 Zig 编译器添加支持分析功能。目前，Zig 编译器使用 Tracy 库<sup>[17]</sup>进行分析，该库的 Zig 接口是编译器本身的一部分，不能在应用程序中使用。修改编译器以自动为应用程序添加调用该库的代码，提供类似于 gprof 的功能。此外，增强 Zig 与 Fortran 之间的互操作性非常重要，这将使 Zig 能够集成到现有的大型 Fortran 代码库中。虽然在本文中我们已经证明了这种集成是可行的，但这需要在编译器内部进行额外的工作，并可能扩展 Fortran 标准，以确保这种方法的可靠性和一致性。</p><p>总之，我们得出结论，Zig 编程语言所提供的性能和安全性组合使其有潜力应用于 HPC 工作负载场景。通过增强编译器以支持 OpenMP 循环指令，我们提供了在 Zig 中基于 pragma 的共享内存并行性的能力，并证明了其缩放性与其他语言相当。此外，对于 HPC 工作负载，性能甚至超过原有语言。</p><h3 id="参考索引"><a href="#参考索引" class="headerlink" title="参考索引"></a>参考索引</h3><ol><li><p>C. Lattner, Architecture of open source applications &#x2F; structure, scale, and a few more fearless hacks.   Lulu Com, 2012, vol. 1. [Online]. Available: <a href="http://www.aosabook.org/en/llvm.html">http://www.aosabook.org/en/llvm.html</a></p></li><li><p>Apple, “Swift compiler,” 2023. [Online]. Available: <a href="https://www.swift.org/swift-compiler/">https://www.swift.org/swift-compiler/</a></p></li><li><p>“The rustc book,” 2023. [Online]. Available: <a href="https://doc.rust-lang.org/rustc/what-is-rustc.html">https://doc.rust-lang.org/rustc/what-is-rustc.html</a></p></li><li><p>A. Kelly, “Introduction to the Zig programming language,” Feb 2016. [Online]. Available: <a href="https://andrewkelley.me/post/intro-to-zig.html">https://andrewkelley.me/post/intro-to-zig.html</a></p></li><li><p>OpenMP Application Programming Interface, nov 2021. [Online]. Available: <a href="https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf">https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf</a></p></li><li><p>The LLVM Compiler Infrastructure, 06 2023. [Online]. Available: <a href="https://llvm.org/">https://llvm.org/</a></p></li><li><p>The LLVM Compiler Infrastructure, 08 2023. [Online]. Available: LLVM’s Analysis and Transform Passes</p></li><li><p>A. Kelley, “bindings.zig,” Mar 2023. [Online]. Available: <a href="https://github.com/ziglang/zig/blob/master/src/codegen/llvm/bindings.zig">https://github.com/ziglang/zig/blob/master/src/codegen/llvm/bindings.zig</a></p></li><li><p>A. Kelly, “Complete C ABI Compatibility,” Feb 2016. [Online]. Available: <a href="https://andrewkelley.me/post/intro-to-zig.html#c-abi">https://andrewkelley.me/post/intro-to-zig.html#c-abi</a></p></li><li><p>MPI: A Message-Passing Interface Standard Version 4.0, Message Passing Interface Forum, jun 2021. [Online]. Available: <a href="https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf">https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf</a></p></li><li><p>13.10 Implementing PARALLEL construct. [Online]. Available: <a href="https://gcc.gnu.org/onlinedocs/libgomp/Implementing-PARALLEL-construct.html">https://gcc.gnu.org/onlinedocs/libgomp/Implementing-PARALLEL-construct.html</a></p></li><li><p>H. Sutter, “How to write a CAS loop using std::atomics,” aug 2012. [Online]. Available: <a href="https://herbsutter.com/2012/08/31/reader-qa-how-to-write-a-cas-loop-using-stdatomics/">https://herbsutter.com/2012/08/31/reader-qa-how-to-write-a-cas-loop-using-stdatomics/</a></p></li><li><p>Zig Language Reference. [Online]. Available: <a href="https://ziglang.org/documentation/0.10.1/#Grammar">https://ziglang.org/documentation/0.10.1/#Grammar</a></p></li><li><p>LLVM&#x2F;OpenMP, 2023. [Online]. Available: <a href="https://openmp.llvm.org/">https://openmp.llvm.org/</a></p></li><li><p>Programming languages — Fortran, ISO&#x2F;IEC, 2004. [Online]. Available: <a href="https://j3-fortran.org/doc/year/04/04-007.pdf">https://j3-fortran.org/doc/year/04/04-007.pdf</a></p></li><li><p>“NAS Parallel Benchmarks,” Jul 2023. [Online]. Available: <a href="https://www.nas.nasa.gov/software/npb.html">https://www.nas.nasa.gov/software/npb.html</a></p></li><li><p>B. Taudul, R. Kupstys, A. Machizaud, and A. Gerstmann, “Tracy Profiler,” aug 2023. [Online]. Available: <a href="https://github.com/wolfpld/tracy">https://github.com/wolfpld/tracy</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ziglang适配OpenMP循环指令以实现共享内存并行计算&quot;&gt;&lt;a href=&quot;#Ziglang适配OpenMP循环指令以实现共享内存并行计算&quot; class=&quot;headerlink&quot; title=&quot;Ziglang适配OpenMP循环指令以实现共享内存并行计算&quot;&gt;</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Dockerless：Containerd方案</title>
    <link href="https://zoues.com/posts/3881eb8f/"/>
    <id>https://zoues.com/posts/3881eb8f/</id>
    <published>2024-11-18T12:23:48.000Z</published>
    <updated>2024-11-18T04:29:54.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>Docker作为最早广泛应用的容器运行时，其普及程度使得用户对其操作方式和功能特性极为熟悉。在Kubernetes的初期，Kubernetes通过内置的dockershim组件与Docker进行适配。然而，随着容器技术的不断发展，Kubernetes推出了CRI（容器运行时接口）标准，以更好地规范和扩展容器的接入能力。这一创新不仅增强了Kubernetes对多种容器运行时的兼容性，也为用户提供了更多选择，提升了灵活性。</p><p>CRI的引入使Kubernetes的系统组件能够与各种容器运行时无缝交互。这一转变不仅扩展了Kubernetes的适用范围，使其不再局限于Docker，也降低了对Docker和dockershim的依赖。如今，Kubernetes用户可以放心选择如Containerd、CRI-O等优秀的容器运行时。这种转变简化了Kubernetes项目的架构，并提升了服务运行效率。</p><p>自Kubernetes 1.24版本起，官方已正式移除了对Docker（通过dockershim实现的CRI支持）的支持。同时，Amazon EKS也明确将Containerd作为其唯一的容器运行时。因此，我们也将集群的容器运行时切换至Containerd，并用nerdctl替换docker命令行。</p><p>在此，有必要介绍一些常见的容器组件，如<code>libcontainer</code>、<code>runc</code>、<code>containerd</code>、<code>CRI</code>、<code>OCI</code>等。这些组件在容器生态系统中扮演着关键角色，各自发挥独特功能，共同构建了一个高效、灵活的容器运行环境。通过深入了解这些组件，我们将更好地掌握容器技术的运维手段，为未来的应用部署与运维奠定坚实基础。如果想直接了解nerdctl的相关内容，请跳转至相应章节。</p><p>从Docker 1.11版本开始，Docker的容器运行除了Docker Daemon，还依赖于多个组件的协作，如containerd、runc等。虽然Docker Daemon守护进程模块不断重构，但其基本功能和定位未发生太大变化，依然保持CS架构，由守护进程负责与Docker Client交互，并管理Docker镜像和容器。</p><p>为了应对Docker引擎的复杂性问题，Docker项目将引擎中的部分核心功能抽象出来并开源，这便是Containerd项目的诞生。Containerd专注于容器的基本操作，如镜像管理、容器运行时和生命周期管理等，为容器管理工具提供了统一的接口。</p><p>在现代架构中，Containerd负责集群节点上容器的生命周期管理，并通过gRPC接口为Docker Daemon提供支持。</p><p><img src="https://pic.imgdb.cn/item/673aba1fd29ded1a8c40c4fb.jpg" alt="img"></p><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><h3 id="OCI-标准-（Open-Container-Initiative）"><a href="#OCI-标准-（Open-Container-Initiative）" class="headerlink" title="OCI 标准 （Open Container Initiative）"></a>OCI 标准 （Open Container Initiative）</h3><p>OCI是由 Docker、CoreOS（被RedHat收购） 等组织对容器格式及运行时建立的统一的行业标准。</p><p>OCI主要定义两个规范：</p><ul><li>镜像规范（image-spec）定义了镜像的主要格式及内容</li><li>运行时规范（runtime-spec） 运行时规范定义镜像文件运行的管理， 而 runC 则是 目前使用最广泛的Low-Level容器运行时（runC 包含libcontainer，包括对namespace和cgroup的调用操作）。</li></ul><p>接下去看下目前市面上最主流的3个High-Level容器运行时（High-Level包括镜像传输、镜像管理、镜像解包和 API等高级功能）</p><ul><li>Containerd :CNCF孵化器的开源项目，由 Docker捐献的</li><li>Podman():Redhat孵化的项目，Podman 工具在RHEL8中作为完全支持的功能发布。</li><li>CRI-O : CNCF孵化器的开源项目</li></ul><p>我们再继续看一下 容器运行关系：</p><p><img src="https://pic.imgdb.cn/item/673ab8f0d29ded1a8c3fc841.webp" alt="ocs"></p><p>基于Containerd作为容器运行时的Docker可以定义为”High-High-Level”容器运行时。</p><p>关于 containerd 和 CRI 的关系要注意一些时间点:</p><ul><li>Containerd 在 2016年初被拆出来</li><li>CRI 标准在2016 年末出来的 （早于 Containerd）</li><li>Containerd 在2017年3月进入CNCF之后才添加了CRI支持.</li></ul><h3 id="Containerd"><a href="#Containerd" class="headerlink" title="Containerd"></a>Containerd</h3><p>Containerd 项目在 2017 年 3 月加入了云原生计算基金会（CNCF），其是一种简单、健壮和可移植性的行业标准容器运行时。它可作为 Linux 和 Windows 的守护进程，可以管理完整容器生命周期，包括镜像传输和存储、容器管理、底层存储和网络管理（通过CNI等）等功能。</p><p><img src="https://pic.imgdb.cn/item/673ab904d29ded1a8c3fdb48.webp" alt="containerd"></p><p>上图展示了 Containerd 的主要组件：</p><ol><li>Runtime：Containerd 负责容器的生命周期管理，包括容器的创建、运行和事件管理等。</li><li>Storage：Containerd 提供镜像存储管理。它负责容器镜像的存储、检索和管理。</li><li>gRPC：Containerd 使用 gRPC 服务器来与上层应用程序通信，为上层提供容器管理服务的接口。gRPC 是一种远程过程调用协议，使容器管理操作可以通过网络进行。</li><li>Metrics：Containerd 提供了有关容器性能和资源使用的指标，主要涉及 cgroup（控制组）的性能指标。</li><li>Metadata：容器的元数据，如镜像和容器的信息，以及与其相关的元数据，存储在 bootfs 中。这些元数据用于容器的管理和操作。</li><li>Tasks：在 Containerd 中，容器结构被管理为任务（tasks）。这包括容器的运行状态、进程信息和其他相关数据。</li><li>Events：Containerd 生成事件以通知上层应用程序容器的状态变化。这使上层应用程序能够订阅这些事件，以获知容器的状态变化，以及采取相应的操作。</li></ol><p>Containerd 项目在 2017 年 3 月加入了云原生计算基金会（CNCF），下图是Docker跟Containerd的对照表。</p><table><thead><tr><th><strong>特性</strong></th><th><strong>Containerd</strong></th><th><strong>Docker</strong></th></tr></thead><tbody><tr><td><strong>架构</strong></td><td>专注于容器运行时，负责容器生命周期的管理。</td><td>提供完整的容器平台，包含运行时和高级管理工具。</td></tr><tr><td><strong>镜像管理</strong></td><td>提供基础的镜像传输和存储功能。</td><td>提供高级镜像构建、版本控制和分发功能。</td></tr><tr><td><strong>生态系统</strong></td><td>模块化设计，常作为容器生态系统的一部分。</td><td>整合多种工具和服务，构建全面的容器生态系统。</td></tr><tr><td><strong>安全性</strong></td><td>针对核心功能进行小型优化和改进。</td><td>提供高级安全功能，如镜像签名和密钥管理。</td></tr><tr><td><strong>可扩展性</strong></td><td>提供精简机制，支持多种容器运行时场景。</td><td>提供插件机制，允许高度自定义和扩展。</td></tr><tr><td><strong>部署模式</strong></td><td>需要额外工具，如 Kubernetes，提供集成容器解决方案。</td><td>提供一体化容器部署管理，支持简单快速启动。</td></tr><tr><td><strong>社区支持</strong></td><td>获得 CNCF 和云原生基金会的支持。</td><td>社区由 Docker 公司主导，资源丰富。</td></tr><tr><td><strong>用户案例</strong></td><td>常用于构建自定义容器解决方案，如 Kubernetes。</td><td>适合构建容器化应用和分布式容器管理。</td></tr></tbody></table><h3 id="CRI"><a href="#CRI" class="headerlink" title="CRI"></a>CRI</h3><p>Kubernetes 提供了一个名为 CRI 的容器运行时接口。那么，CRI 到底是什么呢？这其实与 Docker 的发展有着密切关系。</p><p>在 Kubernetes 的早期阶段，Docker 因其广泛的流行，成为了 Kubernetes 首选支持的容器运行时。Kubernetes 当时通过硬编码的方式直接调用 Docker API。然而，随着 Docker 的不断演进以及 Google 的主导，更多的容器运行时开始涌现。为了支持这些更精简、更灵活的容器运行时，Google 联合红帽推出了 CRI（容器运行时接口）标准，以将 Kubernetes 平台与特定的容器运行时解耦（其中一个目的也是为了减少对 Docker 的依赖）。</p><p>CRI（Container Runtime Interface）本质上是一组 Kubernetes 定义的接口，用于与容器运行时进行交互。因此，任何实现了这套接口的容器运行时都可以无缝对接到 Kubernetes。然而，Kubernetes 推出 CRI 标准时，还没有如今的主导地位，因此一些容器运行时并未直接实现 CRI 接口。为了适配这些容器运行时，便引入了 <code>shim</code>。<code>shim</code> 作为适配器，将各种容器运行时的接口转换为 Kubernetes 可识别的 CRI 接口。其中，<code>dockershim</code> 就是 Kubernetes 为了对接 Docker 而实现的 CRI 适配器。</p><p><img src="https://pic.imgdb.cn/item/673ab91fd29ded1a8c3ff970.png" alt="dockershim"></p><p>Kubelet 通过 gRPC 与容器运行时或 shim 进行通信，其中 kubelet 作为客户端，CRI shim（或容器运行时本身）作为服务端。</p><p>CRI 定义的 <a href="https://github.com/kubernetes/cri-api/tree/master/pkg/apis">API</a> 主要包括两个 gRPC 服务：<code>ImageService</code> 和 <code>RuntimeService</code>。<code>ImageService</code> 负责处理镜像的操作，如拉取、查看和删除镜像；而 <code>RuntimeService</code> 主要管理 Pod 和容器的生命周期，以及与容器交互的操作（如 exec、attach、port-forward）。可以通过 kubelet 的 <code>--container-runtime-endpoint</code> 和 <code>--image-service-endpoint</code> 命令行或者Kubelet配置文件来配置这两个服务的服务地址。</p><p>由于 Docker 当时处于主导地位，Kubernetes 直接将 <code>dockershim</code> 内置于 kubelet 中。因此，如果使用 Docker 作为容器运行时，无需额外安装或配置适配器。不过，这种内置支持也在一定程度上让 Docker 公司忽视了后续发展。</p><p>当使用 Docker 时，在 Kubernetes 中创建一个 Pod 的过程大致如下：首先，kubelet 通过 CRI 接口调用 <code>dockershim</code>，请求创建容器。此时，kubelet 作为CRI 客户端，而 <code>dockershim</code> 是接收请求的服务端代理，二者都内置于 kubelet 中。</p><p><code>dockershim</code> 收到请求后，会将其转换为 Docker Daemon 能识别的请求，发送给 Docker Daemon，请求创建容器。接下来，Docker Daemon 调用 <code>Containerd</code>，再由 <code>containerd</code> 创建 <code>containerd-shim</code> 进程（Containerd后续版本移除了containerd-shim），通过该进程调用 <code>runc</code> 来实际创建容器。</p><p>可以看出，使用 Docker 的调用链较长，而真正与容器相关的操作，<code>Containerd</code> 已经足够。尽管其深受欢迎的一个重要原因是为用户提供了许多友好的功能，但Docker实在是过于复杂和笨重，一些功能对于 Kubernetes 来说并不必要。因此，将容器运行时切换到 <code>Containerd</code> 是一个更为高效的选择。下图展示了插件的发展历程。</p><p><img src="https://pic.imgdb.cn/item/673ab932d29ded1a8c400bfe.png" alt="cri"></p><p>从上图可以看出，在 Containerd 1.0 中，CRI 的适配是通过一个独立的 <code>CRI-Containerd</code> 进程完成的。这是因为最初的 Containerd 还需要适配其他系统（如 Swarm），所以并未直接实现 CRI，而是由 <code>CRI-Containerd</code> 这个 shim 来承担。</p><p>到了 Containerd 1.1 版本，<code>CRI-Containerd</code> shim 被移除，适配逻辑被作为插件直接集成到 Containerd 主进程中，这使得调用流程更加简洁高效。</p><p>随着 CRI 方案的成熟以及其他容器运行时对 CRI 支持的不断完善，Kubernetes 社区在2020年7月启动了移除 dockershim 的计划。在 Kubernetes 1.20 版本中，kubelet 中内置的 dockershim 代码被分离，并将其标记为“维护模式”。尽管此时仍然可以使用 dockershim，但已经在 1.24 版本中彻底移除它，dockershim由</p><blockquote><p> 同时社区也提供相应的命令行工具crictl来与CRI进行交互</p></blockquote><h2 id="命令行盘点"><a href="#命令行盘点" class="headerlink" title="命令行盘点"></a>命令行盘点</h2><p>其中Docker、ctr、nerdctl 和 crictl 都是容器管理工具，但它们各自的设计目标和使用场景有所不同。以下是它们的对比：</p><p><strong>Docker</strong></p><ul><li><strong>简介</strong>：Docker 是一个流行的容器管理工具，提供了从构建、运行到管理容器的完整生态系统。</li><li><strong>功能</strong>：支持容器镜像构建、镜像分发（Docker Hub 集成）、容器生命周期管理（启动、停止、删除等）。</li><li><strong>接口</strong>：提供强大且用户友好的 CLI 和 API 接口，适合开发人员和运维人员快速上手。</li><li><strong>特点</strong>：<ul><li>有丰富的生态系统（如 Docker Compose）。</li><li>支持复杂的容器编排功能（如 Swarm）。</li><li>包含额外的守护进程（<code>dockerd</code>），需要与容器运行时协作。</li></ul></li></ul><p><strong>ctr</strong></p><ul><li><strong>简介</strong>：<code>ctr</code> 是 containerd 提供的原生 CLI，用于直接与 containerd 交互。</li><li><strong>功能</strong>：<code>ctr</code> 可以拉取镜像、运行容器、管理容器和镜像存储等基本功能，但功能相对简化，更适合低级操作和调试。</li><li><strong>接口</strong>：与 Docker 不同，<code>ctr</code> 面向 containerd 用户，没有像 Docker 那样的高级 API 和生态支持。</li><li><strong>特点</strong>：<ul><li>适合需要直接操作 containerd 的开发者和系统集成商。</li><li>不支持镜像构建等高级功能（containerd 专注于容器运行时管理）。</li></ul></li></ul><h3 id="nerdctl"><a href="#nerdctl" class="headerlink" title="nerdctl"></a><strong>nerdctl</strong></h3><ul><li><strong>简介</strong>：<code>nerdctl</code> 是一个基于 containerd 的命令行工具，支持 Docker 风格的 CLI 命令。</li><li><strong>功能</strong>：提供 Docker 类似的用户体验，可以直接用来运行容器、构建镜像、支持 CNI 网络插件和卷等。</li><li><strong>接口</strong>：与 Docker 的 CLI 非常相似，使得从 Docker 迁移到 nerdctl 相对简单。</li><li><strong>特点</strong>：<ul><li>不需要 Docker 守护进程，仅依赖 containerd。</li><li>可以与 Kubernetes（使用 containerd 作为运行时）很好地集成，且支持与 nerdctl 的 Docker Compose 类似功能。</li></ul></li></ul><p><strong>crictl</strong></p><ul><li><strong>简介</strong>：<code>crictl</code> 是一个用于容器运行时接口（CRI）的命令行工具，专为 Kubernetes 容器管理设计。</li><li><strong>功能</strong>：主要用于与 CRI 兼容的容器运行时（如 containerd 和 CRI-O）进行交互，包括管理 Pod、镜像和容器。</li><li><strong>接口</strong>：<code>crictl</code> 的命令集更偏向 Kubernetes 集群的调试和管理，与 Docker 和 nerdctl 相比，更专注于 Kubernetes 场景。</li><li><strong>特点</strong>：<ul><li>集成和兼容 Kubernetes，通常用来调试或管理容器和 Pod 运行状态。</li><li>不能用来构建镜像，专注于容器和 Pod 的生命周期管理。</li></ul></li></ul><p>前面我们了解了可以使用 docker、ctr、crictl 这些命令行工具进行容器管理，接下来我们针对以下几个场景做横向对比：</p><ol><li><strong>镜像相关操作</strong></li></ol><table><thead><tr><th><strong>镜像相关功能</strong></th><th><strong>docker CLI</strong></th><th>*<em>containerd ctr</em></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>显示本地镜像列表</td><td><code>docker images</code></td><td><code>ctr i ls</code></td><td><code>crictl images</code></td><td><code>nerdctl images</code></td></tr><tr><td>下载镜像</td><td><code>docker pull</code></td><td><code>ctr i pull</code></td><td><code>crictl pull</code></td><td><code>nerdctl pull</code></td></tr><tr><td>上传镜像</td><td><code>docker push</code></td><td><code>ctr i push</code></td><td>无</td><td><code>nerdctl push</code></td></tr><tr><td>删除本地镜像</td><td><code>docker rmi</code></td><td><code>ctr i rm</code></td><td><code>crictl rmi</code></td><td><code>nerdctl rmi</code></td></tr><tr><td>查看镜像详情</td><td><code>docker inspect</code></td><td>无</td><td><code>crictl inspecti</code></td><td><code>nerdctl inspect</code></td></tr><tr><td>重命名镜像</td><td><code>docker tag</code></td><td><code>ctr i tag</code></td><td>无</td><td><code>nerdctl tag</code></td></tr><tr><td>导出镜像</td><td><code>docker export</code></td><td><code>ctr i export</code></td><td>无</td><td><code>nerdctl save</code></td></tr><tr><td>导入镜像</td><td><code>docker import</code></td><td><code>ctr i import</code></td><td>无</td><td><code>nerdctl load</code></td></tr><tr><td>构建镜像</td><td><code>docker build</code></td><td>无</td><td>无</td><td><code>nerdctl build</code></td></tr></tbody></table><ol start="2"><li><strong>容器相关操作</strong></li></ol><table><thead><tr><th><strong>容器相关功能</strong></th><th><strong>docker CLI</strong></th><th><strong>containerd ctr</strong></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>显示容器列表</td><td><code>docker ps</code></td><td><code>ctr c ls</code></td><td><code>crictl ps</code></td><td><code>nerdctl ps</code></td></tr><tr><td>创建容器</td><td><code>docker create</code></td><td><code>ctr c create</code></td><td><code>crictl create</code></td><td><code>nerdctl create</code></td></tr><tr><td>启动容器</td><td><code>docker start</code></td><td><code>ctr t start</code></td><td><code>crictl start</code></td><td><code>nerdctl start</code></td></tr><tr><td>停止容器</td><td><code>docker stop</code></td><td>无</td><td><code>crictl stop</code></td><td><code>nerdctl stop</code></td></tr><tr><td>删除容器</td><td><code>docker rm</code></td><td><code>ctr c rm</code></td><td><code>crictl rm</code></td><td><code>nerdctl rm</code></td></tr><tr><td>查看容器详情</td><td><code>docker inspect</code></td><td><code>ctr c info</code></td><td><code>crictl inspect</code></td><td><code>nerdctl inspect</code></td></tr><tr><td>attach</td><td><code>docker attach</code></td><td><code>ctr t attach</code></td><td><code>crictl attach</code></td><td><code>nerdctl attach</code></td></tr><tr><td>exec</td><td><code>docker exec</code></td><td><code>ctr t exec</code></td><td><code>crictl exec</code></td><td><code>nerdctl exec</code></td></tr><tr><td>logs</td><td><code>docker logs</code></td><td>无</td><td><code>crictl logs</code></td><td><code>nerdctl logs</code></td></tr><tr><td>stats</td><td><code>docker stats</code></td><td><code>ctr t metrics</code></td><td><code>crictl stats</code></td><td><code>nerdctl stats</code></td></tr></tbody></table><ol start="3"><li><strong>Pod 相关操作</strong></li></ol><table><thead><tr><th><strong>Pod相关功能</strong></th><th><strong>docker CLI</strong></th><th><strong>containerd ctr</strong></th><th><strong>crictl</strong></th><th><strong>nerdctl</strong></th></tr></thead><tbody><tr><td>显示Pod列表</td><td>无</td><td>无</td><td><code>crictl pods</code></td><td>无</td></tr><tr><td>查看Pod详情</td><td>无</td><td>无</td><td><code>crictl inspectp</code></td><td>无</td></tr><tr><td>运行Pod</td><td>无</td><td>无</td><td><code>crictl runp</code></td><td>无</td></tr><tr><td>停止Pod</td><td>无</td><td>无</td><td><code>crictl stopp</code></td><td>无</td></tr><tr><td>删除Pod</td><td>无</td><td>无</td><td><code>crictl rmp</code></td><td>无</td></tr></tbody></table><p><strong>总结</strong></p><ul><li><strong>Docker</strong>：用户友好，功能全面，适合独立容器管理和开发环境。</li><li><strong>ctr</strong>：轻量、低级工具，适合直接与 containerd 交互，主要用于调试和底层管理。</li><li><strong>nerdctl</strong>：兼容 Docker CLI，适合那些想使用 Docker 命令但又希望直接使用 containerd 的用户。</li><li><strong>crictl</strong>：Kubernetes 环境下的调试工具，专注于 CRI 兼容的运行时，不适合一般的开发场景。</li></ul><p>其中nerdctl 是轻量化的 containerd 替代方案，crictl 是基于 Kubernetes CRI交互的Pod管理工具。</p><h2 id="Containerd使用指南"><a href="#Containerd使用指南" class="headerlink" title="Containerd使用指南"></a>Containerd使用指南</h2><p>nerdctl 是一个与 docker cli 风格兼容的 containerd 客户端工具，而且直接兼容 docker compose 的语法的，这就大大提高了直接将 containerd 作为本地开发、测试或者单机容器部署使用的效率。</p><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>同样直接在 GitHub Release 页面下载对应的压缩包解压到 PATH 路径下即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 如果没有安装 containerd，则可以下载 nerdctl-full-&lt;VERSION&gt;-linux-amd64.tar.gz 包进行安装</span><br><span class="line">➜  ~ wget https://github.com/containerd/nerdctl/releases/download/v1.7.5/nerdctl-1.7.5-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">➜  ~ mkdir -p /usr/local/containerd/bin/ &amp;&amp; tar -zxvf nerdctl-1.7.5-linux-amd64.tar.gz nerdctl &amp;&amp; mv nerdctl /usr/local/containerd/bin/</span><br><span class="line">➜  ~ ln -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl</span><br><span class="line">➜  ~ nerdctl version</span><br><span class="line">WARN[0000] unable to determine buildctl version: exec: &quot;buildctl&quot;: executable file not found in $PATH </span><br><span class="line">Client:</span><br><span class="line"> Version:v1.7.5</span><br><span class="line"> OS/Arch:linux/amd64</span><br><span class="line"> Git commit:cffed372371dcbea3dc9a646ce5a913fc1c09513</span><br><span class="line"> buildctl:</span><br><span class="line">  Version:</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> containerd:</span><br><span class="line">  Version:v2.0.0-beta.2-33-g96bf529cb</span><br><span class="line">  GitCommit:96bf529cbf55940ddb96bb8adc8be51b11922ebb</span><br><span class="line"> runc:</span><br><span class="line">  Version:1.1.4</span><br><span class="line">  GitCommit:v1.1.4-0-g5fd4c4d</span><br></pre></td></tr></table></figure><p>安装完成后接下来学习下 <code>nerdctl</code> 命令行工具的使用。</p><h3 id="2-命令简介"><a href="#2-命令简介" class="headerlink" title="2. 命令简介"></a>2. 命令简介</h3><h4 id="容器管理"><a href="#容器管理" class="headerlink" title="容器管理"></a>容器管理</h4><p><strong>run</strong></p><p>和 <code>docker run</code> 类似可以使用 <code>nerdctl run</code> 命令运行容器，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpine</span><br><span class="line">docker.io/library/nginx:alpine:                                                   resolved       |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">index-sha256:bead42240255ae1485653a956ef41c9e458eb077fcb6dc664cbc3aa9701a05ce:    done           |++++++++++++++++++++++++++++++++++++++| manifest-sha256:ce6ca11a3fa7e0e6b44813901e3289212fc2f327ee8b1366176666e8fb470f24: done           |++++++++++++++++++++++++++++++++++++++| config-sha256:7ce0143dee376bfd2937b499a46fb110bda3c629c195b84b1cf6e19be1a9e23b:   done           |++++++++++++++++++++++++++++++++++++++| elapsed: 5.3 s                                                                    total:  3.1 Ki (606.0 B/s)                                       6e489777d2f73dda8a310cdf8da9df38353c1aa2021d3c2270b30eff1806bcf8</span><br></pre></td></tr></table></figure><p>可选的参数使用和 <code>docker run</code> 基本一直，比如 <code>-i</code>、<code>-t</code>、<code>--cpus</code>、<code>--memory</code> 等选项，可以使用 <code>nerdctl run --help</code> 获取可使用的命令选项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">[root@192 containerd]# nerdctl run --help</span><br><span class="line">Run a command in a new container. Optionally specify &quot;ipfs://&quot; or &quot;ipns://&quot; scheme to pull image from IPFS.</span><br><span class="line"></span><br><span class="line">Usage: nerdctl run [flags] IMAGE [COMMAND] [ARG...]</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --add-host strings                               Add a custom host-to-IP mapping (host:ip)</span><br><span class="line">      --blkio-weight uint16                            Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0)</span><br><span class="line">      --cap-add strings                                Add Linux capabilities</span><br><span class="line">      --cap-drop strings                               Drop Linux capabilities</span><br><span class="line">      --cgroup-conf strings                            Configure cgroup v2 (key=value)</span><br><span class="line">      --cgroup-parent string                           Optional parent cgroup for the container</span><br><span class="line">      --cgroupns string                                Cgroup namespace to use, the default depends on the cgroup version (&quot;host&quot;|&quot;private&quot;) (default &quot;private&quot;)</span><br><span class="line">      --cidfile string                                 Write the container ID to the file</span><br><span class="line">      --cosign-certificate-identity string             The identity expected in a valid Fulcio certificate for --verify=cosign. Valid values include email address, DNS names, IP addresses, and URIs. Either --cosign-certificate-identity or --cosign-certificate-identity-regexp must be set for keyless flows</span><br><span class="line">      --cosign-certificate-identity-regexp string      A regular expression alternative to --cosign-certificate-identity for --verify=cosign. Accepts the Go regular expression syntax described at https://golang.org/s/re2syntax. Either --cosign-certificate-identity or --cosign-certificate-identity-regexp must be set for keyless flows</span><br><span class="line">      --cosign-certificate-oidc-issuer string          The OIDC issuer expected in a valid Fulcio certificate for --verify=cosign, e.g. https://token.actions.githubusercontent.com or https://oauth2.sigstore.dev/auth. Either --cosign-certificate-oidc-issuer or --cosign-certificate-oidc-issuer-regexp must be set for keyless flows</span><br><span class="line">      --cosign-certificate-oidc-issuer-regexp string   A regular expression alternative to --certificate-oidc-issuer for --verify=cosign. Accepts the Go regular expression syntax described at https://golang.org/s/re2syntax. Either --cosign-certificate-oidc-issuer or --cosign-certificate-oidc-issuer-regexp must be set for keyless flows</span><br><span class="line">      --cosign-key string                              Path to the public key file, KMS, URI or Kubernetes Secret for --verify=cosign</span><br><span class="line">      --cpu-period uint                                Limit CPU CFS (Completely Fair Scheduler) period</span><br><span class="line">      --cpu-quota int                                  Limit CPU CFS (Completely Fair Scheduler) quota (default -1)</span><br><span class="line">      --cpu-shares uint                                CPU shares (relative weight)</span><br><span class="line">      --cpus float                                     Number of CPUs</span><br><span class="line">      --cpuset-cpus string                             CPUs in which to allow execution (0-3, 0,1)</span><br><span class="line">      --cpuset-mems string                             MEMs in which to allow execution (0-3, 0,1)</span><br><span class="line">  -d, --detach                                         Run container in background and print container ID</span><br><span class="line">      --detach-keys string                             Override the default detach keys (default &quot;ctrl-p,ctrl-q&quot;)</span><br><span class="line">      --device strings                                 Add a host device to the container</span><br><span class="line">      --dns strings                                    Set custom DNS servers</span><br><span class="line">      --dns-opt strings                                Set DNS options</span><br><span class="line">      --dns-option strings                             Set DNS options</span><br><span class="line">      --dns-search strings                             Set custom DNS search domains</span><br><span class="line">      --entrypoint stringArray                         Overwrite the default ENTRYPOINT of the image</span><br><span class="line">  -e, --env stringArray                                Set environment variables</span><br><span class="line">      --env-file strings                               Set environment variables from file</span><br><span class="line">      --gpus stringArray                               GPU devices to add to the container (&#x27;all&#x27; to pass all GPUs)</span><br><span class="line">      --group-add strings                              Add additional groups to join</span><br><span class="line">      --help                                           show help</span><br><span class="line">  -h, --hostname string                                Container host name</span><br><span class="line">      --init                                           Run an init process inside the container, Default to use tini</span><br><span class="line">      --init-binary string                             The custom binary to use as the init process (default &quot;tini&quot;)</span><br><span class="line">  -i, --interactive                                    Keep STDIN open even if not attached</span><br><span class="line">      --ip string                                      IPv4 address to assign to the container</span><br><span class="line">      --ip6 string                                     IPv6 address to assign to the container</span><br><span class="line">      --ipc string                                     IPC namespace to use (&quot;host&quot;|&quot;private&quot;)</span><br><span class="line">      --ipfs-address string                            multiaddr of IPFS API (default uses $IPFS_PATH env variable if defined or local directory ~/.ipfs)</span><br><span class="line">      --isolation string                               Specify isolation technology for container. On Linux the only valid value is default. Windows options are host, process and hyperv with process isolation as the default (default &quot;default&quot;)</span><br><span class="line">      --kernel-memory string                           Kernel memory limit (deprecated)</span><br><span class="line">  -l, --label stringArray                              Set metadata on container</span><br><span class="line">      --label-file strings                             Set metadata on container from file</span><br><span class="line">      --log-driver string                              Logging driver for the container. Default is json-file. It also supports logURI (eg: --log-driver binary://&lt;path&gt;) (default &quot;json-file&quot;)</span><br><span class="line">      --log-opt stringArray                            Log driver options</span><br><span class="line">      --mac-address string                             MAC address to assign to the container</span><br><span class="line">  -m, --memory string                                  Memory limit</span><br><span class="line">      --memory-reservation string                      Memory soft limit</span><br><span class="line">      --memory-swap string                             Swap limit equal to memory plus swap: &#x27;-1&#x27; to enable unlimited swap</span><br><span class="line">      --memory-swappiness int                          Tune container memory swappiness (0 to 100) (default -1) (default -1)</span><br><span class="line">      --mount stringArray                              Attach a filesystem mount to the container</span><br><span class="line">      --name string                                    Assign a name to the container</span><br><span class="line">      --net strings                                    Connect a container to a network (&quot;bridge&quot;|&quot;host&quot;|&quot;none&quot;|&lt;CNI&gt;) (default [bridge])</span><br><span class="line">      --network strings                                Connect a container to a network (&quot;bridge&quot;|&quot;host&quot;|&quot;none&quot;|&quot;container:&lt;container&gt;&quot;|&lt;CNI&gt;) (default [bridge])</span><br><span class="line">      --oom-kill-disable                               Disable OOM Killer</span><br><span class="line">      --oom-score-adj int                              Tune container’s OOM preferences (-1000 to 1000, rootless: 100 to 1000)</span><br><span class="line">      --pid string                                     PID namespace to use</span><br><span class="line">      --pidfile string                                 file path to write the task&#x27;s pid</span><br><span class="line">      --pids-limit int                                 Tune container pids limit (set -1 for unlimited) (default -1)</span><br><span class="line">      --platform string                                Set platform (e.g. &quot;amd64&quot;, &quot;arm64&quot;)</span><br><span class="line">      --privileged                                     Give extended privileges to this container</span><br><span class="line">  -p, --publish strings                                Publish a container&#x27;s port(s) to the host</span><br><span class="line">      --pull string                                    Pull image before running (&quot;always&quot;|&quot;missing&quot;|&quot;never&quot;) (default &quot;missing&quot;)</span><br><span class="line">      --rdt-class string                               Name of the RDT class (or CLOS) to associate the container with</span><br><span class="line">      --read-only                                      Mount the container&#x27;s root filesystem as read only</span><br><span class="line">      --restart string                                 Restart policy to apply when a container exits (implemented values: &quot;no&quot;|&quot;always|on-failure:n|unless-stopped&quot;) (default &quot;no&quot;)</span><br><span class="line">      --rm                                             Automatically remove the container when it exits</span><br><span class="line">      --rootfs                                         The first argument is not an image but the rootfs to the exploded container</span><br><span class="line">      --runtime string                                 Runtime to use for this container, e.g. &quot;crun&quot;, or &quot;io.containerd.runsc.v1&quot; (default &quot;io.containerd.runc.v2&quot;)</span><br><span class="line">      --security-opt stringArray                       Security options</span><br><span class="line">      --shm-size string                                Size of /dev/shm</span><br><span class="line">      --stop-signal string                             Signal to stop a container (default &quot;SIGTERM&quot;)</span><br><span class="line">      --stop-timeout int                               Timeout (in seconds) to stop a container</span><br><span class="line">      --sysctl stringArray                             Sysctl options</span><br><span class="line">      --tmpfs stringArray                              Mount a tmpfs directory</span><br><span class="line">  -t, --tty                                            Allocate a pseudo-TTY</span><br><span class="line">      --ulimit strings                                 Ulimit options</span><br><span class="line">      --umask string                                   Set the umask inside the container. Defaults to 0022</span><br><span class="line">  -u, --user string                                    Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;])</span><br><span class="line">      --uts string                                     UTS namespace to use</span><br><span class="line">      --verify string                                  Verify the image (none|cosign|notation) (default &quot;none&quot;)</span><br><span class="line">  -v, --volume stringArray                             Bind mount a volume</span><br><span class="line">      --volumes-from stringArray                       Mount volumes from the specified container(s)</span><br><span class="line">  -w, --workdir string                                 Working directory inside the container</span><br><span class="line"></span><br><span class="line">See also &#x27;nerdctl --help&#x27; for the global flags such as &#x27;--namespace&#x27;, &#x27;--snapshotter&#x27;, and &#x27;--cgroup-manager&#x27;.</span><br></pre></td></tr></table></figure><p><strong>exec</strong></p><p>同样也可以使用 <code>exec</code> 命令执行容器相关命令，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl exec -it ea07355852eb date</span><br><span class="line">Mon Nov 18 03:15:06 UTC 2024</span><br></pre></td></tr></table></figure><p><strong>ps</strong></p><p>使用 <code>nerdctl ps</code> 命令可以列出所有容器，与docker不同的是，其提供了基于namespace的隔离，如果需要查看k8s的容器，需要增加<code>-n k8s.io</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io ps</span><br><span class="line">CONTAINER ID    IMAGE                                                                      COMMAND                   CREATED       STATUS    PORTS    NAMES</span><br><span class="line">e7f156d31942    docker.io/prom/node-exporter:v1.4.0                                        &quot;/bin/node_exporter …&quot;    2 days ago    Up                 k8s://node-exporter/node-exporter-5zhjt/main</span><br><span class="line">bf1704937991    hub.cloud.ctripcorp.com/k8s-mirror/pause-amd64:3.1                         &quot;/pause&quot;                  2 days ago    Up                 k8s://node-exporter/node-exporter-5zhjt</span><br></pre></td></tr></table></figure><p>同样可以使用 <code>-a</code> 选项显示所有的容器列表，默认只显示正在运行的容器，不过需要注意的是 <code>nerdctl ps</code> 命令并没有实现 <code>docker ps</code> 下面的 <code>--filter</code>、<code>--format</code>、<code>--last</code>、<code>--size</code> 等选项。</p><p><strong>inspect</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io inspect e7f156d31942</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;e7f156d31942ffec027b48d50d787e18c59e24ae8134f06ee25dab6c74220d83&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2024-11-15T08:37:07.845392486Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/node_exporter&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">           ...</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;running&quot;,</span><br><span class="line">            &quot;Running&quot;: true,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 22800,</span><br><span class="line">            &quot;ExitCode&quot;: 0,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;docker.io/prom/node-exporter:v1.4.0&quot;,</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到显示结果和 <code>docker inspect</code> 也基本一致的。</p><p><strong>logs</strong></p><p>查看容器日志是我们平时经常会使用到的一个功能，同样我们可以使用 <code>nerdctl logs</code> 来获取日志数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io logs e7f156d31942 |more</span><br><span class="line">ts=2024-11-15T08:37:07.980Z caller=node_exporter.go:182 level=info msg=&quot;Starting node_exporter&quot; version=&quot;(version=1.4.0, branch=HEAD, revision=7da1321761b3b8dfc9e496e1a60e6a476fec6018)&quot;</span><br><span class="line">ts=2024-11-15T08:37:07.980Z caller=node_exporter.go:183 level=info msg=&quot;Build context&quot; build_context=&quot;(go=go1.19.1, user=root@83d90983e89c, date=20220926-12:32:56)&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>同样支持 <code>-f</code>、<code>-t</code>、<code>-n</code>、<code>--since</code>、<code>--until</code> 这些选项。</p><p><strong>stop</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io stop e7f156d31942</span><br><span class="line">FATA[0000] 1 errors:</span><br><span class="line">unable to cleanup network for container: e7f156d31942</span><br></pre></td></tr></table></figure><p><strong>rm</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io rm e7f156d31942 </span><br><span class="line">FATA[0000] 1 errors:</span><br><span class="line">container e7f156d31942ffec027b48d50d787e18c59e24ae8134f06ee25dab6c74220d83 is in running status. unpause/stop container first or force removal </span><br><span class="line">➜  ~ nerdctl -n k8s.io rm -f e7f156d31942 </span><br><span class="line">ERRO[0000] 1 errors:</span><br><span class="line">failed to load container networking options from specs: unexpected end of JSON input </span><br></pre></td></tr></table></figure><p>要强制删除同样可以使用 <code>-f</code> 或 <code>--force</code> 选项来操作。</p><h4 id="镜像管理"><a href="#镜像管理" class="headerlink" title="镜像管理"></a>镜像管理</h4><p><strong>images</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io images|grep 4a2c72aa0e18</span><br><span class="line">prom/node-exporter                                                 &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">&lt;none&gt;                                                             &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">prom/node-exporter                                                 v1.4.0                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br></pre></td></tr></table></figure><p>通过cri方式获取的镜像，containerd会生成三个镜像，分为为tag、sha256以及digest，另外需要注意的是<code>docker images</code> 的一些选项暂未实现，比如 <code>--filter</code>、<code>--format</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@192 containerd]# nerdctl image --help</span><br><span class="line">Manage images</span><br><span class="line"></span><br><span class="line">Usage: nerdctl image [flags]</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  build    Build an image from a Dockerfile. Needs buildkitd to be running.</span><br><span class="line">  convert  convert an image</span><br><span class="line">  decrypt  decrypt an image</span><br><span class="line">  encrypt  encrypt image layers</span><br><span class="line">  history  Show the history of an image</span><br><span class="line">  inspect  Display detailed information on one or more images.</span><br><span class="line">  load     Load an image from a tar archive or STDIN</span><br><span class="line">  ls       List images</span><br><span class="line">  prune    Remove unused images</span><br><span class="line">  pull     Pull an image from a registry. Optionally specify &quot;ipfs://&quot; or &quot;ipns://&quot; scheme to pull image from IPFS.</span><br><span class="line">  push     Push an image or a repository to a registry. Optionally specify &quot;ipfs://&quot; or &quot;ipns://&quot; scheme to push image to IPFS.</span><br><span class="line">  rm       Remove one or more images</span><br><span class="line">  save     Save one or more images to a tar archive (streamed to STDOUT by default)</span><br><span class="line">  tag      Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">  -h, --help   help for image</span><br></pre></td></tr></table></figure><p><strong>pull</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io pull prom/node-exporter:v1.4.0</span><br><span class="line">docker.io/prom/node-exporter:v1.4.0:                                              resolved       |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">index-sha256:4a2c72aa0e18fcedfa86e4a2ca5cf8e33010246e3125449015b586f4fcde7f01:    exists         |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">manifest-sha256:2d9dcdf0b2226f0c3d550a64d2667710265462350a3ba9ebe37d0302bc64af0f: exists         |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">config-sha256:d3e443c987ef405e1be101647873d86b5729c9c47bb1dd1ab59ccb24bc9e322c:   exists         |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">elapsed: 0.5 s                                                                    total:   0.0 B (0.0 B/s)</span><br></pre></td></tr></table></figure><p><strong>push</strong></p><p>当然在推送镜像之前也可以使用 <code>nerdctl login</code> 命令登录到镜像仓库，然后再执行 push 操作。</p><p>可以使用 <code>nerdctl login</code> 进行登录，使用 <code>nerdctl logout</code> 可以注销退出登录。</p><p><strong>tag</strong></p><p>使用 <code>tag</code> 命令可以为一个镜像创建一个别名镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io images</span><br><span class="line">REPOSITORY    TAG                  IMAGE ID        CREATED           SIZE</span><br><span class="line">prom/node-exporter                                                 &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">&lt;none&gt;                                                             &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">prom/node-exporter                                                 v1.4.0                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">➜  ~ nerdctl -n k8s.io tag prom/node-exporter:v1.4.0 prom/node-exporter:v1.4.1</span><br><span class="line">➜  ~ nerdctl -n k8s.io images|grep 4a2c72aa0e18</span><br><span class="line">prom/node-exporter                                                 v1.4.1                4a2c72aa0e18    4 seconds ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">prom/node-exporter                                                 &lt;none&gt;                4a2c72aa0e18    2 days ago       linux/amd64    24.56MB    11.47MB</span><br><span class="line">&lt;none&gt;                                                             &lt;none&gt;                4a2c72aa0e18    2 days ago       linux/amd64    24.56MB    11.47MB</span><br><span class="line">prom/node-exporter                                                 v1.4.0                4a2c72aa0e18    2 days ago       linux/amd64    24.56MB    11.47MB</span><br></pre></td></tr></table></figure><p><strong>save</strong></p><p>使用 <code>save</code> 命令可以导出镜像为一个 <code>tar</code> 压缩包。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl save -o busybox.tar.gz busybox:latest</span><br><span class="line">➜  ~ ls -lh busybox.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root 761K Aug 19 15:19 busybox.tar.gz</span><br></pre></td></tr></table></figure><p><strong>rmi</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl -n k8s.io rmi prom/node-exporter:v1.4.1</span><br><span class="line">Untagged: docker.io/prom/node-exporter:v1.4.1@sha256:4a2c72aa0e18fcedfa86e4a2ca5cf8e33010246e3125449015b586f4fcde7f01</span><br><span class="line">Deleted: sha256:084326605ab6715ca698453e530e4d0319d4e402b468894a06affef944b4ef04</span><br><span class="line">Deleted: sha256:5295faa045209ff9800d03fe1ccc94431dac6a54dac2edc7f6d06ee1e58bb0be</span><br><span class="line">Deleted: sha256:bebdf9d4bf7bd2bd40a7e73e5f09a86a87e34578b622328d5a58bf01353f9def</span><br></pre></td></tr></table></figure><p><strong>load</strong></p><p>使用 <code>load</code> 命令可以将上面导出的镜像再次导入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl load -i busybox.tar.gz</span><br><span class="line">unpacking docker.io/library/busybox:latest (sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60)...done</span><br></pre></td></tr></table></figure><p>使用 <code>-i</code> 或 <code>--input</code> 选项指定需要导入的压缩包。</p><p><strong>build</strong></p><p> <code>nerdctl build</code> 需要依赖 <code>buildkit</code> 工具。</p><p><a href="https://github.com/moby/buildkit">buildkit</a> 是 Docker 公司开源的一个构建工具包，支持 OCI 标准的镜像构建。它主要包含以下部分:</p><ul><li>服务端 <code>buildkitd</code>：当前支持 runc 和 containerd 作为 worker，默认是 runc，我们这里使用 containerd</li><li>客户端 <code>buildctl</code>：负责解析 Dockerfile，并向服务端 buildkitd 发出构建请求</li></ul><p>构建完成后查看镜像是否构建成功：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ➜  ~ nerdctl -n k8s.io images</span><br><span class="line">REPOSITORY    TAG                  IMAGE ID        CREATED           SIZE</span><br><span class="line">prom/node-exporter                                                 &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">&lt;none&gt;                                                             &lt;none&gt;                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br><span class="line">prom/node-exporter                                                 v1.4.0                4a2c72aa0e18    2 days ago    linux/amd64    24.56MB    11.47MB</span><br></pre></td></tr></table></figure><p>这样我们就使用 <code>nerdctl + buildkitd</code> 轻松完成了容器镜像的构建。</p><h4 id="compose"><a href="#compose" class="headerlink" title="compose"></a>compose</h4><p>当然，如果你希望在单机环境下使用 Docker Compose，但想使用 <code>containerd</code>，<code>nerdctl</code> 提供了对 Compose 功能的兼容支持。我们可以使用以下 <code>nerdctl compose</code> 系列命令来管理 Compose 服务：</p><ol><li><p><strong>启动 Compose 服务</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nerdctl compose up</span><br></pre></td></tr></table></figure></li><li><p><strong>查看服务日志</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nerdctl compose logs</span><br></pre></td></tr></table></figure></li><li><p><strong>构建镜像</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nerdctl compose build</span><br></pre></td></tr></table></figure></li><li><p><strong>停止并删除服务</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nerdctl compose down</span><br></pre></td></tr></table></figure></li></ol><p>通过结合 <code>containerd</code>、<code>nerdctl</code> 和 <code>buildkit</code>，您可以在镜像构建和容器管理方面完全替代 Docker，提供高效和轻量的容器管理体验。这种方式特别适合希望利用 <code>containerd</code> 的轻量特性，同时又不想放弃 Docker Compose 的用户。</p><h2 id="CRICTL使用指南"><a href="#CRICTL使用指南" class="headerlink" title="CRICTL使用指南"></a>CRICTL使用指南</h2><p><code>crictl</code> 可以追溯到 Kubernetes 中的容器运行时接口（CRI）。在 Kubernetes 1.5 版本中引入了 CRI，它定义了 Kubernetes 和容器运行时之间的标准化接口，以便 Kubernetes 可以与不同的容器运行时（如 Docker、containerd、CRI-O 等）进行交互。</p><p>由于 Kubernetes 对容器运行时的要求日益严格，需要更多的运行时特性和更高的性能，因此出现了对 CRI 的更高需求。而且，CRI 也为容器运行时的实现提供了标准化的接口，使得开发人员可以更轻松地实现自己的容器运行时。</p><p>在这样的背景下，<code>crictl</code> 作为一个命令行工具被开发出来，用于与符合 CRI 标准的容器运行时进行交互。它使得用户可以方便地管理容器和容器镜像，查询容器状态和元数据，并进行容器日志查看等操作。<code>crictl</code> 的出现使得容器运行时的管理和调试变得更加便捷，同时也促进了 CRI 标准的推广和应用。</p><p>以下是 <code>crictl</code> 常见的用法和功能：</p><ol><li><strong>容器管理</strong>：<code>crictl</code> 可以用来启动、停止、删除和查询容器的状态。</li><li><strong>容器镜像管理</strong>：<code>crictl</code> 可以用来拉取、推送、删除和查询容器镜像。</li><li><strong>容器日志查看</strong>：<code>crictl</code> 可以用来查看容器的标准输出和标准错误日志。</li><li><strong>容器信息查询</strong>：<code>crictl</code> 可以用来查询容器的详细信息，如 ID、名称、状态、IP 等。</li><li><strong>容器元数据操作</strong>：<code>crictl</code> 还可以进行容器元数据的操作，如注解和标签的添加和删除。</li></ol><p><code>crictl</code> 是一个强大的工具，可以帮助您管理容器运行时中的容器和容器镜像，以及了解容器的状态和元数据。它通常用于调试和管理容器化应用程序，并与 Kubernetes 等容器编排系统集成使用。</p><h3 id="1-安装-1"><a href="#1-安装-1" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>首先我们需要先安装 <code>crictl</code> 工具，直接从 <a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools</a> 的 release 页面下载对应的二进制包，解压放入 PATH 路径下即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ VERSION=&quot;v1.22.0&quot;</span><br><span class="line">➜  ~ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz</span><br><span class="line"># 如果有限制，也可以替换成下面的 URL 加速下载</span><br><span class="line"># wget https://download.fastgit.org/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz</span><br><span class="line">➜  ~ tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin</span><br><span class="line">➜  ~ rm -f crictl-$VERSION-linux-amd64.tar.gz</span><br><span class="line">➜  ~ crictl -v</span><br><span class="line">crictl version v1.22.0</span><br></pre></td></tr></table></figure><p>到这里证明 <code>crictl</code> 工具安装成功了。</p><h3 id="2-命令简介-1"><a href="#2-命令简介-1" class="headerlink" title="2. 命令简介"></a>2. 命令简介</h3><p><code>crictl</code> 安装完成后，接下来我们来了解下该工具的一些常见使用方法。</p><p>首先需要修改下默认的配置文件，默认为 <code>/etc/crictl.yaml</code>，在文件中指定容器运行时和镜像的 endpoint 地址，内容如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">runtime-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">image-endpoint: unix:///var/run/containerd/containerd.sock</span><br><span class="line">debug: false</span><br><span class="line">pull-image-on-create: false</span><br><span class="line">disable-pull-on-run: false</span><br></pre></td></tr></table></figure><p>配置完成后就可以使用 <code>crictl</code> 命令了。</p><p><strong>Pod 管理</strong></p><p>通过 <code>crictl pods</code> 命令可以获取当前节点上运行的 Pods 列表，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                       NAMESPACE           ATTEMPT             RUNTIME</span><br><span class="line">1f617ebf0524c       8 weeks ago         Ready               node-exporter-ggjwf                node-exporter       0                   (default)</span><br><span class="line">4b93e3d6e8d1f       2 months ago        Ready               node-problem-detector-r6ps6        kube-system         0                   (default)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>还可以使用 <code>--name</code> 参数获取指定的 Pod，也可以根据标签来筛选 Pod 列表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl pods --label app=node-exporter</span><br><span class="line">POD ID              CREATED             STATE               NAME                  NAMESPACE           ATTEMPT             RUNTIME</span><br><span class="line">1f617ebf0524c       8 weeks ago         Ready               node-exporter-ggjwf   node-exporter       0                   (default)</span><br></pre></td></tr></table></figure><p><strong>镜像管理</strong></p><p>使用 <code>crictl images</code> 命令可以获取所有的镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl images</span><br><span class="line">IMAGE                                     TAG                 IMAGE ID            SIZE</span><br><span class="line">hub.cloud.ctripcorp.com/prom/node-exporter                                            v1.4.0                                                                                    d3e443c987ef4       11.5MB</span><br></pre></td></tr></table></figure><p>同样在命令后面可以加上 <code>-v</code> 参数来显示镜像的详细信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl images -v</span><br><span class="line">ID: sha256:d3e443c987ef405e1be101647873d86b5729c9c47bb1dd1ab59ccb24bc9e322c</span><br><span class="line">RepoTags: docker.io/prom/node-exporter:v1.4.0</span><br><span class="line">RepoDigests: docker.io/prom/node-exporter@sha256:4a2c72aa0e18fcedfa86e4a2ca5cf8e33010246e3125449015b586f4fcde7f01</span><br><span class="line">Size: 11474514</span><br><span class="line">Username: nobody</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><strong>容器管理</strong></p><p>使用 <code>crictl ps</code> 命令可以获取正在运行的容器列表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl ps</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID</span><br><span class="line">e7f156d31942f       d3e443c987ef4       2 days ago          Running             main                    2                   bf1704937991a       node-exporter-5zhjt</span><br></pre></td></tr></table></figure><p>还有更多其他可选参数，使用 <code>-s</code> 选项按照状态进行过滤：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl ps -s Exited</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                    ATTEMPT             POD ID              POD</span><br><span class="line">22a0adacf702f       d3e443c987ef4       2 days ago          Exited              main                    1                   bf1704937991a       node-exporter-5zhjt</span><br></pre></td></tr></table></figure><p><code>crictl</code> 也有类似 <code>exec</code> 的命令支持，比如在容器 ID 为 <code>e7f156d31942f</code> 的容器中执行一个 <code>whoami</code> 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  crictl exec -it e7f156d31942f whoami</span><br><span class="line">root</span><br></pre></td></tr></table></figure><p>还可以获取容器日志信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl logs e7f156d31942f </span><br><span class="line">ts=2024-11-15T08:37:07.980Z caller=node_exporter.go:182 level=info msg=&quot;Starting node_exporter&quot; version=&quot;(version=1.4.0, branch=HEAD, revision=7da1321761b3b8dfc9e496e1a60e6a476fec6018)&quot;</span><br><span class="line">ts=2024-11-15T08:37:07.980Z caller=node_exporter.go:183 level=info msg=&quot;Build context&quot; build_context=&quot;(go=go1.19.1, user=root@83d90983e89c, date=20220926-12:32:56)&quot;</span><br><span class="line">ts=2024-11-15T08:37:07.980Z caller=node_exporter.go:185 level=warn msg=&quot;Node Exporter is running as root user. This exporter is designed to run as unprivileged user, root is not required.&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>和 <code>kubectl logs</code> 类似于，还可以使用 <code>-f</code> 选项来 Follow 日志输出，<code>--tail N</code> 也可以指定输出最近的 <code>N</code> 行日志。</p><p>使用 <code>crictl stats</code> 命令可以列出所有或者单一容器资源的使用情况：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ crictl stats e7f156d31942f </span><br><span class="line"></span><br><span class="line">CONTAINER           NAME                CPU %               MEM                 DISK                INODES</span><br><span class="line">e7f156d31942f       main                0.00                37.13MB             0B                  18</span><br></pre></td></tr></table></figure><p>此外镜像和容器相关的一些操作也都支持，更多信息请参考 <a href="https://github.com/kubernetes-sigs/cri-tools">kubernetes-sigs&#x2F;cri-tools</a>。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ol><li><a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/">https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/</a></li><li><a href="https://zhuanlan.zhihu.com/p/666200234">https://zhuanlan.zhihu.com/p/666200234</a></li><li><a href="https://www.qikqiak.com/k8strain2/containerd/runtime/">https://www.qikqiak.com/k8strain2/containerd/runtime/</a></li><li><a href="https://github.com/kubernetes/cri-api/tree/master/pkg/apis">https://github.com/kubernetes/cri-api/tree/master/pkg/apis</a></li><li><a href="http://www.opennaru.com/kubernetes/containerd/">http://www.opennaru.com/kubernetes/containerd/</a></li><li><a href="https://www.slideshare.net/AkihiroSuda/container-plumbing-days-2023-why-was-nerdctl-made">https://www.slideshare.net/AkihiroSuda/container-plumbing-days-2023-why-was-nerdctl-made</a></li><li><a href="https://www.alibabacloud.com/blog/a-discussion-on-container-runtime---starting-with-dockershim-being-deleted-by-kubernetes_600118">https://www.alibabacloud.com/blog/a-discussion-on-container-runtime---starting-with-dockershim-being-deleted-by-kubernetes_600118</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h2&gt;&lt;p&gt;Docker作为最早广泛应用的容器运行时，其普及程度使得用户对其操作方式和功能特性极为熟悉。在Kubernetes的初期，K</summary>
      
    
    
    
    
    <category term="Containerd" scheme="https://zoues.com/tags/Containerd/"/>
    
    <category term="runtime" scheme="https://zoues.com/tags/runtime/"/>
    
  </entry>
  
  <entry>
    <title>超越C++：Ziglang 元编程一文打尽</title>
    <link href="https://zoues.com/posts/ae025efe/"/>
    <id>https://zoues.com/posts/ae025efe/</id>
    <published>2024-11-09T10:00:43.000Z</published>
    <updated>2024-11-09T10:07:04.823Z</updated>
    
    <content type="html"><![CDATA[<p>如果你以前只有在宏、泛型或代码生成场景中体会过编译时执行，那么可以在Zig语言中好好体会一番。</p><p>Zig是由Andrew Kelley开发的一种新的通用编程语言。尽管仍在积极开发中（当前版本0.13），但我认为这种语言已经展现出了巨大的潜力。Zig的目标是成为更好的C，类似于Rust可以被理解为更好的C++。它没有垃圾回收，没有内置事件循环，也没有其他运行时机制。它像C一样精简，并且实际上可以与C轻松互操作。有关完整概述，请访问官网。</p><p>如果你对Zig操作的抽象级别有一般的了解，那么也就不会对在运行时没有反射感到惊讶；在编译时无法做的事情，可以在编译时完成。</p><h2 id="Phase-distinction"><a href="#Phase-distinction" class="headerlink" title="Phase distinction"></a>Phase distinction</h2><p>阶段区分是指在编程语言中，类型和术语之间有严格分隔的一种特性。Luca Cardelli 提出了一个简明的规则，用于判断语言是否保持了阶段区分：如果 A 是一个编译时术语，并且 B 是 A 的一个子术语，那么 B 也必须是一个编译时术语。</p><p>大多数静态类型语言都遵循阶段区分原则。然而，一些拥有特别灵活且表达能力强的类型系统的语言（尤其是依赖类型的编程语言）允许像操作普通术语一样操作类型。类型可以被传递给函数或作为结果返回。</p><p>具有阶段区分的语言可能会为类型和运行时变量设置单独的命名空间。在优化编译器中，阶段区分标记了哪些表达式可以安全删除的边界。</p><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>阶段区分通常与静态检查结合使用。通过基于演算的系统，阶段区分消除了在不同类型和术语之间实施线性逻辑的必要性。</p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>阶段区分将编译时的处理与运行时的处理区分开来。</p><p>一种简单的语言，其术语包括：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t ::= true | false | x | λx : T . t | t t | if t then t else t</span><br></pre></td></tr></table></figure><p>以及类型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T ::= Bool | T -&gt; T </span><br></pre></td></tr></table></figure><p>注意类型和术语是如何分开的。在编译时，类型用于验证术语的正确性。然而，在运行时，类型并不起任何作用。</p><h2 id="编译时与运行时"><a href="#编译时与运行时" class="headerlink" title="编译时与运行时"></a>编译时与运行时</h2><p>编译时与运行时源自“Phase distinction”理论。这个概念是较难理解，尤其是对于编程语言背景不太深的人来说。为了解决这个问题，我觉得有帮助的办法是回答下面几个问题：</p><ol><li>程序满足了哪些不变性？</li><li>在这个阶段可能出什么问题？</li><li>如果这个阶段成功了，我们知道什么后置条件？</li><li>如果有输入和输出，它们是什么？</li></ol><h3 id="编译时"><a href="#编译时" class="headerlink" title="编译时"></a>编译时</h3><p>程序不需要满足任何不变性。实际上，它甚至不需要是一个格式正确的程序。你可以把一段 HTML 代码交给编译器，编译器会直接报错……</p><p>编译时可能出的问题：</p><ul><li>语法错误</li><li>类型检查错误</li><li>（极少情况）编译器崩溃</li></ul><p>如果编译成功，我们知道什么？</p><ul><li>程序格式正确——在所使用的语言中是一个有意义的程序。</li><li>程序可以开始运行。（程序可能会立刻失败，但至少我们可以尝试运行。）</li></ul><p>输入和输出是什么？</p><ul><li>输入是正在编译的程序，加上任何头文件、接口、库，或其他为了编译而需要导入的内容。</li><li>输出通常是汇编代码、可重定位的目标代码，或者甚至是一个可执行程序。或者如果出错了，输出是一堆错误信息。</li></ul><h3 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h3><p>我们对程序的不变性一无所知——它们完全由程序员设定。运行时的不变性很少只靠编译器来保证；这通常需要程序员的帮助。</p><p>运行时可能出的问题：</p><ul><li>运行时错误：<ul><li>除零错误</li><li>解引用空指针</li><li>内存不足</li></ul></li><li>还可能有程序自身检测到的错误：<ul><li>尝试打开一个不存在的文件</li><li>试图查找网页却发现提供的 URL 格式不正确</li></ul></li></ul><p>如果运行时成功，程序就会顺利完成（或继续运行）而不会崩溃。</p><p>输入和输出完全由程序员决定。例如，文件、屏幕上的窗口、网络数据包、发送到打印机的作业等等。假如程序发射导弹，那也是一个输出，并且只能在运行时发生 :-)</p><p><strong>在编译时运行代码</strong></p><p>让我们从基础知识开始：使用<code>comptime</code>关键字可以在编译时运行任意代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fn multiply(a: i64, b: i64) i64 &#123;</span><br><span class="line">    return a * b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const len = comptime multiply(4, 5);</span><br><span class="line">    const my_static_array: [len]u8 = undefined;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是，函数定义没有任何说明在编译时可用的属性。这只是一个普通的函数，我们在调用点请求其在编译时执行。</p><p><strong>在编译时定义块</strong></p><p>你还可以使用<code>comptime</code>在函数内定义编译时块。以下示例是一个处理不区分大小写的字符串比较函数，针对其中一个字符串是硬编码的情况进行了优化。编译时执行确保函数不被滥用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">fn insensitive_eql(comptime uppr: []const u8, str: []const u8) bool &#123;</span><br><span class="line">    comptime &#123;</span><br><span class="line">        var i = 0;</span><br><span class="line">        while (i &lt; uppr.len) : (i += 1) &#123;</span><br><span class="line">            if (uppr[i] &gt;= &#x27;a&#x27; and uppr[i] &lt;= &#x27;z&#x27;) &#123;</span><br><span class="line">                @compileError(&quot;`uppr` must be all uppercase&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    var i = 0;</span><br><span class="line">    while (i &lt; uppr.len) : (i += 1) &#123;</span><br><span class="line">        const val = if (str[i] &gt;= &#x27;a&#x27; and str[i] &lt;= &#x27;z&#x27;)</span><br><span class="line">            str[i] - 32</span><br><span class="line">        else</span><br><span class="line">            str[i];</span><br><span class="line">        if (val != uppr[i]) return false;</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const x = insensitive_eql(&quot;Hello&quot;, &quot;hElLo&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该程序的编译失败并产生以下输出。</p><p><strong>编译时代码消除</strong></p><p>Zig可以静态解析依赖于编译时已知值的控制流表达式。例如，你可以强制在while&#x2F;for循环上进行循环展开，并从if&#x2F;switch语句中省略分支。下面的程序要求用户输入一个数字，然后迭代地对其应用一系列操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">const builtin = @import(&quot;builtin&quot;);</span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const fmt = std.fmt;</span><br><span class="line">const io = std.io;</span><br><span class="line"></span><br><span class="line">const Op = enum &#123;</span><br><span class="line">    Sum,</span><br><span class="line">    Mul,</span><br><span class="line">    Sub,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">fn ask_user() !i64 &#123;</span><br><span class="line">    var buf: [10]u8 = undefined;</span><br><span class="line">    std.debug.warn(&quot;A number please: &quot;);</span><br><span class="line">    const user_input = try io.readLineSlice(buf[0..]);</span><br><span class="line">    return fmt.parseInt(i64, user_input, 10);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn apply_ops(comptime operations: []const Op, num: i64) i64 &#123;</span><br><span class="line">    var acc: i64 = 0;</span><br><span class="line">    inline for (operations) |op| &#123;</span><br><span class="line">        switch (op) &#123;</span><br><span class="line">            .Sum =&gt; acc +%= num,</span><br><span class="line">            .Mul =&gt; acc *%= num,</span><br><span class="line">            .Sub =&gt; acc -%= num,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return acc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    const user_num = try ask_user();</span><br><span class="line">    const ops = [4]Op&#123;.Sum, .Mul, .Sub, .Sub&#125;;</span><br><span class="line">    const x = apply_ops(ops[0..], user_num);</span><br><span class="line">    std.debug.warn(&quot;Result: &#123;&#125;\n&quot;, x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该代码的有趣部分是for循环。<code>inline</code>关键字强制进行循环展开，循环体内有一个在编译时解析的switch语句。简而言之，在前面示例中对<code>apply_ops</code>的调用基本上解析为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">var acc: i64 = 0;</span><br><span class="line">acc +%= num;</span><br><span class="line">acc *%= num;</span><br><span class="line">acc -%= num;</span><br><span class="line">acc -%= num;</span><br><span class="line">return acc;</span><br></pre></td></tr></table></figure><p>为了测试这是否确实发生了，将程序代码粘贴到<a href="https://godbolt.org/">https://godbolt.org</a>，选择Zig作为目标语言，然后选择大于0.4.0的Zig版本。Godbolt将编译代码并显示生成的汇编代码。右键单击代码行，会弹出一个上下文菜单，让你跳转到相应的汇编代码。你会注意到for循环和switch都没有对应的汇编代码。删除<code>inline</code>关键字，它们现在将会显示出来。</p><p><strong>泛型</strong></p><p><code>comptime</code>关键字指示在编译时解析的代码区域和值。在前面的示例中，我们使用它执行类似于模板元编程的操作，但它也可用于泛型编程，因为类型是有效的编译时值。</p><p><strong>泛型函数</strong></p><p>由于泛型编程与<code>comptime</code>参数相关，Zig没有传统的菱形括号语法。除此之外，泛型的基本用法与其他语言非常相似。以下代码是从标准库中提取的Zig的<code>mem.eql</code>实现，用于测试两个切片是否相等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/// Compares two slices and returns whether they are equal.</span><br><span class="line">pub fn eql(comptime T: type, a: []const T, b: []const T) bool &#123;</span><br><span class="line">    if (a.len != b.len) return false;</span><br><span class="line">    for (a) |item, index| &#123;</span><br><span class="line">        if (b[index] != item) return false;</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如你所见，<code>T</code>是<code>type</code>类型的变量，后续的参数将其用作泛型参数。这样，就可以使用<code>mem.eql</code>与任何类型的切片。</p><p>还可以对<code>type</code>类型的值执行内省。在之前的示例中，我们从用户输入解析了一个整数，并请求了一个特定类型的整数。解析函数使用该信息从其泛型实现中省略了一些代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">return fmt.parseInt(i64, user_input, 10);</span><br><span class="line"></span><br><span class="line">// 这是`parseInt`的stdlib实现</span><br><span class="line">pub fn parseInt(comptime T: type, buf: []const u8, radix: u8) !T &#123;</span><br><span class="line">    if (!T.is_signed) return parseUnsigned(T, buf, radix);</span><br><span class="line">    if (buf.len == 0) return T(0);</span><br><span class="line">    if (buf[0] == &#x27;-&#x27;) &#123;</span><br><span class="line">        return math.negate(try parseUnsigned(T, buf[1..], radix));</span><br><span class="line">    &#125; else if (buf[0] == &#x27;+&#x27;) &#123;</span><br><span class="line">        return parseUnsigned(T, buf[1..], radix);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return parseUnsigned(T, buf, radix);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>泛型结构体</strong></p><p>在描述如何创建泛型结构体之前，先简要介绍一下Zig中结构体的工作原理。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const math = std.math;</span><br><span class="line">const assert = std.debug.assert;</span><br><span class="line"></span><br><span class="line">// 结构体定义不包括名称。</span><br><span class="line">// 将结构体分配给变量会为其赋予名称。</span><br><span class="line">const Point = struct &#123;</span><br><span class="line">    x: f64,</span><br><span class="line">    y: f64,</span><br><span class="line">    z: f64,</span><br><span class="line">    </span><br><span class="line">    // 结构体定义还可以包含命名空间函数。</span><br><span class="line">    // 当通过结构体实例调用带有Self参数的结构体函数时，</span><br><span class="line">    // 将自动填充第一个参数，就像方法一样。</span><br><span class="line">    const Self = @This();</span><br><span class="line">    pub fn distance(self: Self, p: Point) f64 &#123;</span><br><span class="line">        const x2 = math.pow(f64, self.x - p.x, 2);</span><br><span class="line">        const y2 = math.pow(f64, self.y - p.y, 2);</span><br><span class="line">        const z2 = math.pow(f64, self.z - p.z, 2);</span><br><span class="line">        return math.sqrt(x2 + y2 + z2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    const p1 = Point&#123; .x = 0, .y = 2, .z = 8 &#125;;</span><br><span class="line">    const p2 = Point&#123; .x = 0, .y = 6, .z = 8 &#125;;</span><br><span class="line">    </span><br><span class="line">    assert(p1.distance(p2) == 4);</span><br><span class="line">    assert(Point.distance(p1, p2) == 4);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在我们可以深入讨论泛型结构体了。要创建泛型结构体，只需创建一个接受类型参数的函数，并在结构体定义中使用该参数。以下是从Zig文档中提取的示例。它是一个双向链接的列表。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fn LinkedList(comptime T: type) type &#123;</span><br><span class="line">    return struct &#123;</span><br><span class="line">        pub const Node = struct &#123;</span><br><span class="line">            prev: ?*Node = null,</span><br><span class="line">            next: ?*Node = null,</span><br><span class="line">            data: T,</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        first: ?*Node = null,</span><br><span class="line">        last: ?*Node = null,</span><br><span class="line">        len: usize = 0,</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数返回一个类型，这意味着它只能在编译时调用。它定义了两个结构体：</p><ul><li>主LinkedList结构体</li><li>命名空间内的Node结构体，嵌套在主结构体中</li></ul><p>就像结构体可以对函数进行命名空间分组一样，它们也可以对变量进行命名空间分组。在创建复合类型时，这对内省非常有用。以下是LinkedList如何与先前的Point结构体组合的示例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const PointList = LinkedList(Point);</span><br><span class="line">const p = Point&#123; .x = 0, .y = 2, .z = 8 &#125;;</span><br><span class="line"></span><br><span class="line">var my_list = PointList&#123;&#125;;</span><br><span class="line"></span><br><span class="line">// 完整实现需要提供一个`append`方法。</span><br><span class="line">// 现在我们手动添加新节点。</span><br><span class="line">var node = PointList.Node&#123; .data = p &#125;;</span><br><span class="line">my_list.first = &amp;node;</span><br><span class="line">my_list.last = &amp;node;</span><br><span class="line">my_list.len = 1;</span><br></pre></td></tr></table></figure><p>Zig标准库中包含了一些完成度非常高的链表实现。</p><p><strong>编译时反射</strong></p><p>现在我们已经涵盖了所有基础知识，我们终于可以进入 Zig 元编程真正强大且有趣的内容。</p><p>在之前的例子中，我们已经看到了在 parseInt 中检查 T.is_signed 时的反射示例，但在这一节中，我想专注于更高级的反射用法。我将通过一个代码示例来介绍这个概念。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fn make_couple_of(x: anytype) [2]@typeOf(x) &#123;</span><br><span class="line">    return [2]@typeOf(x) &#123;x, x&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个几乎没什么用的函数可以接受任何值作为输入，并创建一个包含两个副本的数组。以下调用都是正确的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">make_couple_of(5); // 创建 [2]comptime_int&#123;5, 5&#125;</span><br><span class="line">make_couple_of(i32(5)); // 创建 [2]i32&#123;5, 5&#125;</span><br><span class="line">make_couple_of(u8); // 创建 [2]type&#123;u8, u8&#125;</span><br><span class="line">make_couple_of(type); // 创建 [2]type&#123;type, type&#125;</span><br><span class="line">make_couple_of(make_couple_of(&quot;hi&quot;)); </span><br><span class="line">// 创建 [2][2][2]u8&#123;[2][2]u8&#123;&quot;hi&quot;,&quot;hi&quot;&#125;, [2][2]u8&#123;&quot;hi&quot;,&quot;hi&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><p>anytype 类型的参数非常强大，允许构建经过优化但仍然“动态”的函数。对于下一个例子，我将从标准库中提取一些代码，展示这种功能的更有用的用法。</p><p>以下代码是 math.sqrt 的实现，我们在先前的例子中用它来计算两点之间的欧几里德距离。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">// 为了更好的可读性，我将原始定义的一部分移动到单独的函数中。</span><br><span class="line">fn decide_return_type(comptime T: type) type &#123;</span><br><span class="line">    if (@typeId(T) == TypeId.Int) &#123;</span><br><span class="line">        return @IntType(false, T.bit_count / 2);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return T;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn sqrt(x: anytype) decide_return_type(@typeOf(x)) &#123;</span><br><span class="line">    const T = @typeOf(x);</span><br><span class="line">    switch (@typeId(T)) &#123;</span><br><span class="line">        TypeId.ComptimeFloat =&gt; return T(@sqrt(f64, x)),</span><br><span class="line">        TypeId.Float =&gt; return @sqrt(T, x),</span><br><span class="line">        TypeId.ComptimeInt =&gt; comptime &#123;</span><br><span class="line">            if (x &gt; maxInt(u128)) &#123;</span><br><span class="line">                @compileError(</span><br><span class="line">                    &quot;sqrt not implemented for &quot; ++ </span><br><span class="line">                    &quot;comptime_int greater than 128 bits&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            if (x &lt; 0) &#123;</span><br><span class="line">                @compileError(&quot;sqrt on negative number&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            return T(sqrt_int(u128, x));</span><br><span class="line">        &#125;,</span><br><span class="line">        TypeId.Int =&gt; return sqrt_int(T, x),</span><br><span class="line">        else =&gt; @compileError(&quot;not implemented for &quot; ++ @typeName(T)),</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数的返回类型有点奇怪。如果看一下 sqrt 的签名，它在应声明返回类型的地方调用了一个函数。在 Zig 中，这是允许的。原始代码实际上内联了一个 if 表达式，但出于更好的可读性，我将其移到了一个单独的函数中。</p><p>那么 sqrt 对其返回类型想要做什么呢？当我们传入整数值时，它应用了一个小优化。在这种情况下，函数将其返回类型声明为原始输入的比特大小的一半的无符号整数。这意味着，如果我们传入一个 i64 值，该函数将返回一个 u32 值。这主要考虑到平方根函数的作用。然后，声明的其余部分使用反射进一步类型化，并在适当的情况下报告编译时错误。</p><p>总的来说，编译时执行非常出色，特别是当语言非常具有表达力时。没有良好的编译时元编程，人们必须借助宏或代码生成，或者更糟糕地在运行时执行许多无用的工作。</p><p>如果你希望想看到Zig更酷的例子，请看一下 Andrew 本人的这篇博文。他使用了一些上述技术来为编译时已知的字符串列表生成完美的哈希函数。其结果是用户可以创建一个在 O(1) 时间内匹配字符串的开关。代码非常易于理解，他还提供了关于如何轻松、有趣和安全地使用所有其他次要功能的一些独特见解。</p><h2 id="Zig-元编程"><a href="#Zig-元编程" class="headerlink" title="Zig 元编程"></a>Zig 元编程</h2><p>Zig 的元编程由几个基本概念驱动：</p><ul><li>类型在编译时是有效值。</li><li>大多数运行时代码也可以在编译时工作。</li><li>结构体字段在编译时是鸭子类型（duck-typed）。</li><li>Zig 标准库提供了一些工具来进行编译时反射。</li><li>示例：多分派（multiple dispatch）。</li></ul><p><em><strong>Zig 示例代码</strong></em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(x: anytype) @TypeOf(x) &#123;</span><br><span class="line">    // 注意，这个 if 语句是在编译时执行的，而不是在运行时。</span><br><span class="line">    if (@TypeOf(x) == i64) &#123;</span><br><span class="line">        return x + 2;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return 2 * x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i64 = 47;</span><br><span class="line">    var y: i32 = 47;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;i64-foo: &#123;&#125;\n&quot;, .&#123;foo(x)&#125;);</span><br><span class="line">    std.debug.print(&quot;i32-foo: &#123;&#125;\n&quot;, .&#123;foo(y)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="泛型类型"><a href="#泛型类型" class="headerlink" title="泛型类型"></a>泛型类型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fn Vec2Of(comptime T: type) type &#123;</span><br><span class="line">    return struct &#123;</span><br><span class="line">        x: T,</span><br><span class="line">        y: T</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">const V2i64 = Vec2Of(i64);</span><br><span class="line">const V2f64 = Vec2Of(f64);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var vi = V2i64&#123;.x = 47, .y = 47&#125;;</span><br><span class="line">    var vf = V2f64&#123;.x = 47.0, .y = 47.0&#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;i64 vector: &#123;&#125;\n&quot;, .&#123;vi&#125;);</span><br><span class="line">    std.debug.print(&quot;f64 vector: &#123;&#125;\n&quot;, .&#123;vf&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="编译时执行"><a href="#编译时执行" class="headerlink" title="编译时执行"></a>编译时执行</h3><p>使用 <code>comptime</code> 关键字，可以强制在编译时执行代码块。在这个例子中，变量 <code>x</code> 和 <code>y</code> 是等价的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test &quot;comptime blocks&quot; &#123;</span><br><span class="line">    var x = comptime fibonacci(10);</span><br><span class="line"></span><br><span class="line">    var y = comptime blk: &#123;</span><br><span class="line">        break :blk fibonacci(10);</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>comptime_int和 comptime_float</strong></em></p><ul><li><code>comptime_int</code> 是一种特殊类型，在编译时没有大小限制，并具有任意精度。它可以转换为能够容纳其值的任何整数类型，也可以转换为浮点数。</li><li><code>comptime_float</code> 是 <code>f128</code> 类型，不能转换为整数，即使其值是一个整数。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test &quot;comptime_int&quot; &#123;</span><br><span class="line">    const a = 12;</span><br><span class="line">    const b = a + 10;</span><br><span class="line"></span><br><span class="line">    const c: u4 = a;</span><br><span class="line">    const d: f32 = b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="编译时函数参数"><a href="#编译时函数参数" class="headerlink" title="编译时函数参数"></a>编译时函数参数</h3><p>Zig 的函数参数可以标记为 <code>comptime</code>，这意味着传入的值必须在编译时已知。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fn Matrix(</span><br><span class="line">    comptime T: type,</span><br><span class="line">    comptime width: comptime_int,</span><br><span class="line">    comptime height: comptime_int,</span><br><span class="line">) type &#123;</span><br><span class="line">    return [height][width]T;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test &quot;returning a type&quot; &#123;</span><br><span class="line">    expect(Matrix(f32, 4, 4) == [4][4]f32);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>常见问题</strong></em></p><ul><li>在 <code>comptime</code> 执行中没有“同级”类型解析。</li><li>所有 <code>comptime</code> 值都不遵循常规的生命周期规则，具有“静态”生命周期（可以认为这些值是垃圾回收的）。</li><li>允许结构体字段使用 <code>anytype</code>，这将使结构体成为编译时类型。</li><li>可以使用 <code>comptime var</code> 来创建编译时闭包。</li></ul><hr><h3 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h3><p>在 Zig 中，类型是 <code>type</code> 类型的值，仅在编译时可用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test &quot;branching on types&quot; &#123;</span><br><span class="line">    const a = 5;</span><br><span class="line">    const b: if (a &lt; 10) f32 else i32 = 5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以使用内建的 <code>@typeInfo</code> 来反射类型，这会返回一个标记联合（tagged union）。</p><hr><h3 id="泛型类型与反射"><a href="#泛型类型与反射" class="headerlink" title="泛型类型与反射"></a>泛型类型与反射</h3><p><em><strong>创建泛型类型</strong></em></p><p>使用 <code>@This</code> 获取最内层的结构体、联合或枚举的类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">fn Vec(</span><br><span class="line">    comptime count: comptime_int,</span><br><span class="line">    comptime T: type,</span><br><span class="line">) type &#123;</span><br><span class="line">    return struct &#123;</span><br><span class="line">        data: [count]T,</span><br><span class="line">        const Self = @This();</span><br><span class="line"></span><br><span class="line">        fn abs(self: Self) Self &#123;</span><br><span class="line">            var tmp = Self&#123; .data = undefined &#125;;</span><br><span class="line">            for (self.data) |elem, i| &#123;</span><br><span class="line">                tmp.data[i] = if (elem &lt; 0) -elem else elem;</span><br><span class="line">            &#125;</span><br><span class="line">            return tmp;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        fn init(data: [count]T) Self &#123;</span><br><span class="line">            return Self&#123; .data = data &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">const eql = @import(&quot;std&quot;).mem.eql;</span><br><span class="line"></span><br><span class="line">test &quot;generic vector&quot; &#123;</span><br><span class="line">    const x = Vec(3, f32).init([_]f32&#123; 10, -10, 5 &#125;);</span><br><span class="line">    const y = x.abs();</span><br><span class="line">    expect(eql(f32, &amp;y.data, &amp;[_]f32&#123; 10, 10, 5 &#125;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>动态绑定</strong></em></p><p>Zig 使用 <code>anytype</code> 绑定任意类型，实现基于调用类型的绑定。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fn makeCoupleOf(x: anytype) [2]@TypeOf(x) &#123;</span><br><span class="line">    return [2]@TypeOf(x)&#123; x, x &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>通过这些功能，Zig 的元编程提供了灵活而强大的编译时能力，允许在编译阶段实现类型检查、类型推断和代码生成等高级功能。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://kristoff.it/blog/what-is-zig-comptime/">https://kristoff.it/blog/what-is-zig-comptime/</a></li><li><a href="https://en.wikipedia.org/wiki/Phase_distinction">https://en.wikipedia.org/wiki/Phase_distinction</a></li><li><a href="https://stackoverflow.com/questions/846103/runtime-vs-compile-time">https://stackoverflow.com/questions/846103/runtime-vs-compile-time</a></li><li><a href="https://en.wikipedia.org/wiki/Phase_distinction">https://en.wikipedia.org/wiki/Phase_distinction</a></li><li><a href="https://www.cnblogs.com/cdaniu/p/15456650.html">https://www.cnblogs.com/cdaniu/p/15456650.html</a></li><li><a href="https://ikrima.dev/dev-notes/zig/zig-metaprogramming/">https://ikrima.dev/dev-notes/zig/zig-metaprogramming/</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如果你以前只有在宏、泛型或代码生成场景中体会过编译时执行，那么可以在Zig语言中好好体会一番。&lt;/p&gt;
&lt;p&gt;Zig是由Andrew Kelley开发的一种新的通用编程语言。尽管仍在积极开发中（当前版本0.13），但我认为这种语言已经展现出了巨大的潜力。Zig的目标是成为更</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Go程序员为什么喜欢Ziglang</title>
    <link href="https://zoues.com/posts/ffe3c38f/"/>
    <id>https://zoues.com/posts/ffe3c38f/</id>
    <published>2024-11-06T10:00:43.000Z</published>
    <updated>2024-11-06T02:45:51.727Z</updated>
    
    <content type="html"><![CDATA[<p>在编程语言领域，Zig相对来说是新成员。Zig由Andrew Kelly创立，于2016年正式面世，Zig社区将其定位为“用于开发健壮、优化和可复用软件的通用编程语言”。</p><p>在这简单的描述中，蕴含着一些宏大的目标。例如，Zig被视为一个能够与C语言竞争的编程语言（Rust在Linux内核方向进展更快一些）。此外，Zig提供一整套编译工具链，可以替代现有C编译器（Rust可以使用cargo-zigbuild跨平台编译）。</p><p>作为一名Go开发者，我对Zig及其工具链的提议尤为感兴趣。在研究Zig时，发现这两种语言（Zig和Go）在某些方面有着共同的特性。在这篇博文中，将重点介绍作为Go程序员，对Zig感兴趣的一些特性。</p><h2 id="Zig-与-Go"><a href="#Zig-与-Go" class="headerlink" title="Zig 与 Go"></a>Zig 与 Go</h2><h3 id="简洁性"><a href="#简洁性" class="headerlink" title="简洁性"></a>简洁性</h3><p>两种语言都秉承简洁的设计哲学，以减少语言的干扰，使开发者更快上手并能够高效地完成开发任务。Zig中没有宏、预处理器或操作符重载等功能，避免了执行流程中的“魔法”。</p><p>Go通过运行时处理内存分配和释放。而Zig则坚持其“无隐藏控制流”的准则，没有自动内存管理，而是通过标准库提供内存管理API，让开发者管理内存。</p><h3 id="强类型"><a href="#强类型" class="headerlink" title="强类型"></a>强类型</h3><p>作为一门系统编程语言，Zig围绕类型系统设计了许多功能，注重安全性和C ABI（应用二进制接口）兼容性。这里简要介绍一些有趣的特性：</p><ul><li>有符号&#x2F;无符号整数（预设大小从8位到128位）</li><li>任意大小的有符号&#x2F;无符号整数（例如i7表示7位整数）</li><li>浮点数（精度从16位到128位）</li><li>切片和数组（例如 <code>[]u8&#123;‘h’, ‘i’, ‘!’&#125;</code> 或 <code>[4]i32&#123;1, 2, 3, 4&#125;</code>）</li><li>以UTF-8编码的字符串字面量，存储为以空字符结尾的字节数组</li><li>具有丰富功能的结构体类型，可与C ABI兼容</li><li>具有隐式&#x2F;显式序数值的枚举，并支持方法</li><li>可用于存储多种类型值的联合</li><li>支持使用向量并行操作</li><li>传统指针及多元素指针与切片表达式</li></ul><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>Zig中的错误处理机制非常有特色，融合了try-catch异常语义和Go的错误值模式。</p><p>首先，所有Zig错误都是必须分配和处理的值（否则会导致编译错误）。错误声明使用<code>error</code>关键字，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const DigitError = error&#123; TooLarge, &#125;;</span><br></pre></td></tr></table></figure><p>有趣的是，Zig错误值可以与普通类型的值结合，使用<code>!</code>运算符形成一个联合类型。下面的函数可以返回错误或<code>u32</code>类型的值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fn addSingleDigits(a: u32, b: u32) !u32 &#123;</span><br><span class="line">    if (a &gt; 9) return error.TooLarge;</span><br><span class="line">    if (b &gt; 9) return error.TooLarge;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此外，Zig提供了类似Java等语言的<code>catch</code>关键字，用于错误处理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pub fn main() void &#123;</span><br><span class="line">    const result = addSingleDigits(4, 5) catch |err| &#123;</span><br><span class="line">        std.debug.print(&quot;Error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">        return;</span><br><span class="line">    &#125;;</span><br><span class="line">    std.debug.print(&quot;Result: &#123;&#125;\n&quot;, .&#123;result&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig还支持通过<code>try</code>关键字将错误向上级调用传播。此外，还可以通过<code>if-else-switch</code>语句更精确地筛选和处理错误。</p><h3 id="Zig测试"><a href="#Zig测试" class="headerlink" title="Zig测试"></a>Zig测试</h3><p>在Zig中，测试是语言的关键性能力，使用<code>test</code>关键字声明：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test &quot;test that 1 + 1 equals 2&quot; &#123;</span><br><span class="line">    const result = 1 + 1;</span><br><span class="line">    assert(result == 2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>zig test</code>命令运行源代码中的测试，在标准库中，测试大多跟源代码处于同一文件。</p><h3 id="Zig运行"><a href="#Zig运行" class="headerlink" title="Zig运行"></a>Zig运行</h3><p>类似于<code>go run</code>，Zig提供了<code>zig run</code>命令，将编译和运行源代码的步骤结合：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig run my_program.zig</span><br></pre></td></tr></table></figure><h3 id="Defer"><a href="#Defer" class="headerlink" title="Defer"></a>Defer</h3><p>Zig与Go一样，通过<code>defer</code>概念来管理退出堆栈，当作用域块结束时执行清理操作等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">const print = @import(&quot;std&quot;).debug.print;</span><br><span class="line"></span><br><span class="line">fn addSingleDigits(a: u32, b: u32) !u32 &#123;</span><br><span class="line">    defer print(&quot;this is deferred!&quot;);</span><br><span class="line">    </span><br><span class="line">    if (a &gt; 9) return error.TooLarge;</span><br><span class="line">    if (b &gt; 9) return error.TooLarge;</span><br><span class="line"></span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Comptime"><a href="#Comptime" class="headerlink" title="Comptime"></a>Comptime</h3><p><code>comptime</code>是Zig的一项有趣特性。Zig没有单独的宏系统，而是通过<code>comptime</code>将其代码编写的灵活性扩展到编译阶段。</p><p><code>comptime</code>允许在编译时进行如下操作：</p><ul><li>在编译时解析变量和表达式</li><li>根据编译时值行为的函数</li><li>编译期间有选择性地执行<code>comptime</code>代码块</li><li>编译时执行的元编程</li></ul><h3 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h3><p>在 Zig 中，<code>comptime</code> 提供了对类型值的访问，可以像普通数据值一样存储和传递这些类型值。</p><p>这使得可以创建带有类型参数的函数，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fn max(comptime T: type, a: T, b: T) T &#123;</span><br><span class="line">    return if (a &gt; b) a else b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test &quot;max with different types&quot; &#123;</span><br><span class="line">    const condition = false;</span><br><span class="line">    const result = max(if (condition) f32 else u64, 1234, 5678);</span><br><span class="line">    _ = result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于 <code>comptime</code> 类型值被视为普通类型，Zig 允许使用它们来构建泛型数据结构。例如，<code>MakeList</code> 使用 <code>comptime</code> 类型信息来返回一个编译时结构体：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn MakeList(comptime T: type, comptime size: usize) type &#123;</span><br><span class="line">    return struct &#123;</span><br><span class="line">        items: [size]T,</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var list = MakeList(i32, 3)&#123;</span><br><span class="line">        .items = [3]i32&#123;1, 2, 3&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    std.debug.print(&quot;List &#123;&#125;\n&quot;, .&#123;list&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个示例中，<code>MakeList</code> 函数使用 <code>comptime</code> 类型和大小参数，返回一个包含固定数量元素的结构体。</p><h3 id="Zig-编译"><a href="#Zig-编译" class="headerlink" title="Zig 编译"></a>Zig 编译</h3><ol><li>Zig作为 C (交叉) 编译器</li></ol><p>Zig 工具链包含完整的 C 编译器，因此可以使用 Zig 来替换当前的 C 编译器工具链。以下是 <code>hello.c</code> 的源代码文件：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用以下命令，Zig 可以将该源代码编译成可执行的二进制文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe hello.c --library c</span><br></pre></td></tr></table></figure><ol start="2"><li>Zig 和 C 交叉编译</li></ol><p>Zig 让交叉编译（无论是 C 代码还是 Zig 代码亦或Rust）变得简单。无需繁琐的“自行准备交叉编译工具链”。Zig 提供所有必要的工具和库，确保您可以面向其支持的任何架构进行编译。</p><p>例如，Zig 可以将上述 C 源代码交叉编译成一个面向 Linux 的静态二进制文件（使用 <code>musl</code> 库）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe hello.c --library c -target x86_64-linux-musl</span><br></pre></td></tr></table></figure><ol start="3"><li>Zig 和 CGo 交叉编译</li></ol><p>Zig 对 C 的交叉编译对在交叉编译启用了 CGo 的 Go 源代码时特别有用。例如， <code>add.c</code> 中的C 函数 <code>add</code>：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdint.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int32_t</span> <span class="title function_">add</span><span class="params">(<span class="type">int32_t</span> a, <span class="type">int32_t</span> b)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以在Go中 调用它：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">#include &quot;add.c&quot;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;C&quot;</span></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    a, b := <span class="type">int32</span>(<span class="number">3</span>), <span class="type">int32</span>(<span class="number">4</span>)</span><br><span class="line">    result := C.add(a, b)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;%d + %d = %d\n&quot;</span>, a, b, result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假设我们在 MacOS 上构建代码，可以使用命令 <code>zig cc</code> 来使用 Zig 的 C 编译器，将 C 代码交叉编译成目标文件并与 Go 的目标文件链接，以构建适用于 x86 架构 Linux 的静态二进制文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CGO_ENABLED=1 GOOS=linux CC=<span class="string">&quot;zig cc -target x86_64-linux-musl&quot;</span> go build .</span><br></pre></td></tr></table></figure><p>要让这一步成功，您只需安装 Zig 工具链，无其他依赖项！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇博客为您简单介绍了 Zig 的功能。Zig融合了简洁性、强大性、安全性和对 C 的兼容性，为开发者提供了一个令人兴奋的选择。无论您是为新项目寻找语言，还是像我一样想扩展编程技能，Zig 都是一个值得探索的创新选择。</p><h2 id="译文链接"><a href="#译文链接" class="headerlink" title="译文链接"></a>译文链接</h2><ul><li><a href="https://medium.com/@vladimirvivien/things-i-like-about-zig-as-a-go-programmer-75eb02aab00f">https://medium.com/@vladimirvivien/things-i-like-about-zig-as-a-go-programmer-75eb02aab00f</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在编程语言领域，Zig相对来说是新成员。Zig由Andrew Kelly创立，于2016年正式面世，Zig社区将其定位为“用于开发健壮、优化和可复用软件的通用编程语言”。&lt;/p&gt;
&lt;p&gt;在这简单的描述中，蕴含着一些宏大的目标。例如，Zig被视为一个能够与C语言竞争的编程语言</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>比肩Rust？万字Ziglang项目实战</title>
    <link href="https://zoues.com/posts/22db7120/"/>
    <id>https://zoues.com/posts/22db7120/</id>
    <published>2024-08-12T10:00:43.000Z</published>
    <updated>2024-08-13T14:20:12.542Z</updated>
    
    <content type="html"><![CDATA[<h1 id="比肩Rust？万字Ziglang项目实战"><a href="#比肩Rust？万字Ziglang项目实战" class="headerlink" title="比肩Rust？万字Ziglang项目实战"></a>比肩Rust？万字Ziglang项目实战</h1><p>Zig 是一种命令式、通用、静态类型、编译型系统编程语言和工具链，用于维护健壮、优化和可重用的软件。</p><ul><li>健壮：即使在内存不足等极端情况下，行为也是正确的。</li><li>优化：以最佳方式编写程序，使其能够表现良好。</li><li>可重用：相同的代码在具有不同约束条件的许多环境中都可以使用。</li><li>可维护：精确地向编译器和其他程序员传达意图。该语言对于阅读代码的开销很小，并且对于需求和环境的变化具有韧性。</li></ul><p>它支持编译时泛型、反射和评估、交叉编译和手动内存管理。Zig 的一个主要目标是改进 C 语言，同时也受到 Rust 等其他语言的启发。</p><p>学习 Zig 的资源有很多，主要包括：</p><ul><li><a href="https://ziglang.org/learn/">Zig 文档</a></li><li><a href="https://ziglang.org/documentation/master/std/">Zig 标准库参考</a></li><li><a href="Ziglearn.org">Ziglearn.org</a></li><li><a href="">Zig 简明教程</a></li><li><a href="">三十分钟搞定Ziglang</a>等</li></ul><p>学习一门语言最快捷的方式，当然是用它，上手最快的方式，当属项目实战，在运用中去学习，在学习中输出。通过本文我们将学习到：</p><ul><li>切片的使用</li><li>defer防止内存泄漏</li><li>项目开发与维护</li><li>从0到1开发HTTP服务等</li></ul><h2 id="0x1-从切片开始"><a href="#0x1-从切片开始" class="headerlink" title="0x1: 从切片开始"></a>0x1: 从切片开始</h2><p>以下是用于演示的代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">// 导入标准库</span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">/// 返回一个切片</span><br><span class="line">fn zigBits() []u8 &#123;</span><br><span class="line">    // 创建一个数组字面量</span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    // 将数组作为字符串打印出来</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    // 我们需要使用地址运算符（&amp;）将其强制转换为切片类型 &#x27;[]u8&#x27;。</span><br><span class="line">    return &amp;message;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/// 程序的入口</span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    // 常量赋值</span><br><span class="line">    const message = zigBits();</span><br><span class="line"></span><br><span class="line">    // 打印message</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Q：上述实现是从一个函数中返回一个切片，并打印其值吗？</p><p>A：是的！我们期望看到 zigbits 两次。一次来自函数内部，一次来自主函数。</p><p>让我们运行它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug: �,�$</span><br></pre></td></tr></table></figure><p>这跟我们预期的结果不太一致。retry？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug:</span><br><span class="line">       �;�</span><br></pre></td></tr></table></figure><p>Q：这里打印的是 u8 数组而不是切片？结果显示不是同一个数组？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">+ std.log.debug(&quot;&#123;d&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">$ zig build run</span><br><span class="line">debug: &#123; 122, 105, 103, 98, 105, 116, 115 &#125;</span><br><span class="line">debug: &#123; 80, 129, 179, 51, 255, 127, 0 &#125;</span><br></pre></td></tr></table></figure><h3 id="分析说明"><a href="#分析说明" class="headerlink" title="分析说明"></a>分析说明</h3><p>让我们看看这一行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return &amp;message;</span><br></pre></td></tr></table></figure><p>在这里，我们实际上返回的是一个栈上分配的数组切片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try std.testing.expect(@TypeOf(&amp;message) == *[7]u8);</span><br></pre></td></tr></table></figure><p>由于数组是在栈上分配的，当我们从函数中返回时，它可能会被破坏，即当我们从函数返回时释放时。这在文档的“Lifetime and Ownership”部分有解释：</p><p>Zig 使用者有责任确保指针在指向的内存不再可用时即不会被访问。注意，切片是指针的一种形式，因为它引用其他内存。</p><p>这就是当我们尝试在从函数返回后打印数组内容时为什么会得到随机的无意义字符串的原因。</p><p>此外，在 ziglang&#x2F;zig官方仓库中有一个相关问题说明：<a href="https://github.com/ziglang/zig/issues/5725">https://github.com/ziglang/zig/issues/5725</a></p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>我们可以通过几种方式来解决这个问题：</p><ol><li>将切片作为参数传递给函数</li><li>将数组设置为全局变量</li><li>最常见的做法：分配切片（返回切片的分配副本）</li></ol><p>让我们看看每种解决方案是如何运作的。</p><ol><li>将切片作为参数传递</li></ol><p>不带 len 的话 main 函数不知道写入的字节数，因此最终的日志会打印整个缓冲区，而不单单是实际的消息部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn zigBits(slice: []u8) usize &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    std.mem.copy(u8, slice, &amp;message);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    </span><br><span class="line">    var message: [9]u8 = undefined;</span><br><span class="line"></span><br><span class="line">    const len = zigBits(&amp;message);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message[0..len]&#125;);</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>正如所看到的，将函数的返回值更新为 void，并使其接受一个切片参数。使用 std.mem.cpy 方法在函数中更新切片，而不是使用 return。</p><blockquote><p> 这种方法类似于在 Rust 中将可变引用（&amp;mut）传递给函数</p></blockquote><p>运行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits�,�$</span><br></pre></td></tr></table></figure><ol start="2"><li>使用全局数组</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">fn zigBits() []u8 &#123;</span><br><span class="line">    </span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">    return &amp;message;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    </span><br><span class="line">    const msg = zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;msg&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们将数组字面量声明为全局变量，其将会被延迟解析，并且可以被内部作用域访问。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits</span><br></pre></td></tr></table></figure><ol start="3"><li>分配切片</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits() ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line"></span><br><span class="line">    const message = try zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们在这一行上对切片进行了堆上的复制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br></pre></td></tr></table></figure><p>这使得切片可以在函数外部使用，因为现在它在堆上分配了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits</span><br></pre></td></tr></table></figure><p>更常见和符合惯例的处理此类情况的方式是将 std.mem.Allocator 传递给分配内存的函数。这样一来，我们可以允许调用者决定使用哪种分配器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits(allocator: std.mem.Allocator) ![]u8 &#123;</span><br><span class="line">   </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // https://ziglang.org/documentation/master/#Choosing-an-Allocator</span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line">    const message = try zigBits(allocator);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>让我们改进我们的程序，以返回一个具有指定长度的切片，而非栈上分配的数组。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn zigBits(len: usize) ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    // 切片是指针和长度。数组和切片的区别在于，数组的长度是类型的一部分，并且在编译时已知，而切片的长度在运行时已知。</span><br><span class="line">    try std.testing.expect(@TypeOf(message[0..len]) == []u8);</span><br><span class="line"></span><br><span class="line">    // 我们使用 `len` 参数来使用运行时已知的值进行切片。</span><br><span class="line">    // 如果 `len` 声明为 `comptime len`，那么此值将为 &#x27;*[N]u8&#x27;。</span><br><span class="line">    return message[0..len];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    </span><br><span class="line">    const message = try zigBits(7);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这与第一个示例中的悬挂指针问题相同。在 Zig 中，&amp;message 和 message[0..] 将返回相同的切片。</p><p>我们可以使用 <a href="https://godbolt.org/z/cPWjajYxb">https://godbolt.org/z/cPWjajYxb</a> 检查此行为，该行为显示了在使用 ![]u8 时，“zigbits” 放置在堆栈中的 40 个字节，而在使用 []u8 时，“zigbits” 放置在堆栈中的 16 个字节。</p><h2 id="0x2-使用defer防止内存泄漏"><a href="#0x2-使用defer防止内存泄漏" class="headerlink" title="0x2: 使用defer防止内存泄漏"></a>0x2: 使用defer防止内存泄漏</h2><p>在第一章节中，我们讨论了从函数返回切片以及 Zig 如何管理内存这个话题。我们了解到 Zig 数组是在堆栈上分配的，当它们被访问时，如果指向它们的内存不再可用，它们可能会被破坏。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits() ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line">    </span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    const message = try zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，你可以看到我们使用了一个硬编码的 std.heap.page_allocator 来复制内存。然而，在 Zig 的分配器使用约定中，我们一般会声明一个分配器，然后将分配器作为参数传递给函数，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits(allocator: std.mem.Allocator) ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line">    const message = try zigBits(allocator);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，调用者可以根据定义为通用 std.mem.Allocator 的类型来决定分配器的类型。</p><h3 id="处理文件路径"><a href="#处理文件路径" class="headerlink" title="处理文件路径"></a>处理文件路径</h3><p>下述为我们的示例程序，它的目标是连接文件系统路径并打印结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">/// 连接给定的路径，并返回 &#x27;/home&#x27; 目录下的路径。</span><br><span class="line">fn concatPath(allocator: std.mem.Allocator, p1: []const u8, p2: []const u8) ![]const u8 &#123;</span><br><span class="line">    // 定义路径分隔符（对于 POSIX 系统来说是 &#x27;/&#x27;）。</span><br><span class="line">    const separator = std.fs.path.sep_str;</span><br><span class="line"></span><br><span class="line">    // 连接路径并返回结果。</span><br><span class="line">    const path = try std.fs.path.join(allocator, &amp;.&#123; separator, &quot;home&quot;, p1, p2 &#125;);</span><br><span class="line">    return path;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line"></span><br><span class="line">    const path = try concatPath(allocator, &quot;zig&quot;, &quot;bits&quot;);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;path&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你可能已经猜到，在这个程序中我们会围绕 concatPath 函数进行说明。它接受一个通用的分配器类型（allocator），并连接给定的 p1 和 p2 路径字符串。在最终路径前面添加了 “&#x2F;home”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line"></span><br><span class="line">debug: /home/zig/bits</span><br></pre></td></tr></table></figure><p>你可以看到一切都正常，我们成功地打印出了最终路径，但你有没有发现内存泄漏？</p><h3 id="分析说明-1"><a href="#分析说明-1" class="headerlink" title="分析说明"></a>分析说明</h3><p>首先，什么是内存泄漏，为什么会发生呢？</p><p>在计算机编程中，内存泄漏就像那些永不满足的黑洞一样——程序不断地消耗着越来越多的内存，没有任何限制，程序也不会释放那些它们不再需要的内存。</p><p>在我们的示例中，我们正在使用分配器将我们的字符串连接起来。但问题在于我们从未释放通过分配器分配的内存。所以我们的程序不断地分配内存。</p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>幸运的是，Zig有方法简化我们的工作。与Golang等现代高级语言一样，我们可以使用 defer 语句来确保在函数返回时释放内存。什么是 defer 语句呢？它是一种执行在函数返回之前的语句，类似hook，让我们来看看如何使用它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn concatPath(allocator: std.mem.Allocator, p1: []const u8, p2: []const u8) ![]const u8 &#123;</span><br><span class="line">    const separator = std.fs.path.sep_str;</span><br><span class="line"></span><br><span class="line">    defer allocator.free(path);</span><br><span class="line"></span><br><span class="line">    const path = try std.fs.path.join(allocator, &amp;.&#123; separator, &quot;home&quot;, p1, p2 &#125;);</span><br><span class="line">    return path;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line"></span><br><span class="line">    const path = try concatPath(allocator, &quot;zig&quot;, &quot;bits&quot;);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;path&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过上述示例，我们看看 defer 语句的具体用法。在我们的函数中，我们在调用 std.fs.path.join 之前添加了 defer 语句。这意味着当函数返回之前，我们将调用 allocator.free(path)，从而释放我们分配的内存。这样，就不会再有内存泄漏了！</p><h2 id="0x3-从0到1构建HTTP服务"><a href="#0x3-从0到1构建HTTP服务" class="headerlink" title="0x3: 从0到1构建HTTP服务"></a>0x3: 从0到1构建HTTP服务</h2><p>现在让我们尝试使用 Zig &gt;&#x3D;0.11 的 std.http 模块，并从头开始创建一个 HTTP 服务器&#x2F;客户端（以及一些基准测试）。</p><p>在学习一门新的编程语言时，一般都是从编写 HTTP 服务器&#x2F;客户端开始。 Zig 标准库将从 0.11.0 （现在已经到0.13版本了）开始具有一些令人兴奋的 HTTP 功能。作为第一步，让我们尝试使用新功能进行一些实验，并尝试设计出一些功能性的东西。此外，我们如果对性能&#x2F;速度方面的事情感兴趣，可以创建一些基准测试，以查看与其他编程语言（如 Rust）的比较，该章节包含以下内容</p><ul><li>查看 std.http</li><li>构建一个 HTTP 客户端</li><li>构建一个 HTTP 服务器</li><li>基准测试</li></ul><ol><li>std.http简介</li></ol><p>Zig 最近几个月终于实现了内置的 HTTP 服务器&#x2F;客户端支持，下述为具体说明：</p><ul><li><a href="https://zig.news/nameless/coming-soon-to-a-zig-near-you-http-client-5b81">Coming Soon to a Zig Near You: HTTP Client</a></li><li><a href="https://github.com/ziglang/zig/issues/910">标准库中的 HTTP 服务器（跟踪问题）</a></li><li><a href="https://news.ycombinator.com/item?id=35991684">Zig 现在在 std 中内置了 HTTP 服务器和客户端</a></li></ul><p>总结一下这个新添加的 std.http 模块的功能：</p><ul><li>仅在 Zig 0.11.0-dev.2675-0eebc2588 及以上版本中可用。</li><li>支持 HTTP&#x2F;1.1（请参阅 A barely HTTP&#x2F;2 server in Zig ）</li><li>支持诸如连接池、压缩内容、代理和 TLS 等功能。</li></ul><p>而我认为最令人兴奋的部分是：</p><blockquote><p>使用最新（0.11.0）的 Zig，无需找到额外的库，我们就可以通过Get获取网站的内容或发送 POST 请求，不费吹灰之力…</p></blockquote><p>std.http 具有以下 API：</p><ul><li>Client：HTTP 客户端实现。</li><li>Server：HTTP 服务器实现。</li><li>Connection：连接类型（keep_alive、close）</li><li>ContentEncoding：压缩选项（compress、deflate、gzip 和 zstd）</li><li>Field：名称和值的通用类型</li><li>Headers：HTTP 标头</li><li>Method：HTTP 方法，如 GET 和 POST</li><li>Status：HTTP 状态码（not_found &#x3D; 404、teapot &#x3D; 418 等）</li><li>TransferEncoding：用于传输正文的编码形式（chunked）</li><li>Version：当前为 HTTP&#x2F;1.0 和 HTTP&#x2F;1.1</li></ul><p>正如您已经注意到的那样，这是一个非常简单直接的 HTTP 实现，与当前稳定版本的 Zig（0.10.1）相比，std.http 并不那么简陋，而是具有一些很酷的添加功能，比如内容&#x2F;传输编码和标头，进步很大！</p><ol start="2"><li>构建一个 HTTP 客户端</li></ol><p>在开始之前，我们需要选择一个分配器来使用。这是 Zig 的一个有趣之处，另外，强烈建议你阅读关于分配器的资料，这样你可以精确地知晓如何进行内存分配。</p><p>简单起见，我们使用 <code>std.heap.GeneralPurposeAllocator</code>，这是一个安全的分配器，可以防止双重释放（double-free）、使用已释放的内存（use-after-free），并且可以检测内存泄漏等情况。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">const allocator = gpa.allocator();</span><br></pre></td></tr></table></figure><p>接下来，我们创建 <code>Client</code> 对象：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">const http = std.http;</span><br><span class="line"></span><br><span class="line">var client = http.Client&#123; .allocator = allocator &#125;;</span><br><span class="line">defer client.deinit();</span><br></pre></td></tr></table></figure><p>如你所见，创建 <code>Client</code> 唯一需要的参数是分配器。跟Rust、Golang一样，构造时还可以在选择性地提供证书包、连接池和代理。</p><p>需要注意的是 <code>defer</code> 关键字。它指示在客户端超出作用域时将调用 <code>deinit</code> 方法，这意味着与客户端关联的所有资源将被释放。</p><p>要发送请求，我们需要以下几样东西：</p><ul><li>一个 <code>std.http.Method</code></li><li>一个 <code>std.Uri</code>，由一个 URL 解析而来。</li><li>一个 <code>std.http.Headers</code>，用于保存发送到服务器的请求头。只有当我们向其中追加内容时，它才会分配内存。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const uri = std.Uri.parse(&quot;https://whatthecommit.com&quot;) catch unreachable;</span><br><span class="line"></span><br><span class="line">var headers = std.http.Headers&#123; .allocator = allocator &#125;;</span><br><span class="line">defer headers.deinit();</span><br><span class="line"></span><br><span class="line">try headers.append(&quot;accept&quot;, &quot;*/*&quot;);</span><br></pre></td></tr></table></figure><p>然后们准备创建一个 <code>Request</code> 对象：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.GET, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br></pre></td></tr></table></figure><p>要真正发送请求，我们还需要使用 <code>start</code> 函数，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try request.start();</span><br></pre></td></tr></table></figure><p>这种方式在很有用。例如，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.POST, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br><span class="line"></span><br><span class="line">request.transfer_encoding = .chunked;</span><br><span class="line"></span><br><span class="line">try request.start();</span><br></pre></td></tr></table></figure><p>在发送请求后，我们需要等待服务器发送响应：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try request.wait();</span><br></pre></td></tr></table></figure><p>这个 <code>wait</code> 调用将为我们处理以下任务：</p><ul><li>重定向</li><li>读取&#x2F;存储请求头</li><li>设置解压</li></ul><p>最后，要读取来自服务器的响应，我们可以使用 <code>Request</code> 的 <code>reader()</code> 方法获取一个 <code>std.io.Reader</code>。剩下的就是在 Zig 中从流中读取数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const body = request.reader().readAllAlloc(allocator, 8192) catch unreachable;</span><br><span class="line">defer allocator.free(body);</span><br></pre></td></tr></table></figure><p>以下是将所有步骤结合在一起并打印出响应的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.11.0</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">    defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">    const allocator = gpa.allocator();</span><br><span class="line"></span><br><span class="line">    var client = http.Client&#123; .allocator = allocator &#125;;</span><br><span class="line">    defer client.deinit();</span><br><span class="line"></span><br><span class="line">    const uri = std.Uri.parse(&quot;https://whatthecommit.com/index.txt&quot;) catch unreachable;</span><br><span class="line"></span><br><span class="line">    var headers = std.http.Headers&#123; .allocator = allocator &#125;;</span><br><span class="line">    defer headers.deinit();</span><br><span class="line"></span><br><span class="line">    try headers.append(&quot;accept&quot;, &quot;*/*&quot;);</span><br><span class="line"></span><br><span class="line">    var request = try client.request(.GET, uri, headers, .&#123;&#125;);</span><br><span class="line">    defer request.deinit();</span><br><span class="line"></span><br><span class="line">    try request.start();</span><br><span class="line">    try request.wait();</span><br><span class="line"></span><br><span class="line">    const body = request.reader().readAllAlloc(allocator, 8192) catch unreachable;</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    std.log.info(&quot;&#123;s&#125;&quot;, .&#123;body&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终运行结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">error: TlsInitializationFailed</span><br></pre></td></tr></table></figure><p>结果运行出错，新语言就是这么难查问题，我们在Zig官方issue中发现了一个问题：</p><p><a href="https://github.com/ziglang/zig/issues/14172">https://github.com/ziglang/zig/issues/14172</a></p><p>当前标准库中的 TLS 实现仅支持 TLS 1.3，不幸的是，我们测试的网站 whatthecommit.com 使用的是 TLS 1.2。</p><p>我们可以通过以下命令来验证是否支持 TLS 1.3：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -v --tlsv1.3 https://whatthecommit.com/index.txt</span><br></pre></td></tr></table></figure><p>结果显示网站不支持 TLS 1.3。</p><p>那么我们换个目标网址，更新程序中的 URL 并再次运行它：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">&quot;s|https://.*\&quot;|https://godsays.xyz\&quot;|g&quot;</span> src/main.zig</span><br><span class="line"></span><br><span class="line">$ zig build run</span><br></pre></td></tr></table></figure><p>结果成功了！那 POST 请求呢？我们可以这样写入数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.POST, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br><span class="line">request.transfer_encoding = .chunked;</span><br><span class="line"></span><br><span class="line">try request.start();</span><br><span class="line"></span><br><span class="line">try request.writer().writeAll(&quot;Zig Bits!\n&quot;);</span><br><span class="line">try request.finish();</span><br><span class="line"></span><br><span class="line">try request.wait();</span><br></pre></td></tr></table></figure><ol start="3"><li>构建一个 HTTP 服务器</li></ol><p>现在简单的部分已经完成了，让我们进入正题：如何实现一个HTTP服务器。</p><p>在下面的实现中，我们从标准库中的测试中获得了一些灵感，并提出了一个简化版本，去掉了多线程和断言等。</p><p>为了保持简单，直接添加一个名为 <code>/get</code> 的路径就足够了。</p><p>所以我们想要实现的目标很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl 127.0.0.1:8000/get</span><br><span class="line"></span><br><span class="line">Zig Bits!</span><br></pre></td></tr></table></figure><blockquote><p>你是否知道Python只需要一行就能创建 HTTP 服务器吗？ <code>python -m http.server 8000</code>。</p></blockquote><p>让我们快速定义一下服务器配置的常量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line">const log = std.log.scoped(.server);</span><br><span class="line"></span><br><span class="line">const server_addr = &quot;127.0.0.1&quot;;</span><br><span class="line">const server_port = 8000;</span><br></pre></td></tr></table></figure><p>这里需要注意的是，我们为 <code>log</code> 模块指定了一个作用域，这样日志消息会显示为 <code>info(server)</code>、<code>debug(server)</code> 等等。</p><p>接下来，我们需要选择一个分配器（allocator），再次使用 <code>std.heap.GeneralPurposeAllocator</code>，以保持与 HTTP 客户端示例的一致性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">const allocator = gpa.allocator();</span><br></pre></td></tr></table></figure><p>现在构建 <code>Server</code> 对象并指示在其作用域结束时释放资源。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var server = http.Server.init(allocator, .&#123; .reuse_address = true &#125;);</span><br><span class="line">defer server.deinit();</span><br></pre></td></tr></table></figure><p>将服务器绑定到我们之前定义的地址上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.info(&quot;Server is running at &#123;s&#125;:&#123;d&#125;&quot;, .&#123; server_addr, server_port &#125;);</span><br><span class="line"></span><br><span class="line">const address = std.net.Address.parseIp(server_addr, server_port) catch unreachable;</span><br><span class="line">try server.listen(address);</span><br></pre></td></tr></table></figure><p>和每个 HTTP 服务器的实现一样，我们需要一个机制来阻塞当前线程，等待下一个请求并处理它。为此，在 Zig 中我们可以这样做：</p><ul><li><code>Server</code> 的 <code>accept</code> 方法返回一个 <code>Response</code>。</li><li><code>Response</code> 包含有用的信息，如请求方法和头信息。</li><li><code>Response</code> 还提供了用于处理请求的辅助方法。例如，<code>wait</code> 方法可以用来等待客户端完成请求头的发送。</li></ul><p>当我们把这些拼凑在一起时，服务器运行函数看起来像这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// 运行服务器并处理传入的请求</span><br><span class="line">fn runServer(server: *http.Server, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    outer: while (true) &#123;</span><br><span class="line">        // 接受传入的连接</span><br><span class="line">        var response = try server.accept(.&#123;</span><br><span class="line">            .allocator = allocator,</span><br><span class="line">        &#125;);</span><br><span class="line">        defer response.deinit();</span><br><span class="line"></span><br><span class="line">        while (response.reset() != .closing) &#123;</span><br><span class="line">            // 处理请求处理期间的错误</span><br><span class="line">            response.wait() catch |err| switch (err) &#123;</span><br><span class="line">                error.HttpHeadersInvalid =&gt; continue :outer,</span><br><span class="line">                error.EndOfStream =&gt; continue,</span><br><span class="line">                else =&gt; return err,</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">            // 处理请求</span><br><span class="line">            try handleRequest(&amp;response, allocator);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们首先创建一个无限循环来接受传入的连接。然后还有另一个循环用于通过 <code>wait</code> 方法从流中读取响应。同时，在Header无效时跳过读取循环中的响应。</p><p>正如你所见，处理请求的步骤是在服务器循环的最后通过 <code>handleRequest</code> 函数完成的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// 处理单个请求</span><br><span class="line">fn handleRequest(response: *http.Server.Response, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    log.info(&quot;&#123;s&#125; &#123;s&#125; &#123;s&#125;&quot;, .&#123; @tagName(response.request.method), @tagName(response.request.version), response.request.target &#125;);</span><br><span class="line"></span><br><span class="line">    // 读取请求体</span><br><span class="line">    const body = try response.reader().readAllAlloc(allocator, 8192);</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    // 如果请求头中存在 &quot;connection&quot;，将其设置为 &quot;keep-alive&quot;</span><br><span class="line">    if (response.request.headers.contains(&quot;connection&quot;)) &#123;</span><br><span class="line">        try response.headers.append(&quot;connection&quot;, &quot;keep-alive&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 检查请求目标是否以 &quot;/get&quot; 开头</span><br><span class="line">    if (std.mem.startsWith(u8, response.request.target, &quot;/get&quot;)) &#123;</span><br><span class="line">        // 检查请求目标是否包含 &quot;?chunked&quot;</span><br><span class="line">        if (std.mem.indexOf(u8, response.request.target, &quot;?chunked&quot;) != null) &#123;</span><br><span class="line">            response.transfer_encoding = .chunked;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            response.transfer_encoding = .&#123; .content_length = 10 &#125;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 设置 &quot;content-type&quot; 头为 &quot;text/plain&quot;</span><br><span class="line">        try response.headers.append(&quot;content-type&quot;, &quot;text/plain&quot;);</span><br><span class="line"></span><br><span class="line">        // 写入响应体</span><br><span class="line">        try response.do();</span><br><span class="line">        if (response.request.method != .HEAD) &#123;</span><br><span class="line">            try response.writeAll(&quot;Zig &quot;);</span><br><span class="line">            try response.writeAll(&quot;Bits!\n&quot;);</span><br><span class="line">            try response.finish();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // 设置响应状态为 404（未找到）</span><br><span class="line">        response.status = .not_found;</span><br><span class="line">        try response.do();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们再来逐步分析一遍：</p><ol><li>首先，记录传入请求的详细信息。</li><li>读取请求体并合理分配内存（最大8KB）。</li><li>如有必要，设置 <code>Connection</code> 头。</li><li>如果请求目标以 <code>/get</code> 开头，则将 <code>&quot;Zig Bits!&quot;</code> 写入响应体。</li><li>如果目标包含 <code>?chunked</code>，则启用分块编码，否则使用固定内容长度。</li><li>如果请求未匹配到配置的路由，则返回404。</li></ol><p>需要注意的一点是 <code>finish()</code> 和 <code>do()</code> 方法的区别。<code>do()</code> 方法只是发送响应头，而 <code>finish()</code> 会发送分块消息的最后一个块，或者验证我们发送了约定数量的字节。因此，如果我们在发送数据时，总是应该调用 <code>finish()</code> 来完成响应。</p><p>现在请求已被处理，我们也有了服务器函数，下面看看如何运行服务器并记录错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 运行服务器</span><br><span class="line">runServer(&amp;server, allocator) catch |err| &#123;</span><br><span class="line">    // 处理服务器错误</span><br><span class="line">    log.err(&quot;server error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    if (@errorReturnTrace()) |trace| &#123;</span><br><span class="line">        std.debug.dumpStackTrace(trace.*);</span><br><span class="line">    &#125;</span><br><span class="line">    std.os.exit(1);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>最终的 HTTP 服务器代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本: 0.11.0</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line">const log = std.log.scoped(.server);</span><br><span class="line"></span><br><span class="line">const server_addr = &quot;127.0.0.1&quot;;</span><br><span class="line">const server_port = 8000;</span><br><span class="line"></span><br><span class="line">fn runServer(server: *http.Server, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    outer: while (true) &#123;</span><br><span class="line">        var response = try server.accept(.&#123;</span><br><span class="line">            .allocator = allocator,</span><br><span class="line">        &#125;);</span><br><span class="line">        defer response.deinit();</span><br><span class="line"></span><br><span class="line">        while (response.reset() != .closing) &#123;</span><br><span class="line">           </span><br><span class="line">            response.wait() catch |err| switch (err) &#123;</span><br><span class="line">                error.HttpHeadersInvalid =&gt; continue :outer,</span><br><span class="line">                error.EndOfStream =&gt; continue,</span><br><span class="line">                else =&gt; return err,</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">            try handleRequest(&amp;response, allocator);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn handleRequest(response: *http.Server.Response, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    log.info(&quot;&#123;s&#125; &#123;s&#125; &#123;s&#125;&quot;, .&#123; @tagName(response.request.method), @tagName(response.request.version), response.request.target &#125;);</span><br><span class="line">    const body = try response.reader().readAllAlloc(allocator, 8192);</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    if (response.request.headers.contains(&quot;connection&quot;)) &#123;</span><br><span class="line">        try response.headers.append(&quot;connection&quot;, &quot;keep-alive&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (std.mem.startsWith(u8, response.request.target, &quot;/get&quot;)) &#123;</span><br><span class="line">        if (std.mem.indexOf(u8, response.request.target, &quot;?chunked&quot;) != null) &#123;</span><br><span class="line">            response.transfer_encoding = .chunked;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            response.transfer_encoding = .&#123; .content_length = 10 &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        try response.headers.append(&quot;content-type&quot;, &quot;text/plain&quot;);</span><br><span class="line"></span><br><span class="line">        try response.do();</span><br><span class="line">        if (response.request.method != .HEAD) &#123;</span><br><span class="line">            try response.writeAll(&quot;Zig &quot;);</span><br><span class="line">            try response.writeAll(&quot;Bits!\n&quot;);</span><br><span class="line">            try response.finish();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        response.status = .not_found;</span><br><span class="line">        try response.do();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">    defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">    const allocator = gpa.allocator();</span><br><span class="line"></span><br><span class="line">    var server = http.Server.init(allocator, .&#123; .reuse_address = true &#125;);</span><br><span class="line">    defer server.deinit();</span><br><span class="line"></span><br><span class="line">    log.info(&quot;Server is running at &#123;s&#125;:&#123;d&#125;&quot;, .&#123; server_addr, server_port &#125;);</span><br><span class="line"></span><br><span class="line">    const address = std.net.Address.parseIp(server_addr, server_port) catch unreachable;</span><br><span class="line">    try server.listen(address);</span><br><span class="line"></span><br><span class="line">    runServer(&amp;server, allocator) catch |err| &#123;</span><br><span class="line">        log.err(&quot;server error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">        if (@errorReturnTrace()) |trace| &#123;</span><br><span class="line">            std.debug.dumpStackTrace(trace.*);</span><br><span class="line">        &#125;</span><br><span class="line">        std.os.exit(1);</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>让我们看看运行情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line"></span><br><span class="line">info(server): Server is running at 127.0.0.1:8000</span><br><span class="line">info(server): GET HTTP/1.1 /get</span><br><span class="line">info(server): GET HTTP/1.1 /get?chunked</span><br></pre></td></tr></table></figure><p>在另一个终端中执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl 127.0.0.1:8000/get</span><br><span class="line">Zig Bits!</span><br><span class="line"></span><br><span class="line">$ curl 127.0.0.1:8000/get?chunked</span><br><span class="line">Zig Bits!</span><br></pre></td></tr></table></figure><p>现在我们有了一个纯 Zig 编写的功能性 HTTP 服务器了。</p><ol start="4"><li>基准测试</li></ol><p>为了查看 Zig HTTP 客户端相对于其他编程语言实现的速度，这里使用 hyperfine 创建了一些基准测试。</p><p>这些基准测试可以在这个仓库中找到：<a href="https://github.com/orhun/zig-http-benchmarks">https://github.com/orhun/zig-http-benchmarks</a></p><p>将 HTTP 客户端与以下几种客户端进行比较：</p><ul><li>Rust HTTP 客户端（hyper、reqwest、ureq、attohttpc）</li><li>Go HTTP 客户端（net&#x2F;http）</li><li>Python HTTP 客户端（requests）</li><li>curl</li></ul><p>工作的方式是，运行 Zig HTTP 服务器并接受来自不同客户端的 N 个请求，然后让 hyperfine 完成它。</p><p>要运行基准测试，只需运行 <code>./bench.sh</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rust-ureq 运行结果:</span><br><span class="line">    比 rust-hyper 快 1.18 ± 0.22 倍</span><br><span class="line">    比 rust-reqwest 快 1.30 ± 0.27 倍</span><br><span class="line">    比 go-http-client 快 1.74 ± 0.38 倍</span><br><span class="line">    比 rust-attohttpc 快 1.92 ± 0.40 倍</span><br><span class="line">    比 zig-http-client 快 2.17 ± 0.63 倍</span><br><span class="line">    比 curl 快 4.25 ± 0.73 倍</span><br><span class="line">    比 python-http-client 快 10.31 ± 1.47 倍</span><br></pre></td></tr></table></figure><p>以下是具体的测试数据：</p><table><thead><tr><th>命令</th><th>平均时间 [ms]</th><th>最小时间 [ms]</th><th>最大时间 [ms]</th><th>相对速度</th></tr></thead><tbody><tr><td>curl</td><td>295.2 ± 29.3</td><td>248.6</td><td>367.9</td><td>4.25 ± 0.73</td></tr><tr><td>zig-http-client</td><td>150.9 ± 38.1</td><td>98.5</td><td>250.2</td><td>2.17 ± 0.63</td></tr><tr><td>rust-attohttpc</td><td>133.4 ± 20.6</td><td>101.1</td><td>174.7</td><td>1.92 ± 0.40</td></tr><tr><td>rust-hyper</td><td>82.1 ± 10.1</td><td>65.7</td><td>106.0</td><td>1.18 ± 0.22</td></tr><tr><td>rust-reqwest</td><td>90.0 ± 14.0</td><td>67.8</td><td>126.0</td><td>1.30 ± 0.27</td></tr><tr><td>rust-ureq</td><td>69.5 ± 9.6</td><td>55.3</td><td>92.9</td><td>1.00</td></tr><tr><td>go-http-client</td><td>120.8 ± 20.0</td><td>84.6</td><td>171.6</td><td>1.74 ± 0.38</td></tr><tr><td>python-http-client</td><td>716.5 ± 22.0</td><td>665.9</td><td>765.7</td><td>10.31 ± 1.47</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/orhun/zig-http-benchmarks/output/benchmarks-bw.png"></p><p>通过比较这两个服务器的性能指标，我们可以得出它们在处理相同请求的情况下的性能差异。通常来说，性能测试的结果可能会受到多种因素的影响，包括硬件配置、操作系统、编译优化等，因此需要在真实环境中进行测试以获得更准确的结果。</p><h2 id="0x4-项目实战"><a href="#0x4-项目实战" class="headerlink" title="0x4: 项目实战"></a>0x4: 项目实战</h2><p>在核心功能实现并且 Zig 代码经过了彻底的测试之后，我们可以开始做与项目管理相关的琐事，我们将分享在 Zig 项目管理方面的经验，以便能够正常的使用github等工具维护你的开源代码。</p><p>该章节将涵盖以下主题：</p><ul><li>添加库</li><li>运行测试</li><li>代码覆盖率</li><li>文档生成</li><li>CI&#x2F;CD</li></ul><ol><li>添加库</li></ol><p>对于新手来说，这是一个棘手的问题之一。目前，似乎还没有一个像 cargo add 这样简单和标准的方法来为你的 Zig 项目添加库。但是，有一些用于此目的的包管理器可供选择：</p><ul><li>gyro：一个带有索引、构建运行器和构建依赖项的 Zig 包管理器。</li><li>zigmod：Zig 编程语言的包管理器。</li><li>aquila：一个用于 Zig 项目的包索引。</li></ul><p>对于我们接下来的项目，选择采取了一个更简单直接的方法：Git 子模块</p><p>a. 在项目的根目录下创建 libs 目录。</p><p>b. 将库作为 Git 子模块添加：</p><p>要么运行 git submodule add <remote_url> libs&#x2F;<lib>，要么添加 .gitmodules 文件。例如，对于 zig-clap：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[submodule &quot;libs/zig-clap&quot;]</span><br><span class="line">  path = libs/zig-clap</span><br><span class="line">  url = https://github.com/Hejsil/zig-clap</span><br></pre></td></tr></table></figure><p>c. 然后你需要在 build.zig 中将该包添加到你的项目中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">const exe = b.addExecutable(&quot;exe_name&quot;, &quot;src/main.zig&quot;);</span><br><span class="line">// ...</span><br><span class="line">// ...</span><br><span class="line">exe.addPackagePath(&quot;clap&quot;, &quot;libs/zig-clap/clap.zig&quot;)</span><br></pre></td></tr></table></figure><p>d. 现在可以在我们的源文件中导入库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const clap = @import(&quot;clap&quot;);</span><br></pre></td></tr></table></figure><p>当然，现在还没有标准的方法来安装 Zig 库。人们常用的一些常见方法有：</p><ul><li>使用 git 子模块或将库复制粘贴到自己的项目中。</li><li>在 build.zig 文件中添加 exe.addPackagePath(“clap”, “path&#x2F;to&#x2F;clap.zig”); 以使用该库。</li><li>使用非官方的包管理器，如 zigmod 或 gyro。</li></ul><p>关于如何使用这些包管理器安装包，请查看这些包管理器的文档。</p><ol start="2"><li>运行测试</li></ol><p>在编写项目的测试时，我们需要为每个文件添加测试，并在 build.zig 中指定要测试的文件。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const exe_tests = b.addTest(&quot;src/main.zig&quot;);</span><br><span class="line">exe_tests.setTarget(target);</span><br><span class="line">exe_tests.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">const test_step = b.step(&quot;test&quot;, &quot;Run unit tests&quot;);</span><br><span class="line">test_step.dependOn(&amp;exe_tests.step);</span><br></pre></td></tr></table></figure><p>当运行 zig build test 时，它只会运行 main.zig 中的测试。我们希望运行项目中每个文件中的测试，所以我们做了下面的操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const test_step = b.step(&quot;test&quot;, &quot;Run tests&quot;);</span><br><span class="line"></span><br><span class="line">// loop through the modules and add them for testing</span><br><span class="line">for ([_][]const u8&#123; &quot;main&quot;, &quot;wav&quot;, &quot;file&quot;, &quot;gen&quot; &#125;) |module| &#123;</span><br><span class="line">    const test_module = b.fmt(&quot;src/&#123;s&#125;.zig&quot;, .&#123;module&#125;);</span><br><span class="line">    var exe_tests = b.addTest(test_module);</span><br><span class="line">    test_step.dependOn(&amp;exe_tests.step);</span><br><span class="line">&#125;</span><br><span class="line">sqlCopy code$ zig build test</span><br><span class="line"></span><br><span class="line">1/1 test.run... OK</span><br><span class="line">All 1 tests passed.</span><br><span class="line">1/2 test.encode WAV... OK</span><br><span class="line">2/2 test.stream out WAV... OK</span><br><span class="line">All 2 tests passed.</span><br><span class="line">1/1 test.read bytes from the file... OK</span><br><span class="line">All 1 tests passed.</span><br><span class="line">1/1 test.generate music... OK</span><br><span class="line">All 1 tests passed.</span><br></pre></td></tr></table></figure><p>这个方法运行结果符合预期，但还有另一种更好的方法，在是在 Reddit 帖子中找到的方法（其实我们看ziglang公共库也可以发现）。</p><p>我们需要有一个通用的文件来“引用”测试代码，以便通过一个单独的 “addTest” 语句在你的构建文件中运行它们。例如，在 src&#x2F;myLibrary.zig 中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pub const A = @import(&quot;A.zig&quot;);</span><br><span class="line">pub const B = @import(&quot;B.zig&quot;);</span><br><span class="line">pub const SomeDataType = @import(&quot;C.zig&quot;).SomeDataType;</span><br><span class="line"></span><br><span class="line">test &#123;</span><br><span class="line">  @import(&quot;std&quot;).testing.refAllDecls(@This());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在 build.zig 中，只需要简单地添加它作为 b.addTest(“src&#x2F;myLibrary.zig”)。</p><ol start="3"><li>代码覆盖率</li></ol><p>追踪项目的测试覆盖率。有助于更好地测试功能并消除潜在地错误。有时，甚至需要重构你的代码来为某个函数&#x2F;模块编写测试，这样的代码将会更好。</p><p>对于 Rust 项目，通常会按照以下步骤进行测试&#x2F;覆盖率检查：</p><ul><li>编写测试</li><li>使用 cargo-nextest 运行测试</li><li>使用工具生成代码覆盖率报告<ul><li>cargo-tarpaulin</li><li>cargo-llvm-cov</li></ul></li><li>将其上传到 Codecov.io</li></ul><p>对于 Zig，我们将采取以下步骤：</p><ul><li>编写测试</li><li>使用 zig build test 运行测试</li><li>使用 kcov 生成代码覆盖率报告</li><li>将其上传到 Codecov.io</li></ul><p>在测试通过后，第一步是生成代码覆盖率报告。我们只需要在 build.zig 中添加一个新的标志来生成覆盖率报告：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const coverage = b.option(bool, &quot;test-coverage&quot;, &quot;Generate test coverage&quot;) orelse false;</span><br><span class="line"></span><br><span class="line">const exe_tests = b.addTest(&quot;src/main.zig&quot;);</span><br><span class="line">exe_tests.setTarget(target);</span><br><span class="line">exe_tests.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">if (coverage) &#123;</span><br><span class="line">    exe_tests.setExecCmd(&amp;[_]?[]const u8&#123;</span><br><span class="line">        &quot;kcov&quot;,</span><br><span class="line">        &quot;kcov-output&quot;,</span><br><span class="line">        null,</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在当你运行 zig build test -Dtest-coverage 时，报告将存放在 kcov-output。</p><p>下一步是将这个报告上传到 Codecov。我们只需要编写了一个简单的 GitHub Actions 工作流程即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># contents of .github/workflows/ci.yml</span><br><span class="line"></span><br><span class="line">name: Continuous Integration</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  push:</span><br><span class="line">    branches:</span><br><span class="line">      - main</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  test:</span><br><span class="line">    name: Test</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          # include libraries</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Install kcov</span><br><span class="line">        run: |</span><br><span class="line">          sudo apt-get update</span><br><span class="line">          sudo apt-get install -y \</span><br><span class="line">            --no-install-recommends \</span><br><span class="line">            --allow-unauthenticated \</span><br><span class="line">            kcov</span><br><span class="line"></span><br><span class="line">      - name: Test</span><br><span class="line">        run: zig build test -Dtest-coverage</span><br><span class="line"></span><br><span class="line">      - name: Upload coverage to Codecov</span><br><span class="line">        uses: codecov/codecov-action@v3</span><br><span class="line">        with:</span><br><span class="line">          name: code-coverage-report</span><br><span class="line">          directory: kcov-output</span><br><span class="line">          fail_ci_if_error: true</span><br><span class="line">          verbose: true</span><br></pre></td></tr></table></figure><p><img src="https://blog.orhun.dev/zig-codecov.png" alt="codecov"></p><ol start="4"><li>文档生成</li></ol><p>在 Ziglearn 的第 3 章中，详细解释了文档生成过程：</p><p>Zig 编译器自带自动文档生成功能。你可以通过在 <code>zig build-&#123;exe, lib, obj&#125;</code> 或 <code>zig run</code> 命令中添加 <code>-femit-docs</code> 参数来调用这个功能。生成的文档会保存在 <code>./docs</code> 文件夹中，通常用于小型的静态网站。</p><p>需要注意的是这个文档生成功能目前还是实验性的，对于复杂的例子经常会失败。标准库的文档就是通过这个方式生成的。</p><p>因此，我们只需要激活 <code>emit_docs</code> 标志，就能自动生成文档。我们推荐在 <code>build.zig</code> 文件中添加一个标志，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const documentation = b.option(bool, &quot;docs&quot;, &quot;Generate documentation&quot;) orelse false;</span><br><span class="line"></span><br><span class="line">const exe = b.addExecutable(exe_name, &quot;src/main.zig&quot;);</span><br><span class="line">exe.setTarget(target);</span><br><span class="line">exe.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">if (documentation) &#123;</span><br><span class="line">    exe.emit_docs = .emit;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后，你可以通过 <code>zig build -Ddocs=true</code> 来生成文档。生成的静态网站将保存在 <code>docs/</code> 目录中，展示的样子如下：</p><p><img src="https://blog.orhun.dev/zig-docs.png" alt="docs"></p><p>但我们希望更进一步，把这个网站部署到 GitHub Pages 上。我们觉得在仓库中维护 <code>docs/</code> 文件夹不太合适，所以把它添加到了 <code>.gitignore</code> 中。我们想要的是在推送提交到主分支时，能够自动生成文档并部署它。</p><p><img src="https://blog.orhun.dev/github-pages-1.png" alt="GitHub Pages I"></p><p>要做到这一点，首先需要为 GitHub Pages 启用 GitHub Actions 功能：</p><blockquote><p> 仓库 -&gt; 设置 -&gt; 页面 -&gt; 构建和部署 -&gt; 源 -&gt; 选择 GitHub Actions 而不是以前从分支部署选项。</p></blockquote><p>之后，我们只需添加以下工作流程文件，即可将文档部署到 GitHub Pages：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># contents of .github/workflows/pages.yml</span><br><span class="line"></span><br><span class="line">name: GitHub Pages</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  # Runs on pushes targeting the default branch</span><br><span class="line">  push:</span><br><span class="line">    branches: [main]</span><br><span class="line"></span><br><span class="line">  # Allows you to run this workflow manually from the Actions tab</span><br><span class="line">  workflow_dispatch:</span><br><span class="line"></span><br><span class="line"># Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages</span><br><span class="line">permissions:</span><br><span class="line">  contents: read</span><br><span class="line">  pages: write</span><br><span class="line">  id-token: write</span><br><span class="line"></span><br><span class="line"># Allow one concurrent deployment</span><br><span class="line">concurrency:</span><br><span class="line">  group: &quot;pages&quot;</span><br><span class="line">  cancel-in-progress: true</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  deploy:</span><br><span class="line">    name: Deploy website</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    environment:</span><br><span class="line">      name: github-pages</span><br><span class="line">      url: $&#123;&#123; steps.deployment.outputs.page_url &#125;&#125;</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Generate documentation</span><br><span class="line">        run: zig build -Ddocs=true</span><br><span class="line"></span><br><span class="line">      - name: Setup Pages</span><br><span class="line">        uses: actions/configure-pages@v3</span><br><span class="line">      - name: Upload artifact</span><br><span class="line">        uses: actions/upload-pages-artifact@v1</span><br><span class="line">        with:</span><br><span class="line">          # upload documentation</span><br><span class="line">          path: docs</span><br><span class="line"></span><br><span class="line">      - name: Deploy to GitHub Pages</span><br><span class="line">        id: deployment</span><br><span class="line">        uses: actions/deploy-pages@v2</span><br></pre></td></tr></table></figure><p>你可以在设置 &gt; 页面 中查看部署情况：</p><p><img src="https://blog.orhun.dev/github-pages-2.png" alt="GitHub Pages II"></p><ol start="5"><li>CI&#x2F;CD</li></ol><p>最后，为我们的项目设置一个 CI&#x2F;CD 工作流程。包含两个工作流程文件：</p><ul><li>ci.yml：用于确保项目正常构建<ul><li>在提交时触发</li></ul></li><li>cd.yml：用于为不同平台分发预构建的二进制文件。<ul><li>在提交标签时触发</li></ul></li></ul><p><strong>持续集成</strong></p><p>以下是一个 GitHub Actions 工作流程文件，它会在每次推送或拉取请求到主分支时自动化项目的构建、测试和格式检查过程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">name: Continuous Integration</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  pull_request:</span><br><span class="line">  push:</span><br><span class="line">    branches:</span><br><span class="line">      - main</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  build:</span><br><span class="line">    name: &quot;Build with args: &#x27;$&#123;&#123; matrix.OPTIMIZE &#125;&#125;&#x27;&quot;</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    strategy:</span><br><span class="line">      fail-fast: false</span><br><span class="line">      matrix:</span><br><span class="line">        OPTIMIZE: [&quot;&quot;, &quot;-Drelease-safe&quot;, &quot;-Drelease-fast&quot;, &quot;-Drelease-small&quot;]</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Build</span><br><span class="line">        run: zig build $&#123;&#123; matrix.OPTIMIZE &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Test</span><br><span class="line">        run: zig build test $&#123;&#123; matrix.OPTIMIZE &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Check formatting</span><br><span class="line">        run: zig fmt --check .</span><br></pre></td></tr></table></figure><p>这个二进制文件会针对 3 种不同的优化配置进行测试：</p><ul><li>ReleaseFast<ul><li>较快的运行时性能</li><li>禁用安全检查</li><li>编译速度慢</li><li>二进制文件较大</li></ul></li><li>ReleaseSafe<ul><li>中等的运行时性能</li><li>启用安全检查</li><li>编译速度慢</li><li>二进制文件较大</li></ul></li><li>ReleaseSmall<ul><li>中等的运行时性能</li><li>禁用安全检查</li><li>编译速度慢</li><li>二进制文件较小</li></ul></li></ul><p><strong>持续部署</strong></p><p>以下是一个 GitHub Actions 工作流程文件，它会在每次将新版本标签推送到存储库时自动构建一个特定目标的二进制文件，并将其发布到 GitHub：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">name: Continuous Deployment</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  push:</span><br><span class="line">    tags:</span><br><span class="line">      - &quot;v*.*.*&quot;</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  publish-github:</span><br><span class="line">    name: Publish on GitHub</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    strategy:</span><br><span class="line">      fail-fast: false</span><br><span class="line">      matrix:</span><br><span class="line">        TARGET:</span><br><span class="line">          [</span><br><span class="line">            x86_64-linux,</span><br><span class="line">            x86_64-macos,</span><br><span class="line">            x86_64-windows,</span><br><span class="line">            aarch64-linux,</span><br><span class="line">            aarch64-macos,</span><br><span class="line">            aarch64-windows,</span><br><span class="line">            arm-linux,</span><br><span class="line">            riscv64-linux,</span><br><span class="line">            i386-linux,</span><br><span class="line">          ]</span><br><span class="line"></span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Set the release version</span><br><span class="line">        run: echo &quot;RELEASE_VERSION=$&#123;GITHUB_REF:11&#125;&quot; &gt;&gt; $GITHUB_ENV</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Build</span><br><span class="line">        run: zig build -Drelease-safe -Dtarget=$&#123;&#123; matrix.TARGET &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Upload the binary</span><br><span class="line">        uses: svenstaro/upload-release-action@v2</span><br><span class="line">        with:</span><br><span class="line">          file: zig-out/bin/binary-$&#123;&#123; env.RELEASE_VERSION &#125;&#125;-$&#123;&#123; matrix.TARGET &#125;&#125;*</span><br><span class="line">          file_glob: true</span><br><span class="line">          overwrite: true</span><br><span class="line">          tag: $&#123;&#123; github.ref &#125;&#125;</span><br><span class="line">          repo_token: $&#123;&#123; secrets.GITHUB_TOKEN &#125;&#125;</span><br></pre></td></tr></table></figure><p>正如你在这里所看到的，通过只提供 -Dtarget 选项，轻松实现了 Zig 项目的交叉编译。</p><p>此工作流程中的 TARGET 变量由两部分组成：</p><ul><li>CPU 架构（例如 x86_64）</li><li>操作系统（例如 linux）</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本教程中，我们学习了如何使用 Zig 编程语言构建一个简单的 HTTP 服务器，并使用 std.http 模块来处理 HTTP 请求和发送 HTTP 响应。我们还通过与 Rust 编写的 HTTP 服务器进行基准测试，比较了它们在处理相同请求时的性能表现，通过项目实战，让我们的代码能够通过github实现CICD管理项目的生命周期。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://github.com/ziglang/zig/blob/master/lib/std/mem/Allocator.zig">https://github.com/ziglang/zig/blob/master/lib/std/mem/Allocator.zig</a></li><li><a href="https://blog.orhun.dev/zig-bits-01/">https://blog.orhun.dev/zig-bits-01/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;比肩Rust？万字Ziglang项目实战&quot;&gt;&lt;a href=&quot;#比肩Rust？万字Ziglang项目实战&quot; class=&quot;headerlink&quot; title=&quot;比肩Rust？万字Ziglang项目实战&quot;&gt;&lt;/a&gt;比肩Rust？万字Ziglang项目实战&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Ziglang 简明教程</title>
    <link href="https://zoues.com/posts/7ea0589/"/>
    <id>https://zoues.com/posts/7ea0589/</id>
    <published>2024-08-03T08:32:35.000Z</published>
    <updated>2024-08-03T09:59:57.746Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ziglang-简明教程"><a href="#Ziglang-简明教程" class="headerlink" title="Ziglang 简明教程"></a>Ziglang 简明教程</h1><blockquote><p>译：<a href="https://blog.logrocket.com/getting-started-zig-programming-language/">https://blog.logrocket.com/getting-started-zig-programming-language/</a></p></blockquote><p>在本文中，通过理论与示例相结合的方式来帮助初学者进行Zig语言的学习。</p><h2 id="What‘s-Ziglang"><a href="#What‘s-Ziglang" class="headerlink" title="What‘s Ziglang"></a>What‘s Ziglang</h2><p>Zig是一款开源的规模最小、功能齐全的系统编程语言，其被视作较C更友好的替代品。它具有类似Rust的极简语法，同时保持了C的简单性。</p><p>Zig的目标是通过一种新的、受Rust语法影响的C语法的现代化方法来解决C开发人员面临的问题。它提供了一个高效的C互操作解决方案，让C开发人员可以将他们的C代码迁移到Zig。</p><p>Zig不仅仅是一种语言——其具备是一套完整的、功能齐全的工具链，这也意味着您可以使用Zig来创建、开发、测试和构建程序&#x2F;库，而无需第三方自动化构建工具。Zig工具链还可以交叉编译C&#x2F;C++以及Rust项目，因此您可以有效地使用Zig工具链来构建您现有的C&#x2F;C++项目。Zig被设计为一种低级别、硬件友好的系统编程语言，但其高效、开发者友好的语法和功能使其更适合构建任何现代软件系统。</p><p>Zig项目最初由Andrew Kelley发起，现在由Zig软件基金会（ZSF）维护。</p><h3 id="Zig的突出特点"><a href="#Zig的突出特点" class="headerlink" title="Zig的突出特点"></a>Zig的突出特点</h3><p>Zig致力于成为一个更好的C语言替代品，其不仅适用于低级系统编程，还适用于开发通用软件系统，具有以下突出特点：</p><h4 id="设计简单"><a href="#设计简单" class="headerlink" title="设计简单"></a>设计简单</h4><p>现代化语言的设计目标是提供一套设计良好的语法，而不像汇编语言那样原子化。如果语言的抽象过于接近汇编语言，开发人员可能需要编写冗长的代码。另一方面，当语言被抽象成接近人类可读时，它可能与硬件相距甚远，可能不适合系统编程的需求。</p><p>Zig提供了轻量级的、类Rust的语法，其大多数C提供的能力都已具备，但是它不提供Rust和C++那些复杂的功能集和语法，而是提供了一个像Go那样简单性为先的开发路径。</p><h4 id="性能和安全性"><a href="#性能和安全性" class="headerlink" title="性能和安全性"></a>性能和安全性</h4><p>性能和安全性是选择的关键因素。语言的性能通常取决于其标准库、核心运行时功能的性能，以及编译器生成的二进制文件的质量。同时，安全设计实现边界检查、溢出处理和内存范围，并帮助开发人员减少关键安全漏洞。</p><p>Zig构建系统提供了四种构建模式，开发人员可以根据其性能和安全性要求使用。Zig还可以在编译时理解变量溢出。</p><p>此外，它可以生成带有运行时安全检查的优化二进制文件，就像Rust一样，也可以生成不带运行时安全检查的超轻量级二进制文件，就像C一样。Zig官方文档声称，由于其基于LLVM的优化和改进的未定义行为，Zig在理论上比C更快！</p><h4 id="完整的系统编程解决方案"><a href="#完整的系统编程解决方案" class="headerlink" title="完整的系统编程解决方案"></a>完整的系统编程解决方案</h4><p>大多数编程语言都有一个或多个标准编译器和标准库实现。例如，您可以使用以下编译C：</p><ul><li>GNU C</li><li>Apple Clang</li><li>带有libc、BSD-libc和Microsoft C运行时的MSVC编译器</li></ul><p>但是这两个组件对于现代系统编程需求来说还不够。程序员通常需要建立工具、包管理器和交叉编译工具等。</p><p>因此，在C生态系统中，像CMake、Ninja、Meson这样的构建工具以及类似Conan这样的包管理器逐渐流行，而像Go和Rust这样的现代语言官方内置了包管理器、构建工具及API、交叉编译支持和测试集成等。</p><p>与Go及Rust等现代语言一样，Zig内置了包管理器、构建系统及API、支持交叉编译和测试集成，这提高了Zig成为更好的C的机会，因为它解决了C（和C++）开发人员面临的关键系统编程问题。从语言设计的角度来看，Zig提供了C开发人员期望的现代语言的所有功能，因此C程序员可以逐步将他们的系统迁移到现代Zig，而无需重新编写他们遗留的代码库。</p><h3 id="为什么学习Zig"><a href="#为什么学习Zig" class="headerlink" title="为什么学习Zig"></a>为什么学习Zig</h3><p>虽然编程语言千千万，但只有其中的一小部分能够在开发者社区中流行开来。未来还会有更多的编程语言出现，它们具有各种各样的特性，旨在取代现有的语言。</p><p>我们不需要学习所有开发语言，但是，如果有那么一种语言具有理想中未来语言的意味，并且提供了强有力、有效和经过验证的论据，说明它可以作为现有语言的替代品，那么学习它的内部设计理念无疑比完全忽视它要好。Go 1.0在2012年发布，作为一种新的极简语言，现在已经成为主要技术公司的依赖。在1990年代，Python是一种新的实验性脚本语言，但现在数字世界依赖于它。</p><p>类似地，Zig最初出现于2016年，2017年发布了首个预发布版本，展示了它作为C的现代替代品的能力。Zig甚至提供了一个完整的系统编程工具链，经过几年的积极开发，并确立了一个充满希望的未来。鉴于它与C的相似性和互操作性，它还可以为您打开更广泛的开发机会，包括人工智能开发、游戏开发等领域。</p><p>学习Zig不仅为您的技能组合增加了一种有前途的与C相关的语言，而且由于其聪明、性能安全平衡的设计，还提高了您对系统编程的了解。</p><h3 id="谁在使用Zig？"><a href="#谁在使用Zig？" class="headerlink" title="谁在使用Zig？"></a>谁在使用Zig？</h3><p>以下流行的开源项目和技术公司使用Zig语言及其工具链：</p><ul><li>主要项目</li></ul><p><img src="https://pic.imgdb.cn/item/66adfe70d9c307b7e9a7d9e2.png" alt="image-20240803165637785"></p><ul><li><p>主要公司</p><p><img src="https://pic.imgdb.cn/item/66adfe89d9c307b7e9a7ee54.png" alt="image-20240803165725136"></p></li></ul><h2 id="学习Zig"><a href="#学习Zig" class="headerlink" title="学习Zig"></a>学习Zig</h2><p>现在您已经了解了Zig及其引入原因，让我们通过学习语言语法、概念和特性的方式开始使用Zig进行开发。</p><ol><li>设置开发环境</li></ol><p>与任何其他开源项目一样，您可以从官方网站下载Zig，或者从源代码构建，但最简单、最现代的方法是通过系统软件包管理器安装它。在Ubuntu上使用以下Snap命令（以sudo运行）安装了Zig开发工具链：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">snap install zig --beta --classic</span><br></pre></td></tr></table></figure><p>有关Zig安装方法的更多信息，请参阅官方安装指南。</p><p>安装Zig后，请在终端中输入zig验证您的安装状态</p><ol start="2"><li>Zig初体验</li></ol><p>我们已经安装了Zig工具链，现在让我们开始在Zig中编写程序。我们将编写一个（经典）的Hello, World!类型的程序，以了解基本的源代码结构和工具链基础知识。</p><p>创建一个名为hello.zig的新文件，并添加以下源代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello Zig!\n&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们在第一行导入了标准库，并将其引用加载到std常量中。然后，main函数（返回void）使用print函数在终端上打印调试消息。</p><p>使用以下命令运行上述代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig run hello.zig</span><br></pre></td></tr></table></figure><p>您将在终端中看到<code>Hello Zig！</code>。print函数提供了类似于C的printf函数的API，让我们通过第二个参数使用格式化字符串：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello &#123;s&#125;! &#123;d&#125;\n&quot;, .&#123;&quot;Zig&quot;, 2024&#125;); // Hello Zig! 2024</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>您还可以省略原子的格式类型，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std.debug.print(&quot;Hello &#123;&#125;\n&quot;, .&#123;2024&#125;); // Hello 2024</span><br></pre></td></tr></table></figure><ol start="3"><li>编译Zig二进制文件</li></ol><p>任何软件发布都需创建一个二进制文件。Zig提供四种模式构建配置基于性能和安全性需求来交叉编译二进制文件。</p><p>为您的Hello, World!程序创建一个二进制文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe --name hello-bin hello.zig</span><br></pre></td></tr></table></figure><p>上述命令将使用默认的Debug模式构建生成一个名为hello-bin的调试二进制文件，因此我们可以通过<code>./hello-bin</code>来执行。</p><p>就像 GNU C 一样，默认情况下，Zig 会为当前目标（CPU 和操作系统）生成二进制文件，因此上述命令在我的计算机上为我生成了 x86 Linux 二进制文件。此外，可以使用 -target 标志进行交叉编译二进制文件。</p><p><img src="https://pic.imgdb.cn/item/66adfea0d9c307b7e9a80225.png" alt="image-20240803170630942"></p><p>例如，以下命令会为 x64 Windows 系统交叉编译一个 .exe 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe -target x86_64-windows --name hello-bin.exe hello.zig</span><br></pre></td></tr></table></figure><ol start="4"><li>使用构建器 API 编译 Zig</li></ol><p>Zig 的编译命令行界面与 GNU C 的命令行标志相同，因此我们可能需要经常重复使用编译命令。在 C 中，我们可以通过编写用于构建过程的 shell 脚本或 CMake来解决这个问题。</p><p>构建系统允许您将编译器标志存储在配置文件中，甚至添加自定义构建步骤。Zig 通过 std 包暴露了一个内置的构建器 API，作为独立的、第三方构建系统。它还提供了 zig build 命令来执行存储在 build.zig 文件中的构建步骤。</p><p>您可以根据自己的喜好使用以下命令搭建一个 Zig 构建项目：</p><ul><li>zig init-exe：初始化一个基于 Zig 构建器的可执行应用程序</li><li>zig init-lib：初始化一个基于 Zig 构建器的库</li></ul><p>之前的演示应用程序是一个可执行类型的应用程序，所以我们可以使用第一个命令来了解 Zig 构建系统：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir zig-exe-demo</span><br><span class="line">cd zig-exe-demo</span><br><span class="line"></span><br><span class="line">zig init-exe</span><br></pre></td></tr></table></figure><p>上述命令创建了一个新的可执行程序，其中包含 src&#x2F;main.zig 中的演示源代码。它向 build.zig 文件添加了几个构建步骤，我们可以使用 zig build 命令执行它们，而不是使用 zig run 或 zig build-exe。</p><p>例如，您可以通过执行以下命令来运行程序：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build run</span><br></pre></td></tr></table></figure><p>您可以使用 install 步骤创建二进制文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zig build install</span><br><span class="line"></span><br><span class="line">./zig-out/bin/zig-exe</span><br></pre></td></tr></table></figure><p>就像在其他构建系统（如 CMake）中所做的那样，您可以通过修改 build.zig 文件来添加更多的构建步骤或中间处理。通过zig build 运行所有的构建自动化步骤，这无疑简化了 Zig 开发过程。</p><p>现在，您知道了如何编写一个基本的 Zig 程序，使用 Zig 编译器进行编译，并使用 Zig 的内置构建系统简化开发过程。让我们开始学习语法和特性吧！</p><ol start="5"><li>Zig 基本类型</li></ol><p>与 C 类似，Zig 支持各种形式的整数、浮点数和指针。让我们学习一些您应该了解的基本类型。</p><p>以下代码片段定义了一些整数和浮点数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i8 = -100;     // 有符号 8 位整数</span><br><span class="line">    var y: u8 = 120;      // 无符号 8 位整数</span><br><span class="line">    var z: f32 = 100.324; // 32 位浮点数</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;x=&#123;&#125;\n&quot;, .&#123;x&#125;);        // x=-100</span><br><span class="line">    std.debug.print(&quot;y=&#123;&#125;\n&quot;, .&#123;y&#125;);        // y=120</span><br><span class="line">    std.debug.print(&quot;z=&#123;d:3.2&#125;\n&quot;, .&#123;z&#125;);   // z=100.32</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于上述标识符值永远不会更改，我们可以使用 const 关键字而不是 var：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">const x: i8 = -100;</span><br><span class="line">// ...</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><p>作为现代语言，布尔类型也得到了支持：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var initialized: bool = true;</span><br></pre></td></tr></table></figure><p>您可以将字符存储在无符号字节（8 位整数）中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const l1: u8 = &#x27;Z&#x27;;</span><br><span class="line">    const l2: u8 = &#x27;i&#x27;;</span><br><span class="line">    const l3: u8 = &#x27;g&#x27;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;c&#125;-&#123;c&#125;-&#123;c&#125;\n&quot;, .&#123;l1, l2, l3&#125;); // Z-i-g</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>您还可以定义变量而不写明其数据类型，如下所示。然后，Zig 将使用 comptime 类型将它们存储起来，保证编译时评估：</p><p>Zig 还支持原生 C 类型（即 c_char、c_int 等）。可以在官方文档的表格中查看所有支持的类型。Zig 中没有内置的字符串类型，因此我们必须使用字节数组。我们将在本教程的另一部分讨论数组。</p><ol start="6"><li>枚举</li></ol><p>Zig 提供了一个简单的语法来定义和访问枚举。看看以下示例源代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const LogType = enum &#123;</span><br><span class="line">        info,</span><br><span class="line">        err,</span><br><span class="line">        warn</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const ltInfo = LogType.info;</span><br><span class="line">    const ltErr = LogType.err;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;ltInfo&#125;); // main.main.LogType.info</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;ltErr&#125;);  // main.main.LogType.err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 允许覆盖枚举的序数值，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const LogType = enum(u32) &#123;</span><br><span class="line">    info = 200,</span><br><span class="line">    err = 500,</span><br><span class="line">    warn = 600</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol start="7"><li>数组和切片</li></ol><p>Zig 建议将数组用于编译时已知值（compile-time-known），切片用于运行时已知值（runtime-known）。例如，我们可以将英语元音存储在常量字符数组中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const vowels = [5]u8&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;&#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;vowels&#125;); // aeiou</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;vowels.len&#125;); // 5</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们可以省略大小，因为它在编译时已知：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const vowels = [_]u8&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;&#125;; // 注意 &quot;_&quot;</span><br></pre></td></tr></table></figure><p>您不需要使用这种方法来定义字符串，因为 Zig 允许您以 C 风格定义字符串，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const msg = &quot;Ziglang&quot;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg&#125;); // Zig</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(msg)&#125;); // *const [7:0]u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦将硬编码字符串存储在标识符中，Zig 将自动使用空终止数组引用（数组的指针）*const [7:0]u8 来存储元素。在这里，我们使用了 @TypeOf() 内置函数来获取变量的类型。您可以在官方文档中浏览所有支持的内置函数。</p><p>数组可以使用 ** 和 ++ 运算符进行重复或连接：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const msg1 = &quot;Zig&quot;;</span><br><span class="line">    const msg2 = &quot;lang&quot;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg1 ** 2&#125;); // ZigZig</span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg1 ++ msg2&#125;); // Ziglang</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 切片几乎与数组一样，但用于存储在运行时已知而不在编译时已知的值。看看下面的例子，它从数组中创建一个切片：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const nums = [_]u8&#123;2, 5, 6, 4&#125;;</span><br><span class="line">    var x: usize = 3;</span><br><span class="line">    const slice = nums[1..x];</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;any&#125;\n&quot;, .&#123;slice&#125;);        // &#123; 5, 6 &#125;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(slice)&#125;);  // []const u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，如果 x 是一个运行时已知的变量，slice 标识符就会变成一个切片。如果对 x 使用 const，则 slice 将变成一个数组指针（*const [2]u8），因为 x 在编译时已知。我们将在后面的章节中讨论指针。</p><ol start="8"><li>结构体和联合体</li></ol><p>结构体是用于存储多个值的有用数据结构，甚至可以用来实现面向对象编程（OOP）概念。</p><p>您可以创建结构体并使用以下语法访问其内部字段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const PrintConfig = struct &#123;</span><br><span class="line">        id: *const [4:0] u8,</span><br><span class="line">        width: u8,</span><br><span class="line">        height: u8,</span><br><span class="line">        zoom: f32</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const pc = PrintConfig &#123;</span><br><span class="line">        .id = &quot;BAX1&quot;,</span><br><span class="line">        .width = 200,</span><br><span class="line">        .height = 100,</span><br><span class="line">        .zoom = 0.234</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;ID: &#123;s&#125;\n&quot;, .&#123;pc.id&#125;);  // ID: BAX1</span><br><span class="line">    std.debug.print(&quot;Size: &#123;d&#125;x&#123;d&#125; (zoom: &#123;d:.2&#125;)\n&quot;,</span><br><span class="line">        .&#123;pc.width, pc.height, pc.zoom&#125;);  // Size: 200x100 (zoom: 0.23)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Zig 中，结构体也可以具有方法，所以当我们讨论函数时，下面将展示一个示例。</p><p>Zig 联合体类似于结构体，但一次只能有一个活动字段。看下面的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const ActionResult = union &#123;</span><br><span class="line">        code_int: u8,</span><br><span class="line">        code_float: f32</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const ar1 = ActionResult &#123; .code_int = 200 &#125;;</span><br><span class="line">    const ar2 = ActionResult &#123; .code_float = 200.13 &#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;code1 = &#123;d&#125;\n&quot;, .&#123;ar1.code_int&#125;);  // code1 = 200</span><br><span class="line">    std.debug.print(&quot;code2 = &#123;d:.2&#125;\n&quot;, .&#123;ar2.code_float&#125;);  // code2 = 200.13</span><br><span class="line">    // std.debug.print(&quot;code2 = &#123;d:.2&#125;\n&quot;, .&#123;ar2.code_int&#125;);  // 错误！</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="9"><li>使用控制结构</li></ol><p>每种编程语言通常都提供控制结构来处理程序的逻辑流程。Zig 支持所有常见的控制结构，如 if、switch、for 等。</p><p>看看以下 if…else 语句的示例代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var score: u8 = 100;</span><br><span class="line"></span><br><span class="line">    if(score &gt;= 90) &#123;</span><br><span class="line">        std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">        std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;&quot;*&quot; ** 10&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    else if(score &gt;= 50) &#123;</span><br><span class="line">        std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        std.debug.print(&quot;Try again...\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是 switch 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var score: u8 = 88;</span><br><span class="line"></span><br><span class="line">    switch(score) &#123;</span><br><span class="line">        90...100 =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">            std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;&quot;*&quot; ** 10&#125;);</span><br><span class="line">        &#125;,</span><br><span class="line">        50...89 =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">        &#125;,</span><br><span class="line">        else =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Try again...\n&quot;, .&#123;&#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是while 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: u8 = 0;</span><br><span class="line">    while(x &lt; 11) &#123;</span><br><span class="line">        std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">        x += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是for 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const A = [_]u8 &#123;2, 4, 6, 8&#125;;</span><br><span class="line"></span><br><span class="line">    for (A) |n| &#123;</span><br><span class="line">        std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;n&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="10"><li>函数</li></ol><p>函数通过允许我们使用可调用标识符来命名每个代码段来帮助我们创建可重用的代码段。我们已经使用了main 来启动我们的应用程序，让我们创建更多函数并进一步学习函数。</p><p>下面是一个简单的函数，它返回两个整数的总和：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn add(a: i8, b: i8) i8 &#123;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const a: i8 = 10;</span><br><span class="line">    const b: i8 = -2;</span><br><span class="line">    const c = add(a, b);</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125; + &#123;d&#125; = &#123;d&#125;\n&quot;, .&#123;a, b, c&#125;); // 10 + -2 = 8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归也是 Zig 提供的一种编程功能，与许多其他通用语言一样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn fibonacci(n: u32) u32 &#123;</span><br><span class="line">    if(n == 0 or n == 1) return n;</span><br><span class="line">    return fibonacci(n - 1) + fibonacci(n - 2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;fibonacci(2)&#125;);   // 1</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;fibonacci(12)&#125;);  // 144</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与 Go 一样，Zig 允许您在结构体中创建方法，并将它们用作 OOP 方法，如下例所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Rectangle = struct &#123;</span><br><span class="line">    width: u32,</span><br><span class="line">    height: u32,</span><br><span class="line">    fn calcArea(self: *Rectangle) u32 &#123;</span><br><span class="line">        return self.width * self.height;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var rect = Rectangle &#123; .width = 200, .height = 25 &#125;;</span><br><span class="line">    var area = rect.calcArea();</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;area&#125;);   // 5000</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="11"><li>指针</li></ol><p>Zig 作为硬件友好的语言，其支持类似 C 的指针。看看下面的基本整数指针：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: u8 = 10;</span><br><span class="line">    var ptr_x = &amp;x;</span><br><span class="line">    ptr_x.* = 12;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;x&#125;);   // 12</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;ptr_x&#125;);   // u8@...mem_address</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_x)&#125;);   // *u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，C&#x2F;C++ 开发人员需要注意，我们使用 ptr.* 语法对指针进行解引用，而不是像在 C&#x2F;C++ 中那样使用 *ptr。指向数组元素和指向整个数组的指针也能按预期工作，如下所示的代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var A = [_]u8 &#123;2, 5, 6, 1, 1&#125;;</span><br><span class="line">    var ptr_x = &amp;A[1];</span><br><span class="line">    ptr_x.* = 12;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;A[1]&#125;);            // 12</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;ptr_x&#125;);           // u8@...mem_address</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_x)&#125;);   // *u8</span><br><span class="line"></span><br><span class="line">    var ptr_y = &amp;A;</span><br><span class="line">    ptr_y[2] = 11;</span><br><span class="line">    std.debug.print(&quot;&#123;any&#125;\n&quot;, .&#123;A&#125;);             // &#123; 2, 12, 11, 1, 1 &#125;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_y)&#125;);   // *[5]u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="12"><li>高级语言特性</li></ol><p>以下是您应该了解的一些 Zig 高级语言特性的摘要：</p><ol><li>通过分配器和 defer 关键字</li><li>支持手动内存管理</li><li>使用简单的语法支持泛型</li><li>提供高效的关键字（async、suspend 和 resume）进行现代异步编程（后续版本已经撤回该特性）</li><li>提供自动类型转换和手动类型转换，使用内置的 @as Zig 的 C-interop 允许您调用 C API。使用 -lc 标志进行以下运行以链接到 libc：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const c = @cImport(&#123;</span><br><span class="line">    @cInclude(&quot;stdio.h&quot;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const char_count = c.printf(&quot;Hello %s\n&quot;, &quot;C...&quot;); // Hello C...</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(char_count)&#125;); // c_int</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;char_count&#125;); // 11</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过关注官方 Zig 新闻页面，及时了解最新功能和高级概念。</p><ol start="13"><li>Zig 中的标准库 API</li></ol><p>我们已经讨论了前面示例中的 Zig 语言语法和特性，但这些概念不足以开发通用程序 —— 我们经常需要使用复杂的数据结构、数学公式和操作系统级别的 API。Zig 通过 std 命名空间提供了一个功能齐全但又精简的标准库。</p><p>我们将编写一个简单的 CLI 程序来学习几个 Zig 标准库的特性。将以下代码添加到一个新的 Zig 文件中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const stdout = std.io.getStdOut().writer();</span><br><span class="line"></span><br><span class="line">fn print_help() !void &#123;</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot; , .&#123;&quot;-&quot; ** 25&#125;);</span><br><span class="line">    try stdout.print(&quot;0: 退出\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;1: 显示帮助\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;2: 打印 Node.js 版本\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot; , .&#123;&quot;-&quot; ** 25&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn print_node_version() !void &#123;</span><br><span class="line">    const cmd_res = try std.ChildProcess.exec(.&#123;</span><br><span class="line">        .allocator = std.heap.page_allocator,</span><br><span class="line">        .argv = &amp;[_][]const u8&#123;</span><br><span class="line">            &quot;node&quot;,</span><br><span class="line">            &quot;--version&quot;,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;);</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot;, .&#123;cmd_res.stdout&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn ask_action() !i64 &#123;</span><br><span class="line">    const stdin = std.io.getStdIn().reader();</span><br><span class="line">    var buf: [10]u8 = undefined;</span><br><span class="line"></span><br><span class="line">    try stdout.print(&quot;输入操作：&quot;, .&#123;&#125;);</span><br><span class="line"></span><br><span class="line">    if (try stdin.readUntilDelimiterOrEof(buf[0..], &#x27;\n&#x27;)) |user_input| &#123;</span><br><span class="line">        return std.fmt.parseInt(i64, user_input, 10);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        return @as(i64, 0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    try print_help();</span><br><span class="line">    while(true) &#123;</span><br><span class="line">        const action = ask_action() catch -1;</span><br><span class="line">        switch(action) &#123;</span><br><span class="line">            0 =&gt; &#123;</span><br><span class="line">                std.debug.print(&quot;再见！\n&quot;, .&#123;&#125;);</span><br><span class="line">                break;</span><br><span class="line">            &#125;,</span><br><span class="line">            1 =&gt; &#123;</span><br><span class="line">                try print_help();</span><br><span class="line">            &#125;,</span><br><span class="line">            2 =&gt; &#123;</span><br><span class="line">                try print_node_version();</span><br><span class="line">            &#125;,</span><br><span class="line">            else =&gt; &#123;</span><br><span class="line">                std.debug.print(&quot;无效的操作：&#123;d&#125;\n&quot;, .&#123;action&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个演示的 CLI 支持三种整数操作：</p><ul><li>0：退出程序</li><li>1：显示程序帮助</li><li>2：通过 Zig 子进程 API 打印当前 Node.js 版本</li></ul><p>在这里，我们使用了一些错误处理基础知识以及标准库的 io 命名空间和 ChildProcess 结构。您可以在官方标准库参考文档中查看所有可用的命名空间和结构。</p><h2 id="开源的-Zig-生态系统"><a href="#开源的-Zig-生态系统" class="headerlink" title="开源的 Zig 生态系统"></a>开源的 Zig 生态系统</h2><p>Zig 是一种新的语言，因此开源软件包的可用性和开发者资源仍在不断增长。请查看以下流行的开源 Zig 库：</p><ul><li>zigzap&#x2F;zap：用于构建 Web 后端的微型框架</li><li>JakubSzark&#x2F;zig-string：用于 Zig 的字符串库</li><li>kooparse&#x2F;zalgebra：游戏和实时图形的线性代数库</li><li>zigimg&#x2F;zigimg：用于读写不同图像格式的 Zig 库</li><li>ziglibs&#x2F;ini：用于 Zig 的简单 INI 解析器 您还可以从 awesome-zig 存储库中了解更多与 Zig 有关的开发内容。</li></ul><h2 id="Zig-vs-C-vs-Rust"><a href="#Zig-vs-C-vs-Rust" class="headerlink" title="Zig vs. C vs. Rust"></a>Zig vs. C vs. Rust</h2><table><thead><tr><th>比较因素</th><th>Zig</th><th>C</th><th>Rust</th></tr></thead><tbody><tr><td>语言影响</td><td>受Rust、C和Python影响</td><td>B</td><td>受函数式语言和C++影响</td></tr><tr><td>首次发布</td><td>2017年（0.1.0）</td><td>1972年</td><td>2012年（0.1.0）</td></tr><tr><td>语言语法复杂度</td><td>极简</td><td>极简</td><td>复杂</td></tr><tr><td>主要范式</td><td>过程式</td><td>过程式</td><td>函数式和过程式</td></tr><tr><td>内存管理</td><td>手动（通过分配器）</td><td>手动（通过malloc等）</td><td>手动（但改进了以避免安全问题）</td></tr><tr><td>性能和二进制文件大小</td><td>生成超轻量、速度更快的二进制文件，无专用运行时（可选地链接libc）</td><td>生成超轻量、速度更快的二进制文件，具有一个称为C运行时的最小运行时</td><td>生成更快的二进制文件，没有专用运行时，二进制大小问题可以通过技巧解决</td></tr><tr><td>第三方库生态系统</td><td>作为新语言仍在发展中</td><td>成熟的库生态系统，但没有标准化的库注册和集成方法（取决于构建工具）</td><td>在官方包注册中心Crates中有成熟的库生态系统</td></tr><tr><td>开发者资源</td><td>良好，但除了官方文档外只有少量资源</td><td>良好</td><td>良好</td></tr><tr><td>标准库功能</td><td>良好</td><td>仅具有低级API—需要第三方库或编写特定于平台的代码</td><td>良好</td></tr><tr><td>工具链</td><td>功能齐全</td><td>仅编译器、链接器和汇编器—需要单独的工具来运行测试、进行高级构建、使用包等</td><td>功能齐全</td></tr><tr><td>C语言互操作性</td><td>无需FFI（外部函数接口）即可直接导入</td><td>不适用</td><td>通过Rust FFI</td></tr></tbody></table><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本教程中，我们学习了Zig编程语言开发背后的概念、目标和设计技术。通过测试通用的、通用的编程知识来学习Zig语言，这些知识可以用来构建现代计算机程序。</p><p>Zig仍然是一种新语言，ZSF仍在定期实现和测试更多功能。学习Zig是一个很好的决定，因为它作为一种更好的C语言，有着光明的前景。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ziglang-简明教程&quot;&gt;&lt;a href=&quot;#Ziglang-简明教程&quot; class=&quot;headerlink&quot; title=&quot;Ziglang 简明教程&quot;&gt;&lt;/a&gt;Ziglang 简明教程&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;译：&lt;a href=&quot;https:</summary>
      
    
    
    
    
    <category term="ziglang" scheme="https://zoues.com/tags/ziglang/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Resource Allocation (DRA) for GPUs in Kubernetes</title>
    <link href="https://zoues.com/posts/11831eff/"/>
    <id>https://zoues.com/posts/11831eff/</id>
    <published>2024-07-20T05:29:19.000Z</published>
    <updated>2024-07-20T06:36:02.419Z</updated>
    
    <content type="html"><![CDATA[<p>Original: <a href="https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file">https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Dynamic Resource Allocation (DRA)</a> is an upcoming Kubernetes feature that puts resource scheduling in the hands of 3rd-party developers. From an end-user’s perspective, It moves away from the limited “countable” interface for requesting access to resources  (e.g. “<strong>nvidia.com&#x2F;gpu: 2</strong>“), providing an API more akin to that of persistent volumes. Under the hood it uses <a href="https://github.com/container-orchestrated-devices/container-device-interface">CDI</a> to do its device injection.</p><p>NVIDIA has been working with Intel for the past 2 years on this feature and we are excited to see it finally gain traction within the community. DRA has been <a href="https://github.com/kubernetes/kubernetes/pull/111023">merged</a> as an alpha feature for <a href="https://www.kubernetes.dev/resources/release/#kubernetes-126">Kubernetes 1.26</a> (released in December 2022). Its graduation to beta and GA will follow soon after.</p><p>In the context of GPUs, this unlocks a host of new features without the need for awkward solutions shoehorned on top of the existing device plugin API.</p><p>These features include:</p><ul><li>Controlled GPU Sharing (both within a pod and across pods)</li><li>Multiple GPU models per node (e.g. T4 and A100)</li><li>Specifying arbitrary constraints for a GPU (min&#x2F;max memory, device model, etc.)</li><li>Natural support for MPS</li><li>Dynamic allocation of MIG devices</li><li>Dynamic repurposing of a GPU from full to MIG mode</li><li>Dynamic repurposing of a GPU for use as Passthrough vs. vGPU</li><li>… the list goes on …</li></ul><p>A reference implementation of our <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> is already available and a demo showcasing a subset of the features listed above can be found <a href="https://drive.google.com/file/d/1iLg2FEAEilb1dcI27TnB19VYtbcvgKhS/view?usp=sharing">here</a>.</p><h2 id="User-Facing-API"><a href="#User-Facing-API" class="headerlink" title="User-Facing API"></a>User-Facing API</h2><p><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Dynamic Resource Allocation (DRA)</a> is a generalization of the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a> API for generic resources. As such, it allows one to separate the <em>declaration</em> of a resource to be consumed, from its actual consumption. This allows one to move away from the limited  “countable” API  provided by device-plugins today, to something much more flexible in terms of controlling which resources are consumed (and where).</p><p>Using our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> as an example, the table below shows the difference between how one requests access to two <strong>gpus</strong> under the existing device plugin model vs. DRA.</p><table><tr><th>Existing device plugin</th><th>DRA resource driver for GPUs</th></tr><tr><td><pre>apiVersion: v1kind: Podmetadata:  name: podspec:  containers:  - name: ctr    image: nvidia/cuda    command: ["nvidia-smi", "-L"]    resources:      limits:        nvidia.com/gpu: 2</pre></td><td><pre>apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimTemplatemetadata:  name: gpu-templatespec:  spec:    resourceClassName: gpu.nvidia.com<p>––<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu0</li><li>name: gpu1<br>resourceClaims:</li></ul></li><li>name: gpu0<br>source:<br>resourceClaimTemplate: gpu-template</li><li>name: gpu1<br>source:<br>resourceClaimTemplate: gpu-template<br></pre></td></li></ul></tr></table><p>Moreover, <strong>ResourceClaims</strong> can be annotated with a set of parameters defined by the developer of the DRA resource driver for a given <strong>ResourceClass</strong>. These parameters allow users to attach additional constraints to their resource requests.</p><p>For example, our reference DRA resource driver for GPUs defines the following two claim parameter objects for use with claims against the <strong>gpu.nvidia.com</strong> resource class:</p><table><tr><th>GPU Sharing within a Pod</th><th>GPU Sharing across Pods</th></tr><tr><td><pre>---apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimTemplatemetadata:  name: gpu-templatespec:  spec:    resourceClassName: gpu.nvidia.com<p>––<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod<br>spec:<br>containers:</p><ul><li>name: ctr0<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu</li></ul></li><li>name: ctr1<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimTemplate: gpu-template<br></pre></td></li></ul><td><pre>–--apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimmetadata:  name: shared-gpuspec:  resourceClassName: gpu.nvidia.com<hr><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod0<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimName: shared-gpu</li></ul><hr><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod1<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimName: shared-gpu<br></pre></td></li></ul></tr></table><p>Moreover, <strong>ResourceClaims</strong> can be annotated with a set of parameters defined by the developer of the DRA resource driver for a given <strong>ResourceClass</strong>. These parameters allow users to attach additional constraints to their resource requests.</p><p>For example, our reference DRA resource driver for GPUs defines the following two claim parameter objects for use with claims against the <strong>gpu.nvidia.com</strong> resource class:</p><table><tr><th></th><th></th></tr><tr><td><pre>apiVersion: gpu.resource.nvidia.com/v1alpha1kind: GpuClaimParametersmetadata:  name: ...spec:  count: ...  migEnabled: ...</pre></td><td><pre>apiVersion: gpu.resource.nvidia.com/v1alpha1kind: MigDeviceClaimParametersmetadata:  name: ...spec:  profile: ...  gpuClaimName: ...</pre></td></tr></table><p>The <strong>GpuClaimParameters</strong> object gives users the ability to request more than one GPU from a single resource claim (via the <strong>count</strong> parameter) as well as specify whether they want to receive GPUs that have MIG mode enabled on them or not (via the <strong>migEnabled</strong> parameter) .</p><p>The <strong>MigDeviceClaimParameters</strong> object gives users the ability to specify the profile of a MIG device they would like access to (via the <strong>profile</strong> parameter) as well as an optional reference to the specific GPU they would like their MIG device to be allocated on (via the <strong>gpuClaimName</strong> parameter).</p><p><em><strong>Note:</strong> both of these claim parameter objects are reference implementations, and we plan to extend &#x2F; replace them before they are released. Any feedback on what you would like to see here is greatly appreciated.</em></p><p>An example of using <strong>GpuClaimParameters</strong> to request eight GPUs from a single resource claim can be seen below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: gpu.resource.nvidia.com/v1alpha1</span><br><span class="line">kind: GpuClaimParameters</span><br><span class="line">metadata:</span><br><span class="line">  name: eight-gpus</span><br><span class="line">spec:</span><br><span class="line">  count: 8</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: resource.k8s.io/v1alpha1</span><br><span class="line">kind: ResourceClaimTemplate</span><br><span class="line">metadata:</span><br><span class="line">  name: eight-gpus-template</span><br><span class="line">spec:</span><br><span class="line">  spec:</span><br><span class="line">    resourceClassName: gpu.nvidia.com</span><br><span class="line">    parametersRef:</span><br><span class="line">      apiGroup: gpu.resource.nvidia.com</span><br><span class="line">      kind: GpuClaimParameters</span><br><span class="line">      name: eight-gpus</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: ctr</span><br><span class="line">    image: nvidia/cuda</span><br><span class="line">    command: [&quot;nvidia-smi&quot; &quot;-L&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      claims:</span><br><span class="line">      - eight-gpus</span><br><span class="line">  resourceClaims:</span><br><span class="line">  - name: eight-gpus</span><br><span class="line">    source:</span><br><span class="line">      resourceClaimTemplate: eight-gpus-template</span><br></pre></td></tr></table></figure><p>A more complex example involving both a GpuClaimParameters object and a MigDeviceClaimParameters object can be seen here:</p><p>In this example, we create a single pod with two containers, each of which wants access to its own 3g.40gb MIG device.</p><p><img src="https://pic.imgdb.cn/item/669b5569d9c307b7e917a5e3.png"></p><p>To ensure that the two MIG devices ultimately come from the same underlying GPU, we first create a <strong>GpuClaimParameters</strong> object requesting access to a MIG enabled GPU. We call this claim parameters object <strong>mig-enabled-gpu</strong>.<br>We then create a <strong>ResourceClaimTemplate</strong> also called <strong>mig-enabled-gpu</strong>, which binds the <strong>gpu.nvidia.com</strong> resource class to the <strong>mig-enabled-gpu</strong> claim parameters object.<br>Next we create a <strong>MigDeviceClaimParameters</strong> object specifying the <strong>3g.40gb</strong> profile. This object also includes a forward reference to the (yet-to-be-created) resource claim of the MIG enabled GPU on which this MIG device should be created (<strong>shared-gpu</strong>). Note this is the name of the resource claim itself, <em>not</em> the claim parameters object we called <strong>mig-enabled-gpu</strong>. We call this new claim parameters object <strong>mig-3g.40gb</strong>.<br>We then create a <strong>ResourceClaimTemplate</strong> also called <strong>mig-3g.40gb</strong>, which binds the <strong>gpu.nvidia.com</strong> resource class to the <strong>mig-3g.40gb</strong> claim parameters object.<br>Next we create the actual resource claims themselves inside the pod spec. One resource claim called <strong>shared-gpu</strong>, which references the <strong>mig-enabled-gpu</strong> resource claim template, as well as two other resource claims, each referencing the <strong>mig-3g.40gb</strong> resource claim template. These two resource claims are called <strong>mig-3g-0</strong> and <strong>mig-3g-1</strong>, respectively.<br>Finally, we reference each of these resource claims in the <strong>resources.claims</strong> sections of our two containers. Both containers refer to the same underlying <strong>shared-gpu</strong> claim, with each container pointing to one of <strong>mig-3g-0</strong> or <strong>mig-3g-1</strong>, respectively.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">gpu.resource.nvidia.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">GpuClaimParameters</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">migEnabled:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">resource.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceClaimTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">resourceClassName:</span> <span class="string">gpu.nvidia.com</span></span><br><span class="line">    <span class="attr">parametersRef:</span></span><br><span class="line">      <span class="attr">apiGroup:</span> <span class="string">gpu.resource.nvidia.com</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">GpuClaimParameters</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">gpu.resource.nvidia.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MigDeviceClaimParameters</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">profile:</span> <span class="string">3g.40gb</span></span><br><span class="line">  <span class="attr">gpuClaimName:</span> <span class="string">shared-gpu</span></span><br><span class="line"></span><br><span class="line"><span class="string">–--</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">resource.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceClaimTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">resourceClassName:</span> <span class="string">gpu.nvidia.com</span></span><br><span class="line">    <span class="attr">parametersRef:</span></span><br><span class="line">      <span class="attr">apiGroup:</span> <span class="string">gpu.resource.nvidia.com</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">MigDeviceClaimParameters</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr0</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nvidia/cuda</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;nvidia-smi&quot;</span> <span class="string">&quot;-L&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">claims:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-0</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr1</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nvidia/cuda</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;nvidia-smi&quot;</span> <span class="string">&quot;-L&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">claims:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-1</span></span><br><span class="line">  <span class="attr">resourceClaims:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-0</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-1</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-3g.40gb</span></span><br></pre></td></tr></table></figure><p>As mentioned previously, <strong>GpuClaimParameters</strong> and <strong>MigDeviceClaimParameters</strong> are just reference specifications, and we plan to iterate on them further before they get released. Any feedback on how you would like to see these evolve would be greatly appreciated.<br>In the following section, we discuss the details of the DRA resource driver architecture and how it interacts with Kubernetes to make the user-facing API described above possible.</p><h2 id="DRA-Resource-Driver-Architecture"><a href="#DRA-Resource-Driver-Architecture" class="headerlink" title="DRA Resource Driver Architecture"></a>DRA Resource Driver Architecture</h2><p>At a high-level, a DRA resource driver is responsible for:</p><ul><li>Defining a ResourceClass associated with a specific type of resource (e.g. gpu.nvidia.com)</li><li>Processing any class parameter objects associated with this ResourceClass</li><li>Watching for incoming ResourceClaims that reference this ResourceClass</li><li>Processing any claim parameter objects associated with this ResourceClaim</li><li>Coordinating with the Kubernetes scheduler to find a node where a given ResourceClaim should be allocated</li><li>Allocating the ResourceClaim on that node</li><li>Cleaning up any allocated ResourceClaims once they get deleted</li></ul><p>To accomplish this, DRA resource drivers consists of two separate-but-coordinating components:</p><ul><li>A centralized controller running with high-availability</li><li>A node-local kubelet plugin running as a daemonset</li></ul><p><img src="https://pic.imgdb.cn/item/669b567fd9c307b7e9197041.png"></p><p>The centralized controller serves to:</p><ol><li>Coordinate with the K8s scheduler to decide which nodes an incoming <strong>ResourceClaim</strong> can be serviced on</li><li>Perform the actual <strong>ResourceClaim</strong> allocation once the scheduler picks a node to allocate it on</li><li>Perform the deallocation of a <strong>ResourceClaim</strong> once it has been deleted</li></ol><p>The node-local <strong>kubelet</strong> plugin serves to:</p><ol><li>Advertise any node-local state required by the centralized controller to make its allocation decisions</li><li>Perform any node-local operations required as part of allocating a <strong>ResourceClaim</strong> on a node</li><li>Pass the set of devices associated with an allocated <strong>ResourceClaim</strong> to the kubelet so it can ultimately be forwarded to the underlying container runtime</li><li>Perform any node-local operations required as part of freeing a <strong>ResourceClaim</strong> on a node</li></ol><p>To help illustrate how these responsibilities are carried out by each component, the following section walks through the process of deploying a DRA resource driver and then allocating a <strong>ResourceClaim</strong> associated with a newly created pod.</p><h2 id="Allocating-a-ResourceClaim"><a href="#Allocating-a-ResourceClaim" class="headerlink" title="Allocating a ResourceClaim"></a>Allocating a ResourceClaim</h2><p>The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Kubernetes Enhancement Proposal for DRA</a>  is the definitive source for all of the details about how the internals of DRA work. It defines a number of modes of operation, including delayed vs. immediate allocation, shared vs. non-shared resource claims, etc. As the names suggest, immediate allocation occurs as soon as a resource claim is created (i.e. there is no need to wait for a consumer to begin the allocation process), whereas delayed allocation does not occur until a pod that references the resource claim is first created. Likewise, shared resource claims can have multiple pods consuming them, whereas non-shared resource claims can only have one.</p><p>In this section we walk through the process of allocating a shared resource claim with delayed allocation (which is the default). The steps are broken down into phases, with a diagram showing the steps from each phase in action.  The steps themselves are annotated as “1.x” from Phase 1, “2.x”  from Phase 2, etc.</p><p>Phase 1 - Setup:</p><ol><li>Admin registers a <strong>ResourceClass</strong> pointing to a specific DRA resource driver as its owner</li><li>Admin deploys DRA resource driver in the cluster</li><li>Each DRA kubelet plugin begins advertising its node-local state for the centralized controller to pick up on</li></ol><p>Phase 2 - Pod Creation:</p><ol><li>User creates a <strong>ResourceClaim</strong> referencing a registered ResourceClass</li><li>User submits a pod to the API server referencing the <strong>ResourceClaim</strong> in one of its containers</li></ol><p><img src="https://pic.imgdb.cn/item/669b5833d9c307b7e91b7264.png"></p><p>Phase 3 - Node Selection:</p><ol><li>Scheduler picks up a pod from the API server and begins to schedule it on a node</li><li>Scheduler sees the pod’s <strong>ResourceClaim</strong> and its <strong>ResourceClass</strong> pointing to a specific DRA resource driver</li><li>Scheduler provides a list of potential nodes where it is considering scheduling the pod that has a reference to the <strong>ResourceClaim</strong>.  It does this through a special <strong>PodScheduling</strong> object in the API server</li><li>DRA resource driver picks up the <strong>PodScheduling</strong> object</li><li>DRA resource driver narrows down the list of potential nodes to just those where the <strong>ResourceClaim</strong> could possibly be allocated. It writes this back to the <strong>PodScheduling</strong> object</li></ol><p><img src="https://pic.imgdb.cn/item/669b5901d9c307b7e91c91fd.png"></p><p>Phase 4 - Claim Allocation</p><ol><li>Scheduler considers other scheduling constraints in relation to each of the nodes in the narrowed down list</li><li>Scheduler picks a node and sets a field in the  <strong>ResourceClaim</strong> with a reference to it</li><li>DRA resource driver picks up the selected node from the <strong>ResourceClaim</strong></li><li>DRA resource driver allocates the <strong>ResourceClaim</strong> for use on the node</li><li>DRA resource driver  marks the allocation as complete in the <strong>ResourceClaim</strong> object</li><li>Scheduler picks up the allocation completion from the <strong>ResourceClaim</strong> object</li><li>Scheduler schedules the pod on the node</li><li>Scheduler writes the scheduled node back to the Pod object</li></ol><p><img src="https://pic.imgdb.cn/item/669b597cd9c307b7e91d0a1b.png"></p><p>Phase 5 - Container Start:</p><ol><li>Kubelet picks up the pod from the API server and begins creating its containers</li><li>Kubelet calls out to DRA’s kubelet plugin to get the list of CDI devices associated with the <strong>ResourceClaim</strong></li><li>Kubelet passes the CDI devices to the container-runtime via CRI</li><li>Container runtime starts the container with access to the devices associated with the <strong>ResourceClaim</strong></li></ol><p><img src="https://pic.imgdb.cn/item/669b59bfd9c307b7e91d50c5.png"></p><h2 id="Writing-your-own-DRA-resource-driver"><a href="#Writing-your-own-DRA-resource-driver" class="headerlink" title="Writing your own DRA resource driver"></a>Writing your own DRA resource driver</h2><p>As mentioned in the previous section, DRA resource drivers consist of two separate-but-coordinating components: a centralized controller and a daemonset of node-local kubelet plugins. Most of the work required by the centralized controller to coordinate with the scheduler can be handled by boilerplate code. Only the business logic required to actually allocate ResourceClaims against the ResourceClasses owned by the resource driver needs to be customized.  As such, the following package is provide by Kubernetes to include APIs for invoking this boilerplate code as well as a Driver interface that one can implement to provide their custom business logic:</p><pre><code>k8s.io/dynamic-resource-allocation/controller</code></pre><p>Likewise, boilerplate code can be used to register the kubelet plugin with the kubelet, as well as start a gRPC server to implement DRA’s kubelet plugin API. The following package is provided for this purpose:</p><p><code>k8s.io/dynamic-resource-allocation/kubeletplugin</code></p><p>The set of functions defined by the controller’s Driver interface can be seen below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">type Driver interface &#123;</span><br><span class="line">GetClassParameters()</span><br><span class="line">GetClaimParameters()</span><br><span class="line">Allocate()</span><br><span class="line">Deallocate()</span><br><span class="line">UnsuitableNodes() </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Likewise, the set of functions defined by the gRPC API for the node-local kubelet plugin are:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">service Node &#123;</span><br><span class="line">  rpc NodePrepareResource (NodePrepareResourceRequest)</span><br><span class="line">    returns (NodePrepareResourceResponse) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  rpc NodeUnprepareResource (NodeUnprepareResourceRequest)</span><br><span class="line">    returns (NodeUnprepareResourceResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Examples of implementing these can be found in our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a>.</p><h2 id="Status-Future-Work"><a href="#Status-Future-Work" class="headerlink" title="Status &amp; Future Work"></a>Status &amp; Future Work</h2><p>mentioned previously, the alpha release of DRA has been <a href="https://github.com/kubernetes/kubernetes/pull/111023">merged</a> as part of <a href="https://www.kubernetes.dev/resources/release/#kubernetes-126">Kubernetes 1.26</a> in December 2022. Its graduation to beta and GA will follow soon after. Our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> is feature-complete in terms of supporting all of the APIs required by DRA, but there is still quite a bit of room for improvement. We plan to continue iterating on this resource driver with an official release to coincide with the beta release of DRA itself. These are still the early days of DRA, and we are excited to see what other technologies this new feature helps unlock.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file">https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Original: &lt;a href=&quot;https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file&quot;&gt;https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-fi</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Container Interference Detection and Mitigation:Part 1</title>
    <link href="https://zoues.com/posts/48bcc63c/"/>
    <id>https://zoues.com/posts/48bcc63c/</id>
    <published>2024-07-19T23:48:30.000Z</published>
    <updated>2024-07-20T11:26:17.419Z</updated>
    
    <content type="html"><![CDATA[<p>From the perspective of resource allocation, the cluster appears relatively balanced. However, in terms of actual load, many hotspots have already emerged. This can lead to competition for shared resources between applications, causing performance interference in core business processes. As a result, application response times often exhibit long-tail behavior, leading to a decline in service quality and an increased likelihood of failures. This inter-application resource contention and performance interference (such as the “noisy neighbor” phenomenon) make application scheduling and resource management extremely complex. Therefore, it is crucial to consider how to reduce performance interference between applications to ensure business stability.</p><p><img src="https://pic.imgdb.cn/item/6691e507d9c307b7e9943c62.png"></p><p>The “noisy neighbor” problem is a common phenomenon in cloud infrastructure, referring to a situation where the resources required by an application are heavily consumed by other applications on the same compute node, leading to degraded performance for the application, such as increased latency. This issue is particularly troublesome for business-critical applications but is often difficult to identify.</p><p>The root cause lies in the lack of strict isolation policies during resource sharing. From a resource perspective, this includes CPU, memory (L1&#x2F;L2&#x2F;L3 cache), network, and block I&#x2F;O. From a hardware topology perspective, it involves CPU caches, memory bandwidth, and more. CPU cores are tightly coupled with other shared resources such as the last level cache (LLC) and memory bandwidth. If there are no effective isolation strategies in place when allocating shared resources, problems arise. For instance, sibling hyperthreads share the same physical CPU core, workloads on different physical CPU cores share the same last level cache, memory controller, and I&#x2F;O bus bandwidth, and even workloads on different physical CPU sockets share CPU interconnect bandwidth, the same storage devices, or I&#x2F;O bus. In such cases, the “noisy neighbor” often requires more resources and takes more time to complete the same task. This can lead to critical applications not getting the resources they need, causing performance degradation and stalls.</p><p>Kubernetes offers a CPU manager and device plugin manager for hardware resource allocation, such as CPUs and devices (SR-IOV, GPUs). Currently, Kubernetes also provides a topology manager to achieve NUMA topology awareness, coordinate resources, and ensure optimal performance for critical workloads. However, these features do not directly address the “noisy neighbor” problem.</p><p>To address this issue, the following optimization strategies can be considered:</p><ol><li><strong>Resource Requests and Limits</strong>: Set reasonable resource requests and limits for each Pod in Kubernetes to ensure that applications can obtain sufficient resources during contention.</li><li><strong>Affinity and Anti-affinity Rules</strong>: Use Kubernetes affinity and anti-affinity rules to deploy critical applications separately from other high resource-consuming applications to reduce resource contention.</li><li><strong>Isolation Policies</strong>: Utilize Kubernetes CPU isolation policies to ensure that critical applications have exclusive access to specific CPU cores, reducing competition for shared resources.</li><li><strong>Priority and Preemption</strong>: Set the priority of applications to ensure that critical applications can obtain resources first and preempt resources from lower-priority applications if necessary.</li><li><strong>NUMA-aware Scheduling</strong>: Through Kubernetes topology manager, ensure that critical applications are allocated resources within NUMA nodes to maximize the efficiency of local resource use and reduce latency from cross-node resource access.</li><li><strong>Dedicated Hardware</strong>: For high-performance applications, consider using dedicated hardware (such as GPUs, FPGAs) or isolated physical nodes to avoid sharing resources with other applications.</li><li><strong>Monitoring and Adjustment</strong>: Continuously monitor the resource usage and performance of applications, and adjust resource allocation strategies promptly to ensure the stable operation of critical applications.</li></ol><p>By implementing these optimization strategies, the “noisy neighbor” problem can be largely mitigated, ensuring the performance and stability of critical applications. Isolation policies, priority and preemption, and NUMA-aware scheduling are resource QoS optimization strategies for application scenarios. So, how do we measure performance interference?</p><blockquote><p>NOTE: Mixed deployment, here “mixed” essentially means “distinguishing priorities”. Narrowly, it can be simply understood as “online + offline” (offline) mixed deployment, broadly, it can be extended to a wider range of applications: multi-priority business mixed deployment.</p></blockquote><hr><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><h3 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h3><h4 id="a-CPI"><a href="#a-CPI" class="headerlink" title="a. CPI"></a>a. CPI</h4><p>CPI stands for Cycles Per Instruction, which means the <strong>number of cycles per instruction</strong>. Additionally, in some contexts, you may also encounter IPC, which stands for Instructions Per Cycle, meaning the <strong>number of instructions per cycle</strong>.</p><blockquote><p>The smaller the CPI value, the higher the instruction execution efficiency of the computer system.</p></blockquote><p>The relationship between CPI and IPC is: CPI &#x3D; 1 &#x2F; IPC</p><p><strong>In the context of single CPU program execution performance</strong>, it can actually be expressed as:</p><p><img src="https://pic.imgdb.cn/item/6691e5acd9c307b7e994e785.png" alt="img"></p><p>Due to the limitations of silicon material and manufacturing processes, increasing the processor’s clock speed has reached a bottleneck. Therefore, improving program performance mainly depends on two variables: Instruction Count and CPI.</p><p>Typically, by examining the CPI value, we can roughly determine whether a compute-intensive task is CPU-bound or memory-bound:</p><ul><li>CPI less than 1: The program is usually CPU-bound.</li><li>CPI greater than 1: The program is usually memory-bound.</li></ul><p>This is just a rough estimation method. Different types of tasks may have different CPI value ranges, so it is necessary to judge based on actual circumstances. Additionally, other factors such as memory size, bandwidth, and CPU cache must be considered for a more comprehensive assessment of the task type.</p><p>A key criterion for evaluating the efficiency of a compute-intensive task is the CPU utilization during the program’s execution. Many people believe that high CPU utilization indicates that the program’s code is running intensely. In fact, high CPU utilization can also mean that the CPU is <strong>busy-waiting</strong> for some resources (not iowait), such as encountering bottlenecks when accessing memory.</p><p>Some compute-intensive tasks normally have a low CPI and good performance. The CPU utilization is very high. However, as system load increases and other tasks compete for system resources, the CPI of these compute tasks can significantly rise, leading to a drop in performance. At this time, CPU utilization might still appear high, but this high utilization actually reflects CPU busy-waiting and the effects of pipeline stalls.</p><p>Brendan Gregg pointed out in his blog “CPU Utilization is Wrong” that CPU utilization metrics need to be analyzed in conjunction with CPI&#x2F;IPC metrics.</p><p>By using <code>perf record</code> to generate a CPI flame graph, one can show the association between the program’s call stack and CPU occupancy, and also reveal which parts of the CPU occupancy represent actual effective runtime and which parts are due to CPU busy-waiting caused by some stalls.</p><p>This tool can generally help identify system resource bottlenecks and provide ways to mitigate these bottlenecks; for example, interference from cache thrashing between applications can be resolved by binding applications to different CPUs.</p><p>Application developers can improve program performance by optimizing relevant functions. For instance, by optimizing code to reduce cache misses, thereby lowering the application’s CPI and reducing performance issues caused by memory access stalls.</p><h4 id="b-LLC"><a href="#b-LLC" class="headerlink" title="b. LLC"></a>b. LLC</h4><p>Older CPUs had two levels of cache (L1 and L2), while newer CPUs have three levels of cache (L1, L2, and L3), as shown in the following diagram:</p><img src="https://pic2.zhimg.com/80/v2-f4551e163f574dfe4a98dc1847272e05_720w.webp" alt="img" style="zoom:50%;" /><ul><li><p>L1 cache is divided into two types: instruction cache and data cache. L2 and L3 caches do not differentiate between instructions and data.</p></li><li><p>L1 and L2 caches are located within each CPU core, while the L3 cache is shared among all CPU cores.</p></li><li><p>The closer the cache (L1, L2, L3) is to the CPU, the smaller and faster it is; the farther it is from the CPU, the slower it becomes.</p><p><img src="https://frankdenneman.nl/wp-content/uploads/2019/10/03-Rome-Chiplet.png" alt="img"></p></li></ul><p>The LLC (Last Level Cache) cache resources are crucial for job performance, and interference on these resources cannot be ignored. When multiple applications share the cache, they may experience mutual replacement issues. When LLC is ineffective, the execution time of memory access instructions can increase from 15ns to 70ns. Assuming a CPU clock speed of 3 GHz, this results in each memory access instruction consuming an additional 200 or more cycles to complete. To mitigate such interference, resource partitioning techniques can be employed to physically separate multiple jobs’ access to shared resources. The diagram above shows a classic multi-core architecture where multiple CPUs share the LLC. Jobs running on different CPUs may contend for LLC resources. The Intel Cascade Lake microarchitecture and AMD’s Rome chiplet reduce inter-core competition for LLC by providing each core with its own independent LLC. However, multiple applications running on the same CPU can still experience cache replacement issues when running in a mixed deployment, necessitating application-level cache partitioning techniques. To achieve application-level cache partitioning, Intel introduced RDT (Resource Director Technology), which includes CAT (Cache Allocation Technology). CAT allows for the allocation of private cache space to processes or cgroups to avoid cache replacement.</p><p>The LLC cache hit rate and miss rate are typically calculated as follows:</p><ul><li>LLC Cache Hit Rate &#x3D; L3_CACHE_HITS &#x2F; L3_CACHE_REFERENCES</li><li>LLC Cache Miss Rate &#x3D; L3_CACHE_MISSES &#x2F; L3_CACHE_REFERENCES</li></ul><p>Here, L3_CACHE_REFERENCES represents the event counter for all LLC cache accesses, L3_CACHE_HITS represents the event counter for LLC cache hits, and L3_CACHE_MISSES represents the event counter for LLC cache misses.</p><p>It is important to note that these event counters are not simple accumulative counters; they need to be processed and normalized to obtain accurate results. Detailed processing methods can be found in the documentation for tools like <code>pcm-exporter</code> or other similar resources.</p><p>In a containerized environment, a streaming application running inside a container that continuously reads and writes data can consume a large amount of LLC space. This can lead to data required by an LS application running in another container on the same machine being evicted from the LLC, resulting in decreased performance for the LS application.<br><img src="https://lynnapan.github.io/images/cache/8.png" alt="img"></p><h3 id="Metric-Collection"><a href="#Metric-Collection" class="headerlink" title="Metric Collection"></a>Metric Collection</h3><p>Several implementation paths:</p><ol><li><p><strong>Using cgroup <code>perf_event</code> to obtain CPI metrics for all applications on the host:</strong></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unix.IoctlSetInt(p.fd, unix.PERF_EVENT_IOC_ENABLE, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Using node <code>perf</code> to obtain CPI metrics for each CPU:</strong></p><p>This approach involves using Go’s implementation, which requires locking the OS and then utilizing <code>perf_event_open</code>.</p></li><li><p><strong>Using cAdvisor with the <code>libpfm</code> tool:</strong></p><p>This requires using cgo, with <code>libpfm</code> relying on <code>perf_event_open</code> at a lower level.</p></li></ol><h4 id="a-Node-Exporter"><a href="#a-Node-Exporter" class="headerlink" title="a. Node Exporter"></a>a. Node Exporter</h4><p>Currently, node exporter only supports CPU-level metrics. For more details, refer to the <a href="https://github.com/prometheus/node_exporter/pull/1274">implementation</a>.</p><h4 id="b-cgroup-perf-event"><a href="#b-cgroup-perf-event" class="headerlink" title="b. cgroup perf_event"></a>b. cgroup <code>perf_event</code></h4><p>This approach aims to support cgroup-level metrics, allowing monitoring of all threads belonging to a specific group and threads running on a specific CPU. For more information, see <a href="https://lwn.net/Articles/421574/">LWN.net</a>. cAdvisor supports cgroup <code>perf_event</code>, and its <a href="https://github.com/google/cadvisor/blob/a52ec5d60cf70b22f8b6d204780aec7a222cf6bb/manager/manager.go">implementation</a> can be reviewed for details.</p><p>Other metric collection methods:</p><ul><li><a href="https://github.com/intel/pcm">Intel PCM</a></li></ul><h3 id="Intel-RDT"><a href="#Intel-RDT" class="headerlink" title="Intel RDT"></a>Intel RDT</h3><p>Previously, solutions involved controlling virtual machine logical resources (cgroups), but this approach was too coarse-grained and couldn’t manage sensitive and scarce processor caches effectively. To address this, Intel introduced RDT (Resource Director Technology), which provides more precise control. For more details, refer to the <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">related introduction</a>.</p><p>RDT, which stands for Resource Director Technology, offers two main capabilities: monitoring and allocation. It enables users to directly monitor and allocate L2 and L3 caches (LLC) and memory bandwidth for each CPU core (or each logical core after Hyper-Threading) using a set of CPU instructions.</p><p>The Intel RDT implementation architecture was introduced in Linux Kernel 4.10, which provided L3 Cache Allocation Technology (CAT), L3 Code and Data Prioritization (CDP), and L2 CAT through the <code>resctrl</code> filesystem. Linux Kernel 4.12 further introduced support for Memory Bandwidth Allocation (MBA) for memory bandwidth management.</p><p>Intel RDT provides a series of allocation (resource control) capabilities, including Cache Allocation Technology (CAT), Code and Data Prioritization (CDP), and Memory Bandwidth Allocation (MBA).</p><p>The Intel Xeon E5-xxxx v4 series (Broadwell) offers L3 cache configuration and CAT mechanisms, with some communication-related features introduced in the E5-xxxx v3 series (Haswell). Some Intel processor series (e.g., Intel Atom processors) may support control of L2 caches. Additionally, the MBA functionality provides memory bandwidth management at the processor core level.</p><p>To use resource allocation technologies in Linux, the <code>resctrl</code> interface needs to be introduced in both the kernel and user space. From Linux Kernel 4.10 onwards, L3 CAT, L3 CDP, and L2 CAT are available with the <code>resctrl</code> architecture. Starting with Linux Kernel 4.12, MBA technology was introduced and is under development. For kernel usage instructions, see the <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">kernel documentation</a>.</p><h5 id="RDT-Technology-Architecture"><a href="#RDT-Technology-Architecture" class="headerlink" title="RDT Technology Architecture"></a>RDT Technology Architecture</h5><p>The core goal of Cache Allocation Technology (CAT) is to allocate resources based on Class of Service (COS or CLOS). Applications or individual threads can be tagged according to a series of service levels provided by the processor. This allows the allocation and restriction of cache usage based on the service classification of applications and threads. Each CLOS can use Capacity Bitmasks (CBMs) to indicate and specify the degree of overlap or isolation in service classification.</p><p>For each logical processor, there is a register (known as <code>IA32_PQR_ASSOC</code> MSR or PQR) that allows the operating system (OS) or Virtual Machine Monitor (VMM) to specify the CLOS when scheduling applications, threads, or virtual machines (VMs).</p><p>RDT is divided into five functional modules:</p><ul><li><strong>Cache Monitoring Technology (CMT)</strong>: Cache monitoring</li><li><strong>Cache Allocation Technology (CAT)</strong>: Cache allocation</li><li><strong>Memory Bandwidth Monitoring (MBM)</strong>: Memory bandwidth monitoring</li><li><strong>Memory Bandwidth Allocation (MBA)</strong>: Memory bandwidth allocation</li><li><strong>Code and Data Prioritization (CDP)</strong>: Code and data prioritization</li></ul><blockquote><p>RDT technology targets caches and memory bandwidth, divided into monitoring and control, forming four functional modules. Adding Code and Data Prioritization (control technology) results in a total of five functional modules.</p></blockquote><img src="https://pic.imgdb.cn/item/6691e693d9c307b7e995d1f8.png" alt="英特尔® RDT 内核架构" style="zoom:50%;" /><p>After enabling RDT control, you can create user directories in the root directory (e.g., “CG1” and “CG2”, as shown in Figure 4: Intel® RDT Hierarchy in the resctrl Filesystem) and specify different levels of control for each shared resource. The RDT control groups include the following files:</p><ul><li><strong>“tasks”</strong>: Reading this file displays a list of all tasks in the group. Writing task IDs to this file adds tasks to the group.</li><li><strong>“cpus”</strong>: Reading this file displays the bitmask of logical CPUs owned by the group. Writing a bitmask to this file adds CPUs to the group or removes CPUs from the group.</li><li><strong>“schemata”</strong>: A list of all resources accessible by the group.</li></ul><p>After enabling RDT monitoring features, the root directory and other top-level directories will include a “mon_groups” directory, where user directories (e.g., “M1” and “M2”, as shown in Figure 4: Intel® RDT Hierarchy in the resctrl Filesystem) can be created to monitor task groups. The “Mon_data” directory contains a set of files organized by resource domain and RDT events. Each directory within “Mon_data” has files for each event (e.g., “llc_occupancy”, “mbm_total_bytes”, and “mbm_local_bytes”). These files provide counters for the current values of events for all tasks in the group.<br><img src="https://pic.imgdb.cn/item/6691e6c8d9c307b7e9960b0d.png" alt="英特尔® RDT 在 resctrl 文件系统中的监测和控制示意图" style="zoom:50%;" /></p><h2 id="Theoretical-Foundation"><a href="#Theoretical-Foundation" class="headerlink" title="Theoretical Foundation"></a>Theoretical Foundation</h2><h3 id="CPI2-CPU-Performance-Isolation-for-Shared-Compute-Clusters"><a href="#CPI2-CPU-Performance-Isolation-for-Shared-Compute-Clusters" class="headerlink" title="CPI2: CPU Performance Isolation for Shared Compute Clusters"></a>CPI2: CPU Performance Isolation for Shared Compute Clusters</h3><h4 id="Performance-Metrics"><a href="#Performance-Metrics" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h4><p>CPI</p><h4 id="Core-Methodology"><a href="#Core-Methodology" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>Google’s approach relies entirely on statistical analysis based on historical data, without requiring separate stress testing, making the method straightforward. Google demonstrated the correlation between CPI and RT (Response Time) using historical data, finding a correlation of 0.97 for <strong>leaf-level links</strong> primarily for <strong>CPU-bound</strong> applications. Other services, such as some I&#x2F;O-bound services and intermediate node services, still showed a correlation above 0.7. Thus, CPI was confirmed as a valid proxy for performance.</p><p>Online jobs are typically long-running tasks, and their CPI trends on the same CPU model usually follow a predictable pattern. Therefore, traditional <strong>sliding window prediction</strong> methods are used to forecast the CPI for the next period.</p><img src="https://justadogistaken.github.io/images/cpi-1.png" alt="/images/cpi-1.png" style="zoom:50%;" /><p>Furthermore, the distribution of CPI data remains relatively consistent day-to-day, allowing the use of statistical methods to calculate the average CPI (CPIavg) and standard deviation (stddev) from the previous day. A threshold is set at CPIavg + 2 * stddev; if the CPI exceeds this value, QoS (Quality of Service) is considered to be impacted. Additionally, to avoid false positives, the rule requires that this threshold be exceeded three times within a 5-minute window before determining that QoS is affected.<br><img src="https://justadogistaken.github.io/images/cpi-2.png" alt="/images/cpi-2.png" style="zoom:30%;" /></p><h4 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h4><p>Experimental data indicate that the linear correlation coefficient between CPI and interference is 0.97.</p><h3 id="LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management"><a href="#LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management" class="headerlink" title="LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management"></a>LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management</h3><h4 id="Performance-Metrics-1"><a href="#Performance-Metrics-1" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h4><p>CPI</p><h4 id="Core-Methodology-1"><a href="#Core-Methodology-1" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>The primary contribution of this article from Alibaba is in how to adjust LLC (Last Level Cache) more finely. Intel RDT provides two technologies—CAT (Cache Allocation Technology) and MBA (Memory Bandwidth Allocation)—which are similar to cgroups in usage. Due to the slow integration into container runtimes, Intel open-sourced <code>intel-resource-manager</code> to support these technologies. CAT isolates LLC size, while MBA isolates L2-L3 memory bandwidth. However, the adjustment granularity for these two dimensions is 10%, which is too coarse. Alibaba combined CAT and MBA to achieve finer-grained adjustments.</p><p>In this approach, CPI is used for interference detection, but Alibaba calculated it through stress testing. They established a linear relationship <code>RT = k * CPI + l</code> to relate RT (Response Time) to CPI. Real-time CPI data is then used to estimate RT values and assess whether application QoS exceeds SLA (Service Level Agreement).</p><h4 id="Conclusions-1"><a href="#Conclusions-1" class="headerlink" title="Conclusions"></a>Conclusions</h4><p>CPI is linearly related to interference. By calculating RT from CPI, adjustments to LLC and MBA can be made to manage resource isolation effectively.</p><h3 id="PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services"><a href="#PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services" class="headerlink" title="PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services"></a>PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services</h3><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>Latency (ms)</p><h4 id="Core-Methodology-2"><a href="#Core-Methodology-2" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>Stress testing is used to determine the optimal latency target QoS for single applications (by applying pressure until the latency-pressure curve shows an inflection point). Resources are categorized as &lt;cpu core, cache way, cpu frequency, memory space, disk bandwidth&gt;, with corresponding adjustment granularity of &lt;1 core, 1 way, 100 MHz, 1 GB, 1 GB&#x2F;s&gt;. QoS is checked every 500 ms, and if deviations from target QoS are detected, resources are adjusted. For each application, different resource adjustments are attempted each round (similar to guessing the resource causing interference) until the QoS for all applications on the machine is ensured. The article provides a good approach by treating resources as interchangeable. For example, when interference is detected, resources are not uniformly scaled up or down; instead, resources are exchanged, such as reallocating CPU from I&#x2F;O-intensive applications to CPU-intensive ones, ensuring that each application has the appropriate resources.</p><p>Due to the author’s limited time, perspective, and understanding, there may be errors or omissions in this article. Feedback and corrections from readers and industry experts are welcomed. The troubleshooting information above has been updated to reflect community content.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1759977">Caelus—全场景在离线混部解决方案</a></li><li>Google Borg 2015：<a href="https://research.google/pubs/pub43438/">https://research.google/pubs/pub43438/</a></li><li>Google Borg 2019：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387517">https://dl.acm.org/doi/pdf/10.1145/3342195.3387517</a></li><li>Google Autopilot：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387524">https://dl.acm.org/doi/pdf/10.1145/3342195.3387524</a></li><li>百度千寻：<a href="https://www.infoq.cn/article/aEut*ZAIffp0q4MSKDSg">百度大规模战略性混部系统演进</a></li><li>阿里伏羲：<a href="https://yq.aliyun.com/articles/651202">https://yq.aliyun.com/articles/651202</a></li><li>阿里k8s混部：<a href="https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf">https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf</a></li><li>CPI论文: <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf</a></li><li>Heracles论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf</a></li><li>Bubble-up论文：<a href="https://ieeexplore.ieee.org/document/7851476">https://ieeexplore.ieee.org/document/7851476</a></li><li><a href="http://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/08/29/perf-arch">Linux kernel perf architecture (terenceli.github.io)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/noisy-neighbors-problem-in-kubernetes.html">resolving noisy neighbors (intel.com)</a></li><li><a href="https://facebookmicrosites.github.io/cgroup2/docs/overview.html">Maximizing Resource Utilization with cgroup2</a></li><li><a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html">字节跳动：混布环境下集群的性能评估与优化</a></li><li><a href="https://www.cnblogs.com/tencent-cloud-native/p/14754230.html">混部之殇-论云原生资源隔离技术之CPU隔离(一) </a></li><li><a href="https://patents.google.com/patent/CN106776005A/zh">CN106776005A - 一种面向容器化应用的资源管理系统及方法</a></li><li><a href="https://www.aboutyun.com/thread-27867-1-1.html">阿里K8s之动态解决容器资源按需分配</a></li><li><a href="https://justadogistaken.github.io/posts/handle-interference/">混部场景下的单机服务质量保障 </a></li><li>[Cache高速缓存和缓存隔离](<a href="https://lynnapan.github.io/2017/04/18/understand">https://lynnapan.github.io/2017/04/18/understand</a> Cache&#x2F;)</li><li><a href="https://github.com/hodgesds/perf-utils">hodgesds&#x2F;perf-utils</a></li><li><a href="https://cloud.tencent.com/developer/article/1517979">用CPI火焰图分析Linux性能问题</a></li><li><a href="https://www.openeuler.org/zh/blog/rubik/index.html">openEuler资源利用率提升之道</a></li><li><a href="http://jos.org.cn/html/2020/10/6066.htm">在离线混部作业调度与资源管理技术研究综述</a></li><li><a href="https://qiankunli.github.io/2021/11/22/hybrid_deployment.html">从混部到统一调度</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;From the perspective of resource allocation, the cluster appears relatively balanced. However, in terms of actual load, many hotspots hav</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>容器干扰检测与治理（上篇）</title>
    <link href="https://zoues.com/posts/e9953e7c/"/>
    <id>https://zoues.com/posts/e9953e7c/</id>
    <published>2024-07-12T23:48:30.000Z</published>
    <updated>2024-07-13T02:32:15.895Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>从资源分配视图来看，集群分配相对平衡的，但从实际负载情况来看，已经出现较多的热点，此时容易发生应用间竞争共享资源, 导致核心业务出现性能干扰，应用的响应时间往往会出现长尾现象，导致应用服务质量下降，且会增加其故障的可能性，这种应用间的资源竞争及性能干扰（如noisy neighbor现象）使得应用调度与资源管理变得十分复杂，因此考虑如何降低应用间的性能干扰, 以保障业务的稳定性。</p><p><img src="https://pic.imgdb.cn/item/6691e507d9c307b7e9943c62.png"></p><p>“noisy neighbor”问题是云基础设施中的一种常见现象，指的是当应用程序所需的资源被同一计算节点中的其他应用程序大量占用时，导致应用程序性能降低，如延迟时间增加。这对业务敏感型应用来说，尤其棘手，但通常难以识别。</p><p>问题的根源在于资源共享时缺乏严格的隔离策略。从资源层面来看，包括CPU、内存（L1&#x2F;L2&#x2F;L3缓存）、网络和块I&#x2F;O等。从硬件拓扑层面来看，则涉及到CPU缓存、内存带宽等。CPU核心紧密连接着其他可共享资源，如最后一级缓存（LLC）和内存带宽。在分配共享资源时，如果没有有效的隔离策略，问题就会出现。例如，同级超线程共享相同的物理CPU核心，不同物理CPU核心上的工作负载共享相同的最后一级缓存、内存控制器和I&#x2F;O总线带宽，甚至不同物理CPU插槽上的工作负载也共享CPU互连带宽、相同的存储设备或I&#x2F;O总线。在这种情况下，”noisy neighbor”往往需要更多的资源，并且需要消耗更多时间才能完成同一任务。这会导致核心应用无法获得所需资源，导致性能下降和stall情况的出现。</p><p>Kubernetes提供了CPU管理器和设备插件管理器，用于硬件资源分配，例如CPU和设备（SR-IOV、GPU）。目前，Kubernetes还提供了拓扑管理器，以实现NUMA拓扑感知，协调资源并保证关键工作负载的最佳性能。然而，这些功能并未直接解决”noisy neighbor”问题。</p><p>要解决这个问题，可以考虑以下优化策略：</p><ol><li><strong>资源请求和限制</strong>：在Kubernetes中为每个Pod设置合理的资源请求和限制，以确保应用程序在资源竞争时能获得足够的资源。</li><li><strong>亲和性和反亲和性规则</strong>：使用Kubernetes的亲和性和反亲和性规则，将关键应用与其他高资源消耗的应用分开部署，减少资源竞争。</li><li><strong>隔离策略</strong>：利用Kubernetes的CPU隔离策略，确保关键应用独占特定的CPU核心，减少共享资源的争夺。</li><li><strong>优先级和抢占</strong>：设置应用程序的优先级，确保关键应用可以优先获得资源，并在必要时抢占低优先级应用的资源。</li><li><strong>NUMA感知调度</strong>：通过Kubernetes的拓扑管理器，确保关键应用在NUMA节点内分配资源，最大化本地资源的使用效率，减少跨节点资源访问的延迟。</li><li><strong>使用专用硬件</strong>：对于需要高性能的应用，考虑使用专用硬件（如GPU、FPGA）或独立的物理节点，避免与其他应用共享资源。</li><li><strong>监控和调整</strong>：持续监控应用程序的资源使用情况和性能表现，及时调整资源分配策略，确保关键应用的稳定运行。</li></ol><p>通过实施这些优化策略，可以在很大程度上缓解”noisy neighbor”问题，保障关键应用程序的性能和稳定性,其中隔离策略、优先级和抢占以及numa感知调度都是针对应用场景的资源QoS优化策略，那么我们如何衡量性能干扰？</p><blockquote><p>NOTE: 混部(混合部署)，这里的“混”，本质上就是“区分优先级”。狭义上，可以简单的理解为“在线+离线”(在离线)混部，广义上，可以扩展到更广的应用范围：多优先级业务混合部署</p></blockquote><hr><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><h4 id="a-CPI"><a href="#a-CPI" class="headerlink" title="a. CPI"></a>a. CPI</h4><p>CPI 即 Cycle Per Instruction 的缩写，它的含义就是<strong>每指令周期数</strong>。此外，在一些场合，也可以经常看到 IPC，即 Instruction Per Cycle，含义为<strong>每周期指令数</strong>。</p><blockquote><p>CPI 值越小，表示计算机系统的指令执行效率越高。</p></blockquote><p>CPI 和 IPC 的关系为: CPI &#x3D; 1 &#x2F; IPC</p><p><strong>如果具体到单 CPU 的程序执行性能场景</strong>，实际上可以表示为：</p><p><img src="https://pic.imgdb.cn/item/6691e5acd9c307b7e994e785.png" alt="img"></p><p>由于受到硅材料和制造工艺的限制，处理器主频的提高已经面临瓶颈，因此，程序性能的提高，主要的变量在 Instruction Count 和 CPI 这两个方面</p><p>通常情况下，通过 CPI 的取值，我们可以大致判断一个计算密集型任务，到底是 CPU 密集型的还是 Memory 密集型的：</p><ul><li>CPI 小于 1，程序通常是 CPU 密集型的；</li><li>CPI 大于 1，程序通常是 Memory 密集型的;</li></ul><p>这只是一个大致的判断方法，不同的任务类型可能会有不同的CPI取值范围，因此需要结合实际情况进行判断。同时，还需要考虑到其它因素，比如内存大小、带宽、CPU的缓存等，才能更全面地判断任务类型。</p><p>判断一个计算密集型任务运行效率的重要依据就是看程序运行时的 CPU 利用率。很多人认为 CPU 利用率高就是程序的代码在疯狂运行。实际上，CPU 利用率高，也有可能是 CPU 正在<strong>忙等</strong>一些资源(非iowait)，如访问内存遇到了瓶颈。</p><p>一些计算密集型任务，在正常情况下，CPI 很低，性能原本很好。CPU 利用率很高。但是随着系统负载的增加，其它任务对系统资源的争抢，导致这些计算任务的 CPI 大幅上升，性能下降。而此时，很可能 CPU 利用率上看，还是很高的，但是这种 CPU 利用率的高，实际上体现的是 CPU 的忙等，及流水线的停顿带来的效应。</p><p>Brendan Gregg 曾在 CPU Utilization is Wrong 这篇博客中指出，CPU 利用率指标需要结合 CPI&#x2F;IPC 指标一起来分析。</p><p>通过perf record，生成CPI 火焰图，其可以展示了程序的 Call Stack 与 CPU 占用率的关联性，而且还揭示了这些 CPU 占用率里，哪些部分是真正的有效的运行时间，哪些部分实际上是 CPU 因某些停顿造成的忙等。</p><p>一般可以通过此工具发现系统存在的资源瓶颈，并且通过一些方式来缓解资源的瓶颈；例如，应用间的 Cache 颠簸干扰，可以通过将应用绑到不同的 CPU 上解决。</p><p>而应用开发者则可以通过优化相关函数，来提高程序的性能。例如，通过优化代码减少 Cache Miss，从而降低应用的 CPI 来减少处理器因访存停顿造成的性能问题。</p><h4 id="b-LLC"><a href="#b-LLC" class="headerlink" title="b. LLC"></a>b. LLC</h4><p>旧式的 CPU 会有两级内存（L1 和 L2），新的CPU会有三级内存（L1，L2，L3 ），如下图所示：</p><img src="https://pic2.zhimg.com/80/v2-f4551e163f574dfe4a98dc1847272e05_720w.webp" alt="img" style="zoom:50%;" /><p>其中：</p><ul><li><p>L1 缓存分成两种，一种是指令缓存，一种是数据缓存。L2 缓存和 L3 缓存不分指令和数据。</p></li><li><p>L1 和 L2 缓存在每一个 CPU 核中，L3 则是所有 CPU 核心共享的内存。</p></li><li><p>L1、L2、L3 的越离CPU近就越小，速度也越快，越离 CPU 远，速度也越慢。</p><p><img src="https://frankdenneman.nl/wp-content/uploads/2019/10/03-Rome-Chiplet.png" alt="img"></p></li></ul><p>LLC(last level cache)缓存资源作为影响作业性能的重要资源, 其上的干扰同样不可忽略.多个应用在共享缓存时可能出现相互替换的现象, LLC失效时原本访存指令的执行时间将从15ns上升至70ns, 假设CPU主频为3Ghz, 则一条访存指令需要多消耗200多个周期才能完成，消除此类干扰的方法是使用资源划分技术, 在物理上划分多个作业对共享资源的使用，上图所示为经典的多核体系结构, 多个CPU共享了LLC, 运行于不同CPU上的作业会在LLC上发生竞争, Intel Cascade Lake微架构与amd的Rome chiplet, 通过为每个核设置独立的LLC以减少核间对于LLC的资源竞争; 但是, 同一CPU上的多个应用在混部运行时仍然会出现缓存相互替换问题, 因此需要应用级别的缓存划分技术.为了实现应用级别的缓存划分, Intel提出了RDT技术，其中, CAT(cache allocation technology)可为进程或者CGroup分配私有的缓存空间, 避免缓存相互替换;</p><p>LLC 缓存的命中率和缺失率的计算方式通常如下：</p><ul><li>LLC 缓存命中率 &#x3D; L3_CACHE_HITS &#x2F; L3_CACHE_REFERENCES</li><li>LLC 缓存缺失率 &#x3D; L3_CACHE_MISSES &#x2F; L3_CACHE_REFERENCES</li></ul><p>其中，L3_CACHE_REFERENCES 表示所有访问 LLC 缓存的事件计数器，L3_CACHE_HITS 表示 LLC 缓存的命中事件计数器，L3_CACHE_MISSES 表示 LLC 缓存的缺失事件计数器。</p><p>需要注意的是，这里的事件计数器并不是简单的累加计数器，而是需要进行一定的处理和归一化，才能得到准确的结果。具体处理方法可以参考 <code>pcm-exporter</code> 的文档或者其他类似的文档。</p><p>在container环境下，一个运行在container里面的streaming应用不停的读写数据导致大量的LLC占用，会导致同机器上另外一个container里运行的LS需要的数据被evict出LLC，从而导致LS应用性能下降。</p><p><img src="https://lynnapan.github.io/images/cache/8.png" alt="img"></p><h3 id="指标采集"><a href="#指标采集" class="headerlink" title="指标采集"></a>指标采集</h3><p>几种实现路径：</p><ol><li><p>通过cgroup perf_event获取主机所以的应用的CPI指标</p><p><code>unix.IoctlSetInt(p.fd, unix.PERF_EVENT_IOC_ENABLE, 0)</code></p></li><li><p>通过node perf获取各cpu的CPI指标，这里使用的go的实现，需要LockOS，然后使用perf_event_open</p></li><li><p>cadvisor使用libpfm工具，这里要cgo，其中libpfm底层用的perf_event_open</p></li></ol><h4 id="a-node-exporter"><a href="#a-node-exporter" class="headerlink" title="a. node exporter"></a>a. node exporter</h4><p>目前node exporter只支持cpu级别的metrics，具体<a href="https://github.com/prometheus/node_exporter/pull/1274">实现</a></p><h4 id="b-cgroup-perf-event"><a href="#b-cgroup-perf-event" class="headerlink" title="b. cgroup perf_event"></a>b. cgroup perf_event</h4><p>期望支持cgroup级别的metrics，即可以监测属于某个特定的 group 的所有线程以及运行在特定 CPU 上的线程,<a href="https://lwn.net/Articles/421574/">LWN.net]</a>，其中cadvisor支持cgroup perf_event，具体<a href="https://github.com/google/cadvisor/blob/a52ec5d60cf70b22f8b6d204780aec7a222cf6bb/manager/manager.go">实现</a></p><p>其他指标采集方式：</p><ul><li><a href="https://github.com/intel/pcm">https://github.com/intel/pcm</a></li></ul><h3 id="Intel-RDT"><a href="#Intel-RDT" class="headerlink" title="Intel RDT"></a>Intel RDT</h3><p>以往解决方法是通过控制虚拟机逻辑资源(cgroup)但是调整粒度太粗，并且无法控制处理器缓存这样敏感而且稀缺的资源。为此Intel推出了RDT技术, <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">相关介绍</a></p><p>RDT技术，全称为Resource Director Technology，提供了两种能力：监控和分配。该技术旨在通过一系列的CPU指令从而允许用户直接对每个CPU核心（附加了HT技术后为每个逻辑核心）的L2缓存、L3缓存（LLC）以及内存带宽进行监控和分配。</p><p>Linux Kernel 4.10引入了Intel RDT实现架构，基于 <code>resctrl</code> 文件系统提供了 L3 CAT (Cache Allocation Technology)，L3 CDP(Code and Data Prioritization)，以及L2 CAT。并且Linux Kernel 4.12进一步实现支持了MBA(Memory Bandwidth Allocation)内存带宽分配技术。</p><p>Intel RDT提供了一系列分配(资源控制)能力，包括缓存分配技术(Cache Allocation Technology, CAT)，代码和数据优先级(Code and Data Prioritization, CDP) 以及 内存带宽分配(Memory Bandwidth Allocation, MBA)。</p><p>Intel志强处理器 E5-xxxx v4系列(即Broadwell)提供了L3缓存的配置以及CAT机制，其中部分通讯相关功能在 E5-xxxx v3系列(即Haswell)引入。一些Intel处理器系列(例如Intel Atom处理器系列)可能支持对L2缓存的控制。此外，MBA共功能提供了相应的处理器核心级别的内存带宽管理。</p><p>为了能够在Linux中使用资源分配技术，需要在内核和用户空间引入 <code>resctl</code> 接口。从Linux Kernel 4.10开始，可以使用 L3 CAT, L3 CDP 和 L2 CAT 以及 <code>resctrl</code> 架构。从Linux Kernel 4.12开始，开始引入并正在开发MBA技术, <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">内核使用说明</a></p><h5 id="RDT技术架构"><a href="#RDT技术架构" class="headerlink" title="RDT技术架构"></a>RDT技术架构</h5><p>缓存分配技术CAT(Cache Allocation Technology)的核心目标是基于服务级别(Class of Service, COS 或 CLOS)来实现资源分配。应用程序或者独立线程可以按照处理器提供的一系列服务级别来标记。这样就会按照应用程序和线程的服务分类来限制和分配其使用的缓存。每个CLOS可以使用能力掩码(capacity bitmasks, CBMs)来标志并在服务分类中指定覆盖(overlap)或隔离(isolation)的程度。</p><p>对于每个逻辑处理器，都有一个寄存器(被称为 <code>IA32_PQR_ASSOC</code> MSR或PQR)来允许操作系统(OS)或虚拟机管理器(VMM)在应用程序、线程、虚拟机(VM)调度(scheduled)的时候指定它的CLOS。</p><p>RDT分为5个功能模块：</p><ul><li>Cache Monitoring Technology (CMT) 缓存检测技术</li><li>Cache Allocation Technology (CAT) 缓存分配技术</li><li>Memory Bandwidth Monitoring (MBM) 内存带宽监测</li><li>Memory Bandwidth Allocation (MBA) 内存带宽分配</li><li>Code and Data Prioritization (CDP) 代码和数据优先级</li></ul><blockquote><p>RDT技术针对的是缓存和内存带宽，分别又分为监控和控制，就形成了4个功能模块，再加上代码和数据优先级(控制技术)，合起来形成5个功能模块。</p></blockquote><img src="https://pic.imgdb.cn/item/6691e693d9c307b7e995d1f8.png" alt="英特尔® RDT 内核架构" style="zoom:50%;" /><p>启用 RDT 控制后，可在根目录中创建用户目录（“CG1” 和 “CG2”，见图 4：英特尔® RDT 在 resctrl 文件系统中的分层结构），为每个共享资源指定不同的控制力度。RDT 控制组包含以下文件：“tasks”：读取该文件会显示该群组所有任务的列表。将任务 ID 写入文件会添加任务到群组。“cpus”：读取该文件会显示该群组拥有的逻辑 CPU 的位掩码。将掩码写入文件会添加 CPU 到群组或从群组中移除 CPU。“schemata”：该群组可访问的所有资源的列表。</p><p>启用 RDT 监控功能后，根目录和其他顶层目录会包含 “mon_groups” 目录，在此目录中可以创建用户目录（“M1” 和 “M2”，见图 4：英特尔® RDT 在 resctrl 文件系统中的分层结构），以监控任务群组。“Mon_data” 目录包含一组按照资源域和 RDT 事件组织的文件。这些目录中，每个目录针对每个事件都有一个文件（“llc_occupancy”、“mbm_ total_bytes” 和 “mbm_local_bytes”）。这些文件为群组中的所有任务提供了事件当前值的计数器。</p><img src="https://pic.imgdb.cn/item/6691e6c8d9c307b7e9960b0d.png" alt="英特尔® RDT 在 resctrl 文件系统中的监测和控制示意图" style="zoom:50%;" /><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="CPI2-CPU-performance-isolation-for-shared-compute-clusters"><a href="#CPI2-CPU-performance-isolation-for-shared-compute-clusters" class="headerlink" title="CPI2 : CPU performance isolation for shared compute clusters"></a>CPI2 : CPU performance isolation for shared compute clusters</h3><h4 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h4><p>CPI</p><h4 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h4><p>Google的方法，完全基于历史数据做统计分析，不需要单独做压测，方法简单。Google基于历史数据，对CPI与RT的相关性做论证，得出对于<strong>链路处于叶子</strong>，且主要为<strong>CPU型</strong>的应用，相关性有0.97；其他一些服务，如部分IO型，中间节点服务，仍然0.7+的相关性。所以确定CPI可以作为性能的proxy。在线作业通常为常驻作业，这类作业在同一CPU型号的CPI数据走向通常呈现一定规律，是可预测的。所以用传统的<strong>滑动窗口预测</strong>方法，对下一周期的CPI进行预测。</p><img src="https://justadogistaken.github.io/images/cpi-1.png" alt="/images/cpi-1.png" style="zoom:50%;" /><p>并且每天CPI的数据分布都相差不大，所以可以直接用统计的方法，算前一天CPI的平均值CPIavg，及标准差stddev，设置 CPIavg + 2 * stddev为阈值，超过该值，认定QoS受到影响。同时，为了避免误判，规则为5min中内发现3次超过，才确定QoS受到影响。</p><img src="https://justadogistaken.github.io/images/cpi-2.png" alt="/images/cpi-2.png" style="zoom:30%;" /><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>实验数据表明,CPI与干扰相关性线性系数为0.97</p><h3 id="LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management"><a href="#LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management" class="headerlink" title="LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management"></a>LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management</h3><h4 id="性能指标-1"><a href="#性能指标-1" class="headerlink" title="性能指标"></a>性能指标</h4><p>CPI</p><h4 id="核心方法-1"><a href="#核心方法-1" class="headerlink" title="核心方法"></a>核心方法</h4><p>阿里这篇文章的主要贡献在于如何更细粒度的调整LLC；Intel RDT提供了CAT和MBA两种技术（用法与cgroups相似，由于推入container runtime太慢了，intel专门开源了intel-resource-manager支持他家的黑科技），前者是对LLC size的隔离，后者是对L2-L3内存带宽的隔离；由于两个维度单独调整的粒度都是10%，粒度太粗；阿里通过CAT和MBA结合，实现更细粒度的调整。其中用CPI做干扰检测，但是阿里是用压测的方式计算出；RT与CPI的相关性，构建<code>RT=k*CPI+l</code> like线性方程；从而用实时的CPI，计算出大致的RT值，判断应用QoS是否超过SLA。</p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>CPI与干扰线性相关，根据CPI计算RT，据此调整LLC、MBA等资源隔离</p><h3 id="PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services"><a href="#PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services" class="headerlink" title="PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services"></a>PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services</h3><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>Latency(ms)</p><h4 id="核心方法-2"><a href="#核心方法-2" class="headerlink" title="核心方法"></a>核心方法</h4><p>压测得出单应用最合适的Latency-targetQoS（加压直至Latency与压力曲线出现拐点），资源维度为&lt;cpu core， cache way， cpu frequency， mem space， disk bandwidth&gt;，对应的调整粒度为&lt;1 core, 1 way, 100MHz, 1GB, 1GB&#x2F;s&gt;* 500ms检测一次QoS，如果发现与targetQoS偏离过大，则开始调整资源，对每个应用每轮尝试不同的资源up&#x2F;down（等于猜受干扰资源），直至保证了机器所有应用的QoS。文章提供了很好的思路，资源是可交换的，即发现干扰时，不用统一扩容或缩容资源，文章的背景是所有在线应用部署在一个集群里，一台机器各维度资源有限，那就通过交换，比如把io密集型应用的cpu让给cpu密集型的。从而保证每个应用具有合适的资源。</p><p>由于笔者时间、视野、认知有限，本文难免出现错误、疏漏等问题，期待各位读者朋友、业界专家指正交流，上述排障信息已修改为社区内容。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1759977">Caelus—全场景在离线混部解决方案</a></li><li>Google Borg 2015：<a href="https://research.google/pubs/pub43438/">https://research.google/pubs/pub43438/</a></li><li>Google Borg 2019：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387517">https://dl.acm.org/doi/pdf/10.1145/3342195.3387517</a></li><li>Google Autopilot：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387524">https://dl.acm.org/doi/pdf/10.1145/3342195.3387524</a></li><li>百度千寻：<a href="https://www.infoq.cn/article/aEut*ZAIffp0q4MSKDSg">百度大规模战略性混部系统演进</a></li><li>阿里伏羲：<a href="https://yq.aliyun.com/articles/651202">https://yq.aliyun.com/articles/651202</a></li><li>阿里k8s混部：<a href="https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf">https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf</a></li><li>CPI论文: <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf</a></li><li>Heracles论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf</a></li><li>Bubble-up论文：<a href="https://ieeexplore.ieee.org/document/7851476">https://ieeexplore.ieee.org/document/7851476</a></li><li><a href="http://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/08/29/perf-arch">Linux kernel perf architecture (terenceli.github.io)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/noisy-neighbors-problem-in-kubernetes.html">resolving noisy neighbors (intel.com)</a></li><li><a href="https://facebookmicrosites.github.io/cgroup2/docs/overview.html">Maximizing Resource Utilization with cgroup2</a></li><li><a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html">字节跳动：混布环境下集群的性能评估与优化</a></li><li><a href="https://www.cnblogs.com/tencent-cloud-native/p/14754230.html">混部之殇-论云原生资源隔离技术之CPU隔离(一) </a></li><li><a href="https://patents.google.com/patent/CN106776005A/zh">CN106776005A - 一种面向容器化应用的资源管理系统及方法</a></li><li><a href="https://www.aboutyun.com/thread-27867-1-1.html">阿里K8s之动态解决容器资源按需分配</a></li><li><a href="https://justadogistaken.github.io/posts/handle-interference/">混部场景下的单机服务质量保障 </a></li><li>[Cache高速缓存和缓存隔离](<a href="https://lynnapan.github.io/2017/04/18/understand">https://lynnapan.github.io/2017/04/18/understand</a> Cache&#x2F;)</li><li><a href="https://github.com/hodgesds/perf-utils">hodgesds&#x2F;perf-utils</a></li><li><a href="https://cloud.tencent.com/developer/article/1517979">用CPI火焰图分析Linux性能问题</a></li><li><a href="https://www.openeuler.org/zh/blog/rubik/index.html">openEuler资源利用率提升之道</a></li><li><a href="http://jos.org.cn/html/2020/10/6066.htm">在离线混部作业调度与资源管理技术研究综述</a></li><li><a href="https://qiankunli.github.io/2021/11/22/hybrid_deployment.html">从混部到统一调度</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</title>
    <link href="https://zoues.com/posts/186b05db/"/>
    <id>https://zoues.com/posts/186b05db/</id>
    <published>2024-06-08T13:10:08.000Z</published>
    <updated>2024-07-13T00:50:10.065Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>针对该问题的信息做了部分加工处理,thanks <strong><a href="https://github.com/polarathene">polarathene</a></strong></p></blockquote><p>我们在Fedora系统上将containerd.io从1.4.13版本升级到了1.5.10之后，发现多个项目中所有MySQL 容器实例消耗内存暴涨超过20GB，而在此之前它们仅消耗不到300MB。同事直接上了重启大招，但重启后问题依旧存在。最后选择回滚到1.4.13版本，该现象也随之消失。</p><p>值得注意的是，在Ubuntu 18.04.6系统上运行相同版本的containerd和runc时，MySQL 容器实例一切工作正常。只有在Fedora 35系统(配置相同的containerd与runc版本)，出现了内存消耗异常的情况。下面是出现异常的容器组件版本信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">go1.16.15</span><br><span class="line">containerd: 1.5.11</span><br><span class="line">runc: 1.0.3</span><br></pre></td></tr></table></figure><p>在Fedora 35上，执行以下命令执行会引发系统崩溃：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm mysql:5.7.36</span><br><span class="line">docker run -it --rm mysql:5.5.62</span><br></pre></td></tr></table></figure><p>但是mysql 8.0.29版本在Fedora 35上却运行正常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm mysql:8.0.29</span><br></pre></td></tr></table></figure><p>OOM相关信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2023-06-06T17:23:24.094275-04:00 laptop kernel: oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=user.slice,mems_allowed=0,global_oom,task_memcg=/system.slice/docker-xxx.scope,task=mysqld,pid=38421,uid=0</span><br><span class="line">2023-06-06T17:23:24.094288-04:00 laptop kernel: Out of memory: Killed process 38421 (mysqld) total-vm:16829404kB, anon-rss:12304300kB, file-rss:108kB, shmem-rss:0kB, UID:0 pgtables:28428kB oom_score_adj:0</span><br><span class="line">2022-06-06T17:23:24.094313-04:00 laptop systemd[1]: docker-xxx.scope: A process of this unit has been killed by the OOM killer.</span><br><span class="line">2022-06-06T17:23:24.856029-04:00 laptop systemd[1]: docker-xxx.scope: Deactivated successfully.</span><br></pre></td></tr></table></figure><p>原先在空闲状态下，<code>mysql</code>容器使用内存大约在200MB左右；但在某些操作系统上，如RedHat、Arch Linux或Fedora，一旦为容器设置了非常高的打开文件数（<code>nofile</code>）限制，则可能会导致<code>mysql</code>容器异常地占用大量内存。</p><pre><code>cat /proc/$(pgrep dockerd)/limits | grep &quot;Max open files&quot;cat /proc/$(pgrep containerd)/limits | grep &quot;Max open files&quot;</code></pre><p>如果输出值为1073741816或更高，那么您可能也会遇到类似异常。</p><p>在相关社区，我们发现了类似的案例:</p><h4 id="xinetd-slowly"><a href="#xinetd-slowly" class="headerlink" title="xinetd slowly"></a>xinetd slowly</h4><p>xinetd服务启动极其缓慢，我们查看了dockerd的系统设置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/$(pidof dockerd)/limits | grep &quot;Max open files&quot;</span><br><span class="line">Max open files            1048576              1048576              files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show docker | grep LimitNOFILE</span><br><span class="line">LimitNOFILE=1048576</span><br></pre></td></tr></table></figure><p>但是，在容器内部，则是一个非常巨大的数字——1073741816</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm ubuntu bash -c &quot;cat /proc/self/limits&quot; | grep  &quot;Max open files&quot;</span><br><span class="line">Max open files            1073741816           1073741816           files</span><br></pre></td></tr></table></figure><p><code>xinetd</code>程序在初始化时使用<code>setrlimit(2)</code>设置文件描述符的数量，这会消耗大量的时间及CPU资源去关闭1073741816个文件描述符。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@1b3165886528# strace xinetd</span><br><span class="line">execve(&quot;/usr/sbin/xinetd&quot;, [&quot;xinetd&quot;], 0x7ffd3c2882e0 /* 9 vars */) = 0</span><br><span class="line">brk(NULL)                               = 0x557690d7a000</span><br><span class="line">arch_prctl(0x3001 /* ARCH_??? */, 0x7ffee17ce6f0) = -1 EINVAL (Invalid argument)</span><br><span class="line">mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb14255c000</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">close(12024371)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024372)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024373)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024374)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024375)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024376)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024377)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024378)                         = -1 EBADF (Bad file descriptor)</span><br></pre></td></tr></table></figure><h4 id="yum-hang"><a href="#yum-hang" class="headerlink" title="yum hang"></a>yum hang</h4><p>从docker社区获取Rocky Linux 9对应的Docker版本，在容器中执行yum操作时速度非常缓慢。</p><p>在CentOS 7和Rocky Linux 9宿主机上，我们都进行了以下操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><p>在CentOS 7宿主机上，耗时在2分钟左右； 而在Rocky Linux 9上，一个小时也未能完成。</p><p>复现步骤如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><h4 id="rpm-slow"><a href="#rpm-slow" class="headerlink" title="rpm slow"></a>rpm slow</h4><p>在宿主机上执行下述命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion</span><br></pre></td></tr></table></figure><p>消耗的各类时间如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m11.248s</span><br><span class="line">user    0m7.316s</span><br><span class="line">sys     0m1.932s</span><br></pre></td></tr></table></figure><p>在容器中执行测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --net=none --log-driver=none -v &quot;/workspace:/workspace&quot; -v &quot;/disks:/disks&quot; opensuse bash -c &quot;time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion&quot;</span><br></pre></td></tr></table></figure><p>消耗的各类时间激增：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m31.089s</span><br><span class="line">user    0m14.876s</span><br><span class="line">sys     0m12.524s</span><br></pre></td></tr></table></figure><p>我们找到了RPM的触发问题的根因，其属于RPM内部POSIX lua库 <a href="https://github.com/rpm-software-management/rpm/commit/7a7c31f">rpm-software-management&#x2F;rpm@<code>7a7c31f</code></a>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">static int Pexec(lua_State *L) /** exec(path,[args]) */</span><br><span class="line">&#123;</span><br><span class="line">/* ... */</span><br><span class="line">open_max = sysconf(_SC_OPEN_MAX);</span><br><span class="line">if (open_max == -1) &#123;</span><br><span class="line">    open_max = 1024;</span><br><span class="line">&#125;</span><br><span class="line">for (fdno = 3; fdno &lt; open_max; fdno++) &#123;</span><br><span class="line">    flag = fcntl(fdno, F_GETFD);</span><br><span class="line">    if (flag == -1 || (flag &amp; FD_CLOEXEC))</span><br><span class="line">continue;</span><br><span class="line">    fcntl(fdno, F_SETFD, FD_CLOEXEC);</span><br><span class="line">&#125;</span><br><span class="line">/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类似的，如果设置的最大打开文件数限制过高，那么<code>luaext/Pexec()</code>和<code>lib/doScriptExec()</code>在尝试为所有这些文件描述符设置<code>FD_CLOEXEC</code>标志时，会花费过多的时间，从而导致执行如<code>rpm</code>或<code>dnf</code>等命令的时间显著增加。</p><h4 id="PtyProcess-spawn-slowdown-in-close-loop"><a href="#PtyProcess-spawn-slowdown-in-close-loop" class="headerlink" title="PtyProcess.spawn slowdown in close() loop"></a>PtyProcess.spawn slowdown in close() loop</h4><p>ptyprocess存在问题的相关代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Do not allow child to inherit open file descriptors from parent, </span><br><span class="line"> # with the exception of the exec_err_pipe_write of the pipe </span><br><span class="line"> # and pass_fds. </span><br><span class="line"> # Impose ceiling on max_fd: AIX bugfix for users with unlimited </span><br><span class="line"> # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange() </span><br><span class="line"> # occasionally raises out of range error </span><br><span class="line"> max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0]) </span><br><span class="line"> spass_fds = sorted(set(pass_fds) | &#123;exec_err_pipe_write&#125;) </span><br><span class="line"> for pair in zip([2] + spass_fds, spass_fds + [max_fd]): </span><br><span class="line">     os.closerange(pair[0]+1, pair[1]) </span><br></pre></td></tr></table></figure><p>当处理文件描述符时，为了提高效率，应避免遍历所有可能的文件描述符来关闭它们，尤其是在Linux系统上，因为这会通过<code>close()</code>系统调用消耗大量时间。尤其是当打开文件描述符的限制（可以通过<code>ulimit -n</code>、<code>RLIMIT_NOFILE</code>或<code>SC_OPEN_MAX</code>查看）被设置得非常高时，这种遍历方式将导致数百万次不必要的系统调用，显著增加了处理时间。</p><p>一个更为高效的解决方案是仅关闭那些实际上已打开的文件描述符。在Python 3中，<code>subprocess</code>模块已经实现了这一功能，而对于使用Python 2的用户，<code>subprocess32</code>的兼容库可以作为回退选项。通过利用这些库或类似的技术，我们可以显著减少不必要的系统调用，从而提高程序的运行效率。</p><h3 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h3><h4 id="1-RLIMIT-NOFILE"><a href="#1-RLIMIT-NOFILE" class="headerlink" title="1. RLIMIT_NOFILE"></a>1. RLIMIT_NOFILE</h4><blockquote><p><a href="https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94">https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94</a></p></blockquote><p>当前Linux内核对于用户空间进程的RLIMIT_NOFILE资源限制默认设置为1024（软限制）和4096（硬限制）。以前，systemd在派生进程时会直接传递这些未修改的限制。在systemd240版本中，systemd传递的硬限制增加到了512K，其覆盖了内核的默认值，并大大增加了非特权用户空间进程可以同时分配的文件描述符数量。</p><p>注意，为了兼容性考虑，软限制仍保持在1024，传统的UNIX select()调用无法处理大于或等于1024的文件描述符（FD_SET宏不管是否越界以及越界的后果，fd_set也并非严格限制在1024，FD_SET超过1024的值，会造成越界），因此如果全局提升了软限制，那么在使用select()时可能出现异常（在现代编程中，程序不应该再使用select()，而应该选择poll()&#x2F;epoll，但遗憾的是这个调用仍然大规模存在）。</p><p>在较新的内核中，分配大量文件描述符在内存和性能上比以前消耗少得多。Systemd社区中有用户称在实际应用中他们使用了约30万个文件描述符，因此Systemd认为512K作为新的默认值是足够高的。但是需要注意的是，也有报告称使用非常高的硬限制（例如1G）是有问题的，因此，超高硬限制会触发部分应用程序中过大的内存分配。</p><h4 id="2-File-Descriptor-Limits"><a href="#2-File-Descriptor-Limits" class="headerlink" title="2. File Descriptor Limits"></a>2. File Descriptor Limits</h4><p>最初，文件描述符（fd）主要用于引用打开的文件和目录等资源。如今，它们被用来引用Linux用户空间中几乎所有类型的运行时资源，包括打开的设备、内存（<a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd_create(2)</code></a>）、定时器（<a href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"><code>timefd_create(2)</code></a>）甚至进程（通过新的<a href="https://man7.org/linux/man-pages/man2/pidfd_open.2.html"><code>pidfd_open(2)</code></a>系统调用）。文件描述符的广泛应用使得“万物皆文件描述符”成为UNIX的座右铭。</p><p>由于文件描述符的普及，现代软件往往需要同时处理更多的文件描述符。与Linux上的大多数运行时资源一样，文件描述符也有其限制：一旦达到通过<a href="https://man7.org/linux/man-pages/man2/getrlimit.2.html"><code>RLIMIT_NOFILE</code></a>配置的限制，任何进一步的分配尝试都会被拒绝，并返回<code>EMFILE</code>错误，除非关闭一些已经打开的文件描述符。</p><p>以前文件描述符的限制普遍较低。当Linux内核首次调用用户空间时，<code>RLIMIT_NOFILE</code>的默认值设置为软限制1024和硬限制4096。软限制是实际生效的限制，可以通过程序自身调整到硬限制，但超过硬限制则需要更高权限。1024个文件描述符的限制使得文件描述符成为一种稀缺资源，导致开发者在使用时非常谨慎。这也引发了一些次要描述符的使用，例如inotify观察描述符，以及代码中频繁的文件描述符关闭操作（例如<code>ftw()</code>&#x2F;<code>nftw()</code>），以避免达到限制。</p><p>一些操作系统级别的API在设计时只考虑了较低的文件描述符限制，例如BSD&#x2F;POSIX的<a href="https://man7.org/linux/man-pages/man2/select.2.html"><code>select(2)</code></a>系统调用，它只能处理数字范围在0到1023内的文件描述符。如果文件描述符超出这个范围，<code>select()</code>将越界出现异常。</p><p>Linux中的文件描述符以整数形式暴露，并且通常分配为最低未使用的整数，随着文件描述符用于引用各种资源（例如eBPF程序、cgroup等），确实需要提高这个限制。</p><p>在2019年的systemd v240版本中，采取了一些措施：</p><ul><li>在启动时，自动将两个系统控制参数<code>fs.nr_open</code>和<code>fs.file-max</code>设置为最大值，使其实际上无效，从而简化了配置。</li><li>将<code>RLIMIT_NOFILE</code>的硬限制大幅提高到512K。</li><li>保持<code>RLIMIT_NOFILE</code>的软限制为1024，以避免破坏使用<code>select()</code>的程序。但每个程序可以自行将软限制提高到硬限制，无需特权。</li></ul><p>通过这种方法，文件描述符变得不再稀缺，配置也更简便。程序可以在启动时自行提高软限制，但要确保避免使用<code>select()</code>。</p><p>具体建议如下：</p><ol><li>**不要再使用<code>select()</code>**。使用<code>poll()</code>、<code>epoll</code>、<code>io_uring</code>等更现代的API。</li><li><strong>如果程序需要大量文件描述符</strong>，在启动时将<code>RLIMIT_NOFILE</code>的软限制提高到硬限制，但确保避免使用<code>select()</code>。</li><li><strong>如果程序会fork出其他程序</strong>，在fork之前将<code>RLIMIT_NOFILE</code>的软限制重置为1024，因为子进程可能无法处理高于1024的文件描述符。</li></ol><p>这些建议能帮助你在处理大量文件描述符时避免常见问题。</p><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p><strong>supervisord</strong></p><ul><li>在2011年，<a href="https://github.com/Supervisor/supervisor/issues/26"><code>supervisord</code>报告了一个与<code>select()</code>相关的问题</a>，并在2014年得到修复。这表明<code>supervisord</code>早期版本可能使用了<code>select()</code>，但后续版本已更新。</li></ul><p><strong>Nginx</strong></p><ul><li>Nginx允许用户通过配置提高文件描述符的软限制。2015年的<a href="https://github.com/nginx/nginx/issues/311">bug报告</a>指出了Nginx在某些情况下使用<code>select()</code>并受限于1024个文件描述符的问题。目前，提供了多种方法来处理高并发场景。</li></ul><p><strong>Redis</strong></p><ul><li>Redis文档建议使用高达2^16的文件描述符数量，具体取决于实际工作负载。<ul><li><a href="https://github.com/redis/redis-py/issues/419#issuecomment-41134427">2013年12月，<code>redis-py</code>的<code>select()</code>问题</a>，在2014年6月修复。</li><li>2015年<a href="https://github.com/redis/hiredis/issues/385#issue-123387718"><code>redis/hiredis</code>的问题</a>，用户依赖<code>select()</code>。</li><li><a href="https://blog.pjam.me/posts/select-syscall-in-rust/">2020年11月的文章</a>提到Redis仍将<code>select()</code>作为后备方案，参考了<a href="https://github.com/redis/redis/blob/e12f2decc1cf7742878d516d89d38af178119b17/src/ae_select.c"><code>ae_select.c</code></a>文件。</li></ul></li></ul><p><strong>Apache HTTP Server</strong></p><ul><li>2002年的<a href="https://github.com/apache/httpd/commit/d7bff9e33d304ff95e2b888fd1f0e3b56a62e041">commit</a>显示了Apache HTTP Server早期使用<code>select()</code>。尽管Apache后续增加了对其他I&#x2F;O多路复用机制的支持，但在处理较低并发连接时，仍可能使用<code>select()</code>。</li></ul><p><strong>PostgreSQL</strong></p><ul><li>PostgreSQL没有硬限制，以避免对其他运行的软件产生负面影响。在容器化环境中，这个问题不太严重，因为可以为容器设置适当的限制。PostgreSQL提供了一个配置选项<a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS"><code>max_files_per_process</code></a>，限制每个进程可以打开的最大文件数。</li><li>PostgreSQL的源代码中仍然有使用<code>select()</code>的地方。</li></ul><p><strong>MongoDB</strong></p><ul><li>2014年，<a href="https://narkive.com/CrxObBlI.2">MongoDB仍在使用<code>select()</code></a>。在<a href="https://github.com/mongodb/mongo/blob/91fc5673cab5d1267fd805f1375577df9072ea1b/src/mongo/util/net/listen.cpp#L252-L270">3.7.5版本</a>中，<code>select()</code>仍在<code>listen.cpp</code>中使用，但在<a href="https://github.com/mongodb/mongo/commit/55aac9ac800531ad021f18f56d69c69ac5619245">3.7.6版本</a>（<strong>2018年4月</strong>）中被移除。不过，<a href="https://github.com/mongodb/mongo/blob/8de341d0d2011b51eb1d140fb4820424d29fe510/src/mongo/transport/asio/asio_utils.cpp#L131">MongoDB的源代码</a>中仍然存在<code>select()</code>的调用。</li></ul><h3 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h3><p>虽然 cgroup 控制器在现代资源管理中起着重要作用，但 <code>ulimit</code> 作为一种传统的资源管理机制，依然不可或缺。</p><p>在容器中，默认的 <code>ulimit</code> 设置是从 <code>containerd</code> 继承的（而非 <code>dockerd</code>），这些设置在 <code>containerd.service</code> 的 systemd 单元文件中被配置为无限制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep ^Limit /lib/systemd/system/containerd.service</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>虽然这些设置满足 <code>containerd</code> 自身的需求，但对于其运行的容器来说，这样的配置显得过于宽松。相比之下，主机系统上的用户（包括 root 用户）的 <code>ulimit</code> 设置则相当保守（以下是来自 Ubuntu 18.04 的示例）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sh复制代码$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 62435</span><br><span class="line">max locked memory       (kbytes, -l) 16384</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 62435</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure><p>这种宽松的容器设置可能会引发一系列问题，例如容器滥用系统资源，甚至导致 DoS 攻击。尽管 cgroup 限制通常用于防止这些问题，但将 <code>ulimit</code> 设置为更合理的值也是必要的。</p><p>特别是 <code>RLIMIT_NOFILE</code>（打开文件的数量限制）被设置为 2^30（即 1073741816），这会导致一些程序运行缓慢，因为这些程序会遍历所有可能打开的文件描述符，并在每次 fork&#x2F;exec 之前关闭这些文件描述符（或设置 CLOEXEC 位）。以下是一些具体情况：</p><ul><li><strong>rpm</strong>：在<a href="https://github.com/moby/moby/issues/23137">安装 RPM 以创建新的 Docker 镜像时性能缓慢 #23137</a> 和 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1537564">Red Hat Bugzilla #1537564</a> 中有报告，修复方案为：<a href="https://github.com/rpm-software-management/rpm/pull/444">优化并统一在文件描述符上设置 CLOEXEC 的 rpm-software-management&#x2F;rpm#444</a>（在 Fedora 28 中修复）。</li><li><strong>python2</strong>：在 <a href="https://github.com/docker/for-linux/issues/502">Docker 18.09 上 PTY 进程的创建速度大大降低 #502</a> 中有报告，建议的修复方案为：<a href="https://github.com/python/cpython/pull/11584">subprocess.Popen: 在 Linux 上优化 close_fds python&#x2F;cpython#11584</a>（由于 python2 已经冻结，所以此修复方案不会被采用）。</li><li><strong>python 的 pexpect&#x2F;ptyprocess 库</strong>：在 <a href="https://github.com/pexpect/ptyprocess/issues/50">PtyProcess.spawn（以及因此 pexpect）在 close() 循环中速度降低 #50</a> 中有报告。</li></ul><p>逐一解决这些问题既复杂且收益低，其中一些软件已经过时，另外有一些软件难以修复。上述列表并不全面，可能还有更多类似的问题尚未觉察到。</p><h4 id="探究资源消耗"><a href="#探究资源消耗" class="headerlink" title="探究资源消耗"></a>探究资源消耗</h4><p><code>2^16</code>（65k）个<code>busybox</code>容器的预估资源使用情况如下所示：</p><ul><li>在 <code>containerd</code> 中，共需 688k 个任务和 206 GB（192 GiB）的内存（每个容器约需 10.5 个任务和 3 MiB 的内存）。</li><li>至少需要将 <code>containerd.service</code> 的 <code>LimitNOFILE</code> 设置为 262144。</li><li>打开的文件数达到 249 万（其中<code>fs.file-nr</code> 必须低于 <code>fs.file-max</code> 限制），每个容器大约需要 38 个文件描述符。</li><li>容器的 cgroup 需要 25 GiB 的内存（每个容器大约需要 400 KiB）。</li></ul><p>因此<code>LimitNOFILE=524288</code>（自 v240 版本以来，systemd 的默认值）对于大多数系统作为默认值已经足够，其能满足 <code>docker.service</code> 和 <code>containerd.service</code> 支持 65k 个容器的资源需求。</p><p>从GO 1.19开始将隐式地将 <code>fork</code> &#x2F; <code>exec</code> 进程的软限制恢复到默认值。在此之前，Docker 守护进程可以通过配置 <code>default-ulimit</code> 设置来强制容器使用 <code>1024</code> 的软限制。</p><ol><li>测试详情</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fedora 37 VM 6.1.9 kernel x86_64 (16 GB memory)</span><br><span class="line">Docker v23, containerd 1.6.18, systemd v251</span><br><span class="line"></span><br><span class="line"># Additionally verified with builds before Go 1.19 to test soft limit lower than the hard limit:</span><br><span class="line">dnf install docker-ce-3:20.10.23 docker-ce-cli-1:20.10.23 containerd.io-1.6.8</span><br></pre></td></tr></table></figure><p>在Fedora 37 VM上大约有 1800 个文件描述符被打开（<code>sysctl fs.file-nr</code>）。通过 shell 循环运行 <code>busybox</code> 容器直到失败，并调整 <code>docker.service</code> 和 <code>containerd.service</code> 的 <code>LimitNOFILE</code> 来收集测试数据：</p><ul><li><code>docker.service</code> - <code>6:1</code> 的比例（使用 <code>--network=host</code> 时是 <code>5:1</code>），在 <code>LimitNOFILE=5120</code> 下大约能运行 853 个容器（使用主机网络时为 1024）。</li><li><code>containerd.service</code> - <code>4:1</code> 的比例（未验证 <code>--network=host</code> 是否会降低了比例），<code>LimitNOFILE=1024</code> 能支持 256 个容器，前提是 <code>docker.service</code> 的 <code>LimitNOFILE</code> 也足够高（如 <code>LimitNOFILE=2048</code>）。</li></ul><p>每个容器的资源使用模式：</p><ul><li>每个容器的 systemd <code>.scope</code> 有 1 个任务和大约 400 KiB 的内存（<code>alpine</code> 和 <code>debian</code> 稍少）。</li><li>每个容器增加了 10.5 个任务和 3 MiB 的内存。</li><li>每个正在运行的容器大约打开了 38 个文件。</li></ul><p>在 <code>docker.service</code> 中设置 <code>LimitNOFILE=768</code>，然后执行 <code>systemctl daemon-reload &amp;&amp; systemctl restart docker</code>。通过 <code>cat /proc/$(pidof dockerd)/limits</code> 确认该限制是否已应用。</p><p>运行以下命令列出：</p><ul><li>正在运行的容器数量。</li><li>打开的文件数量。</li><li><code>containerd</code> 和 <code>dockerd</code> 守护进程分别使用的任务和内存数量。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Useful to run before the loop to compare against output after the loop is done</span><br><span class="line">(pgrep containerd-shim | wc -l) &amp;&amp; sysctl fs.file-nr \</span><br><span class="line">  &amp;&amp; (echo &#x27;Containerd service:&#x27; &amp;&amp; systemctl status containerd | grep -E &#x27;Tasks|Memory&#x27;) \</span><br><span class="line">  &amp;&amp; (echo &#x27;Docker service:&#x27; &amp;&amp; systemctl status docker | grep -E &#x27;Tasks|Memory&#x27;)</span><br></pre></td></tr></table></figure><p>运行以下循环时，最后几个容器将失败，大约创建 123 个容器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># When `docker.service` limit is the bottleneck, you may need to `CTRL + C` to exit the loop</span><br><span class="line"># if it stalls while waiting for new FDs once exhausted and outputting errors:</span><br><span class="line">for i in $(seq 1 130); do docker run --rm -d busybox sleep 180; done</span><br></pre></td></tr></table></figure><p>可以添加额外的选项：</p><ul><li><code>--network host</code>：避免每次 <code>docker run</code> 时向默认的 Docker 桥接器创建新的 veth 接口（参见 <code>ip link</code>）。</li><li><code>--ulimit &quot;nofile=1023456789&quot;</code>：不会影响内存使用，但在基于 Debian 的发行版中，值高于 <code>fs.nr_open</code>（1048576）将失败，请使用该值或更低的值。</li><li><code>--cgroup-parent=LimitTests.slice</code>：类似 <code>docker stats</code> 但与其他容器隔离，<code>systemd-cgtop</code> 报告内存使用时包括磁盘缓存（可使用 <code>sync &amp;&amp; sysctl vm.drop_caches=3</code> 清除）。</li></ul><p>为更好了解所有创建容器的资源使用情况，创建一个用于测试的临时 slice：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/LimitTests.slice</span><br><span class="line">systemd-cgtop --order=memory LimitTests.slice</span><br></pre></td></tr></table></figure><p>显示整个 slice 和每个容器的内存使用情况，一个 <code>busybox</code> 容器大约使用 400 KiB 的内存。</p><ol start="2"><li>限制对子进程的影响</li></ol><p>我原以为子进程会继承父进程的文件描述符（FD）限制。然而实际却是，每个进程继承限制但有独立的计数。</p><ul><li>可以通过以下命令观察 <code>dockerd</code> 和 <code>containerd</code> 进程打开的文件描述符数量：<code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>。</li><li>这不适用于负责容器的 <code>containerd-shim</code> 进程，所以 <code>ls -1 /proc/$(pgrep --newest --exact containerd-shim)/fd | wc -l</code> 不会有用。</li></ul><p>为了验证这一点，可以运行以下测试容器：<code>docker run --rm -it --ulimit &quot;nofile=1024:1048576&quot; alpine bash</code>。然后尝试以下操作：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹并添加许多文件：</span></span><br><span class="line"><span class="built_in">mkdir</span> /tmp/test &amp;&amp; <span class="built_in">cd</span> /tmp/test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建空文件：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 3 2048); <span class="keyword">do</span> <span class="built_in">touch</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开文件并指定文件描述符：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 1000 1030); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>&quot;</span>; <span class="built_in">eval</span> <span class="string">&quot;exec <span class="variable">$&#123;x&#125;</span>&lt; <span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"><span class="comment"># 因为软限制在 1024，所以会失败。提高限制：</span></span><br><span class="line"><span class="built_in">ulimit</span> -Sn 2048</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在前面的循环将成功。</span></span><br><span class="line"><span class="comment"># 你可以覆盖整个初始软限制范围（不包括 FDs 0-2：stdin、stdout、stderr）：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 3 1024); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>&quot;</span>; <span class="built_in">eval</span> <span class="string">&quot;exec <span class="variable">$&#123;x&#125;</span>&lt; <span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多个容器进程/子进程打开尽可能多的文件：</span></span><br><span class="line"><span class="comment"># 可以在新 shell 进程中运行相同的循环 `ash -c &#x27;for ... done&#x27;`</span></span><br><span class="line"><span class="comment"># 或通过另一个终端的 `docker exec` 进入容器并在 `/tmp/test` 再次运行循环。</span></span><br><span class="line"><span class="comment"># 每个进程可以根据其当前软限制打开文件，`dockerd`、`containerd` 或容器的 PID 1 的限制无关。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"><span class="comment">### 提示 ###</span></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以观察当前应用的限制：</span></span><br><span class="line"><span class="built_in">cat</span> /proc/self/limits</span><br><span class="line"><span class="comment"># 如果未达到软限制（由于管道），这将报告已使用的限制：</span></span><br><span class="line"><span class="built_in">ls</span> -1 /proc/self/fd | <span class="built_in">wc</span> -l</span><br><span class="line"><span class="comment"># 否则，若这是唯一运行的 `ash` 进程，可以查询其 PID 获取信息：</span></span><br><span class="line"><span class="built_in">ls</span> -1 /proc/$(pgrep --newest --exact ash)/fd | <span class="built_in">wc</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 容器中的进程数：</span></span><br><span class="line"><span class="comment"># `docker stats` 列出容器的 PIDs 数量，</span></span><br><span class="line"><span class="comment"># `systemd-cgtop` 的 Tasks 列也报告相同值。</span></span><br><span class="line"><span class="comment"># 或者如果知道 cgroup 名称，如 `docker-&lt;CONTAINER_ID&gt;.scope`：</span></span><br><span class="line"><span class="comment"># （注意：路径可能因 `--cgroup-parent` 不同）</span></span><br><span class="line"><span class="built_in">cat</span> /sys/fs/cgroup/system.slice/docker-&lt;CONTAINER_ID&gt;.scope/pids.current</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出进程及其 PIDs：</span></span><br><span class="line"><span class="comment"># 对于单个容器，可以可视化进程树：</span></span><br><span class="line">pstree --arguments --show-pids $(pgrep --newest --exact containerd-shim)</span><br><span class="line"><span class="comment"># 或者如果知道 cgroup 名称，如 `docker-&lt;CONTAINER_ID&gt;.scope`：</span></span><br><span class="line">systemd-cgls --unit docker-&lt;CONTAINER_ID&gt;.scope</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察内存监控中的磁盘缓存，通过创建 1GB 文件：</span></span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=bigfile bs=1M count=1000</span><br><span class="line">free -h</span><br><span class="line"><span class="comment"># `systemd-cgtop` 会将此容器的内存使用量增加 1GB，</span></span><br><span class="line"><span class="comment"># 而 `docker stats` 仅增加约 30MiB（按比例）。</span></span><br><span class="line"><span class="comment"># 在容器外清除缓存后再次观察内存使用情况：</span></span><br><span class="line"><span class="built_in">sync</span> &amp;&amp; sysctl vm.drop_caches=3</span><br></pre></td></tr></table></figure><ol start="3"><li>结果观察</li></ol><ul><li>每个进程将这些文件描述符添加到 <code>fs.file-nr</code> 返回的打开文件计数中，并在该进程关闭时释放它们。</li><li>重新运行同一进程的循环不会变化，因为文件已经被计算为该进程打开的。</li><li>这涉及到内存成本：<ul><li>每个通过 <code>touch</code> 创建的文件大约占用 <code>2048</code> 字节（仅在打开前占用磁盘缓存）。</li></ul></li><li>每个打开的文件（每个文件描述符引用都会使 <code>fs.file-nr</code> 增加）大约需要 <code>512</code> 字节的内存。<ul><li>以这种方式创建 512k 个文件大约会占用 1.1 GiB 的内存（当至少有一个文件描述符打开时，使用 <code>sysctl vm.drop_caches=3</code> 也不会释放），每个进程打开等量的文件描述符还会额外使用 250 MiB（262 MB）。</li></ul></li></ul><ol start="4"><li>错误处理</li></ol><p>这些问题主要与系统服务的文件描述符限制有关，不同服务的限制耗尽会导致不同错误。</p><p>有时这会导致任何<code>docker</code>命令（如<code>docker ps</code>）挂起（守护进程耗尽限制）。常见现象包括：</p><ul><li>容器未运行（*<code>pgrep containerd-shim</code>没有输出，但<code>docker ps</code>列出的容器超出预期的退出时间*）。</li><li>容器在<code>containerd-shim</code>进程中占用内存，即使执行了<code>systemctl stop docker containerd</code>。有时需要<code>pkill containerd-shim</code>来清理，并且<code>systemctl start docker containerd</code>会在<code>journalctl</code>中记录错误，处理已死的shims的清理（<em>根据容器数量，这可能会超时，需要再次启动<code>containerd</code>服务</em>）。</li><li>即使排除了所有这些因素，仍然有额外的几百MB内存使用。由于它似乎不属于任何进程，推测是内核内存。我尝试运行的最大容器数量大约是1600个左右。</li></ul><h4 id="docker-service超出限制"><a href="#docker-service超出限制" class="headerlink" title="docker.service超出限制"></a><code>docker.service</code>超出限制</h4><p>每次<code>docker run</code>时，系统会输出不同的错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0000] Error waiting for container: container caff476371b6897ef35a95e26429f100d0d929120ff1abecc8a16aa674d692bf: driver &quot;overlay2&quot; failed to remove root filesystem: open /var/lib/docker/overlay2/35f26ec862bb91d7c3214f76f8660938145bbb36eda114f67e711aad2be89578-init/diff/etc: too many open files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: time=&quot;2023-03-12T02:26:20Z&quot; level=fatal msg=&quot;failed to create a netlink handle: could not get current namespace while creating netlink socket: too many open files&quot;: unknown.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to initialize logging driver: open /var/lib/docker/containers/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f-json.log: too many open files.</span><br></pre></td></tr></table></figure><h4 id="containerd-service限制超出"><a href="#containerd-service限制超出" class="headerlink" title="containerd.service限制超出"></a><code>containerd.service</code>限制超出</h4><p>我也观察到一些类似的错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to start shim: start failed: : pipe2: too many open files: unknown.</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>2023年8月</strong>：在<code>docker.service</code>中<a href="https://github.com/moby/moby/pull/42373">移除了<code>LimitNOFILE=infinity</code></a>。</li><li><strong>2021年5月</strong>：<code>LimitNOFILE=infinity</code> 和 <code>LimitNPROC=infinity</code> <a href="https://github.com/moby/moby/pull/42373">重新添加回<code>docker.service</code></a>，以与Docker CE的配置同步。<ul><li>这个PR是一个合并提交，源自<a href="https://github.com/moby/moby/commit/80039b4699e36ceb0eb81109cd1686aaa805c5ec">2018年9月的提交</a>。</li></ul></li><li><strong>2016年7月</strong>：<a href="https://github.com/moby/moby/pull/24555"><code>LimitNOFILE=infinity</code>更改为<code>LimitNOFILE=1048576</code></a>。<ul><li>讨论引用了<a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible">2009年StackOverflow上的回答</a>，关于特定发行版&#x2F;内核中<code>infinity</code>被限制为<code>2^20</code>。今天的一些系统上，这个上限可以是1024倍更高（*<code>2^30 == 1073741816</code>，超过10亿*）。</li></ul></li><li><strong>2016年7月</strong>：<a href="https://github.com/moby/moby/pull/24307"><code>LimitNOFILE</code>和<code>LimitNPROC</code>从<code>1048576</code>更改为<code>infinity</code></a>。<ul><li>这个PR不久后撤回了对<code>LimitNOFILE</code>的更改。</li></ul></li><li><strong>2014年3月</strong>：<a href="https://github.com/moby/moby/pull/4455#issuecomment-36679884">原始<code>LimitNOFILE</code> + <code>LimitNPROC</code>以<code>1048576</code>添加</a>。<ul><li>链接的PR评论提到这个<code>2^20</code>的值已经高于Docker所需。</li></ul></li></ul><p><strong>当前状态：</strong></p><ul><li>在Docker v25之前，<code>LimitNOFILE=infinity</code>仍然是默认设置，除非将其回退。</li><li><code>containerd</code> 已经<a href="https://github.com/containerd/containerd/pull/8924">合并了相应的更改</a>，从他们的systemd服务文件中移除了<code>LimitNOFILE</code>设置。</li></ul><h4 id="Systemd-240"><a href="#Systemd-240" class="headerlink" title="Systemd &lt; 240"></a>Systemd &lt; 240</h4><p>在某些systemd版本中，设置<code>LimitNOFILE</code>为无穷大可能导致它被限制为65536。请检查服务配置：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># ulimit -n -u</span></span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">max user processes              (-u) 499403</span><br></pre></td></tr></table></figure><p>containerd的systemd服务配置如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /usr/lib/systemd/system/containerd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target local-fs.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line">Type=notify</span><br><span class="line">Delegate=<span class="built_in">yes</span></span><br><span class="line">KillMode=process</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">OOMScoreAdjust=-999</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>查看配置对docker和containerd进程的影响：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># cat /proc/$(pidof dockerd)/limits</span></span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max open files            1048576              1048576              files     </span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># cat /proc/$(pidof containerd)/limits</span></span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max open files            1048576              1048576              files     </span><br></pre></td></tr></table></figure><p>这个补丁使systemd查看<code>/proc/sys/fs/nr_open</code>来找到内核中编译的当前最大打开文件数，并尝试将<code>RLIMIT_NOFILE</code>的最大值设置为此值。这样做的好处是所选的限制值不太随意，并且改善了在设置了rlimit的容器中systemd的行为。</p><p>详细讨论见：<a href="https://github.com/systemd/systemd/issues/6559">systemd GitHub issue</a>。</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li><a href="https://github.com/moby/moby/issues/45838">https://github.com/moby/moby/issues/45838</a></li><li><a href="https://github.com/moby/moby/issues/23137">https://github.com/moby/moby/issues/23137</a></li><li><a href="https://0pointer.net/blog/file-descriptor-limits.html">https://0pointer.net/blog/file-descriptor-limits.html</a></li><li><a href="https://www.codenong.com/cs105896693/">https://www.codenong.com/cs105896693/</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li><li><a href="https://github.com/cri-o/cri-o/issues/7703">https://github.com/cri-o/cri-o/issues/7703</a></li><li><a href="https://github.com/envoyproxy/envoy/issues/31502">https://github.com/envoyproxy/envoy/issues/31502</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="container" scheme="https://zoues.com/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>Why does RLIMIT_NOFILE slow down your containerized application</title>
    <link href="https://zoues.com/posts/5cdbe8ce/"/>
    <id>https://zoues.com/posts/5cdbe8ce/</id>
    <published>2024-04-30T05:32:59.000Z</published>
    <updated>2024-04-30T06:11:34.615Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>thanks <strong><a href="https://github.com/polarathene">polarathene</a></strong></p></blockquote><h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>The max open files limit <code>NOFILE</code> of dockerd is 1048576, which is defined in dockerd’s systemd unit file.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/$(pidof dockerd)/limits | grep &quot;Max open files&quot;</span><br><span class="line">Max open files            1048576              1048576              files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show docker | grep LimitNOFILE</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNOFILESoft=1048576</span><br></pre></td></tr></table></figure><p>However, inside the container, the value of the limit is a very large number — 1073741816:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm ubuntu bash -c &quot;cat /proc/self/limits&quot; | grep  &quot;Max open files&quot;</span><br><span class="line">Max open files            1073741816           1073741816           files</span><br></pre></td></tr></table></figure><p>It may cause the program iterate all available fds until the limit is reached; for example, the <code>xinetd</code> program sets the number of file descriptors using <code>setrlimit(2)</code> at initialization, which causes unnecessary waste of CPU resources and time on closing 1073741816 fds.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@1b3165886528# strace xinetd</span><br><span class="line">execve(&quot;/usr/sbin/xinetd&quot;, [&quot;xinetd&quot;], 0x7ffd3c2882e0 /* 9 vars */) = 0</span><br><span class="line">brk(NULL)                               = 0x557690d7a000</span><br><span class="line">arch_prctl(0x3001 /* ARCH_??? */, 0x7ffee17ce6f0) = -1 EINVAL (Invalid argument)</span><br><span class="line">mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb14255c000</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">close(12024371)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024372)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024373)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024374)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024375)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024376)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024377)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024378)                         = -1 EBADF (Bad file descriptor)</span><br></pre></td></tr></table></figure><p>we found similar cases:</p><h3 id="yum-hang"><a href="#yum-hang" class="headerlink" title="yum hang"></a>yum hang</h3><p>I noticed that newest version of docker, on rockylinux-9, taken from <a href="https://download.docker.com/linux/centos/$releasever/$basearch/stable">https://download.docker.com/linux/centos/$releasever/$basearch/stable</a> are a bit slow especially for operations done by yum</p><p>On both centos-7 and rocky-9 hosts I did:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><p>On centos7 host it takes ~2 minutes<br>On rocky-9 host after an hour it did not complete the process, I can leave it under tmux to discover how much time it takes</p><p>reproduce steps:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><h3 id="rpm-slow"><a href="#rpm-slow" class="headerlink" title="rpm slow"></a>rpm slow</h3><p>run below comamnd on host:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion</span><br></pre></td></tr></table></figure><p>spend time：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m11.248s</span><br><span class="line">user    0m7.316s</span><br><span class="line">sys     0m1.932s</span><br></pre></td></tr></table></figure><p>when test it in container</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --net=none --log-driver=none -v &quot;/workspace:/workspace&quot; -v &quot;/disks:/disks&quot; opensuse bash -c &quot;time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion&quot;</span><br></pre></td></tr></table></figure><p>spend time：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m31.089s</span><br><span class="line">user    0m14.876s</span><br><span class="line">sys     0m12.524s</span><br></pre></td></tr></table></figure><p>Here’s the relevant section of code from RPM. It’s part of the POSIX lua library that’s inside RPM, and was added by <a href="https://github.com/rpm-software-management/rpm/commit/7a7c31f551ff167f8718aea6d5048f6288d60205">rpm-software-management&#x2F;rpm@<code>7a7c31f</code></a>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">static int Pexec(lua_State *L) /** exec(path,[args]) */</span><br><span class="line">&#123;</span><br><span class="line">/* ... */</span><br><span class="line">open_max = sysconf(_SC_OPEN_MAX);</span><br><span class="line">if (open_max == -1) &#123;</span><br><span class="line">    open_max = 1024;</span><br><span class="line">&#125;</span><br><span class="line">for (fdno = 3; fdno &lt; open_max; fdno++) &#123;</span><br><span class="line">    flag = fcntl(fdno, F_GETFD);</span><br><span class="line">    if (flag == -1 || (flag &amp; FD_CLOEXEC))</span><br><span class="line">continue;</span><br><span class="line">    fcntl(fdno, F_SETFD, FD_CLOEXEC);</span><br><span class="line">&#125;</span><br><span class="line">/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So the reason for doing <code>F_GETFD</code> is because they are setting all of the FDs to <code>CLOEXEC</code> before doing the requested <code>exec(2)</code>. There’s a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=919801">redhat bugzilla entry</a> in the commit message, which says that this was an SELinux issue where Fedora (or RHEL) have an SELinux setup where you cannot execute a process if it will inherit FDs it shouldn’t have access to?</p><p>I guess if this is an SELinux issue it should be handled by only applying this fix when SELinux is used (though there are arguably security reasons why you might want to <code>CLOEXEC</code> every file descriptor).</p><h3 id="PtyProcess-spawn-slowdown-in-close-loop"><a href="#PtyProcess-spawn-slowdown-in-close-loop" class="headerlink" title="PtyProcess.spawn slowdown in close() loop"></a>PtyProcess.spawn slowdown in close() loop</h3><p>The following code in ptyprocess</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Do not allow child to inherit open file descriptors from parent, </span><br><span class="line"> # with the exception of the exec_err_pipe_write of the pipe </span><br><span class="line"> # and pass_fds. </span><br><span class="line"> # Impose ceiling on max_fd: AIX bugfix for users with unlimited </span><br><span class="line"> # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange() </span><br><span class="line"> # occasionally raises out of range error </span><br><span class="line"> max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0]) </span><br><span class="line"> spass_fds = sorted(set(pass_fds) | &#123;exec_err_pipe_write&#125;) </span><br><span class="line"> for pair in zip([2] + spass_fds, spass_fds + [max_fd]): </span><br><span class="line">     os.closerange(pair[0]+1, pair[1]) </span><br></pre></td></tr></table></figure><p>is looping through all possible file descriptors in order to close those (note that <code>closerange()</code> implemented as a loop at least on Linux). In case the limit of open fds (aka <code>ulimit -n</code>, aka <code>RLIMIT_NOFILE</code>, aka <code>SC_OPEN_MAX</code>) is set too high (for example, with recent docker it is 1024*1024), this loop takes considerable time (as it results in about a million <code>close()</code> syscalls).</p><p>The solution (at least for Linux and Darwin) is to obtain the list of actually opened fds, and only close those. This is implemented in <code>subprocess</code> module in Python3, and there is a backport of it to Python2 called subprocess32.</p><h3 id="MySQL-has-been-known-to-allocate-excessive-memory"><a href="#MySQL-has-been-known-to-allocate-excessive-memory" class="headerlink" title="MySQL has been known to allocate excessive memory"></a><a href="https://github.com/overhangio/tutor/pull/810/files#diff-a7c720c9cc5fa1b93c92c33e3e75a0b0880d80915d867d57d48464ff4f472b0aR115-R117">MySQL has been known to allocate excessive memory</a></h3><p>In idle mode, the “mysql” container should use ~200MB memory; ~200-300MB for the the “lms” and “cms” containers.</p><p>On some operating systems, such as RedHat, Arch Linux or Fedora, a very high limit of the number of open files (<code>nofile</code>) per container may cause the “mysql”, “lms” and “cms” containers to use a lot of memory: up to 8-16GB. To check whether you might impacted, run::</p><pre><code>cat /proc/$(pgrep dockerd)/limits | grep &quot;Max open files&quot;</code></pre><p>If the output is 1073741816 or higher, then it is likely that you are affected by <code>this mysql issue &lt;https://github.com/docker-library/mysql/issues/579&gt;</code><strong>. To learn more about the root cause, read <code>this containerd issue comment &lt;https://github.com/containerd/containerd/pull/7566#issuecomment-1285417325&gt;</code></strong>. Basically, the OS is hard-coding a very high limit for the allowed number of open files, and this is causing some containers to fail. To resolve the problem, you should configure the Docker daemon to enforce a lower value, as described <code>here &lt;https://github.com/docker-library/mysql/issues/579#issuecomment-1432576518&gt;</code>__. Edit <code>/etc/docker/daemon.json</code> and add the following contents::</p><pre><code>&#123;    &quot;default-ulimits&quot;: &#123;        &quot;nofile&quot;: &#123;            &quot;Name&quot;: &quot;nofile&quot;,            &quot;Hard&quot;: 1048576,            &quot;Soft&quot;: 1048576        &#125;    &#125;&#125;</code></pre><p>Check your configuration is valid with:</p><pre><code>dockerd --validate</code></pre><p>Then restart the Docker service:</p><pre><code>sudo systemctl restart docker.service</code></pre><h2 id="Technical-Background-Introduction"><a href="#Technical-Background-Introduction" class="headerlink" title="Technical Background Introduction"></a>Technical Background Introduction</h2><h3 id="1-RLIMIT-NOFILE"><a href="#1-RLIMIT-NOFILE" class="headerlink" title="1. RLIMIT_NOFILE"></a>1. RLIMIT_NOFILE</h3><blockquote><p><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></p></blockquote><p>Don’t use. Be careful when raising the soft limit above 1024, since <a href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a> cannot function with file descriptors above 1023 on Linux. Nowadays, the hard limit defaults to 524288, a very high value compared to historical defaults. Typically applications should increase their soft limit to the hard limit on their own, if they are OK with working with file descriptors above 1023, i.e. do not use <a href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a>. Note that file descriptors are nowadays accounted like any other form of memory, thus there should not be any need to lower the hard limit. Use <code>MemoryMax=</code> to control overall service memory use, including file descriptor memory.</p><blockquote><p><a href="https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94">https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94</a></p></blockquote><p>The Linux kernel’s current default RLIMIT_NOFILE resource limit for userspace processes is set to 1024 (soft) and 4096 (hard). Previously, systemd passed this on unmodified to all processes it forked off. With this systemd release the hard limit systemd passes on is increased to 512K, overriding the kernel’s defaults and substantially increasing the number of simultaneous file descriptors unprivileged userspace processes can allocate. Note that the soft limit remains at 1024 for compatibility reasons: the traditional UNIX select() call cannot deal with file descriptors &gt;&#x3D; 1024 and increasing the soft limit globally might thus result in programs unexpectedly allocating a high file descriptor and thus failing abnormally when attempting to use it with select() (of course, programs shouldn’t use select() anymore, and prefer poll()&#x2F;epoll, but the call unfortunately remains undeservedly popular at this time). This change reflects the fact that file descriptor<br>handling in the Linux kernel has been optimized in more recent kernels and allocating large numbers of them should be much cheaper both in memory and in performance than it used to be. Programs that<br>want to take benefit of the increased limit have to “opt-in” into high file descriptors explicitly by raising their soft limit. Of course, when they do that they must acknowledge that they cannot use select() anymore (and neither can any shared library they use — or any shared library used by any shared library they use and so on). Which default hard limit is most appropriate is of course hard to decide. However, given reports that ~300K file descriptors are used in real-life applications we believe 512K is sufficiently high as new default for now. Note that there are also reports that using very high hard limits (e.g. 1G) is problematic: some software allocates large arrays with one element for each potential file descriptor  (Java, …) — a high hard limit thus triggers excessively large memory allocations in these applications. Hopefully, the new default of 512K is a good middle ground: higher than what real-life applications currently need, and low enough for avoid triggering excessively large allocations in problematic software. (And yes, somebody should fix<br>Java.)</p><p>systemd v240 release in 2018Q4. Both Docker and Containerd projects have recently removed the line from their configs to rely on the <code>1024:524288</code> default systemd v240 provides (<em>unless the system has been configured explicitly to some other value, which the system administrator may do so when they know they need higher limits</em>).</p><h3 id="2-File-Descriptor-Limits"><a href="#2-File-Descriptor-Limits" class="headerlink" title="2.  File Descriptor Limits"></a>2.  File Descriptor Limits</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.)  to exceed this limit yield the error EMFILE.  (Historically, this limit was named RLIMIT_OFILE on BSD.)</span><br><span class="line">Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have &quot;in</span><br><span class="line">flight&quot; to other processes, by being passed across UNIX domain sockets.  This limit applies to the sendmsg(2) system call.  For further details, see unix(7).</span><br></pre></td></tr></table></figure><p>The primary way to reference, allocate and pin runtime OS resources on Linux today are file descriptors (“fds”). Originally they were used to reference open files and directories and maybe a bit more, but today they may be used to reference almost any kind of runtime resource in Linux userspace, including open devices, memory (<a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd_create(2)</code></a>), timers (<a href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"><code>timefd_create(2)</code></a>) and even processes (with the new <a href="https://man7.org/linux/man-pages/man2/pidfd_open.2.html"><code>pidfd_open(2)</code></a> system call). In a way, the philosophically skewed UNIX concept of “everything is a file” through the proliferation of fds actually acquires a bit of sensible meaning: “everything <em>has</em> a file <em>descriptor</em>“ is certainly a much better motto to adopt.</p><p>Because of this proliferation of fds, non-trivial modern programs tend to have to deal with substantially more fds at the same time than they traditionally did. Today, you’ll often encounter real-life programs that have a few thousand fds open at the same time.</p><p>Like on most runtime resources on Linux limits are enforced on file descriptors: once you hit the resource limit configured via <a href="https://man7.org/linux/man-pages/man2/getrlimit.2.html"><code>RLIMIT_NOFILE</code></a> any attempt to allocate more is refused with the <code>EMFILE</code> error — until you close a couple of those you already have open.</p><p>Because fds weren’t such a universal concept traditionally, the limit of <code>RLIMIT_NOFILE</code> used to be quite low. Specifically, when the Linux kernel first invokes userspace it still sets <code>RLIMIT_NOFILE</code> to a low value of 1024 (soft) and 4096 (hard). (Quick explanation: the <em>soft</em> limit is what matters and causes the <code>EMFILE</code> issues, the <em>hard</em> limit is a secondary limit that processes may bump their soft limit to — if they like — without requiring further privileges to do so. Bumping the limit further would require privileges however.). A limit of 1024 fds made fds a <em>scarce</em> resource: APIs tried to be careful with using fds, since you simply couldn’t have that many of them at the same time. This resulted in some questionable coding decisions and concepts at various places: often secondary descriptors that are very similar to fds — but were not actually fds — were introduced (e.g. inotify watch descriptors), simply to avoid for them the low limits enforced on true fds. Or code tried to aggressively close fds when not absolutely needing them (e.g. <code>ftw()</code>&#x2F;<code>nftw()</code>), losing the nice + stable “pinning” effect of open fds.</p><p>Worse though is that certain OS level APIs were designed having only the low limits in mind. The worst offender being the BSD&#x2F;POSIX <a href="https://man7.org/linux/man-pages/man2/select.2.html"><code>select(2)</code></a> system call: it only works with fds in the numeric range of 0…1023 (aka <code>FD_SETSIZE</code>-1). If you have an fd outside of this range, tough luck: select() won’t work, and only if you are lucky you’ll detect that and can handle it somehow.</p><p>Linux fds are exposed as simple integers, and for most calls it is guaranteed that the lowest unused integer is allocated for new fds. Thus, as long as the <code>RLIMIT_NOFILE</code> soft limit is set to 1024 everything remains compatible with <code>select()</code>: the resulting fds will also be below 1024. Yay. If we’d bump the soft limit above this threshold though and at some point in time an fd higher than the threshold is allocated, this fd would not be compatible with <code>select()</code> anymore.</p><p>Because of that, indiscriminately increasing the soft <code>RLIMIT_NOFILE</code> resource limit today for every userspace process is problematic: as long as there’s userspace code still using <code>select()</code> doing so will risk triggering hard-to-handle, hard-to-debug errors all over the place.</p><p>However, given the nowadays ubiquitous use of fds for all kinds of resources (did you know, an eBPF program is an fd? and a cgroup too? and attaching an eBPF program to cgroup is another fd? …), we’d really like to raise the limit anyway.</p><p>So before we continue thinking about this problem, let’s make the problem more complex (…uh, I mean… “more exciting”) first. Having just one hard and one soft per-process limit on fds is boring. Let’s add more limits on fds to the mix. Specifically on Linux there are two system-wide sysctls: <code>fs.nr_open</code> and <code>fs.file-max</code>. (Don’t ask me why one uses a dash and the other an underscore, or why there are two of them…) On today’s kernels they kinda lost their relevance. They had some originally, because fds weren’t accounted by any other counter. But today, the kernel tracks fds mostly as small pieces of memory allocated on userspace requests — because that’s ultimately what they are —, and thus charges them to the memory accounting done anyway.</p><p>So now, we have four limits (actually: five if you count the memory accounting) on the same kind of resource, and all of them make a resource artificially scarce that we don’t want to be scarce. So what to do?</p><p>Back in systemd v240 already (i.e. 2019) we decided to do something about it. Specifically:</p><ul><li>Automatically at boot we’ll now bump the two sysctls to their maximum, making them effectively ineffective. This one was easy. We got rid of two pretty much redundant knobs. Nice!</li><li>The <code>RLIMIT_NOFILE</code> hard limit is bumped substantially to 512K. Yay, cheap fds! <em>You</em> may have an fd, and <em>you</em>, and <em>you</em> as well, <em>everyone</em> may have an fd!</li><li>But … we left the soft <code>RLIMIT_NOFILE</code> limit at 1024. We weren’t quite ready to break all programs still using <code>select()</code> in 2019 yet. But it’s not as bad as it might sound I think: given the hard limit is bumped every program can easily opt-in to a larger number of fds, by setting the soft limit to the hard limit early on — without requiring privileges.</li></ul><p>So effectively, with this approach fds should be much less scarce (at least for programs that opt into that), and the limits should be much easier to configure, since there are only two knobs now one really needs to care about:</p><ul><li>Configure the <code>RLIMIT_NOFILE</code> hard limit to the maximum number of fds you actually want to allow a process.</li><li>In the program code then either bump the soft to the hard limit, or not. If you do, you basically declare “I understood the problem, I promise to not use <code>select()</code>, drown me fds please!”. If you don’t then effectively everything remains as it always was.</li></ul><p>Apparently this approach worked, since the negative feedback on change was even scarcer than fds traditionally were (ha, fun!). We got reports from pretty much only two projects that were bitten by the change (one being a JVM implementation): they already bumped their soft limit automatically to their hard limit during program initialization, and then allocated an array with one entry per possible fd. With the new high limit this resulted in one massive allocation that traditionally was just a few K, and this caused memory checks to be hit.</p><p>Anyway, here’s the take away of this blog story:</p><ul><li>Don’t use <code>select()</code> anymore in 2021. Use <code>poll()</code>, <code>epoll</code>, <code>iouring</code>, …, but for heaven’s sake don’t use <code>select()</code>. It might have been all the rage in the 1990s but it doesn’t scale and is simply not designed for today’s programs. I wished the man page of <code>select()</code> would make clearer how icky it is and that there are plenty of more preferably APIs.</li><li>If you hack on a program that potentially uses a lot of fds, add <a href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L373">some simple code</a> somewhere to its start-up that bumps the <code>RLIMIT_NOFILE</code> soft limit to the hard limit. But if you do this, you have to make sure your code (and any code that you link to from it) refrains from using <code>select()</code>. (Note: there’s at least one glibc NSS plugin using <code>select()</code> internally. Given that NSS modules can end up being loaded into pretty much <em>any</em> process such modules should probably be considered just buggy.)</li><li>If said program you hack on forks off foreign programs, make sure to reset the <code>RLIMIT_NOFILE</code> soft limit <a href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L394">back to 1024</a> for them. Just because your program might be fine with fds &gt;&#x3D; 1024 it doesn’t mean that those foreign programs might. And unfortunately <code>RLIMIT_NOFILE</code> is inherited down the process tree unless explicitly set.</li></ul><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><ul><li><a href="https://github.com/Supervisor/supervisor/issues/26"><code>supervisord</code> with <code>select()</code></a> (<em>2011 reported, <strong>2014</strong> fixed</em>).</li><li>Nginx will <a href="https://nginx.org/en/docs/ngx_core_module.html#worker_rlimit_nofile">raise soft limit when you tell it to</a> via config (<em><a href="https://forum.nginx.org/read.php?2,258259,258259">bug report in <strong>2015</strong>, where <code>select()</code> was used</a> by nginx limiting it to <code>1024</code></em>). Alternatively if you’re using the nginx container, you can raise the soft limit at the container level.</li><li>Redis docs advising <code>2^16</code> in example, will be dependent upon your workload.<ul><li><a href="https://github.com/redis/redis-py/issues/419#issuecomment-41134427"><code>redis-py</code> Dec 2013 issue with <code>select()</code></a>, fixed in June <strong>2014</strong></li><li><code>redis/hiredis</code> 2015 issue where a <a href="https://github.com/redis/hiredis/issues/385#issue-123387718">user was relying on <code>select()</code></a>.</li><li><a href="https://blog.pjam.me/posts/select-syscall-in-rust/">Nov 2020 article on <code>select()</code></a>, references Redis still carries <code>select()</code> as a fallback (<a href="https://github.com/redis/redis/blob/e12f2decc1cf7742878d516d89d38af178119b17/src/ae_select.c"><code>ae_select.c</code></a>)</li></ul></li><li>httpd has <code>select()</code> (see <a href="https://github.com/apache/httpd/commit/8286cb80fd160a4259ddf10c3ea016777d34d083">this 2002 commit</a>, which is <a href="https://github.com/apache/httpd/blob/bc0e56cdd3eebbe0fae3f9f5770b09236e8a4a17/modules/http/http_core.c#L241">still present today</a> in 2024_)</li><li>Postgres hasthis 2010 response for why they don’t use the hard limit to not negatively impact other software running. Less of an issue in a container, especially when you can set the limits. The limits work a little bit differently now , that the issue shouldn’t be applicable anymore (the global FD limit for a system is notably higher than the hard limit tends to be per process).<ul><li><a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS">Docs on <code>max_files_per_process</code></a>. Similar to nginx, there is a specific setting here, and it’s also per process this software manages. Both of these are doing the correct thing by not having a monolithic process sharing the same FD limit. The soft limit applied is per process, if you have 1024 processes with the soft limit of 1024, you also have <code>2^20</code> FDs available across them…not only 1024.</li><li>Postgres still has source with usage of <code>select()</code> <a href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pg_basebackup/receivelog.c#L904">here</a>, <a href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pgbench/pgbench.c#L7862">here</a> and various other locations if you want to look through it.</li></ul></li><li><a href="https://narkive.com/CrxObBlI.2">MongoDB using <code>select()</code> in 2014</a>, in the <a href="https://github.com/mongodb/mongo/blob/91fc5673cab5d1267fd805f1375577df9072ea1b/src/mongo/util/net/listen.cpp#L252-L270">3.7.5 release it was still using it for <code>listen.cpp</code></a>, but that was <a href="https://github.com/mongodb/mongo/commit/55aac9ac800531ad021f18f56d69c69ac5619245">dropped in the 3.7.6 release</a> (April <strong>2018</strong>). A <a href="https://github.com/mongodb/mongo/blob/8de341d0d2011b51eb1d140fb4820424d29fe510/src/mongo/transport/asio/asio_utils.cpp#L131"><code>select()</code> call still exists though</a>.</li></ul><h2 id="Dig-deeping-into"><a href="#Dig-deeping-into" class="headerlink" title="Dig deeping into"></a>Dig deeping into</h2><p>ulimit, being an archaic resource management mechanism, is not completely obsoleted by cgroup controllers, and it is still an essential part of system administration.</p><p>Default ulimits for a new container are derived from those of <del>dockerd</del> containerd itself. They are set in <code>containerd.service</code> systemd unit file to <code>unlimited</code> values:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep ^Limit /lib/systemd/system/containerd.service</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>This is required for containerd itself, but is way too generous for containers it runs. For comparison, ulimits for a user (including root) on the host system are pretty modest (this is an example from Ubuntu 18.04):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 62435</span><br><span class="line">max locked memory       (kbytes, -l) 16384</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 62435</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure><p>This can create a number of problems, such as container abusing system resources (e.g. DoS attacks). In general, cgroup limits should be used to prevent those, yet I think ulimits should be set to a saner values.</p><p>In particular, <code>RLIMIT_NOFILE</code>, a number of open files limit, which is set to 2^20 (aka 1048576), causes a slowdown in a number of programs, as they use the upper limit value to iterate over all potentially opened file descriptors, closing those (or setting CLOEXEC bit) before every fork&#x2F;exec. I am aware of the following cases:</p><ul><li>rpm, reported in <a href="https://github.com/moby/moby/issues/23137">Slow performance when installing RPMs to create new Docker images #23137</a>, <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1537564">https://bugzilla.redhat.com/show_bug.cgi?id=1537564</a>, fix: <a href="https://github.com/rpm-software-management/rpm/pull/444">Optimize and unite setting CLOEXEC on fds rpm-software-management&#x2F;rpm#444</a> (fixed in Fedora 28).</li><li>python2, reported in <a href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09 docker&#x2F;for-linux#502</a>, proposed fix: [<a href="https://github.com/python/cpython/pull/11584">2.7] bpo-35757: subprocess.Popen: optimize close_fds for Linux python&#x2F;cpython#11584</a> (WONTFIX as python2 is frozen)</li><li>python’s pexpect&#x2F;ptyprocess library, reported in <a href="https://github.com/pexpect/ptyprocess/issues/50">PtyProcess.spawn (and thus pexpect) slowdown in close() loop pexpect&#x2F;ptyprocess#50</a>.</li></ul><p>Attacking those one by one proved complicated and not very fruitful, as some software is obsoleted, some is hard to fix, etc. In addition, the above list is not a concise one, so there might be more cases like this we’re not aware of.</p><h3 id="Investigated-limits-impact-and-costs"><a href="#Investigated-limits-impact-and-costs" class="headerlink" title="Investigated limits impact and costs"></a>Investigated limits impact and costs</h3><p><code>2^16</code> (65k) <code>busybox</code> containers estimated resource usage:</p><ul><li>688k tasks + 206 GB (192 GiB) memory in <code>containerd</code> (<em>10.5 tasks + 3MiB per container</em>)</li><li>Requiring at minimum <code>LimitNOFILE=262144</code> (<code>containerd.service</code>) + <code>LimitNOFILE=393216</code> (<code>docker.service</code>) - based on <code>4:1</code> + <code>6:1</code> service FDs needed per container ratio.</li><li>2.49 million open files (<em><code>fs.file-nr</code> must be below <code>fs.file-max</code> limit</em>) - approx 38 FDs per container.</li><li>25 GiB memory for the containers cgroup (<em>approx 400KiB per container</em>).</li></ul><p><code>LimitNOFILE=524288</code> (<em>systemd default since v240</em>) should be more than enough for most systems as a sane default. This should be more than enough for both <code>docker.service</code> and <code>containerd.service</code> resource needs, capable of supporting 65k containers.</p><p>Containers that do need higher limits can explicitly declare that (<em>via <code>--ulimit</code> or equivalent</em>), as the upper bound is not impacted by <code>containerd.service</code>. The same can be done for lowering limits if necessary, both should rarely be necessary for most containers.</p><p>While <code>docker.service</code> and <code>containerd.service</code> need the higher soft limit (<em>enforced implicitly since Go 1.19</em>), it would be unlikely required for containers. An upcoming release of Go (with backports to 1.19) will implicitly restore the soft limit to <code>fork</code> &#x2F; <code>exec</code> processes AFAIK. Until then, the Docker daemon can be configured with <code>default-ulimit</code> setting to enforce a <code>1024</code> soft limit on containers.</p><h4 id="System-details"><a href="#System-details" class="headerlink" title="System details"></a>System details</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fedora 37 VM 6.1.9 kernel x86_64 (16 GB memory)</span><br><span class="line">Docker v23, containerd 1.6.18, systemd v251</span><br><span class="line"></span><br><span class="line"># Additionally verified with builds before Go 1.19 to test soft limit lower than the hard limit:</span><br><span class="line">dnf install docker-ce-3:20.10.23 docker-ce-cli-1:20.10.23 containerd.io-1.6.8</span><br></pre></td></tr></table></figure><h4 id="Observations-in-service-files-for-LimitNOFILE"><a href="#Observations-in-service-files-for-LimitNOFILE" class="headerlink" title="Observations in .service files for LimitNOFILE"></a>Observations in <code>.service</code> files for <code>LimitNOFILE</code></h4><p>On a fresh install (<em>via VM on Vultr</em>) there was approx 1800 file descriptors open (<code>sysctl fs.file-nr</code>). I used a shell loop to run <code>busybox</code> containers until failure and adjusted the <code>LimitNOFILE</code> for <code>docker.service</code> and <code>containerd.service</code> to collect metrics for insights.</p><p>I noticed a consistent ratio of number of FDs needed per container:</p><ul><li><code>docker.service</code> - <code>6:1</code> ratio (<em><code>5:1</code> with <code>--network=host</code></em>), approx 853 containers with <code>LimitNOFILE=5120</code> (<em>1024 with host network</em>).</li><li><code>containerd.service</code> - <code>4:1</code> ratio (<em>I did not verify if <code>--network=host</code> reduced this</em>), <code>LimitNOFILE=1024</code> should be capable of 256 containers, provided <code>docker.service</code> is also high enough (eg: <code>LimitNOFILE=2048</code>_).</li></ul><p>In <code>containerd.service</code> there was also a clear pattern in resources per container, where the <code>LimitNOFILE</code> value, image used (<em><code>busybox</code>, <code>alpine</code>, <code>debian</code></em>), and number of containers remained constant:</p><ul><li><p>Each containers systemd <code>.scope</code> has 1 task and approx 400KiB memory (little bit less for <code>alpine</code> and <code>debian</code>).</p></li><li><p>10.5 tasks + 3MiB memory added per container to <code>systemctl status containerd</code> report.</p></li><li><p>Approx 38 open files per container running (<em><code>fs.file-nr</code> after, minus the before value, divided by number of containers</em>).</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mailserver/docker-mailserver:edge</span><br></pre></td></tr></table></figure><p>  was also tested to compare to the</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sleep 180</span><br></pre></td></tr></table></figure><p>  containers:</p><pre><code>- 33 tasks per container `.scope` and 85MiB memory reported via `systemd-cgtop` (*10GiB needed min to run 120 of these containers*).- In `containerd` per container average resources were 11 tasks + 3.4MiB memory (*approx 400MiB usage for 120 of these containers*). Roughly consistent with the lighter images resource usage in `containerd`.- Files opened per container also increased to 291 (*approx 35k files open for 120 of these containers*).- If you want to reproduce for this image, `docker run` should include these extra options: `--hostname example.test --env SMTP_ONLY=1` (*hostname required to init, `SMTP_ONLY=1` skips needing an account configured*).</code></pre><p>Operations like <code>docker stats</code> need to open as many file descriptors as total containers running, otherwise it’ll hang waiting. You can observe if the daemon has reached the limit with <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</p><h4 id="Reproduction"><a href="#Reproduction" class="headerlink" title="Reproduction"></a>Reproduction</h4><p>Set <code>LimitNOFILE=768</code> in <code>docker.service</code>, then <code>systemctl daemon-reload &amp;&amp; systemctl restart docker</code>. You can confirm the limit is applied to the daemon process with <code>cat /proc/$(pidof dockerd)/limits</code>.</p><p>Running the following should list:</p><ul><li>How many containers are running.</li><li>Number of open files.</li><li>How many tasks and memory both the <code>containerd</code> and <code>dockerd</code> daemons are using.</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Useful to run before the loop to compare against output after the loop is done</span><br><span class="line">(pgrep containerd-shim | wc -l) &amp;&amp; sysctl fs.file-nr \</span><br><span class="line">  &amp;&amp; (echo &#x27;Containerd service:&#x27; &amp;&amp; systemctl status containerd | grep -E &#x27;Tasks|Memory&#x27;) \</span><br><span class="line">  &amp;&amp; (echo &#x27;Docker service:&#x27; &amp;&amp; systemctl status docker | grep -E &#x27;Tasks|Memory&#x27;)</span><br></pre></td></tr></table></figure><p>Running this loop below should fail on the last few containers, about 123 should be created:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># When `docker.service` limit is the bottleneck, you may need to `CTRL + C` to exit the loop</span><br><span class="line"># if it stalls while waiting for new FDs once exhausted and outputting errors:</span><br><span class="line">for i in $(seq 1 130); do docker run --rm -d busybox sleep 180; done</span><br></pre></td></tr></table></figure><p>You can add additional options:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--network host</span><br></pre></td></tr></table></figure><pre><code>- Avoids creating a new veth interface (see `ip link`) to the default Docker bridge each `docker run`.- Without this `docker run` [may fail after `1023` interfaces are present on a single bridge](https://github.com/moby/moby/issues/44973#issuecomment-1543747718)?- Creation will be a bit faster, and the FD to container ratio for `dockerd` is lowered to `5:1`.</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit &quot;nofile=1023456789&quot;</span><br></pre></td></tr></table></figure><pre><code>- Useful to observe that it does not affect memory usage on it&#39;s own.- Also shows `dockerd` + `containerd` limits don&#39;t affect how high this can go.- For Debian based distros this would fail as it&#39;s higher than `fs.nr_open` (`1 048 576`), use that or a lower value.</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--cgroup-parent=LimitTests.slice</span><br></pre></td></tr></table></figure><pre><code>- Similar to `docker stats` but isolated from other running containers. `systemd-cgtop` does include disk cache (*file-backed memory pages*) in it&#39;s reported memory usage however (*use `sync &amp;&amp; sysctl vm.drop_caches=3` to clear that*).- This can be useful if you want a better overview of resource usage across all the containers created:    - Create a temporary slice for testing with: `mkdir /sys/fs/cgroup/LimitTests.slice`.    - Run `systemd-cgtop --order=memory LimitTests.slice` to only view the containers running in this cgroup slice sorted by memory usage.    - Memory usage shown for the entire slice and per container. A `busybox` container uses roughly 400KB per container.</code></pre><h3 id="Limits-impact-across-process-children"><a href="#Limits-impact-across-process-children" class="headerlink" title="Limits impact across process children"></a>Limits impact across process children</h3><p>I had a misconception that child processes contributed to the parents open files limit. However as my notes in this section detail, it’s only inheriting the limits applied, each process seems to have it’s own individual count.</p><p>Although, I’m probably missing something here as I have read of processes passing down FDs to children, which is also why daemons have a common hygiene practice to close the FD range available I think? This is lower level than I’m familiar with 😅</p><ul><li>You can also observe the number of file descriptors open for the <code>dockerd</code> and <code>containerd</code> processes like this: <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</li><li>This isn’t applicable to the <code>containerd-shim</code> process that is responsible for the container, so <code>ls -1 /proc/$(pgrep --newest --exact containerd-shim)/fd | wc -l</code> won’t be useful there.</li></ul><p>To confirm this, run a container to test with: <code>docker run --rm -it --ulimit &quot;nofile=1024:1048576&quot; alpine ash</code>. Then try the following:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Create a folder to add many files to:</span><br><span class="line">mkdir /tmp/test; cd /tmp/test</span><br><span class="line"></span><br><span class="line"># Create empty files:</span><br><span class="line">for x in $(seq 3 2048); do touch &quot;$&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Open files to specific file descriptors:</span><br><span class="line">for x in $(seq 1000 1030); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"># Fails after 1024 because soft limit is capped there. Raise it:</span><br><span class="line">ulimit -Sn 2048</span><br><span class="line"></span><br><span class="line"># Now the loop before will be successful.</span><br><span class="line"># You could cover the whole original soft limit range (excluding FDs 0-2: stdin, stdout, stderr):</span><br><span class="line">for x in $(seq 3 1024); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Multiple container processes / children opening as many files:</span><br><span class="line"># You can run the same loop in a new shell process with `ash -c &#x27;for ... done&#x27;`</span><br><span class="line"># Or `docker exec` into the container from another terminal and run the loop at `/tmp/test` again.</span><br><span class="line"># Each can open files up to their current soft limit, it doesn&#x27;t matter what limits are set on `dockerd`, `containerd` or the containers PID 1.</span><br><span class="line"></span><br><span class="line">############</span><br><span class="line">### Tips ###</span><br><span class="line">############</span><br><span class="line"></span><br><span class="line"># You can observe the current limit applied:</span><br><span class="line">cat /proc/self/limits</span><br><span class="line"># And if you have not exhausted your FDs soft limit (due to the pipe),</span><br><span class="line"># this will report how much of the limit is used:</span><br><span class="line">ls -1 /proc/self/fd | wc -l</span><br><span class="line"># Otherwise, outside of the container if this is your only `ash` process running,</span><br><span class="line"># you can query it&#x27;s PID to get this information:</span><br><span class="line">ls -1 /proc/$(pgrep --newest --exact ash)/fd | wc -l</span><br><span class="line"></span><br><span class="line"># Process count in container:</span><br><span class="line"># `docker stats` should list a containers PIDs count for number of processes,</span><br><span class="line"># `systemd-cgtop` should report the same value in it&#x27;s Tasks column.</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line"># (NOTE: Path may differ if you used `--cgroup-parent`)</span><br><span class="line">cat /sys/fs/cgroup/system.slice/docker-&lt;CONTAINER_ID&gt;.scope/pids.current</span><br><span class="line"></span><br><span class="line"># List the processes with their PIDs:</span><br><span class="line"># For a single container, you can visualize the process tree:</span><br><span class="line">pstree --arguments --show-pids $(pgrep --newest --exact containerd-shim)</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line">systemd-cgls --unit docker-&lt;CONTAINER_ID&gt;.scope</span><br><span class="line"></span><br><span class="line"># You can also observe disk cache in memory monitoring by creating a 1GB file:</span><br><span class="line">dd if=/dev/zero of=bigfile bs=1M count=1000</span><br><span class="line">free -h</span><br><span class="line"># `systemd-cgtop` will include 1GB more in memory usage for the container,</span><br><span class="line"># while `docker stats` appears to account only 30MiB (scales proportionally).</span><br><span class="line"># Now clear the cache outside of the container and observe memory usage again:</span><br><span class="line">sync &amp;&amp; sysctl vm.drop_caches=3</span><br></pre></td></tr></table></figure><p>You will notice that:</p><ul><li><p>Each process adds those FDs to the open file count returned from <code>fs.file-nr</code>, and frees them when that process is closed.</p></li><li><p>You can re-run the loops for the same process and observe no change, the files are already counted as open for that process.</p></li><li><p>There is a memory cost involved:</p><ul><li>Each file <code>touch</code> costs about <code>2048</code> bytes (<em>disk-cache only until opened</em>).</li><li>Each file open (<em>1 or more FD references each increment <code>fs.file-nr</code></em>) costs about <code>512</code> bytes per FD open for it.</li><li>Creating 512k files this way uses approx 1.1GiB memory (<em>not released with <code>sysctl vm.drop_caches=3</code> while opened by at least one FD</em>), while each process opening the equivalent amount of file descriptors additionally uses 250MiB (262MB).</li></ul></li></ul><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>Nothing useful here, other than depending on which service limit was exhausted first resulted in slightly different errors.</p><p>Sometimes this made any <code>docker</code> command like <code>docker ps</code> hang (daemon exhausted limit). I had also observed:</p><ul><li>Containers not running (<em>no <code>pgrep containerd-shim</code> output, but <code>docker ps</code> listed containers running well beyond when they should have exited</em>).</li><li>Containers running with <code>containerd-shim</code> process (using memory), despite <code>systemctl stop docker containerd</code>. Sometimes this needed <code>pkill containerd-shim</code> to cleanup and <code>systemctl start docker containerd</code> would log a bunch of errors in <code>journalctl</code> handling cleanup of dead shims (<em>depending on the number of containers, this may time out and need to start the <code>containerd</code> service again</em>).</li><li>Even with all that out of the way, there was some memory usage of several hundred MB from the baseline that lingered. As it didn’t seem to belong to any process, I assume it was kernel memory. I think the largest number of containers I experimented running with was around 1600-ish.</li></ul><h4 id="docker-service-limit-exceeded"><a href="#docker-service-limit-exceeded" class="headerlink" title="docker.service limit exceeded"></a><code>docker.service</code> limit exceeded</h4><p>This failure output more errors per <code>docker run</code> but the errors varied:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0000] Error waiting for container: container caff476371b6897ef35a95e26429f100d0d929120ff1abecc8a16aa674d692bf: driver &quot;overlay2&quot; failed to remove root filesystem: openfdat /var/lib/docker/overlay2/35f26ec862bb91d7c3214f76f8660938145bbb36eda114f67e711aad2be89578-init/diff/etc: too many open files </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: time=&quot;2023-03-12T02:26:20Z&quot; level=fatal msg=&quot;failed to create a netlink handle: could not get current namespace while creating netlink socket: too many open files&quot;: unknown.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to initialize logging driver: open /var/lib/docker/containers/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f-json.log: too many open files.</span><br></pre></td></tr></table></figure><h4 id="containerd-service-limit-exceeded"><a href="#containerd-service-limit-exceeded" class="headerlink" title="containerd.service limit exceeded"></a><code>containerd.service</code> limit exceeded</h4><p>I think I’ve seen some others, but it’s usually this one:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to start shim: start failed: : pipe2: too many open files: unknown.</span><br></pre></td></tr></table></figure><h2 id="Scope-and-Explanation"><a href="#Scope-and-Explanation" class="headerlink" title="Scope and Explanation"></a>Scope and Explanation</h2><ul><li><p>Aug 2023: <code>LimitNOFILE=infinity</code>  <a href="https://github.com/moby/moby/pull/42373">remove from <code>docker.service</code></a></p></li><li><p>May 2021: <code>LimitNOFILE=infinity</code> + <code>LimitNPROC=infinity</code> <a href="https://github.com/moby/moby/pull/42373">brought back into <code>docker.service</code></a> to sync with Docker CE’s equivalent config.</p><ul><li>This PR was a merge commit of <a href="https://github.com/moby/moby/commit/80039b4699e36ceb0eb81109cd1686aaa805c5ec">this one from Sep 2018</a> (<em>commit history interleaved ranges from 2017-2021</em>).</li><li>As the PR main diff shows, <code>LimitNPROC=infinity</code> was already added, and <code>LimitNOFILE=1048576</code> was changed to <code>infinity</code> by the PR merge (<em>initially confused since I’m using <code>git blame</code> on the master branch</em>).</li></ul></li><li><p>July 2016: <a href="https://github.com/moby/moby/pull/24555"><code>LimitNOFILE=infinity</code> changed to <code>LimitNOFILE=1048576</code></a> (<em>this number is <code>2^20</code></em>).</p><ul><li>Discussion references a <a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible">2009 StackOverflow answer</a> about <code>infinity</code> being capped to <code>2^20</code> in a specific distro release &#x2F; kernel. On some systems today, that ceiling can be 1024 times higher (<em><code>2^30 == 1073741816</code>, over 1 billion</em>).</li></ul></li><li><p>July 2016: <a href="https://github.com/moby/moby/pull/24307"><code>LimitNOFILE</code> and <code>LimitNPROC</code> changed from <code>1048576</code> to <code>infinity</code></a></p><ul><li>This PR reverted the <code>LimitNOFILE</code> change shortly after as described above.</li></ul></li><li><p>March 2014: <a href="https://github.com/moby/moby/pull/4455#issuecomment-36679884">Original <code>LimitNOFILE</code> + <code>LimitNPROC</code> added with <code>1048576</code></a>.</p><ul><li><p>Linked PR comment mentions that this 2^20 value is already higher than Docker needs:</p></li><li><p>It appears it was later changed to <code>infinity</code> to improve CI times where a smaller limit was applied (<em>like <a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible#comment76259289_1213069">this comment about Ubuntu 14.04 adjusting any limit exceeding <code>2^20</code> to <code>2^10</code>?</a></em>).</p></li><li><p>PR also [referenced relevant systemd docs](<a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process">https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process</a> Properties) (<em>which may have changed since 2014</em>)</p></li></ul></li></ul><hr><p><strong>Current Status:</strong></p><ul><li><code>LimitNOFILE=infinity</code> is still the case until Docker v25, unless the team is backporting it to any releases built with Go 1.19+</li><li><code>containerd</code> has <a href="https://github.com/containerd/containerd/pull/8924">merged the equivalent change to remove <code>LimitNOFILE</code></a> from their systemd service file.</li></ul><h3 id="Systemd-240"><a href="#Systemd-240" class="headerlink" title="Systemd &lt; 240"></a>Systemd &lt; 240</h3><p>Why is LimitNOFILE not set to infinity when configured in the service?</p><p>After setting LimitNOFILE to infinity in the service, when checking the limit of the process ID (pid), it is observed that the open file limit is 65536 instead of infinity.</p><p>Please review the service configuration.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# ulimit -n -u</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">max user processes              (-u) 499403</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>containerd systemd configuration:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/lib/systemd/system/containerd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target local-fs.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"># Having non-zero Limit*s causes performance problems due to accounting overhead</span><br><span class="line"># in the kernel. We recommend using cgroups to do container-local accounting.</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line"># Comment TasksMax if your systemd version does not supports it.</span><br><span class="line"># Only systemd 226 and above support this version.</span><br><span class="line">TasksMax=infinity</span><br><span class="line">OOMScoreAdjust=-999</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Viewing the configuration effect</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# cat /proc/$(pidof dockerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us        </span><br><span class="line">[root@XXX ~]# cat /proc/$(pidof containerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">This has systemd look at /proc/sys/fs/nr_open to find the current maximum of</span><br><span class="line">open files compiled into the kernel and tries to set the RLIMIT_NOFILE max to</span><br><span class="line">it. This has the advantage the value chosen as limit is less arbitrary and also</span><br><span class="line">improves the behavior of systemd in containers that have an rlimit set: When</span><br><span class="line">systemd currently starts in a container that has RLIMIT_NOFILE set to e.g.</span><br><span class="line">100000 systemd will lower it to 65536. With this patch systemd will try to set</span><br><span class="line">the nofile limit to the allowed kernel maximum. If this fails, it will compute</span><br><span class="line">the minimum of the current set value (the limit that is set on the container)</span><br><span class="line">and the maximum value as soft limit and the currently set maximum value as the</span><br><span class="line">maximum value. This way it retains the limit set on the container.</span><br></pre></td></tr></table></figure><p>see: <a href="https://github.com/systemd/systemd/issues/6559">https://github.com/systemd/systemd/issues/6559</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://github.com/moby/moby/issues/45838">https://github.com/moby/moby/issues/45838</a></li><li><a href="https://github.com/moby/moby/issues/23137">https://github.com/moby/moby/issues/23137</a></li><li><a href="https://0pointer.net/blog/file-descriptor-limits.html">https://0pointer.net/blog/file-descriptor-limits.html</a></li><li><a href="https://www.codenong.com/cs105896693/">https://www.codenong.com/cs105896693/</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li><li><a href="https://github.com/cri-o/cri-o/issues/7703">https://github.com/cri-o/cri-o/issues/7703</a></li><li><a href="https://github.com/envoyproxy/envoy/issues/31502">https://github.com/envoyproxy/envoy/issues/31502</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;thanks &lt;strong&gt;&lt;a href=&quot;https://github.com/polarathene&quot;&gt;polarathene&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Description&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="container" scheme="https://zoues.com/categories/container/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="container" scheme="https://zoues.com/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>Interpreting common exit codes in Kubernetes You Need to Know</title>
    <link href="https://zoues.com/posts/45fc99a8/"/>
    <id>https://zoues.com/posts/45fc99a8/</id>
    <published>2024-03-06T12:23:48.000Z</published>
    <updated>2024-03-06T10:21:52.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Interpreting-common-exit-codes-in-Kubernetes"><a href="#Interpreting-common-exit-codes-in-Kubernetes" class="headerlink" title="Interpreting common exit codes in Kubernetes"></a>Interpreting common exit codes in Kubernetes</h2><p>In this article, we will delve into Kubernetes exit codes 127 and 137, explaining what they are, common causes in K8s and Docker, and how to fix them!</p><p>We will cover:</p><ol><li>History of exit codes</li><li>Exit code 127</li><li>Exit code 137</li></ol><h2 id="The-History-of-Exit-Codes"><a href="#The-History-of-Exit-Codes" class="headerlink" title="The History of Exit Codes"></a>The History of Exit Codes</h2><p>The history of process exit codes can be traced back to the early days of the Unix operating system. In Unix systems, a process exit code is an integer value passed to its parent process upon termination, used to indicate the termination status of the process. This integer value typically falls between 0 and 255, where 0 indicates successful termination, and other values are usually used to represent different errors or exceptional conditions.</p><p>Process exit codes were initially designed to provide a simple mechanism for parent processes to understand the outcome of their child processes’ execution. This allows parent processes to take appropriate actions based on the exit code of the child process, such as handling error conditions or continuing with other operations.</p><p>In Unix systems, specific exit code values often have specific meanings, such as:</p><ul><li>0: Indicates successful execution without errors.</li><li>1: Typically signifies a general error.</li><li>2: Indicates a syntax error in the command.</li><li>127: Indicates command not found.</li></ul><p>Over time, with the development of Unix operating systems and different implementations, the meanings of process exit codes may vary, but the basic concept remains unchanged.</p><p>In Linux systems, the usage of process exit codes is similar to Unix systems. Linux inherits Unix’s process management mechanism and extends and improves upon it. Therefore, process exit codes remain an important concept in Linux, used to aid in understanding and diagnosing the execution status of processes.</p><p>The history of process exit codes can be traced back to the early days of Unix systems and is an important concept in both Unix and Linux operating systems, providing a simple yet effective mechanism for inter-process communication. When an application or command terminates or fails to execute due to a fatal error, it produces exit codes in the 128 series (<code>128+n</code>), where <code>n</code> represents the signal number. <code>n</code> includes all types of termination codes, such as <code>SIGTERM</code>, <code>SIGKILL</code>, etc.</p><h2 id="Exit-Code-127"><a href="#Exit-Code-127" class="headerlink" title="Exit Code 127"></a>Exit Code 127</h2><p>Exit code 127 is not a Kubernetes-specific error code, but rather a standard exit code used in Linux and similar Unix-like operating systems. However, it is frequently encountered in Kubernetes and typically indicates that a command or binary executed within a container could not be found.</p><p>Some standard exit codes include:</p><table><thead><tr><th>Exit Code</th><th>Explanation</th></tr></thead><tbody><tr><td>0</td><td>Command executed successfully</td></tr><tr><td>1</td><td>General error</td></tr><tr><td>2</td><td>Misuse of shell builtins</td></tr><tr><td>126</td><td>Permission denied, command invoked cannot execute</td></tr><tr><td>127</td><td>Command not found, incorrect PATH</td></tr><tr><td>128+n</td><td>Command terminated by signal, fatal error encountered</td></tr><tr><td>&gt;255</td><td>Exit codes beyond 255 will be re-calculated (mod 256)</td></tr></tbody></table><p>Let’s take a look at some common reasons for exit code 127:</p><ol><li><p>Command or binary not installed</p><p>The executable specified in the <code>command</code> field of a Kubernetes container is not installed in the container’s file system. Ensure that the required binary or command is available.</p></li><li><p>Incorrect path or command</p><p>The command specified in the Pod definition is incorrect or does not exist in the specified path. This is one of the most common errors, often caused by incorrect input in the Dockerfile or pod spec’s <code>entrypoint</code> or <code>command</code>.</p></li><li><p>Missing dependencies</p><p>The application or script running inside the container lacks necessary dependencies. Ensure that all required dependencies are included in the container image.</p></li><li><p>Shell interpreter</p><p>If a script is specified as a command, ensure that the script is valid (e.g., <code>#!/bin/bash</code>) and available in the container.</p></li><li><p>Shell script syntax error</p><p>If the shell script exits with code 127, check if there are syntax errors in the script or issues preventing its execution.</p></li><li><p>Insufficient permissions</p><p>The user running the command inside the container may not have the necessary permissions to execute the specified command. Ensure that the container runs with appropriate privileges.</p></li><li><p>Image compatibility issues</p><p>Ensure that the container image used is compatible with the host architecture and operating system. Mismatched images may result in commands not being found, such as running an x86 image on an ARM machine.</p></li><li><p>Volume mounts</p><p>If the command relies on files mounted from a volume, check if the volume mount is configured correctly and the required files are accessible.</p></li><li><p>Environment variables</p><p>Some commands may depend on specific environment variables. Ensure that the necessary environment variables are set correctly.</p></li><li><p>Kubernetes RBAC policies</p><p>If RBAC is enabled, ensure that the necessary permissions are granted to execute the specified command.</p></li></ol><h3 id="How-to-Troubleshoot"><a href="#How-to-Troubleshoot" class="headerlink" title="How to Troubleshoot"></a>How to Troubleshoot</h3><p>To diagnose the issue, you can use the following commands to check the logs of the Pod:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f </span><br></pre></td></tr></table></figure><p>You can also inspect the Pod’s status, which provides detailed information about the Pod, including its current state, recent events, and any error messages.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod </span><br></pre></td></tr></table></figure><p>Additionally, you can attach a debugging container to the Pod, which includes a shell (e.g., BusyBox). This allows you to enter the container and manually check the availability of environment, paths, and commands.</p><p>Example of debugging with BusyBox:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    command: [&quot;/bin/sleep&quot;, &quot;36000&quot;]</span><br><span class="line">  - name: debug</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br></pre></td></tr></table></figure><p>If you are using a higher version of Kubernetes, you can also utilize Ephemeral Containers, which are temporary containers. This is a new feature introduced as alpha in Kubernetes v1.16, and enabling the feature of Ephemeral Containers is straightforward. Just configure <code>--feature-gates=EphemeralContainers=true</code> in the kube-api and kubelet services, then restart.</p><p>By carefully examining the logs and investigating in the above directions, you should be able to determine the cause of the exit code 127 issue.</p><h3 id="How-to-Fix"><a href="#How-to-Fix" class="headerlink" title="How to Fix"></a>How to Fix</h3><p>Now that we know the common causes of exit code 127 and how to troubleshoot them, let’s see how to fix them.</p><ol><li>Command or binary not installed</li></ol><p>If the required command or binary is missing, it may need to be installed in the container image. Modify the Dockerfile or the build process to install the necessary software.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add &lt;package&gt; </span><br></pre></td></tr></table></figure><ol><li>Incorrect path or command</li></ol><p>When specifying commands in the Pod definition, consider using the absolute path of the binary. This helps ensure that the binary is found by the runtime, regardless of the current working directory.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    command: [&quot;/usr/bin/command&quot;]</span><br></pre></td></tr></table></figure><ol><li>Missing dependencies</li></ol><p>The reason for the command not running may be that additional software needs to be installed in the container image. If the command requires additional setup or installation steps, you can use init containers to perform these tasks before the main container starts.</p><p>Example (installing a package using init container):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">  - name: install-package</span><br><span class="line">    image: alpine:latest</span><br><span class="line">    command: [&quot;apk&quot;, &quot;--no-cache&quot;, &quot;add&quot;, &quot;&lt;package-name&gt;&quot;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-data</span><br><span class="line">      mountPath: /data</span><br></pre></td></tr></table></figure><ol><li>Shell interpreter</li></ol><p>If a script is specified as a command, ensure that the script is valid (e.g., <code>#!/bin/bash</code>) and available in the container.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br></pre></td></tr></table></figure><ol><li>Volume mounts</li></ol><p>Check the Pod’s configuration to ensure that volumes are correctly mounted. Verify that the volume names, mount paths, and subPaths are correct.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: test</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: test</span><br><span class="line">      mountPath: /path in container</span><br></pre></td></tr></table></figure><p>Additionally, confirm that the volume specified in the Pod definition exists and is accessible. If it’s a persistent volume (PV), check its status. If it’s an emptyDir or other types of volumes, verify that they are created and mounted correctly. If subPaths are used in the volume mount, ensure that the specified subPaths exist in the source directory or file.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumeMounts:</span><br><span class="line">  - name: test</span><br><span class="line">    mountPath: /path in container</span><br><span class="line">    subPath: my-file.txt</span><br></pre></td></tr></table></figure><h2 id="Exit-Code-137"><a href="#Exit-Code-137" class="headerlink" title="Exit Code 137"></a>Exit Code 137</h2><p>In Kubernetes, the exit code 137 indicates that the process was terminated forcibly. In Unix and Linux systems, when a process is terminated due to a signal, the exit code is determined by adding the signal number to 128. Since the signal number for “SIGKILL” is 9, adding 128 to 9 results in exit code 137.</p><p>When a container exceeds its memory limit in a Kubernetes cluster, it may be terminated by the Kubernetes system with an “OOMKilled” error, indicating that the process was terminated due to insufficient memory. The exit code for this error is 137, where OOM stands for “out-of-memory”.</p><p>If the Pod state shows as “OOMKilled”, you can check it using the following command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pods PODNAME</span><br></pre></td></tr></table></figure><h3 id="OOMKiller"><a href="#OOMKiller" class="headerlink" title="OOMKiller"></a>OOMKiller</h3><p>OOMKiller is a mechanism in the Linux kernel responsible for preventing the system from running out of memory by terminating processes that consume too much memory. When the system runs out of memory, the kernel invokes OOMKiller to select a process to terminate in order to free up memory and keep the system running.</p><p>There are two different OOM Killers in the kernel; one is the global OOM Killer, and the other is the OOM Killer based on cgroup memory control, which can be either cgroup v1 or cgroup v2.</p><p>In summary, when the kernel encounters issues allocating physical memory pages, the global OOM Killer is triggered. When the kernel attempts to allocate memory pages (whether for kernel use or for processes needing pages) and initially fails, it tries various ways to reclaim and consolidate memory. If this attempt is successful or at least makes some progress, the kernel will continue retrying allocation (from the code I can see); if it fails to free pages or make progress, it will often trigger the OOM Killer in many cases.</p><p>Once the OOMKiller selects a process to terminate, it sends a signal to that process requesting it to gracefully terminate. If the process does not respond to the signal, the kernel forcibly terminates the process and releases its memory.</p><p>Note: Pods terminated due to memory issues may not necessarily be evicted from the node; if their restart policy is set to “Always”, they will attempt to restart the Pod.</p><p>At the system level, the Linux kernel maintains an oom_score for each process running on the host. The likelihood of a process being terminated depends on how high the score is.</p><p>The oom_score_adj value allows users to customize OOM processes and define when processes should be terminated. Kubernetes uses the oom_score_adj value when defining the Quality of Service (QoS) of Pods.</p><p>Kubernetes defines three types of QoS for Pods, each with a corresponding oom_score_adj value:</p><ul><li>Guaranteed: -997</li><li>BestEffort: 1000</li><li>Burstable: <em>min(max(2, 1000 — (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</em></li></ul><p>Where Pods of Guaranteed QoS have an oom_score_adj value of -997, so they are the last ones to be terminated when the node runs out of memory. BestEffort Pod configurations have an oom_score_adj value of 1000, so they are the first ones to be terminated.</p><p>To check the QoS of a Pod, you can use the following command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o jsonpath=&#x27;&#123;.status.qosClass&#125;&#x27;</span><br></pre></td></tr></table></figure><p>Here’s the calculation policy for defining Pods of <code>Guaranteed</code> QoS type:</p><ul><li>Each container in the Pod must have memory limits and memory requests.</li><li>For each container in the Pod, the memory limit must be equal to the memory request.</li><li>Each container in the Pod must have CPU limits and CPU requests.</li><li>For each container in the Pod, the CPU limit must be equal to the CPU request.</li></ul><p>Exit code 137 typically has two scenarios:</p><p>First and foremost, the most common cause is related to resource constraints. In this scenario, Kubernetes typically exceeds the memory allocation limit of the container. When this happens, it terminates the container to ensure the stability of the node.</p><p>The other scenario involves manual intervention - a user or a script may send a “SIGKILL” signal to the container process, resulting in this exit code. OOMKilled (exit code 137)</p><h3 id="How-to-Troubleshoot-1"><a href="#How-to-Troubleshoot-1" class="headerlink" title="How to Troubleshoot"></a>How to Troubleshoot</h3><ol><li>Check Pod logs</li></ol><p>The first step in diagnosing OOMKilled errors is to check the Pod logs for any error messages indicating memory issues. The events section of the describe command will provide further confirmation and the time&#x2F;date of the error occurrence.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br><span class="line">State:          Running</span><br><span class="line">       Started:      Fri, 12 May 2023 11:14:13 +0200</span><br><span class="line">       Last State:   Terminated</span><br><span class="line">       Reason:       OOMKilled</span><br><span class="line">       Exit Code:    137</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><p>You can also query the Pod logs:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/pods/&lt;podname&gt;</span><br></pre></td></tr></table></figure><p>Of course, you can also do it via (standard output)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f &lt;podname&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>Monitor memory usage</li></ol><p>Monitor memory usage in Pods and containers using monitoring systems like Prometheus or Grafana. This can help us identify which containers are consuming too much memory and triggering OOMKilled errors, and you can use dmesg on the container host to check the scene of the oomkiller.</p><ol start="3"><li>Use memory profilers</li></ol><p>Use memory profilers like pprof to identify memory leaks or inefficient code that may be causing excessive memory usage.</p><h3 id="How-to-Fix-1"><a href="#How-to-Fix-1" class="headerlink" title="How to Fix"></a>How to Fix</h3><p>Below are common causes of OOMKilled Kubernetes errors and their solutions.</p><ol><li>Container memory limit reached</li></ol><p>This may be due to improper setting of the memory limit value specified in the container. The solution is to increase the value of the memory limit or investigate the root cause of the increased load and correct it. Common causes of this situation include large file uploads, as uploading large files can consume a significant amount of memory resources, especially when multiple containers are running in a single Pod, and sudden increases in traffic volume.</p><ol start="2"><li>Application memory leak, container memory usage reaches the upper limit</li></ol><p>Debug the application to locate the cause of the memory leak.</p><ol start="3"><li>Total memory used by all Pods exceeds available node memory</li></ol><p>Increase the available memory of the node by increasing the memory of the node, or migrate Pods to nodes with more memory. Alternatively, adjust the memory limits of Pods running on the node to comply with memory constraints.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://spacelift.io/blog/oomkilled-exit-code-137">https://spacelift.io/blog/oomkilled-exit-code-137</a></p><p><a href="https://spacelift.io/blog/exit-code-127">https://spacelift.io/blog/exit-code-127</a></p><p><a href="https://cloud.tencent.com/developer/news/1152344">https://cloud.tencent.com/developer/news/1152344</a></p><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen">https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Interpreting-common-exit-codes-in-Kubernetes&quot;&gt;&lt;a href=&quot;#Interpreting-common-exit-codes-in-Kubernetes&quot; class=&quot;headerlink&quot; title=&quot;Inte</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>解读Kubernetes常见错误码</title>
    <link href="https://zoues.com/posts/b421b57/"/>
    <id>https://zoues.com/posts/b421b57/</id>
    <published>2024-03-02T12:23:48.000Z</published>
    <updated>2024-07-13T00:50:26.165Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>在这篇文章中，我们将深入分析Kubernetes中的退出码127与137，解释它们是什么，K8s和Docker中常见的原因是什么，以及如何修复它们！</p><p>我们将涵盖以下内容：</p><ol><li><p>退出码的历史 </p></li><li><p>退出码 127</p></li><li><p>退出码 137</p></li></ol><h2 id="退出码的历史"><a href="#退出码的历史" class="headerlink" title="退出码的历史"></a>退出码的历史</h2><p>进程退出码的历史可以追溯到Unix操作系统的早期。在Unix系统中，进程退出码是进程终止时向其父进程传递的一个整数值，用于表示进程的终止状态。这个整数值通常在0到255之间，其中0表示进程成功终止，其他值通常用来表示不同的错误或异常情况。</p><p>进程退出码最初被设计用于提供一种简单的机制，使父进程能够了解子进程的执行结果。这使得父进程能够根据子进程的退出码来采取适当的行动，比如处理错误情况或继续执行其他操作。</p><p>在Unix系统中，特定的退出码值通常具有特定的含义，例如：</p><ul><li>0：表示成功执行，没有错误。</li><li>1：通常表示通用的错误。</li><li>2：表示命令的语法错误。</li><li>127：表示命令未找到。</li></ul><p>随着时间的推移，Unix操作系统的发展和不同的实现，进程退出码的含义可能有所不同，但基本的概念保持不变。</p><p>在Linux系统中，进程退出码的使用与Unix系统类似。Linux继承了Unix的进程管理机制，并在其基础上进行了扩展和改进。因此，Linux中的进程退出码仍然是一个重要的概念，用于帮助理解和诊断进程的执行状态。</p><p>进程退出码的历史可以追溯到早期的Unix系统，是Unix和Linux操作系统中的一个重要概念，为进程间通信提供了一种简单而有效的机制。当应用程序或命令因致命错误而终止或执行失败时，将产生 128 系列退出码（<code>128+n</code>），其中 <code>n</code> 为信号编号。<code>n</code> 包括所有类型的终止代码，如 <code>SIGTERM</code>、<code>SIGKILL</code> 等。</p><h2 id="退出码127"><a href="#退出码127" class="headerlink" title="退出码127"></a>退出码127</h2><p>退出码 127 不是特定于 Kubernetes 的错误代码，而是 Linux 和类 Unix 操作系统中使用的标准退出码。当然，我们在Kubernetes中经常看到它，并且通常表示容器内执行的命令或二进制文件找不到。</p><p>一些标准的退出码包括：</p><table><thead><tr><th>退出码</th><th>解释</th></tr></thead><tbody><tr><td>0</td><td>命令成功执行</td></tr><tr><td>1</td><td>通用错误</td></tr><tr><td>2</td><td>命令（参数）使用不当</td></tr><tr><td>126</td><td>权限被拒绝、无法执行</td></tr><tr><td>127</td><td>未找到命令行、PATH错误</td></tr><tr><td>128+n</td><td>命令被信号从外部终止、遇到致命错误</td></tr><tr><td>&gt;255</td><td>退出码超过255范围的，会重新计算（mod 256）</td></tr></tbody></table><p>让我们看一下退出码 127 的一些常见原因：</p><ol><li><p>命令或二进制文件未安装 </p><p>Kubernetes 容器的 command 字段中指定的可执行文件未安装在容器的文件系统中。需要确保所需的二进制文件或命令可用。</p></li><li><p>路径或命令不正确</p><p>Pod 定义中指定的命令不正确或在指定的路径中不存在。这是错误的最常见原因之一，通常是由于 Dockerfile 或 pod  spec中的entrypoint或command输入不正确造成的。</p></li><li><p>缺少依赖</p><p>在容器内运行的应用程序或脚本未安装相关依赖。需要确保所有必需的依赖项包含在容器映像中。</p></li><li><p>shell 解释器</p><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p></li><li><p>shell 脚本语法错误</p><p>如果 shell 脚本退出码是127，请检查脚本是否存有语法错误或可能阻止其执行的问题。</p></li><li><p>权限不足</p><p>在容器内运行命令的用户可能没有执行指定命令所需的必要权限。确保容器以适当的特权运行。</p></li><li><p>镜像兼容性问题</p><p>确保使用的容器镜像与宿主机架构和操作系统兼容。不匹配的映像可能导致命令找不到，比如x86的镜像运行在arm的机器上</p></li><li><p>卷挂载</p><p>如果命令是卷挂载的文件，请检查卷挂载是否配置正确，且所需的文件可以被访问到。</p></li><li><p>环境变量</p><p>一些命令可能依赖于特定的环境变量。确保必需的环境变量设置正确。</p></li><li><p>Kubernetes RBAC 策略</p><p> 如果启用了RBAC，需要确保具有执行指定命令所需的权限。</p></li></ol><h3 id="如何排查"><a href="#如何排查" class="headerlink" title="如何排查"></a>如何排查</h3><p>要排除问题，可以使用以下命令检查 Pod 的日志：</p><p><code>kubectl logs -f &lt;pod-name&gt; </code></p><p>还可以检查 Pod 状态，该状态提供有关 Pod 的详细信息，包括其当前状态、最近事件和任何错误消息。</p><p><code>kubectl describe pod &lt;pod-name&gt; </code></p><p>还可以为把调试容器attach到Pod 中，该容器包括一个 shell（例如 BusyBox）。这允许您进入容器并手动检查环境、路径和命令的可用性。</p><p>使用 BusyBox 进行调试的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/bin/sleep&quot;, &quot;infinity&quot;]</span><br><span class="line">  - name: debug-container</span><br><span class="line">    image: busybox:latest</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br><span class="line">    tty: true</span><br><span class="line">    stdin: true</span><br></pre></td></tr></table></figure><p>如果是高版本K8s，也可以使用Ephemeral Containers，它就是一个临时容器。这是一个自Kubernetes v1.16中作为alpha引入的新功能，启用临时容器的特性也非常简单，在kubernetes v1.16之后的版本中将启动参数<code>--feature-gates=EphemeralContainers=true</code>配置到kube-api和kubelet服务上重启即可。</p><p>通过仔细查看日志并排查上述几个方向，应该能够确定退出码 127 问题的原因。</p><h3 id="如何修复"><a href="#如何修复" class="headerlink" title="如何修复"></a>如何修复</h3><p> 我们知道了退出码 127 的常见原因以及排查方式，现在让我们看看如何修复它们。</p><ol><li>命令或二进制文件未安装</li></ol><p>如果所需的命令或二进制文件丢失，则可能需要在容器镜像中安装。修改 Dockerfile 或构建过程安装所需软件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add &lt;package-name&gt; </span><br></pre></td></tr></table></figure><ol start="2"><li>路径或命令不正确</li></ol><p>在 Pod 定义中指定命令时，考虑使用二进制文件的绝对路径。这有助于确保不受当前工作目录的影响， runtime可以找到二进制文件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/usr/local/bin/my-command&quot;]</span><br></pre></td></tr></table></figure><ol start="3"><li>缺少依赖项</li></ol><p>导致命令无法运行的原因可能是容器镜像需要安装额外的软件。如果命令需要额外的设置或安装步骤，可以使用init容器在主容器启动之前执行这些任务。</p><p>示例（使用init容器安装软件包）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">  - name: install-package</span><br><span class="line">    image: alpine:latest</span><br><span class="line">    command: [&quot;apk&quot;, &quot;--no-cache&quot;, &quot;add&quot;, &quot;&lt;package-name&gt;&quot;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-data</span><br><span class="line">      mountPath: /data</span><br></pre></td></tr></table></figure><ol start="4"><li>shell解释器</li></ol><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br></pre></td></tr></table></figure><ol start="5"><li>卷挂载</li></ol><p>检查Pod的配置，确保卷已正确挂载。验证卷名称、挂载路径和 subPaths是否正确。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: my-volume</span><br><span class="line">      mountPath: /path/in/container</span><br></pre></td></tr></table></figure><p>同时我们需要确认Pod 定义指定的卷存在且可用。如果是持久卷（PV），需要检查其状态。如果是 emptyDir 或其他类型的卷，需要验证其是否正确创建和挂载。如果在卷挂载中使用了 subPaths，需要确保源目录或文件中存在指定的 subPaths。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumeMounts:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    mountPath: /path/in/container</span><br><span class="line">    subPath: my-file.txt</span><br></pre></td></tr></table></figure><h2 id="退出码137"><a href="#退出码137" class="headerlink" title="退出码137"></a>退出码137</h2><p>在Kubernetes中，137退出码表示进程被强制终止。在Unix和Linux系统中，当进程由于信号而终止时，退出码由信号编号加上128确定。信号编号为9，意味着“SIGKILL”，因此将9加上128，得到137退出码。</p><p>当Kubernetes集群中容器超出其内存限制时，它可能会被Kubernetes系统终止，并显示“OOMKilled”错误，这表示进程因内存不足而被终止。此错误的退出码为137OOM代表“内存耗尽（out-of-memory）”。</p><p>如果Pod状态将显示为“OOMKilled”，你可以使用以下命令查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br></pre></td></tr></table></figure><h3 id="OOMKiller"><a href="#OOMKiller" class="headerlink" title="OOMKiller"></a>OOMKiller</h3><p>OOMKiller是Linux内核中的一种机制，它负责通过终止消耗过多内存的进程来防止系统耗尽内存。当系统内存耗尽时，内核会调用OOMKiller来选择一个要终止的进程，以释放内存并保持系统运行。</p><p>内核中有两种不同的OOM Killer；一种是全局的OOM Killer，另一种是基于cgroup内存控制器的OOM Killer，可以是cgroup v1或cgroup v2。</p><p>简单来说是，当内核在分配物理内存页面时遇到问题时，全局的OOM Killer 会触发。当内核尝试分配内存页面（无论是用于内核使用还是用于需要页面的进程），并且最初失败时，它将尝试各种方式来回收和整理内存。如果这种尝试成功或者至少取得了一些进展，内核将继续重试分配（从代码中我可以看到）；如果无法释放页面或者取得进展，它将在许多情况下触发OOM Killer。</p><p>一旦OOMKiller选择要终止的进程，它会向该进程发送信号，要求其优雅地终止。如果进程不响应信号，则内核会强制终止该进程并释放其内存。</p><p>注意：由于内存问题而被终止的Pod不一定会被节点驱逐，如果其设置的重启策略设置为“Always”，它将尝试重新启动Pod。</p><p>在系统层面，Linux内核为运行在主机上的每个进程维护一个oom_score。进程被终止的机率取决于分数有多高。</p><p>oom_score_adj值允许用户自定义OOM进程，并定义何时应终止进程。Kubernetes在定义Pod的Quality of Service（QoS）时使用oom_score_adj值。</p><p>K8s针对Pod定义了三种QoS，每个类型具有对应的oom_score_adj值：</p><ul><li>Guaranteed: -997</li><li>BestEffort: 1000</li><li>Burstable: <em>min(max(2, 1000 — (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</em></li></ul><p>其中Pod为Guaranteed QoS，则其oom_score_adj的值是-997，因此它们在节点内存不足时最后一个被终止。BestEffort Pod配置的是1000，所以它们第一个被被终止。</p><p>要查看Pod的QoS，可以通过下述命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o jsonpath=&#x27;&#123;.status.qosClass&#125;&#x27;</span><br></pre></td></tr></table></figure><p>下面是定义Pod<code>Guaranteed</code> QoS 类型的计算策略：</p><ul><li>Pod 中的每个容器必须有内存 limit 和内存 request。</li><li>对于 Pod 中的每个容器，内存 limit 必须等于内存 request。</li><li>Pod 中的每个容器必须有 CPU limit 和 CPU request。</li><li>对于 Pod 中的每个容器，CPU limit 必须等于 CPU request。</li></ul><p>退出码137通常有两种情况：</p><ul><li><p>首先，也是最常见的原因是与资源限制相关。在这种情况下，通常情况下，Kubernetes超出了容器的分配内存限制，当发生这种情况时，它将终止容器以确保节点的稳定性。 </p></li><li><p>另一种情况是手动干预 - 用户或脚本可能会向容器进程发送“SIGKILL”信号，导致此退出码。 OOMKilled（退出码137）</p></li></ul><h3 id="如何排查-1"><a href="#如何排查-1" class="headerlink" title="如何排查"></a>如何排查</h3><ol><li>检查Pod日志</li></ol><p>诊断OOMKilled错误的第一步是检查Pod日志，查看是否有任何指示内存问题的错误消息。描述命令的事件部分将提供进一步的确认以及发生错误的时间&#x2F;日期。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br><span class="line">State:          Running</span><br><span class="line">       Started:      Fri, 12 May 2023 11:14:13 +0200</span><br><span class="line">       Last State:   Terminated</span><br><span class="line">       Reason:       OOMKilled</span><br><span class="line">       Exit Code:    137</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><p>您还可以查询Pod日志：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/pods/&lt;podname&gt;</span><br></pre></td></tr></table></figure><p>当然也可以通过(标准输出)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f &lt;podname&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>监视内存使用情况</li></ol><p>使用监视系统（如Prometheus或Grafana）监视Pod和容器中的内存使用情况。这可以帮助我们确定哪些容器消耗了过多的内存从而触发了OOMKilled错误，也可以在容器宿主机使用dmesg查看当时oomkiller的现场</p><ol start="3"><li>使用内存分析器</li></ol><p>使用内存分析器（如pprof）来识别可能导致过多内存使用的内存泄漏或低效代码。</p><h3 id="如何修复-1"><a href="#如何修复-1" class="headerlink" title="如何修复"></a>如何修复</h3><p>以下是OOMKilled Kubernetes错误的常见原因及其解决方法。</p><ol><li>容器内存限制已达到</li></ol><p>这可能是由于在容器指定的内存限制值设置不当导致的。解决方法是增加内存限制的值，或者调查导致负载增加的根本原因并进行纠正。导致这种情况的常见原因包括大文件上传，因为上传大文件可能会消耗大量内存资源，特别是当多个容器在一个Pod内运行时，以及突然增加的流量量。</p><ol start="2"><li>因为应用程序内存泄漏,容器内存使用达到上限</li></ol><p>需要调试应用程序来定位内存泄漏的原因，</p><ol start="3"><li>所有Pod使用的总内存大于节点可用内存</li></ol><p>通过增加节点可用内存来增加节点内存，或者将Pod迁移到内存更多的节点。当然也可以调整运行在节点上的Pod的内存限制，使其符合内存限制，注意你还应该注意内存请求设置，它指定了Pod应该使用的最小内存量。如果设置得太高，可能不是有效利用可用内存，关于资源配置相关的建议，可以参看VPA组件</p><p>在调整内存请求和限制时，当节点过载时，Kubernetes按照以下优先级顺序终止Pod：</p><ul><li><p>没有请求或限制的Pod。 </p></li><li><p>具有请求但没有限制的Pod。</p></li><li><p>使用超过其内存请求值的内存 - 指定的最小内存值 - 但低于其内存限制的Pod。 </p></li><li><p>使用超过其内存限制的Pod。</p></li></ul><h3 id="如何预防"><a href="#如何预防" class="headerlink" title="如何预防"></a>如何预防</h3><p>有几种方法可以防止OOMKilled的发生：</p><ol><li>设置适当的内存限制</li></ol><p>通过压测及监控来确定应用程序的内存使用，通过上述方式配置容器允许使用的最大内存量。过度保守可能会导致因资源利用率低效而造成资金的浪费，同时低估会导致频繁出现OOMKilled现象。</p><ol start="2"><li>HPA</li></ol><p>最佳做法是利用K8s提供的HPA机制，当应用程序的内存使用升高时自动增加Pod副本数量。</p><ol start="3"><li>节点资源分配</li></ol><p>确保节点具有足够的资源来处理业务。</p><ol start="4"><li>优化应用程序内存使用</li></ol><p>监视应用程序并进行适当优化，以减少内存消耗。</p><ol start="5"><li>避免应用程序中的内存泄漏</li></ol><p>从应用程序来看，需要长期检查并修复内存泄漏。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><p><a href="https://spacelift.io/blog/oomkilled-exit-code-137">https://spacelift.io/blog/oomkilled-exit-code-137</a></p></li><li><p><a href="https://spacelift.io/blog/exit-code-127">https://spacelift.io/blog/exit-code-127</a></p></li><li><p><a href="https://cloud.tencent.com/developer/news/1152344">https://cloud.tencent.com/developer/news/1152344</a></p></li><li><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen">https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Exploring the Root Cause of Kubernetes 1.28 Scheduler OOM</title>
    <link href="https://zoues.com/posts/e4e37b07/"/>
    <id>https://zoues.com/posts/e4e37b07/</id>
    <published>2024-03-01T09:13:10.000Z</published>
    <updated>2024-03-01T09:17:38.524Z</updated>
    
    <content type="html"><![CDATA[<p>Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a look together. In the context of fluctuating business tidal changes affecting both pods and nodes in the cluster, memory usage shows a continuous upward trend until reaching Out Of Memory (OOM) conditions.</p><h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><blockquote><p>The following data is all publicly available information from the community.</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>The triggering scenarios include the following two types (there are also other reproduction methods in the community):</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=60</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>Our findings in the community revealed multiple instances of similar memory anomaly scenarios, with various methods of reproduction. The conclusion regarding the above issue is as follows:</p><blockquote><p>The Kubernetes community defaulted to enabling the scheduling feature SchedulerQueueingHints in version 1.28, which led to memory anomalies in the scheduler component. To temporarily address memory-related issues, the community adjusted this feature to default to disabled in version 1.28.5. However, as the problem has not been completely resolved, it is advisable to exercise caution when enabling this feature.</p></blockquote><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><p>This section introduces the following content:</p><ul><li>Introduction to Kubernetes scheduler related data structures</li><li>Introduction to Kubernetes scheduler QueueingHint</li><li>Doubly linked lists in Golang</li></ul><h3 id="K8s-Scheduler-Introduction"><a href="#K8s-Scheduler-Introduction" class="headerlink" title="K8s-Scheduler Introduction"></a>K8s-Scheduler Introduction</h3><p>The PriorityQueue is an interface implementation of the SchedulingQueue. It holds the highest priority pod ready for scheduling at its head. PriorityQueue contains the following important fields:</p><ol><li>activeQ: This queue holds pods ready for scheduling. Newly added pods are placed into this queue. When the scheduling queue needs to perform scheduling, it fetches pods from this queue. activeQ is implemented using a heap.</li><li>backoffQ: This queue holds pods that have been determined to be unschedulable for various reasons (such as not meeting node requirements). These pods will be moved to activeQ to attempt scheduling again after a certain backoff period. backoffQ is also implemented using a heap.</li><li>unschedulablePods: This map data structure holds pods that cannot be scheduled for various reasons. Instead of directly placing them in backoffQ, they are recorded here. When conditions are met, they will be moved to activeQ or backoffQ. The scheduling queue periodically cleans up pods in unschedulablePods.</li><li>inFlightEvents: This is used to store events received by the scheduling queue (with the entry value as clusterEvent) and pods that are currently being processed (with the entry value as *v1.Pod). It’s based on a doubly linked list implemented in Go.</li><li>inFlightPods: This holds the UIDs of all pods that have been popped but have not yet had Done called on them. In other words, it keeps track of all pods that are currently being processed (i.e., in scheduling, in admit, or in binding phases).</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p> For a comprehensive introduction to Kubernetes, please refer to <a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">An In-depth Introduction to Kubernetes Scheduling: Framework</a>. Updates on the latest Kubernetes scheduler will be provided subsequently.</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>Kubernetes scheduler introduced the <code>QueueingHint</code> feature to provide recommendations for re-queueing pods from each plugin. This aims to reduce unnecessary scheduling retries, thereby enhancing scheduling throughput. Additionally, it skips backoff under appropriate circumstances to further improve pod scheduling efficiency.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Currently, each plugin can define when to retry scheduling pods that have been rejected by the plugin through the <code>EventsToRegister</code> mechanism.</p><p>For example, the <code>NodeAffinity</code> plugin may retry scheduling pods when nodes are added or updated because newly added or updated nodes may have labels that match the node affinity on the pod. However, in practice, a large number of node update events occur in the cluster, which does not guarantee successful scheduling of pods previously rejected by <code>NodeAffinity</code>.</p><p>To address this issue, the scheduler introduced more refined callback functions to filter out irrelevant events, thereby only retrying pods that are likely to be successfully scheduled in the next scheduling cycle.</p><p>Furthermore, the Dynamic Resource Allocation (DRA) scheduling plugin sometimes needs to reject pods to wait for status updates from device drivers. Therefore, certain pods may require several scheduling cycles to be scheduled. For this scenario, the wait time for fallback is longer compared to waiting for device driver status updates. Hence, there is a need to allow plugins to skip fallback in specific cases to improve scheduling performance.</p><h3 id="Implementation-Goals"><a href="#Implementation-Goals" class="headerlink" title="Implementation Goals"></a>Implementation Goals</h3><p>To improve scheduling throughput, the community proposed the following enhancements:</p><ol><li>Introduction of QueueingHint<ul><li>Introducing <code>QueueingHint</code> into the <code>EventsToRegister</code> mechanism, allowing plugins to provide recommendations for re-queuing pods.</li></ul></li><li>Enhancement of Pod Tracking and Re-queueing Mechanism<ul><li>Optimizing the implementation for tracking pods currently being processed in the scheduling queue.</li><li>Implementing a mechanism to re-queue rejected pods to appropriate queues.</li><li>Optimizing the backoff strategy for rejected pods, allowing plugins to skip backoff in specific cases to improve scheduling throughput.</li></ul></li></ol><h3 id="Potential-Risks"><a href="#Potential-Risks" class="headerlink" title="Potential Risks"></a>Potential Risks</h3><blockquote><h4 id="1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods"><a href="#1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods" class="headerlink" title="1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods"></a>1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods</h4></blockquote><p>If a plugin is configured with QueueingHint but misses some events that could make pods schedulable, pods rejected by that plugin may remain stuck in unschedulablePods for a long time.</p><p>Although the scheduling queue periodically cleans up pods in unschedulablePods (default is 5 minutes, configurable).</p><blockquote><h4 id="2-Increase-in-Memory-Usage"><a href="#2-Increase-in-Memory-Usage" class="headerlink" title="2. Increase in Memory Usage"></a>2. Increase in Memory Usage</h4></blockquote><p>As the scheduling queue needs to retain events occurring during scheduling, the memory usage of kube-scheduler will increase. Therefore, the busier the cluster, the more memory it may require.</p><p>Although it’s not possible to completely eliminate memory growth, releasing cached events as soon as possible can slow down the rate of memory growth.</p><blockquote><h4 id="3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension"><a href="#3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension" class="headerlink" title="3. Significant Changes in EventsToRegister in EnqueueExtension"></a>3. Significant Changes in <code>EventsToRegister</code> in <code>EnqueueExtension</code></h4></blockquote><p>Developers of custom scheduler plugins need to perform compatibility upgrades, as the return type of <code>EventsToRegister</code> in <code>EnqueueExtension</code> has changed from <code>ClusterEvent</code> to <code>ClusterEventWithHint</code>. <code>ClusterEventWithHint</code> allows each plugin to filter out more useless events through a callback function called <code>QueueingHintFn</code>.</p><p>To simplify the migration work, an empty <code>QueueingHintFn</code> is considered to always return <code>Queue</code>. Thus, if they only want to maintain the existing behavior, they only need to change <code>ClusterEvent</code> to <code>ClusterEventWithHint</code> without registering any <code>QueueingHintFn</code>.</p><h3 id="Design-of-QueueingHints"><a href="#Design-of-QueueingHints" class="headerlink" title="Design of QueueingHints"></a>Design of QueueingHints</h3><p>The return type of the <code>EventsToRegister</code> method has been changed to <code>[]ClusterEventWithHint</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Each <code>ClusterEventWithHint</code> structure consists of a <code>ClusterEvent</code> and a <code>QueueingHintFn</code>. When an event occurs, the <code>QueueingHintFn</code> is executed to determine whether the event can make the pod eligible for scheduling.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The type <code>QueueingHintFn</code> is a function with a return type of <code>(QueueingHint, error)</code>. Here, <code>QueueingHint</code> is an enumeration type with possible values of <code>QueueSkip</code> and <code>Queue</code>. The invocation of <code>QueueingHintFn</code> occurs before moving pods from <code>unschedulableQ</code> to <code>backoffQ</code> or <code>activeQ</code>. If an error is returned, the <code>QueueingHint</code> provided by the caller will be treated as <code>QueueAfterBackoff</code>, which ensures that the pod does not remain indefinitely in the <code>unschedulableQ</code> queue, regardless of the returned result.</p><h3 id="When-to-Skip-Not-Skip-Backoff"><a href="#When-to-Skip-Not-Skip-Backoff" class="headerlink" title="When to Skip&#x2F;Not Skip Backoff"></a>When to Skip&#x2F;Not Skip Backoff</h3><p>The <code>backoffQ</code> prevents pods that are “long-term unschedulable” from blocking the queue, maintaining a lightweight queue with high throughput.</p><p>The longer a pod is rejected during the scheduling cycle, the longer it waits in the <code>backoffQ</code>.</p><p>For example, when <code>NodeAffinity</code> rejects a pod, and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod needs to wait for backoff before retrying scheduling.</p><p>However, certain plugins are designed to experience some failures during the scheduling cycle itself. For instance, the built-in DRA (Dynamic Resource Allocation) plugin, at the Reserve extension, informs the resource driver of the scheduling result and rejects the pod once to wait for a response from the resource driver. For such rejection scenarios, it should not be considered wasted scheduling cycles. Although a specific scheduling cycle fails, the scheduling result based on that cycle can facilitate pod scheduling. Therefore, pods rejected for this reason do not need to be penalized (backoff).</p><p>To support this scenario, we introduce a new state, <code>Pending</code>. When the DRA plugin rejects a pod using <code>Pending</code> and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod skips backoff and is rescheduled.</p><h3 id="How-QueueingHint-Works"><a href="#How-QueueingHint-Works" class="headerlink" title="How QueueingHint Works"></a>How QueueingHint Works</h3><p>When Kubernetes cluster events occur, the scheduling queue executes the <code>QueueingHintFn</code> of those plugins that rejected pods in the previous scheduling cycle.</p><p>The following scenarios describe how they are executed and how pods are moved:</p><blockquote><p>Pod Rejected by One or More Plugins</p></blockquote><p>Suppose there are three nodes. When a pod enters the scheduling cycle, one node rejects the pod due to insufficient resources, and the other two nodes reject it due to mismatching node affinity.</p><p>In this scenario, the pod is rejected by the <code>NodeResourceFit</code> and <code>NodeAffinity</code> plugins and is eventually placed in <code>unschedulableQ</code>.</p><p>Subsequently, whenever cluster events registered in these plugins occur, the scheduling queue notifies them via <code>QueueingHint</code>. If any <code>QueueingHintFn</code> from <code>NodeResourceFit</code> or <code>NodeAffinity</code> returns <code>Queue</code>, the pod is moved to <code>activeQ</code> or <code>backoffQ</code>. (For example, when a <code>NodeAdded</code> event occurs, <code>QueueingHint</code> from <code>NodeResourceFit</code> returns <code>Queue</code> because the pod may be schedulable to the new node.)</p><p>Whether it moves to <code>activeQ</code> or <code>backoffQ</code> depends on how long the pod has been in <code>unschedulableQ</code>. If the time spent in <code>unschedulableQ</code> exceeds the expected pod backoff delay time, it is moved directly to <code>activeQ</code>. Otherwise, it is moved to <code>backoffQ</code>.</p><blockquote><p>Pod Rejected due to Pending State</p></blockquote><p>When the DRA plugin returns <code>Pending</code> for a pod during the Reserve extension phase, the scheduling queue adds the DRA plugin to the pod’s <code>pendingPlugins</code> dictionary and returns the pod.</p><p>When a call to the <code>QueueingHint</code> of the DRA plugin returns <code>Queue</code> in subsequent invocations, the scheduling queue places the pod directly into <code>activeQ</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Tracking-Pods-Being-Processed-in-the-Scheduling-Queue"><a href="#Tracking-Pods-Being-Processed-in-the-Scheduling-Queue" class="headerlink" title="Tracking Pods Being Processed in the Scheduling Queue"></a>Tracking Pods Being Processed in the Scheduling Queue</h4><p>By introducing <code>QueueingHint</code>, we can only retry scheduling when specific events occur. But what if these events happen during pod scheduling?</p><p>The scheduler takes a snapshot of the cluster data and schedules pods based on the snapshot. The snapshot is updated each time a scheduling cycle is initiated, meaning the same snapshot is used within the same scheduling cycle.</p><p>Consider a scenario where, during the scheduling of a pod, it is rejected due to no nodes meeting the pod’s node affinity, but a new node matching the pod’s node affinity is added during the scheduling process.</p><p>As mentioned earlier, this new node is not considered a candidate node within the current scheduling cycle. Therefore, the pod is still rejected by the node affinity plugin. The issue arises if the scheduling queue puts the pod into <code>unschedulableQ</code>, as the pod would still need to wait for another event even though a node matching the pod’s node affinity requirement is available.</p><p>To prevent scenarios where pods miss events during scheduling, the scheduling queue records events occurring during pod scheduling and determines the pod’s queueing position based on these events and <code>QueueingHint</code>.</p><p>Therefore, the scheduling queue caches events from the time a pod leaves the scheduling queue until it returns to the scheduling queue or is scheduled. When the cached events are no longer needed, they are discarded.</p><h2 id="Golang-Doubly-Linked-List"><a href="#Golang-Doubly-Linked-List" class="headerlink" title="Golang Doubly Linked List"></a>Golang Doubly Linked List</h2><p><code>*list.List</code> is a data structure in Go’s standard library <code>container/list</code> package, representing a doubly linked list. In Go, doubly linked lists are a common data structure used to provide efficient performance for operations like element insertion, deletion, and traversal.</p><p>Here’s a brief overview of the <code>*list.List</code> structure:</p><ul><li><strong>Definition</strong>: <code>*list.List</code> is a pointer to a doubly linked list. It contains pointers to the head and tail of the list, as well as information about the length of the list.</li><li><strong>Features</strong>: Each node in the doubly linked list contains pointers to the previous and next nodes, making operations like inserting and deleting elements in the list efficient.</li><li><strong>Usage</strong>: <code>*list.List</code> is commonly used in scenarios where frequent insertion and deletion operations are required, especially when the number of elements is not fixed or the order may change frequently.</li></ul><p>Here’s a demonstration of how to use <code>*list.List</code> in Go:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">goCopy codepackage main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // Create a new doubly linked list</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // Add elements to the end of the list</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // Iterate over the list and print elements</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>PushBack</code> method adds a new element to the end of the list and returns a pointer to the new element (<code>*list.Element</code>). This pointer can be used for further operations on the element, such as removal or modification.</p><p>The <code>*list.Element</code> structure contains pointers to the previous and next elements in the list, as well as a field for storing the element’s value. By returning a <code>*list.Element</code> pointer, we can conveniently access the newly added element when needed for further operations. To remove an element from the doubly linked list, you can use the <code>list.Remove()</code> method. This method takes a list element as input and removes the element from the list.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>outputs：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>In this example, we remove the second element (with a value of 2) from the linked list.</p><h2 id="A-brief-analysis"><a href="#A-brief-analysis" class="headerlink" title="A brief analysis"></a>A brief analysis</h2><p>Let’s dive straight into analyzing the memory usage using pprof. Here’s a partial list of pprof profiles:</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>Here, we can observe that memory usage is primarily concentrated in protobuf’s <code>Decode</code>. Without delving into specific pprof analysis, we can consider three potential factors:</p><ul><li>Whether grpc-go has memory issues</li><li>Whether there are issues with Go itself</li><li>Whether Kubernetes has memory issues</li></ul><p>Regarding the first assumption, we can check related issues in grpc-go. However, recent reports do not indicate any memory anomalies. As for issues with Go itself, it doesn’t seem likely, although we did find a related Transparent Huge Pages (THP) issue, which we can briefly discuss later. Thus, the most probable cause would be an issue within Kubernetes itself. However, considering that <code>(*FieldsV1).Unmarshal</code> hasn’t been modified for 5 years, it’s highly unlikely to be the source of the problem. Therefore, let’s delve deeper into analyzing pprof.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>In the continuously growing list of pods, I noticed some unreleased data that seemed to align with the results of my previous analysis using pprof. Interestingly, pods are the only continuously changing objects. Therefore, I attempted another troubleshooting method to verify if the community had resolved this issue. I used minikube to launch Kubernetes version 1.18.5 locally for investigation. Fortunately, I couldn’t reproduce the issue, indicating that the problem might have been fixed after version 1.18.5.</p><p>To narrow down the investigation further, I asked my colleagues to inspect the commit history between these three minor versions. Eventually, we found a PR that closed the <code>SchedulerQueueingHints</code> feature. As mentioned in the technical background, the <code>SchedulerQueueingHints</code> feature could potentially lead to memory growth issues.</p><p>By examining the <code>PriorityQueue</code> structure, it was evident that the logic handling the feature was controlled by <code>isSchedulingQueueHintEnabled</code>. If the <code>QueueingHint</code> feature is enabled, when executing the <code>Pop</code> method to schedule pods, the UID of the corresponding pod in <code>inFlightPods</code> needs to be populated with the same linked list as <code>inFlightEvents</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So, when are the linked list fields removed? We can observe that the only time they are removed is when the pod completes its scheduling cycle, that is, when the <code>Done</code> method is called.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, it can be observed that the later the timing of <code>Done</code>, the more pronounced the memory growth, and if pod events are ignored or missed, abnormal memory growth in the linked list can also occur. Some fixes for the above scenario can be seen:</p><ul><li>A PR, such as #120586, which emphasizes calling <code>Done()</code> as soon as possible.</li><li>The <code>QueueingHint</code> of the NodeAffinity&#x2F;NodeUnschedulable plugins missed relevant Node events, as seen in PR#122284.</li></ul><p>Despite these modifications, such issues are far from over.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://openai.com/">https://openai.com/</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>探索Kubernetes 1.28调度器OOM的根源</title>
    <link href="https://zoues.com/posts/e46bd846/"/>
    <id>https://zoues.com/posts/e46bd846/</id>
    <published>2024-02-25T12:25:08.000Z</published>
    <updated>2024-07-13T00:51:35.754Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>年前，同事升级K8s调度器至1.28.3，观察到内存异常现象，帮忙一起看看，在集群pod及node随业务潮汐变动的情况下，内存呈现不断上升的趋势，直至OOM</p><blockquote><p>下面数据均为社区公开信息</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>触发场景有以下两种(社区还有其他复现方式)：</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=50</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>我们在社区的发现多起类似内存异常场景，复现方式不尽相同，关于上述问题的结论是：</p><blockquote><blockquote><p>Kubernetes社区在1.28版本中默认开启了调度特性SchedulerQueueingHints，导致调度组件内存异常。为了临时解决内存等问题，社区在1.28.5中将该特性调整为默认关闭。因为问题并未完全修复，所以建议审慎开启该特性。</p></blockquote></blockquote><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>介绍K8s调度器相关结构体</li><li>介绍K8s调度器QueueingHint</li><li>golang的双向链表</li></ul><h3 id="调度器简介"><a href="#调度器简介" class="headerlink" title="调度器简介"></a>调度器简介</h3><p>PriorityQueue是SchedulingQueue的接口实现。它的头部存放着优先级最高的待调度Pod。PriorityQueue包含以下重要字段：</p><ol><li>activeQ：存放准备好调度的Pod。新添加的Pod会被放入该队列。调度队列需要执行调度时，会从该队列中获取Pod。activeQ由堆来实现。</li><li>backoffQ：存放因各种原因（比如未满足节点要求）而被判定为无法调度的Pod。这些Pod会在一段退避时间后，被移到activeQ以尝试再次调度。backoffQ也由堆来实现。</li><li>unschedulablePods：存放因各种原因无法调度的Pod，是一个map数据结构。这些Pod被认定为无法调度，不会直接放入backoffQ，而是被记录在这里。待条件满足时，它们将被移到activeQ或者backoffQ中，调度队列会定期清理unschedulablePods 中的 Pod。</li><li>inFlightEvents：用于保存调度队列接收到的事件（entry的值是clusterEvent），以及正在处理中的Pod（entry的值是*v1.Pod），基于golang内部实现的双向链表</li><li>inFlightPods：保存了所有已经Pop，但尚未调用Done的Pod的UID，换句话说，所有当前正在处理中的Pod（正在调度、在admit中或在绑定周期中）。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>关于K8s完整介绍，参看<a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">kuberneter调度由浅入深：框架</a>，后续会更新最新的K8s调度器梳理</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>K8s调度器引入了<code>QueueingHint</code>特性，通过从每个插件获取有关Pod重新入队的建议，以减少不必要的调度重试，从而提升调度吞吐量。同时，在适当情况下跳过退避，进一步提高Pod调度效率。</p><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><p>当前，每个插件可以通过EventsToRegister定义何时重试调度被插件拒绝的Pod。</p><p>比如，NodeAffinity会在节点添加或更新时重试调度Pod，因为新添加或更新的节点可能具有与Pod上的NodeAffinity匹配的标签。然而，实际上，在集群中会发生大量节点更新事件，这并不能保证之前被NodeAffinity拒绝的Pod能够成功调度。</p><p>为了解决这个问题，调度器引入了更精细的回调函数，以过滤掉无关的事件，从而在下一个调度周期中仅重试可能成功调度的Pod。</p><p>另外，DRA（动态资源分配）调度插件有时需要拒绝Pod以等待来自设备驱动程序的状态更新。因此，某些Pod可能需要经过几个调度周期才能完成调度。针对这种情况，与等待设备驱动程序状态更新相比，回退等待的时间更长。因此，希望能够使插件在特定情况下跳过回退以改善调度性能。</p><h4 id="实现目标"><a href="#实现目标" class="headerlink" title="实现目标"></a>实现目标</h4><p>为了提高调度吞吐量，社区提出以下改进：</p><ol><li><strong>引入QueueingHint</strong><ul><li>将 <code>QueueingHint</code> 引入到 <code>EventsToRegister</code> 机制中，允许插件提供针对Pods重新入队的建议</li></ul></li><li><strong>增强 Pod 跟踪和重新入队机制</strong>：<ul><li>优化追踪调度队列内正在处理的 Pods实现</li><li>实现一种机制，将被拒绝的 Pods 重新入队到适当的队列</li><li>优化被拒绝的Pods的退避策略，能够使插件在特定情况下跳过回退，从而提高调度吞吐量。</li></ul></li></ol><h4 id="潜在风险"><a href="#潜在风险" class="headerlink" title="潜在风险"></a>潜在风险</h4><p><em><strong>1. 实现中的错误可能导致 Pod 在 unschedulablePods 中长时间无法被调度</strong></em></p><p>如果一个插件配置了 QueueingHint，但它错过了一些可以让 Pod 可调度的事件， 被该插件拒绝的 Pod 可能会长期困在 unschedulablePods 中。</p><p>虽然调度队列会定期清理unschedulablePods 中的 Pod。（默认为 5 分钟，可配）</p><p><em><strong>2. 内存使用量的增加</strong></em></p><p>因为调度队列需要保留调度过程中发生的事件，kube-scheduler的内存使用量会增加。 所以集群越繁忙，它可能需要的内存就越多。</p><p>虽然无法完全消除内存增长，但如果能够尽快释放缓存的事件，就可以延缓内存增长的速度。</p><p><em><strong>3.<code>EnqueueExtension</code> 中 <code>EventsToRegister</code> 中的重大变更</strong></em></p><p>自定义调度器插件的开发者需要进行兼容性升级， <code>EnqueueExtension</code> 中的 <code>EventsToRegister</code> 将返回值从 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code>。<code>ClusterEventWithHint</code> 允许每个插件通过名为 <code>QueueingHintFn</code> 的回调函数过滤更多无用的事件。</p><p>社区为了简化迁移工作，空的 <code>QueueingHintFn</code> 被视为始终返回 <code>Queue</code>。 因此，如果他们只想保持现有行为，他们只需要将 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code> 并不需要注册任何 <code>QueueingHintFn</code>。</p><h4 id="QueueingHints设计"><a href="#QueueingHints设计" class="headerlink" title="QueueingHints设计"></a>QueueingHints设计</h4><p>EventsToRegister 方法的返回类型已更改为 []ClusterEventWithHint</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// EnqueueExtensions 是一个可选接口，插件可以实现在内部调度队列中移动无法调度的 Pod。可以导</span><br><span class="line">// 致Pod无法调度（例如，Filter 插件）的插件可以实现此接口。</span><br><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 ClusterEventWithHint结构体包含一个 ClusterEvent 和一个 QueueingHintFn，当事件发生时执行 QueueingHintFn，并确定事件是否可以让 Pod满足调度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>类型 QueueingHintFn 是一个函数，其返回类型为 (QueueingHint, error)。其中，QueueingHint 是一个枚举类型，可能的值有 QueueSkip 和 Queue。QueueingHintFn 调用时机位于将 Pod 从 unschedulableQ 移动到 backoffQ 或 activeQ 之前，如果返回错误，将把调用方返回的 QueueingHint 处理为 <code>QueueAfterBackoff</code>，这种处理无论返回的结果是什么，都可以防止 Pod 永远待在unschedulableQ 队列中。</p><p><em><strong>何时跳过&#x2F;不跳过 backoff</strong></em></p><p>BackoffQ 通过防止“长期无法调度”的 Pod 阻塞队列以保持高吞吐量的轻量级队列。</p><p>Pod 在调度周期中被拒绝的次数越多，Pod 需要等待的时间就越长，即在BackoffQ 待得时间就越长。</p><p>例如，当 NodeAffinity 拒绝了 Pod，后来在其 QueueingHintFn 中返回 Queue 时，Pod 需要等待 backoff 后才能重试调度。</p><p>但是，某些插件的设计本身就需要在调度周期中经历一些失败。比如内置插件DRA（动态资源分配），在 Reserve extension处，它告诉资源驱动程序调度结果，并拒绝 Pod 一次以等待资源驱动程序的响应。针对这种拒绝情况，不能将其视作调度周期的浪费，尽管特定调度周期失败了，但基于该周期的调度结果可以促进 Pod 的调度。因此，由于这种原因被拒绝的 Pod 不需要受到惩罚（backoff）。</p><p>为了支持这种情况，我们引入了一个新的状态 Pending。当 DRA 插件使用 Pending 拒绝 Pod，并且后续在其 QueueingHintFn 中返回 Queue 时，Pod 跳过 backoff，Pod 被重新调度。</p><p><em><strong>QueueingHint 如何工作</strong></em></p><p>当K8s集群事件发生时，调度队列将执行在之前调度周期中拒绝 Pod 的那些插件的 QueueingHintFn。</p><p>通过下述几个场景，描述一下它们如何被执行以及如何移动 Pod。</p><p>a. Pod被一个或多个插件拒绝</p><p>假设有三个节点。当 Pod 进入调度周期时，一个节点由于资源不足拒绝了Pod，其他两个节因为Pod 的 NodeAffinity不匹配拒绝了Pod。</p><p>在这种情况下，Pod 被 NodeResourceFit 和 NodeAffinity 插件拒绝，最终被放到 unschedulableQ 中。</p><p>此后，每当注册在这些插件中的集群事件发生时，调度队列通过 QueueingHint 通知它们。如果来自 NodeResourceFit 或 NodeAffinity 的任何一个的 QueueingHintFn 返回 Queue，则将 Pod 移动到 activeQ或者backoffQ中。 （例如，当 NodeAdded 事件发生时，NodeResourceFit 的 QueueingHint 返回 Queue，因为 Pod 可能可调度到该新节点。）</p><p>它是移动到 activeQ 还是 backoffQ，这取决于此 Pod 在unschedulableQ 中停留的时间有多长。如果在unschedulableQ 停留的时间超过了预期的 Pod 的 backoff 延迟时间，则它将直接移动到 activeQ。否则，它将移动到 backoffQ。</p><p>b. Pod因 Pending 状态而被拒绝 </p><p>当 DRA 插件在 Reserve extension 阶段针对Pod返回 Pending时，调度队列将 DRA 插件添加到 Pod 的pendingPlugins 字典中的同时，Pod 返回调度队列。</p><p>当 DRA 插件的 QueueingHint 之后的调用中返回 Queue 时，调度队列将此 Pod 直接放入 activeQ。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>c. 跟踪调度队列中正在处理的 Pod</p><p>通过引入 QueueingHint，我们只能在特定事件发生时重试调度。但是，如果这些事件发生在Pod 的调度期间呢？</p><p>调度器对集群数据进行快照，并根据快照调度 Pod。每次启动调度周期时都会更新快照，换句话说，相同的快照在相同的调度周期中使用。</p><p>考虑到这样一个情景，比如，在调度一个 Pod 时，由于没有任何节点符合 Pod 的节点亲和性(NodeAffinity)，因此被拒绝，但是在调度过程中加入了一个新的节点，它与 Pod 的节点亲和性匹配。</p><p>如前所述，这个新节点在本次调度周期内不被视为候选节点，因此 Pod 仍然被节点亲和性插件拒绝。问题在于，如果调度队列将 Pod 放入unschedulableQ中，那么即使已经有一个节点匹配了 Pod 的节点亲和性要求，该 Pod 仍需要等待另一个事件。</p><p>为了避免类似Pod 在调度过程中错过事件的场景，调度队列会记录 Pod 调度期间发生的事件，并根据这些事件和QueueingHint来决定Pod 入队的位置。</p><p>因此，调度队列会缓存自 Pod 离开调度队列直到 Pod 返回调度队列或被调度的所有事件。当不再需要缓存的事件时，缓存的事件将被丢弃。</p><h3 id="Golang双向链表"><a href="#Golang双向链表" class="headerlink" title="Golang双向链表"></a>Golang双向链表</h3><p><code>*list.List</code> 是 Go 语言标准库 <code>container/list</code> 包中的一种数据结构，表示一个双向链表。在 Go 中，双向链表是一种常见的数据结构，用于在元素的插入、删除和遍历等操作上提供高效性能。</p><p>以下是 <code>*list.List</code> 结构的简要介绍：</p><ul><li><strong>定义</strong>：<code>*list.List</code> 是一个指向双向链表的指针，它包含了链表的头部和尾部指针，以及链表的长度信息。</li><li><strong>特性</strong>：双向链表中的每个节点都包含指向前一个节点和后一个节点的指针，这使得在链表中插入和删除元素的操作效率很高。</li><li><strong>用途</strong>：<code>*list.List</code> 常用于需要频繁插入和删除操作的场景，尤其是当元素的数量不固定或顺序可能经常变化时。</li></ul><p>演示了如何在 Go 中使用 <code>*list.List</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // 创建一个新的双向链表</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // 在链表尾部添加元素</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // 遍历链表并打印元素</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>PushBack</code> 方法会向链表的尾部添加一个新元素，并返回表示新元素的 <code>*list.Element</code> 指针。这个指针可以用于后续对该元素的操作，例如删除或修改。</p><p><code>*list.Element</code> 结构体包含了指向链表中前一个和后一个元素的指针，以及一个存储元素值的字段。通过返回 <code>*list.Element</code> 指针，我们可以方便地在需要时访问到新添加的元素，以便进行进一步的操作。要从双向链表中删除元素，你可以使用<code>list.Remove()</code>方法。这个方法需要传入一个链表元素，然后会将该元素从链表中移除。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">// 创建一个新的双向链表</span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">// 在链表尾部添加元素</span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">// 找到要删除的元素</span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">// 从链表中移除该元素</span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">// 打印剩余的元素</span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码会输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>在这个例子中，我们移除了链表中第二个元素（值为2）。</p><h2 id="浅析一番"><a href="#浅析一番" class="headerlink" title="浅析一番"></a>浅析一番</h2><p>直接上pprof来分析一下内存使用情况,部分pprof列表，如下所示：</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>这里可以发现，内存主要集中在protobuf的Decode，在不具体分析pprof的前提下，我们的思路有三点：</p><ul><li>grpc-go是否有内存问题</li><li>go本身是否问题</li><li>K8s内存问题</li></ul><p>针对第一个的假设，可以捞一下grpc-go的相关issue，可以发现近期未见相关内存异常的报告，go本身的问题，看起来也不太像，但倒是找到一个THP的相关问题，以后可以简单介绍一下，那么只剩一个结果，就是K8s本身存在问题，但其中<code>(*FieldsV1).Unmarshal</code>5年没动了，大概率不会存在问题，那么我们深入分析一下pprof吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>在持续增长的 Pod 列表中，发现了一些未释放的数据似乎与先前使用 pprof 分析的结果吻合，仅发现 Pod 是持续变更的对象。因此，我尝试了另一种排查方法，验证社区是否已解决此问题。我使用 minikube 在本地启动了 Kubernetes 1.18.5 版本进行排查。幸运的是，我未能复现这一现象，表明问题可能在 1.18.5 版本后已修复。</p><p>为了进一步缩小排查范围，我让同事检查了这三个小版本之间的提交记录。最终发现了一个关闭了 SchedulerQueueingHints 特性的 PR。正如在技术背景中提到的，SchedulerQueueingHints 特性可能导致内存增长问题。</p><p>通过PriorityQueue结构体可以发现其通过isSchedulingQueueHintEnabled来控制特性的逻辑处理，如果开启了<code>QueueingHint</code> 特性，那么在执行Pop方法来调度Pod时，需要为inFlightPods对应pod的UID填充相同inFlightEvents的链表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么链表字段何时移除？我们可以观察到移除的唯一时间点在pod完成调度周期时，也就是调用Done方法时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以发现如何done的时机越晚，内存的增长将越明显，并且如果Pod的事件被忽视或者遗漏，链表的内存同样会出现异常增加的现象，可以看到针对上述场景的一些修复：</p><ul><li>出现了call Done() as soon as possible这样的PR，参看PR#120586</li><li>NodeAffinity&#x2F;NodeUnschedulable插件的QueueingHint 遗漏相关Node事件，参看PR#122284</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>揭开K8s适配CgroupV2内存虚高的迷局</title>
    <link href="https://zoues.com/posts/3f237e52/"/>
    <id>https://zoues.com/posts/3f237e52/</id>
    <published>2024-01-27T12:40:08.000Z</published>
    <updated>2024-07-13T00:50:34.272Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>在Almalinux替换CentOS的过程中，我们通过kubectl top nodes命令观察到了两个相同规格的节点（只有cgroup版本不同）。在分别调度两个相同的Pod后，我们预期它们的内存使用量应该相近。然而，我们发现使用了cgroupv2的节点的内存使用量比使用了cgroupv1的节点多了约280Mi。</p><p>初步分析表明，可能是cAdvisor在统计cgroupv1和v2的内存使用量时存在逻辑上的不一致。</p><p>理论上，无论使用cgroupv1还是cgroupv2，两个相同配置的节点的内存使用量应该相近。实际上，在比较&#x2F;proc&#x2F;meminfo时，我们发现了总内存使用量近似的情况。那么问题出在哪里呢？</p><p>我们发现，这个问题只影响了节点级别的内存统计数据，而不影响Pod级别的统计数据。</p><p>问题的根本原因是cAdvisor调用了runc的接口，其计算root cgroup的内存数据方面存在差异。在cgroupv2中，root cgroup不存在memory.current这个文件，但在cgroupv1中root cgroup是存在memory.usage_in_bytes文件的。这导致了在统计cgroupv2内存使用量时出现了不一致的情况。</p><p>这个问题可能需要在cAdvisor或runc的逻辑中进行修复，以确保在cgroupv1和cgroupv2中的内存统计一致性。下面我们基于社区issue展开介绍。</p><p>v1.28.3 commit:a8a1abc25cad87333840cd7d54be2efaf31a3177</p><blockquote><p>NOTE: Containerd:1.6.21，K8s:1.28, Kernel:5.15.0<br>(同步以前的文章)</p></blockquote><hr><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为false）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">klet.containerRuntime,</span><br><span class="line">klet.statusManager,</span><br><span class="line">hostStatsProvider)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">kubeDeps.RemoteRuntimeService,</span><br><span class="line">kubeDeps.RemoteImageService,</span><br><span class="line">hostStatsProvider,</span><br><span class="line">utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在<code>/stats/</code>暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p>图片</p><p>从 Kubernetes 1.12 版本开始，kubelet 直接从 cAdvisor 暴露了多个接口。包括以下接口：</p><ul><li>cAdvisor 的 Prometheus 指标位于 <code>/metrics/cadvisor</code>。</li><li>cAdvisor v1 Json API 位于 <code>/stats/</code>、<code>/stats/container</code>、<code>/stats/&#123;podName&#125;/&#123;containerName&#125;</code> 和 <code>/stats/&#123;namespace&#125;/&#123;podName&#125;/&#123;uid&#125;/&#123;containerName&#125;</code>。</li><li>cAdvisor 的机器信息位于 &#x2F;spec。</li></ul><p>此外，kubelet还暴露了<code>summary API</code>，其中cAdvisor 是该接口指标来源之一。在社区的监控架构文档中描述了“核心”指标和“监控”指标的定义。这个文档中规定了一组核心指标及其用途，并且目标是通过拆分监控架构来实现以下两个目标：</p><ul><li><p>减小核心指标的统计收集性能影响，允许更频繁地收集这些指标。</p></li><li><p>使监控方案可替代且可扩展。</p></li></ul><p>因此移除cadvisor的接口，成了一项长期目标，目前进度如下(进度状态的标记略为滞后)：</p><ul><li><p>[1.13] 引入 Kubelet 的 pod-resources gRPC 端点；KEP: 支持设备监控社区#2454</p></li><li><p>[1.14] 引入 Kubelet 资源指标 API</p></li><li><p>[1.15] 通过添加和弃用 <code>--enable-cadvisor-json-endpoints</code> 标志，废弃“直接” cAdvisor API 端点</p></li><li><p>[1.18] 默认将 –enable-cadvisor-json-endpoints 标志设置为禁用</p></li><li><p>[1.21] 移除 <code>--enable-cadvisor-json-endpoints</code> 标志</p></li><li><p>[1.21] 将监控服务器过渡到 Kubelet 资源指标 API（需要3个版本的差异）</p></li><li><p>[TBD] 为 kubelet 监控端点提出外部替代方案</p></li><li><p>[TBD] 通过添加和废弃 <code>--enable-container-monitoring-endpoints</code> 标志，废弃摘要 API 和 cAdvisor Prometheus 端点</p></li><li><p>[TBD+2] 移除“直接”的 cAdvisor API 端点</p></li><li><p>[TBD+2] 默认将 –enable-container-monitoring-endpoints 标志设置为禁用</p></li><li><p>[TBD+4] 移除摘要 API、cAdvisor Prometheus 指标和移除 –enable-container-monitoring-endpoints 标志。</p></li></ul><p>当前版本的cadvisor接口已经做了部分废弃，例如<code>/spec及/stats/*</code>等<br><a href="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></a></p><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>kubelet 使用 cadvisor 来获取节点级别的统计信息（无论是使用 cri 还是通过cadvisor 来统计提供程序来获取 pod 的统计信息）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/provider.go</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// NewCRIStatsProvider returns a Provider that provides the node stats</span><br><span class="line">// from cAdvisor and the container stats from CRI.</span><br><span class="line">func NewCRIStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">runtimeService internalapi.RuntimeService,</span><br><span class="line">imageService internalapi.ImageManagerService,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">podAndContainerStatsFromCRI bool,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCRIStatsProvider(cadvisor, resourceAnalyzer,</span><br><span class="line">runtimeService, imageService, hostStatsProvider, podAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// NewCadvisorStatsProvider returns a containerStatsProvider that provides both</span><br><span class="line">// the node and the container stats from cAdvisor.</span><br><span class="line">func NewCadvisorStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">imageService kubecontainer.ImageService,</span><br><span class="line">statusProvider status.PodStatusProvider,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCadvisorStatsProvider(cadvisor, resourceAnalyzer, imageService, statusProvider, hostStatsProvider))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下述两种方式获取节点的内存使用情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl top node</span><br><span class="line">kubectl get --raw /api/v1/nodes/foo/proxy/stats/summary | jq -C .node.memory</span><br></pre></td></tr></table></figure><p>结果显示cgroupv2节点的内存使用量比相同节点配置但使用 cgroupv1的高一些。kubectl top node 获取节点信息的逻辑在：<a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40">https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40</a></p><p>kubelet使用 cadvisor 来获取 cgroup 统计信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/server/stats/summary.go</span><br><span class="line"></span><br><span class="line">rootStats, err := sp.provider.GetCgroupCPUAndMemoryStats(&quot;/&quot;, false)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return nil, fmt.Errorf(&quot;failed to get root cgroup stats: %v&quot;, err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里GetCgroupCPUAndMemoryStats调用以下cadvisor逻辑</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/helper.go</span><br><span class="line"></span><br><span class="line">infoMap, err := cadvisor.ContainerInfoV2(containerName, cadvisorapiv2.RequestOptions&#123;</span><br><span class="line">IdType:    cadvisorapiv2.TypeName,</span><br><span class="line">Count:     2, // 2 samples are needed to compute &quot;instantaneous&quot; CPU</span><br><span class="line">Recursive: false,</span><br><span class="line">MaxAge:    maxAge,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>cadvisor 基于 cgroup v1&#x2F;v2 获取不同 cgroup manager接口实现，然后调用GetStats()获取监控信息。</p><p>这些实现在计算root cgroup 的内存使用方面存在差异。</p><ul><li><p>v1 使用来自 memory.usage_in_bytes 的内存使用情况：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224</a></p></li><li><p>v2 使用 &#x2F;proc&#x2F;meminfo 并计算使用情况为总内存 - 空闲内存：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217</a></p></li></ul><p>usage_in_bytes 大致等于 RSS + Cache。workingset是 usage - 非活动文件。</p><p>在 cadvisor 中，在workingset中排除了非活动文件：<a href="https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844">https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844</a>“</p><p>因此可以判断在cgroupv2计算内存使用使用了total-free，这里面包含了inactive_anon，而内核以及cgroupv1计算内存使用量时不会计入 inactive_anon：<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720</a></p><p>通过下面的测试中，inactive_anon 解释数据看到了差异。</p><p>下述分别为cgroupv1及cgroupv2的两个集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   98m          2%     1512Mi          12%</span><br><span class="line">node2   99m          2%     1454Mi          11%</span><br><span class="line">node3   94m          2%     1448Mi          11%</span><br></pre></td></tr></table></figure><p>其中cgroupv1节点的root cgroup内存使用如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /sys/fs/cgroup/memory/memory.usage_in_bytes</span><br><span class="line">6236864512</span><br><span class="line">~ # cat /sys/fs/cgroup/memory/memory.stat</span><br><span class="line">cache 44662784</span><br><span class="line">rss 3260416</span><br><span class="line">rss_huge 2097152</span><br><span class="line">shmem 65536</span><br><span class="line">mapped_file 11083776</span><br><span class="line">dirty 135168</span><br><span class="line">writeback 0</span><br><span class="line">pgpgin 114774</span><br><span class="line">pgpgout 103506</span><br><span class="line">pgfault 165891</span><br><span class="line">pgmajfault 99</span><br><span class="line">inactive_anon 135168</span><br><span class="line">active_anon 3645440</span><br><span class="line">inactive_file 5406720</span><br><span class="line">active_file 39333888</span><br><span class="line">unevictable 0</span><br><span class="line">hierarchical_memory_limit 9223372036854771712</span><br><span class="line">total_cache 5471584256</span><br><span class="line">total_rss 767148032</span><br><span class="line">total_rss_huge 559939584</span><br><span class="line">total_shmem 1921024</span><br><span class="line">total_mapped_file 605687808</span><br><span class="line">total_dirty 270336</span><br><span class="line">total_writeback 0</span><br><span class="line">total_pgpgin 51679194</span><br><span class="line">total_pgpgout 50291069</span><br><span class="line">total_pgfault 97383769</span><br><span class="line">total_pgmajfault 5610</span><br><span class="line">total_inactive_anon 1081344</span><br><span class="line">total_active_anon 772235264</span><br><span class="line">total_inactive_file 4648124416</span><br><span class="line">total_active_file 820551680</span><br><span class="line">total_unevictable 0</span><br></pre></td></tr></table></figure><p>meminfo文件如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /proc/meminfo</span><br><span class="line">MemTotal:       16393244 kB</span><br><span class="line">MemFree:         9744148 kB</span><br><span class="line">MemAvailable:   15020900 kB</span><br><span class="line">Buffers:          132344 kB</span><br><span class="line">Cached:          5207356 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:          1557252 kB</span><br><span class="line">Inactive:        4526668 kB</span><br><span class="line">Active(anon):     745916 kB</span><br><span class="line">Inactive(anon):      792 kB</span><br><span class="line">Active(file):     811336 kB</span><br><span class="line">Inactive(file):  4525876 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               636 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        618992 kB</span><br><span class="line">Mapped:           624384 kB</span><br><span class="line">Shmem:              2496 kB</span><br><span class="line">KReclaimable:     285824 kB</span><br><span class="line">Slab:             423600 kB</span><br><span class="line">SReclaimable:     285824 kB</span><br><span class="line">SUnreclaim:       137776 kB</span><br><span class="line">KernelStack:        8400 kB</span><br><span class="line">PageTables:         9060 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8196620 kB</span><br><span class="line">Committed_AS:    2800016 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       40992 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             4432 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    270336 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">CmaTotal:              0 kB</span><br><span class="line">CmaFree:               0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      302344 kB</span><br><span class="line">DirectMap2M:     3891200 kB</span><br><span class="line">DirectMap1G:    14680064 kB</span><br></pre></td></tr></table></figure><p>当前的计算</p><p>memory.current - memory.stat.total_inactive_file &#x3D; 6236864512 - 4648124416 &#x3D; 1515 Mi -&gt; kubelet 报告的结果</p><p>cgroupv2 集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   113m         2%     2196Mi          17%</span><br><span class="line">node2   112m         2%     2171Mi          17%</span><br><span class="line">node3   113m         2%     2180Mi          17%</span><br></pre></td></tr></table></figure><p>其中一节点的meminfo文件如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">MemTotal:       16374584 kB</span><br><span class="line">MemFree:         9505980 kB</span><br><span class="line">MemAvailable:   14912544 kB</span><br><span class="line">Buffers:          155164 kB</span><br><span class="line">Cached:          5335576 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:           872420 kB</span><br><span class="line">Inactive:        5399340 kB</span><br><span class="line">Active(anon):       2568 kB</span><br><span class="line">Inactive(anon):   791340 kB</span><br><span class="line">Active(file):     869852 kB</span><br><span class="line">Inactive(file):  4608000 kB</span><br><span class="line">Unevictable:       30740 kB</span><br><span class="line">Mlocked:           27668 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               148 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        716552 kB</span><br><span class="line">Mapped:           608424 kB</span><br><span class="line">Shmem:              6320 kB</span><br><span class="line">KReclaimable:     274360 kB</span><br><span class="line">Slab:             355976 kB</span><br><span class="line">SReclaimable:     274360 kB</span><br><span class="line">SUnreclaim:        81616 kB</span><br><span class="line">KernelStack:        8064 kB</span><br><span class="line">PageTables:         7692 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8187292 kB</span><br><span class="line">Committed_AS:    2605012 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       48092 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             3472 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    409600 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      271624 kB</span><br><span class="line">DirectMap2M:     8116224 kB</span><br><span class="line">DirectMap1G:    10485760 kB</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">usage = total - free = 16374584 - 9505980</span><br><span class="line"></span><br><span class="line">workingset = 总内存 - 空闲内存 - 非活动文件 = 16374584 - 9505980 - 4608000 = 2207 Mi（kubelet 报告的结果）</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如上所述，在Linux kernel及runc cgroupv1计算内存使用为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mem_cgroup_usage =NR_FILE_PAGES + NR_ANON_MAPPED + nr_swap_pages (如果swap启用的话)</span><br><span class="line"></span><br><span class="line">// - rss (NR_ANON_MAPPED)</span><br><span class="line">// - cache (NR_FILE_PAGES)</span><br></pre></td></tr></table></figure><p>但是runc在cgroupv2计算使用了total-free，因此在相似负载下，同一台机器上v1和v2版本的节点级别报告确实会相差约250-750Mi，为了让cgroup v2的内存使用计算更接近 cgroupv1，  cgroup v2调整计算内存使用量方式为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.MemoryStats.Usage.Usage = stats.MemoryStats.Stats[&quot;anon&quot;] + stats.MemoryStats.Stats[&quot;file&quot;]</span><br></pre></td></tr></table></figure><p>当然，我们同时还需要处理cadvisor的woringset的处理逻辑</p><p>由于笔者时间、视野、认知有限，本文难免出现错误、疏漏等问题，期待各位读者朋友、业界专家指正交流，上述排障信息已修改为社区内容。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680">https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680</a><br>2.<a href="https://github.com/kubernetes/kubernetes/issues/68522">https://github.com/kubernetes/kubernetes/issues/68522</a><br>3.<a href="https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/">https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一条K8s命令行引发的血案</title>
    <link href="https://zoues.com/posts/5a8a6c8d/"/>
    <id>https://zoues.com/posts/5a8a6c8d/</id>
    <published>2024-01-27T01:25:08.000Z</published>
    <updated>2024-07-13T00:50:17.283Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h2 id="一条K8s命令行引发的血案"><a href="#一条K8s命令行引发的血案" class="headerlink" title="一条K8s命令行引发的血案"></a>一条K8s命令行引发的血案</h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>因为Centos EOL的缘故，去年内部忙着换OS，打算趁此机会从cgroup v1切到cgroup v2，然而，在低版本K8s适配cgroupv2的过程中，遇到了一些问题，前期kubelet在cgroup v1的环境下，使用<code>-enable_load_reader</code>暴露容器的cpu load等相关监控数据，但在cgroup v2环境下，使用该配置会导致kubelet发生panic</p><p>下述为关键性信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container.go:422] Could not initialize cpu load reader for &quot;/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podXXX.slice&quot;: failed to create a netlink based cpuload reader: failed to get netlink family id for task stats: binary.Read: invalid type int32</span><br></pre></td></tr></table></figure><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>容器指标如何生成</li><li>K8s如何集成容器监控</li><li>cpu load如何计算等</li></ul><h3 id="cadvisor"><a href="#cadvisor" class="headerlink" title="cadvisor"></a>cadvisor</h3><p>cAdvisor是一款强大的Docker容器监控工具，专为容器场景设计，方便监控资源使用和性能分析。它用于收集、汇总、处理和输出容器的相关信息。cAdvisor支持Docker容器，同时支持其他类型的容器运行时。</p><p>Kubelet内置了对cAdvisor的支持，用户可以直接通过Kubelet组件获取有关节点上容器的监控指标。</p><blockquote><p>K8s 1.19使用的cAdvisor版本为0.39.3，而这里的简要介绍使用的是版本0.48.1。</p></blockquote><p>以下是主要功能代码，其中包含了一些注释以提高可读性。代码路径为：&#x2F;cadvisor&#x2F;cmd&#x2F;cadvisor.go。</p><p>cAdvisor主要完成以下几项任务：</p><ul><li>对外提供外部使用的API，包括一般的API接口和Prometheus接口。</li><li>支持第三方数据存储，包括BigQuery、Elasticsearch、InfluxDB、Kafka、Prometheus、Redis、StatsD和标准输出。</li><li>收集与容器、进程、机器、Go运行时以及自定义业务相关的监控。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func init() &#123;</span><br><span class="line">optstr := container.AllMetrics.String()</span><br><span class="line">flag.Var(&amp;ignoreMetrics, &quot;disable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be disabled. Options are %s.&quot;, optstr))</span><br><span class="line">flag.Var(&amp;enableMetrics, &quot;enable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be enabled. If set, overrides &#x27;disable_metrics&#x27;. Options are %s.&quot;, optstr))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上述代码可以看到，cadvisor支持是否开启相关指标的能力，其中<code>AllMetrics</code>主要是下述指标:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/container/factory.go#L72</span><br><span class="line"></span><br><span class="line">var AllMetrics = MetricSet&#123;</span><br><span class="line">CpuUsageMetrics:                struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessSchedulerMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerCpuUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryNumaMetrics:              struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CpuLoadMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskIOMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskUsageMetrics:               struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkTcpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkAdvancedTcpUsageMetrics: struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUdpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">AppMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">HugetlbUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerfMetrics:                    struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ReferencedMemoryMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUTopologyMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ResctrlMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUSetMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">OOMMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">var includedMetrics container.MetricSet</span><br><span class="line">if len(enableMetrics) &gt; 0 &#123;</span><br><span class="line">includedMetrics = enableMetrics</span><br><span class="line">&#125; else &#123;</span><br><span class="line">includedMetrics = container.AllMetrics.Difference(ignoreMetrics)</span><br><span class="line">&#125;</span><br><span class="line">// 上述处理需要开启的指标</span><br><span class="line">klog.V(1).Infof(&quot;enabled metrics: %s&quot;, includedMetrics.String())</span><br><span class="line">setMaxProcs()</span><br><span class="line">// 内存方式存在监控指标</span><br><span class="line">memoryStorage, err := NewMemoryStorage()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to initialize storage driver: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 这是cadvisor核心逻辑，kubelet内部就是直接调用的manager.New</span><br><span class="line">resourceManager, err := manager.New(memoryStorage, sysFs, manager.HousekeepingConfigFlags, includedMetrics, &amp;collectorHTTPClient, strings.Split(*rawCgroupPrefixWhiteList, &quot;,&quot;), strings.Split(*envMetadataWhiteList, &quot;,&quot;), *perfEvents, *resctrlInterval)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to create a manager: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 注册对外的HTTP接口.</span><br><span class="line">err = cadvisorhttp.RegisterHandlers(mux, resourceManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm, *urlBasePrefix)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to register HTTP handlers: %v&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line">// 这里是容器标签的处理，kubelet 1.28切换到CRI之后需要修改kubelet</span><br><span class="line">containerLabelFunc := metrics.DefaultContainerLabels</span><br><span class="line">if !*storeContainerLabels &#123;</span><br><span class="line">whitelistedLabels := strings.Split(*whitelistedContainerLabels, &quot;,&quot;)</span><br><span class="line">// Trim spacing in labels</span><br><span class="line">for i := range whitelistedLabels &#123;</span><br><span class="line">whitelistedLabels[i] = strings.TrimSpace(whitelistedLabels[i])</span><br><span class="line">&#125;</span><br><span class="line">containerLabelFunc = metrics.BaseContainerLabels(whitelistedLabels)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中cpu load是否生成指标，同时也由命令行<code>enable_load_reader</code>控制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/42bb3d13a0cf9ab80c880a16c4ebb4f36e51b0c9/manager/container.go#L455</span><br><span class="line"></span><br><span class="line">if *enableLoadReader &#123;</span><br><span class="line">// Create cpu load reader.</span><br><span class="line">loadReader, err := cpuload.New()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Warningf(&quot;Could not initialize cpu load reader for %q: %s&quot;, ref.Name, err)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cont.loadReader = loadReader</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为<strong>false</strong>）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      klet.containerRuntime,</span><br><span class="line">      klet.statusManager,</span><br><span class="line">      hostStatsProvider)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      kubeDeps.RemoteRuntimeService,</span><br><span class="line">      kubeDeps.RemoteImageService,</span><br><span class="line">      hostStatsProvider,</span><br><span class="line">      utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在&#x2F;stats&#x2F;暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p><img src="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></p><p>最终可以看到cadvisor组件如何在kubelet完成初始化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L80</span><br><span class="line"></span><br><span class="line">func New(imageFsInfoProvider ImageFsInfoProvider, rootPath string, cgroupRoots []string, usingLegacyStats, localStorageCapacityIsolation bool) (Interface, error) &#123;</span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line">// 这里就是kubelet默认暴露的监控指标类型</span><br><span class="line">includedMetrics := cadvisormetrics.MetricSet&#123;</span><br><span class="line">...</span><br><span class="line">cadvisormetrics.CpuLoadMetrics:      struct&#123;&#125;&#123;&#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">// 创建cAdvisor container manager.</span><br><span class="line">m, err := manager.New(memory.New(statsCacheDuration, nil), sysFs, housekeepingConfig, includedMetrics, http.DefaultClient, cgroupRoots, nil /* containerEnvMetadataWhiteList */, &quot;&quot; /* perfEventsFile */, time.Duration(0) /*resctrlInterval*/)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里就是直接调用的cadvisor的manager.New的函数接口，更详细的信息可参看：<a href="https://zoues.com/posts/3f237e52/">https://zoues.com/posts/3f237e52/</a></p><h3 id="CPU-Load指标"><a href="#CPU-Load指标" class="headerlink" title="CPU Load指标"></a>CPU Load指标</h3><p>CPU使用率反映的是当前cpu的繁忙程度，CPU平均负载（load average）是指某段时间内占用cpu时间的进程和等待cpu时间的进程数，这里等待cpu时间的进程是指等待被唤醒的进程，不包括处于wait状态进程。</p><p>在对设备做相关诊断时，需要结合cpu使用率、平均负载以及任务状态来进行判断，比如CPU使用率低但负载高，可能是IO瓶颈等，对此不作深入介绍。</p><p>在cadvisor中对外暴露的指标名称为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container_cpu_load_average_10s</span><br></pre></td></tr></table></figure><p>那么我们来看看是如何被计算出来的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L632</span><br><span class="line"></span><br><span class="line">// Calculate new smoothed load average using the new sample of runnable threads.</span><br><span class="line">// The decay used ensures that the load will stabilize on a new constant value within</span><br><span class="line">// 10 seconds.</span><br><span class="line">func (cd *containerData) updateLoad(newLoad uint64) &#123;</span><br><span class="line">if cd.loadAvg &lt; 0 &#123;</span><br><span class="line">cd.loadAvg = float64(newLoad) // initialize to the first seen sample for faster stabilization.</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>公式计算：<code>cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</code></p><p>大体意思是取的上一次采集计算出来的值cd.loadAvg乘以计算因子cd.loadDecay，然后加上当前采集</p><p>到的newLoad值乘以(1.0-cd.loadDecay)最后得出当前的cd.loadAvg值</p><p>其中<code>cont.loadDecay</code>计算逻辑如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L453</span><br><span class="line"></span><br><span class="line">cont.loadDecay = math.Exp(float64(-cont.housekeepingInterval.Seconds() / 10))</span><br></pre></td></tr></table></figure><p>这里是跟<code>housekeepingInterval</code>相关的固定值，衰变窗口</p><blockquote><p>关于容器cpu load的详细介绍可以看引用链接</p></blockquote><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>cpu load的cd.loadAvg前值通过如下方式获取：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L650</span><br><span class="line"></span><br><span class="line">if cd.loadReader != nil &#123;</span><br><span class="line">// TODO(vmarmol): Cache this path.</span><br><span class="line">path, err := cd.handler.GetCgroupPath(&quot;cpu&quot;)</span><br><span class="line">if err == nil &#123;</span><br><span class="line">loadStats, err := cd.loadReader.GetCpuLoad(cd.info.Name, path)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return fmt.Errorf(&quot;failed to get load stat for %q - path %q, error %s&quot;, cd.info.Name, path, err)</span><br><span class="line">&#125;</span><br><span class="line">stats.TaskStats = loadStats</span><br><span class="line">cd.updateLoad(loadStats.NrRunning)</span><br><span class="line">// convert to &#x27;milliLoad&#x27; to avoid floats and preserve precision.</span><br><span class="line">stats.Cpu.LoadAverage = int32(cd.loadAvg * 1000)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>深入探究可以发现使用了netlink来获取系统指标，关键调用路径:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">updateStats-&gt;GetCpuLoad-&gt;getLoadStats-&gt;prepareCmdMessage-&gt;prepareMessage</span><br></pre></td></tr></table></figure><p>经过上述分析可知， cAdvisor通过发送CGROUPSTATS_CMD_GET请求来获取CPU负载信息，通过netlink消息进行通信：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cadvisor/utils/cpuload/netlink/netlink.go</span><br></pre></td></tr></table></figure><p>在<code>v0.48.1</code>分支的第128到132行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func prepareCmdMessage(id uint16, cfd uintptr) (msg netlinkMessage) &#123; </span><br><span class="line">buf := bytes.NewBuffer([]byte&#123;&#125;) </span><br><span class="line">addAttribute(buf, unix.CGROUPSTATS_CMD_ATTR_FD, uint32(cfd), 4) </span><br><span class="line">return prepareMessage(id, unix.CGROUPSTATS_CMD_GET, buf.Bytes()) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终内核在<code>cgroupstats_user_cmd</code>中处理获取请求：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/* user-&gt;kernel request/get-response */</span><br></pre></td></tr></table></figure><p><a href="https://github.com/torvalds/linux/blob/master/kernel/taskstats.c#L407"><code>kernel/taskstats.c#L407</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">static int cgroupstats_user_cmd(struct sk_buff *skb, struct genl_info *info)</span><br><span class="line">&#123;</span><br><span class="line">int rc = 0;</span><br><span class="line">struct sk_buff *rep_skb;</span><br><span class="line">struct cgroupstats *stats;</span><br><span class="line">struct nlattr *na;</span><br><span class="line">size_t size;</span><br><span class="line">u32 fd;</span><br><span class="line">struct fd f;</span><br><span class="line"></span><br><span class="line">na = info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD];</span><br><span class="line">if (!na)</span><br><span class="line">return -EINVAL;</span><br><span class="line"></span><br><span class="line">fd = nla_get_u32(info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD]);</span><br><span class="line">f = fdget(fd);</span><br><span class="line">if (!f.file)</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">size = nla_total_size(sizeof(struct cgroupstats));</span><br><span class="line"></span><br><span class="line">rc = prepare_reply(info, CGROUPSTATS_CMD_NEW, &amp;rep_skb,</span><br><span class="line">size);</span><br><span class="line">if (rc &lt; 0)</span><br><span class="line">goto err;</span><br><span class="line"></span><br><span class="line">na = nla_reserve(rep_skb, CGROUPSTATS_TYPE_CGROUP_STATS,</span><br><span class="line">sizeof(struct cgroupstats));</span><br><span class="line">if (na == NULL) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">rc = -EMSGSIZE;</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stats = nla_data(na);</span><br><span class="line">memset(stats, 0, sizeof(*stats));</span><br><span class="line"></span><br><span class="line">rc = cgroupstats_build(stats, f.file-&gt;f_path.dentry);</span><br><span class="line">if (rc &lt; 0) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rc = send_reply(rep_skb, info);</span><br><span class="line"></span><br><span class="line">err:</span><br><span class="line">fdput(f);</span><br><span class="line">return rc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并在<code>cgroupstats_build</code>函数中构建cgroup stats结果：</p><p><a href="https://github.com/torvalds/linux/blob/5c1ee569660d4a205dced9cb4d0306b907fb7599/kernel/cgroup/cgroup-v1.c#L699"><code>kernel/cgroup/cgroup-v1.c#L699</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * cgroupstats_build - build and fill cgroupstats</span><br><span class="line"> * @stats: cgroupstats to fill information into</span><br><span class="line"> * @dentry: A dentry entry belonging to the cgroup for which stats have</span><br><span class="line"> * been requested.</span><br><span class="line"> *</span><br><span class="line"> * Build and fill cgroupstats so that taskstats can export it to user</span><br><span class="line"> * space.</span><br><span class="line"> *</span><br><span class="line"> * Return: %0 on success or a negative errno code on failure</span><br><span class="line"> */</span><br><span class="line">int cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)</span><br><span class="line">&#123;</span><br><span class="line">……</span><br><span class="line">/* it should be kernfs_node belonging to cgroupfs and is a directory */</span><br><span class="line">if (dentry-&gt;d_sb-&gt;s_type != &amp;cgroup_fs_type || !kn ||</span><br><span class="line">    kernfs_type(kn) != KERNFS_DIR)</span><br><span class="line">return -EINVAL;  // 导致返回EINVAL错误码</span><br></pre></td></tr></table></figure><p>这里可以发现<code>cgroup_fs_type</code>是cgroup v1的类型，而没有处理cgroup v2。因此，<code>cgroupstats_build</code>函数在路径类型判断语句上返回EINVAL。</p><p>在内核社区也有相关问题的说明：<a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">kernel community issue</a></p><p>那么我们看看tejun(meta，cgroupv2 owner)如何解释的：</p><blockquote><p>The exclusion of cgroupstats from v2 interface was intentional due to the duplication and inconsistencies with other statistics. If you need these numbers, please justify and add them to the appropriate cgroupfs stat file.</p></blockquote><p>简单翻译：对v2接口中排除cgroupstats的操作是有意的，因为它与其他统计数据存在重复和不一致之处。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>那么他的建议是什么？</p><p>他建议我们使用psi，而不是通过CGROUPSTATS_CMD_GET netlink api获取CPU统计信息，直接从<code>cpu.pressure</code>、<code>memory.pressure</code>以及<code>io.pressure</code>文件中获取，后续我们会介绍psi在容器领域的相关进展，当前Containerd已经支持PSI相关监控.</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://github.com/containerd/cgroups/pull/308">https://github.com/containerd/cgroups/pull/308</a></li><li><a href="https://cloud.tencent.com/developer/article/2329489">https://cloud.tencent.com/developer/article/2329489</a></li><li><a href="https://github.com/google/cadvisor/issues/3137">https://github.com/google/cadvisor/issues/3137</a></li><li><a href="https://www.cnblogs.com/vinsent/p/15830271.html">https://www.cnblogs.com/vinsent/p/15830271.html</a></li><li><a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
</feed>
