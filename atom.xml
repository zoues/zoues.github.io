<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zouyee</title>
  
  <subtitle>life is short, enjoy it</subtitle>
  <link href="https://zoues.com/atom.xml" rel="self"/>
  
  <link href="https://zoues.com/"/>
  <updated>2024-08-13T14:20:12.542Z</updated>
  <id>https://zoues.com/</id>
  
  <author>
    <name>zouyee</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>比肩Rust？万字Ziglang项目实战</title>
    <link href="https://zoues.com/posts/22db7120/"/>
    <id>https://zoues.com/posts/22db7120/</id>
    <published>2024-08-12T10:00:43.000Z</published>
    <updated>2024-08-13T14:20:12.542Z</updated>
    
    <content type="html"><![CDATA[<h1 id="比肩Rust？万字Ziglang项目实战"><a href="#比肩Rust？万字Ziglang项目实战" class="headerlink" title="比肩Rust？万字Ziglang项目实战"></a>比肩Rust？万字Ziglang项目实战</h1><p>Zig 是一种命令式、通用、静态类型、编译型系统编程语言和工具链，用于维护健壮、优化和可重用的软件。</p><ul><li>健壮：即使在内存不足等极端情况下，行为也是正确的。</li><li>优化：以最佳方式编写程序，使其能够表现良好。</li><li>可重用：相同的代码在具有不同约束条件的许多环境中都可以使用。</li><li>可维护：精确地向编译器和其他程序员传达意图。该语言对于阅读代码的开销很小，并且对于需求和环境的变化具有韧性。</li></ul><p>它支持编译时泛型、反射和评估、交叉编译和手动内存管理。Zig 的一个主要目标是改进 C 语言，同时也受到 Rust 等其他语言的启发。</p><p>学习 Zig 的资源有很多，主要包括：</p><ul><li><a href="https://ziglang.org/learn/">Zig 文档</a></li><li><a href="https://ziglang.org/documentation/master/std/">Zig 标准库参考</a></li><li><a href="Ziglearn.org">Ziglearn.org</a></li><li><a href="">Zig 简明教程</a></li><li><a href="">三十分钟搞定Ziglang</a>等</li></ul><p>学习一门语言最快捷的方式，当然是用它，上手最快的方式，当属项目实战，在运用中去学习，在学习中输出。通过本文我们将学习到：</p><ul><li>切片的使用</li><li>defer防止内存泄漏</li><li>项目开发与维护</li><li>从0到1开发HTTP服务等</li></ul><h2 id="0x1-从切片开始"><a href="#0x1-从切片开始" class="headerlink" title="0x1: 从切片开始"></a>0x1: 从切片开始</h2><p>以下是用于演示的代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">// 导入标准库</span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">/// 返回一个切片</span><br><span class="line">fn zigBits() []u8 &#123;</span><br><span class="line">    // 创建一个数组字面量</span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    // 将数组作为字符串打印出来</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    // 我们需要使用地址运算符（&amp;）将其强制转换为切片类型 &#x27;[]u8&#x27;。</span><br><span class="line">    return &amp;message;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/// 程序的入口</span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    // 常量赋值</span><br><span class="line">    const message = zigBits();</span><br><span class="line"></span><br><span class="line">    // 打印message</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Q：上述实现是从一个函数中返回一个切片，并打印其值吗？</p><p>A：是的！我们期望看到 zigbits 两次。一次来自函数内部，一次来自主函数。</p><p>让我们运行它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug: �,�$</span><br></pre></td></tr></table></figure><p>这跟我们预期的结果不太一致。retry？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug:</span><br><span class="line">       �;�</span><br></pre></td></tr></table></figure><p>Q：这里打印的是 u8 数组而不是切片？结果显示不是同一个数组？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">+ std.log.debug(&quot;&#123;d&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">$ zig build run</span><br><span class="line">debug: &#123; 122, 105, 103, 98, 105, 116, 115 &#125;</span><br><span class="line">debug: &#123; 80, 129, 179, 51, 255, 127, 0 &#125;</span><br></pre></td></tr></table></figure><h3 id="分析说明"><a href="#分析说明" class="headerlink" title="分析说明"></a>分析说明</h3><p>让我们看看这一行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return &amp;message;</span><br></pre></td></tr></table></figure><p>在这里，我们实际上返回的是一个栈上分配的数组切片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try std.testing.expect(@TypeOf(&amp;message) == *[7]u8);</span><br></pre></td></tr></table></figure><p>由于数组是在栈上分配的，当我们从函数中返回时，它可能会被破坏，即当我们从函数返回时释放时。这在文档的“Lifetime and Ownership”部分有解释：</p><p>Zig 使用者有责任确保指针在指向的内存不再可用时即不会被访问。注意，切片是指针的一种形式，因为它引用其他内存。</p><p>这就是当我们尝试在从函数返回后打印数组内容时为什么会得到随机的无意义字符串的原因。</p><p>此外，在 ziglang&#x2F;zig官方仓库中有一个相关问题说明：<a href="https://github.com/ziglang/zig/issues/5725">https://github.com/ziglang/zig/issues/5725</a></p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>我们可以通过几种方式来解决这个问题：</p><ol><li>将切片作为参数传递给函数</li><li>将数组设置为全局变量</li><li>最常见的做法：分配切片（返回切片的分配副本）</li></ol><p>让我们看看每种解决方案是如何运作的。</p><ol><li>将切片作为参数传递</li></ol><p>不带 len 的话 main 函数不知道写入的字节数，因此最终的日志会打印整个缓冲区，而不单单是实际的消息部分。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn zigBits(slice: []u8) usize &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    std.mem.copy(u8, slice, &amp;message);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    </span><br><span class="line">    var message: [9]u8 = undefined;</span><br><span class="line"></span><br><span class="line">    const len = zigBits(&amp;message);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message[0..len]&#125;);</span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>正如所看到的，将函数的返回值更新为 void，并使其接受一个切片参数。使用 std.mem.cpy 方法在函数中更新切片，而不是使用 return。</p><blockquote><p> 这种方法类似于在 Rust 中将可变引用（&amp;mut）传递给函数</p></blockquote><p>运行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">plaintextCopy codedebug: zigbits</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits�,�$</span><br></pre></td></tr></table></figure><ol start="2"><li>使用全局数组</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">fn zigBits() []u8 &#123;</span><br><span class="line">    </span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">    return &amp;message;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    </span><br><span class="line">    const msg = zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;msg&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们将数组字面量声明为全局变量，其将会被延迟解析，并且可以被内部作用域访问。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits</span><br></pre></td></tr></table></figure><ol start="3"><li>分配切片</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits() ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line"></span><br><span class="line">    const message = try zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们在这一行上对切片进行了堆上的复制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br></pre></td></tr></table></figure><p>这使得切片可以在函数外部使用，因为现在它在堆上分配了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">debug: zigbits</span><br><span class="line">debug: zigbits</span><br></pre></td></tr></table></figure><p>更常见和符合惯例的处理此类情况的方式是将 std.mem.Allocator 传递给分配内存的函数。这样一来，我们可以允许调用者决定使用哪种分配器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits(allocator: std.mem.Allocator) ![]u8 &#123;</span><br><span class="line">   </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // https://ziglang.org/documentation/master/#Choosing-an-Allocator</span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line">    const message = try zigBits(allocator);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>让我们改进我们的程序，以返回一个具有指定长度的切片，而非栈上分配的数组。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn zigBits(len: usize) ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    // 切片是指针和长度。数组和切片的区别在于，数组的长度是类型的一部分，并且在编译时已知，而切片的长度在运行时已知。</span><br><span class="line">    try std.testing.expect(@TypeOf(message[0..len]) == []u8);</span><br><span class="line"></span><br><span class="line">    // 我们使用 `len` 参数来使用运行时已知的值进行切片。</span><br><span class="line">    // 如果 `len` 声明为 `comptime len`，那么此值将为 &#x27;*[N]u8&#x27;。</span><br><span class="line">    return message[0..len];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    </span><br><span class="line">    const message = try zigBits(7);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这与第一个示例中的悬挂指针问题相同。在 Zig 中，&amp;message 和 message[0..] 将返回相同的切片。</p><p>我们可以使用 <a href="https://godbolt.org/z/cPWjajYxb">https://godbolt.org/z/cPWjajYxb</a> 检查此行为，该行为显示了在使用 ![]u8 时，“zigbits” 放置在堆栈中的 40 个字节，而在使用 []u8 时，“zigbits” 放置在堆栈中的 16 个字节。</p><h2 id="0x2-使用defer防止内存泄漏"><a href="#0x2-使用defer防止内存泄漏" class="headerlink" title="0x2: 使用defer防止内存泄漏"></a>0x2: 使用defer防止内存泄漏</h2><p>在第一章节中，我们讨论了从函数返回切片以及 Zig 如何管理内存这个话题。我们了解到 Zig 数组是在堆栈上分配的，当它们被访问时，如果指向它们的内存不再可用，它们可能会被破坏。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits() ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line">    </span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try std.heap.page_allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    const message = try zigBits();</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，你可以看到我们使用了一个硬编码的 std.heap.page_allocator 来复制内存。然而，在 Zig 的分配器使用约定中，我们一般会声明一个分配器，然后将分配器作为参数传递给函数，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn zigBits(allocator: std.mem.Allocator) ![]u8 &#123;</span><br><span class="line">    </span><br><span class="line">    var message = [_]u8&#123; &#x27;z&#x27;, &#x27;i&#x27;, &#x27;g&#x27;, &#x27;b&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;s&#x27; &#125;;</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line"></span><br><span class="line">    var message_copy = try allocator.dupe(u8, &amp;message);</span><br><span class="line">    return message_copy;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line">    const message = try zigBits(allocator);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;message&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，调用者可以根据定义为通用 std.mem.Allocator 的类型来决定分配器的类型。</p><h3 id="处理文件路径"><a href="#处理文件路径" class="headerlink" title="处理文件路径"></a>处理文件路径</h3><p>下述为我们的示例程序，它的目标是连接文件系统路径并打印结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">/// 连接给定的路径，并返回 &#x27;/home&#x27; 目录下的路径。</span><br><span class="line">fn concatPath(allocator: std.mem.Allocator, p1: []const u8, p2: []const u8) ![]const u8 &#123;</span><br><span class="line">    // 定义路径分隔符（对于 POSIX 系统来说是 &#x27;/&#x27;）。</span><br><span class="line">    const separator = std.fs.path.sep_str;</span><br><span class="line"></span><br><span class="line">    // 连接路径并返回结果。</span><br><span class="line">    const path = try std.fs.path.join(allocator, &amp;.&#123; separator, &quot;home&quot;, p1, p2 &#125;);</span><br><span class="line">    return path;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line"></span><br><span class="line">    const path = try concatPath(allocator, &quot;zig&quot;, &quot;bits&quot;);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;path&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你可能已经猜到，在这个程序中我们会围绕 concatPath 函数进行说明。它接受一个通用的分配器类型（allocator），并连接给定的 p1 和 p2 路径字符串。在最终路径前面添加了 “&#x2F;home”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line"></span><br><span class="line">debug: /home/zig/bits</span><br></pre></td></tr></table></figure><p>你可以看到一切都正常，我们成功地打印出了最终路径，但你有没有发现内存泄漏？</p><h3 id="分析说明-1"><a href="#分析说明-1" class="headerlink" title="分析说明"></a>分析说明</h3><p>首先，什么是内存泄漏，为什么会发生呢？</p><p>在计算机编程中，内存泄漏就像那些永不满足的黑洞一样——程序不断地消耗着越来越多的内存，没有任何限制，程序也不会释放那些它们不再需要的内存。</p><p>在我们的示例中，我们正在使用分配器将我们的字符串连接起来。但问题在于我们从未释放通过分配器分配的内存。所以我们的程序不断地分配内存。</p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>幸运的是，Zig有方法简化我们的工作。与Golang等现代高级语言一样，我们可以使用 defer 语句来确保在函数返回时释放内存。什么是 defer 语句呢？它是一种执行在函数返回之前的语句，类似hook，让我们来看看如何使用它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.10.1</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn concatPath(allocator: std.mem.Allocator, p1: []const u8, p2: []const u8) ![]const u8 &#123;</span><br><span class="line">    const separator = std.fs.path.sep_str;</span><br><span class="line"></span><br><span class="line">    defer allocator.free(path);</span><br><span class="line"></span><br><span class="line">    const path = try std.fs.path.join(allocator, &amp;.&#123; separator, &quot;home&quot;, p1, p2 &#125;);</span><br><span class="line">    return path;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    </span><br><span class="line">    const allocator = std.heap.page_allocator;</span><br><span class="line"></span><br><span class="line">    const path = try concatPath(allocator, &quot;zig&quot;, &quot;bits&quot;);</span><br><span class="line"></span><br><span class="line">    std.log.debug(&quot;&#123;s&#125;&quot;, .&#123;path&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过上述示例，我们看看 defer 语句的具体用法。在我们的函数中，我们在调用 std.fs.path.join 之前添加了 defer 语句。这意味着当函数返回之前，我们将调用 allocator.free(path)，从而释放我们分配的内存。这样，就不会再有内存泄漏了！</p><h2 id="0x3-从0到1构建HTTP服务"><a href="#0x3-从0到1构建HTTP服务" class="headerlink" title="0x3: 从0到1构建HTTP服务"></a>0x3: 从0到1构建HTTP服务</h2><p>现在让我们尝试使用 Zig &gt;&#x3D;0.11 的 std.http 模块，并从头开始创建一个 HTTP 服务器&#x2F;客户端（以及一些基准测试）。</p><p>在学习一门新的编程语言时，一般都是从编写 HTTP 服务器&#x2F;客户端开始。 Zig 标准库将从 0.11.0 （现在已经到0.13版本了）开始具有一些令人兴奋的 HTTP 功能。作为第一步，让我们尝试使用新功能进行一些实验，并尝试设计出一些功能性的东西。此外，我们如果对性能&#x2F;速度方面的事情感兴趣，可以创建一些基准测试，以查看与其他编程语言（如 Rust）的比较，该章节包含以下内容</p><ul><li>查看 std.http</li><li>构建一个 HTTP 客户端</li><li>构建一个 HTTP 服务器</li><li>基准测试</li></ul><ol><li>std.http简介</li></ol><p>Zig 最近几个月终于实现了内置的 HTTP 服务器&#x2F;客户端支持，下述为具体说明：</p><ul><li><a href="https://zig.news/nameless/coming-soon-to-a-zig-near-you-http-client-5b81">Coming Soon to a Zig Near You: HTTP Client</a></li><li><a href="https://github.com/ziglang/zig/issues/910">标准库中的 HTTP 服务器（跟踪问题）</a></li><li><a href="https://news.ycombinator.com/item?id=35991684">Zig 现在在 std 中内置了 HTTP 服务器和客户端</a></li></ul><p>总结一下这个新添加的 std.http 模块的功能：</p><ul><li>仅在 Zig 0.11.0-dev.2675-0eebc2588 及以上版本中可用。</li><li>支持 HTTP&#x2F;1.1（请参阅 A barely HTTP&#x2F;2 server in Zig ）</li><li>支持诸如连接池、压缩内容、代理和 TLS 等功能。</li></ul><p>而我认为最令人兴奋的部分是：</p><blockquote><p>使用最新（0.11.0）的 Zig，无需找到额外的库，我们就可以通过Get获取网站的内容或发送 POST 请求，不费吹灰之力…</p></blockquote><p>std.http 具有以下 API：</p><ul><li>Client：HTTP 客户端实现。</li><li>Server：HTTP 服务器实现。</li><li>Connection：连接类型（keep_alive、close）</li><li>ContentEncoding：压缩选项（compress、deflate、gzip 和 zstd）</li><li>Field：名称和值的通用类型</li><li>Headers：HTTP 标头</li><li>Method：HTTP 方法，如 GET 和 POST</li><li>Status：HTTP 状态码（not_found &#x3D; 404、teapot &#x3D; 418 等）</li><li>TransferEncoding：用于传输正文的编码形式（chunked）</li><li>Version：当前为 HTTP&#x2F;1.0 和 HTTP&#x2F;1.1</li></ul><p>正如您已经注意到的那样，这是一个非常简单直接的 HTTP 实现，与当前稳定版本的 Zig（0.10.1）相比，std.http 并不那么简陋，而是具有一些很酷的添加功能，比如内容&#x2F;传输编码和标头，进步很大！</p><ol start="2"><li>构建一个 HTTP 客户端</li></ol><p>在开始之前，我们需要选择一个分配器来使用。这是 Zig 的一个有趣之处，另外，强烈建议你阅读关于分配器的资料，这样你可以精确地知晓如何进行内存分配。</p><p>简单起见，我们使用 <code>std.heap.GeneralPurposeAllocator</code>，这是一个安全的分配器，可以防止双重释放（double-free）、使用已释放的内存（use-after-free），并且可以检测内存泄漏等情况。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">const allocator = gpa.allocator();</span><br></pre></td></tr></table></figure><p>接下来，我们创建 <code>Client</code> 对象：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">const http = std.http;</span><br><span class="line"></span><br><span class="line">var client = http.Client&#123; .allocator = allocator &#125;;</span><br><span class="line">defer client.deinit();</span><br></pre></td></tr></table></figure><p>如你所见，创建 <code>Client</code> 唯一需要的参数是分配器。跟Rust、Golang一样，构造时还可以在选择性地提供证书包、连接池和代理。</p><p>需要注意的是 <code>defer</code> 关键字。它指示在客户端超出作用域时将调用 <code>deinit</code> 方法，这意味着与客户端关联的所有资源将被释放。</p><p>要发送请求，我们需要以下几样东西：</p><ul><li>一个 <code>std.http.Method</code></li><li>一个 <code>std.Uri</code>，由一个 URL 解析而来。</li><li>一个 <code>std.http.Headers</code>，用于保存发送到服务器的请求头。只有当我们向其中追加内容时，它才会分配内存。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const uri = std.Uri.parse(&quot;https://whatthecommit.com&quot;) catch unreachable;</span><br><span class="line"></span><br><span class="line">var headers = std.http.Headers&#123; .allocator = allocator &#125;;</span><br><span class="line">defer headers.deinit();</span><br><span class="line"></span><br><span class="line">try headers.append(&quot;accept&quot;, &quot;*/*&quot;);</span><br></pre></td></tr></table></figure><p>然后们准备创建一个 <code>Request</code> 对象：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.GET, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br></pre></td></tr></table></figure><p>要真正发送请求，我们还需要使用 <code>start</code> 函数，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try request.start();</span><br></pre></td></tr></table></figure><p>这种方式在很有用。例如，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.POST, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br><span class="line"></span><br><span class="line">request.transfer_encoding = .chunked;</span><br><span class="line"></span><br><span class="line">try request.start();</span><br></pre></td></tr></table></figure><p>在发送请求后，我们需要等待服务器发送响应：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">try request.wait();</span><br></pre></td></tr></table></figure><p>这个 <code>wait</code> 调用将为我们处理以下任务：</p><ul><li>重定向</li><li>读取&#x2F;存储请求头</li><li>设置解压</li></ul><p>最后，要读取来自服务器的响应，我们可以使用 <code>Request</code> 的 <code>reader()</code> 方法获取一个 <code>std.io.Reader</code>。剩下的就是在 Zig 中从流中读取数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const body = request.reader().readAllAlloc(allocator, 8192) catch unreachable;</span><br><span class="line">defer allocator.free(body);</span><br></pre></td></tr></table></figure><p>以下是将所有步骤结合在一起并打印出响应的代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本：0.11.0</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">   </span><br><span class="line">    var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">    defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">    const allocator = gpa.allocator();</span><br><span class="line"></span><br><span class="line">    var client = http.Client&#123; .allocator = allocator &#125;;</span><br><span class="line">    defer client.deinit();</span><br><span class="line"></span><br><span class="line">    const uri = std.Uri.parse(&quot;https://whatthecommit.com/index.txt&quot;) catch unreachable;</span><br><span class="line"></span><br><span class="line">    var headers = std.http.Headers&#123; .allocator = allocator &#125;;</span><br><span class="line">    defer headers.deinit();</span><br><span class="line"></span><br><span class="line">    try headers.append(&quot;accept&quot;, &quot;*/*&quot;);</span><br><span class="line"></span><br><span class="line">    var request = try client.request(.GET, uri, headers, .&#123;&#125;);</span><br><span class="line">    defer request.deinit();</span><br><span class="line"></span><br><span class="line">    try request.start();</span><br><span class="line">    try request.wait();</span><br><span class="line"></span><br><span class="line">    const body = request.reader().readAllAlloc(allocator, 8192) catch unreachable;</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    std.log.info(&quot;&#123;s&#125;&quot;, .&#123;body&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终运行结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line">error: TlsInitializationFailed</span><br></pre></td></tr></table></figure><p>结果运行出错，新语言就是这么难查问题，我们在Zig官方issue中发现了一个问题：</p><p><a href="https://github.com/ziglang/zig/issues/14172">https://github.com/ziglang/zig/issues/14172</a></p><p>当前标准库中的 TLS 实现仅支持 TLS 1.3，不幸的是，我们测试的网站 whatthecommit.com 使用的是 TLS 1.2。</p><p>我们可以通过以下命令来验证是否支持 TLS 1.3：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -v --tlsv1.3 https://whatthecommit.com/index.txt</span><br></pre></td></tr></table></figure><p>结果显示网站不支持 TLS 1.3。</p><p>那么我们换个目标网址，更新程序中的 URL 并再次运行它：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">&quot;s|https://.*\&quot;|https://godsays.xyz\&quot;|g&quot;</span> src/main.zig</span><br><span class="line"></span><br><span class="line">$ zig build run</span><br></pre></td></tr></table></figure><p>结果成功了！那 POST 请求呢？我们可以这样写入数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var request = try client.request(.POST, uri, headers, .&#123;&#125;);</span><br><span class="line">defer request.deinit();</span><br><span class="line">request.transfer_encoding = .chunked;</span><br><span class="line"></span><br><span class="line">try request.start();</span><br><span class="line"></span><br><span class="line">try request.writer().writeAll(&quot;Zig Bits!\n&quot;);</span><br><span class="line">try request.finish();</span><br><span class="line"></span><br><span class="line">try request.wait();</span><br></pre></td></tr></table></figure><ol start="3"><li>构建一个 HTTP 服务器</li></ol><p>现在简单的部分已经完成了，让我们进入正题：如何实现一个HTTP服务器。</p><p>在下面的实现中，我们从标准库中的测试中获得了一些灵感，并提出了一个简化版本，去掉了多线程和断言等。</p><p>为了保持简单，直接添加一个名为 <code>/get</code> 的路径就足够了。</p><p>所以我们想要实现的目标很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl 127.0.0.1:8000/get</span><br><span class="line"></span><br><span class="line">Zig Bits!</span><br></pre></td></tr></table></figure><blockquote><p>你是否知道Python只需要一行就能创建 HTTP 服务器吗？ <code>python -m http.server 8000</code>。</p></blockquote><p>让我们快速定义一下服务器配置的常量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line">const log = std.log.scoped(.server);</span><br><span class="line"></span><br><span class="line">const server_addr = &quot;127.0.0.1&quot;;</span><br><span class="line">const server_port = 8000;</span><br></pre></td></tr></table></figure><p>这里需要注意的是，我们为 <code>log</code> 模块指定了一个作用域，这样日志消息会显示为 <code>info(server)</code>、<code>debug(server)</code> 等等。</p><p>接下来，我们需要选择一个分配器（allocator），再次使用 <code>std.heap.GeneralPurposeAllocator</code>，以保持与 HTTP 客户端示例的一致性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">const allocator = gpa.allocator();</span><br></pre></td></tr></table></figure><p>现在构建 <code>Server</code> 对象并指示在其作用域结束时释放资源。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var server = http.Server.init(allocator, .&#123; .reuse_address = true &#125;);</span><br><span class="line">defer server.deinit();</span><br></pre></td></tr></table></figure><p>将服务器绑定到我们之前定义的地址上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.info(&quot;Server is running at &#123;s&#125;:&#123;d&#125;&quot;, .&#123; server_addr, server_port &#125;);</span><br><span class="line"></span><br><span class="line">const address = std.net.Address.parseIp(server_addr, server_port) catch unreachable;</span><br><span class="line">try server.listen(address);</span><br></pre></td></tr></table></figure><p>和每个 HTTP 服务器的实现一样，我们需要一个机制来阻塞当前线程，等待下一个请求并处理它。为此，在 Zig 中我们可以这样做：</p><ul><li><code>Server</code> 的 <code>accept</code> 方法返回一个 <code>Response</code>。</li><li><code>Response</code> 包含有用的信息，如请求方法和头信息。</li><li><code>Response</code> 还提供了用于处理请求的辅助方法。例如，<code>wait</code> 方法可以用来等待客户端完成请求头的发送。</li></ul><p>当我们把这些拼凑在一起时，服务器运行函数看起来像这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// 运行服务器并处理传入的请求</span><br><span class="line">fn runServer(server: *http.Server, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    outer: while (true) &#123;</span><br><span class="line">        // 接受传入的连接</span><br><span class="line">        var response = try server.accept(.&#123;</span><br><span class="line">            .allocator = allocator,</span><br><span class="line">        &#125;);</span><br><span class="line">        defer response.deinit();</span><br><span class="line"></span><br><span class="line">        while (response.reset() != .closing) &#123;</span><br><span class="line">            // 处理请求处理期间的错误</span><br><span class="line">            response.wait() catch |err| switch (err) &#123;</span><br><span class="line">                error.HttpHeadersInvalid =&gt; continue :outer,</span><br><span class="line">                error.EndOfStream =&gt; continue,</span><br><span class="line">                else =&gt; return err,</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">            // 处理请求</span><br><span class="line">            try handleRequest(&amp;response, allocator);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们首先创建一个无限循环来接受传入的连接。然后还有另一个循环用于通过 <code>wait</code> 方法从流中读取响应。同时，在Header无效时跳过读取循环中的响应。</p><p>正如你所见，处理请求的步骤是在服务器循环的最后通过 <code>handleRequest</code> 函数完成的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// 处理单个请求</span><br><span class="line">fn handleRequest(response: *http.Server.Response, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    log.info(&quot;&#123;s&#125; &#123;s&#125; &#123;s&#125;&quot;, .&#123; @tagName(response.request.method), @tagName(response.request.version), response.request.target &#125;);</span><br><span class="line"></span><br><span class="line">    // 读取请求体</span><br><span class="line">    const body = try response.reader().readAllAlloc(allocator, 8192);</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    // 如果请求头中存在 &quot;connection&quot;，将其设置为 &quot;keep-alive&quot;</span><br><span class="line">    if (response.request.headers.contains(&quot;connection&quot;)) &#123;</span><br><span class="line">        try response.headers.append(&quot;connection&quot;, &quot;keep-alive&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 检查请求目标是否以 &quot;/get&quot; 开头</span><br><span class="line">    if (std.mem.startsWith(u8, response.request.target, &quot;/get&quot;)) &#123;</span><br><span class="line">        // 检查请求目标是否包含 &quot;?chunked&quot;</span><br><span class="line">        if (std.mem.indexOf(u8, response.request.target, &quot;?chunked&quot;) != null) &#123;</span><br><span class="line">            response.transfer_encoding = .chunked;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            response.transfer_encoding = .&#123; .content_length = 10 &#125;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 设置 &quot;content-type&quot; 头为 &quot;text/plain&quot;</span><br><span class="line">        try response.headers.append(&quot;content-type&quot;, &quot;text/plain&quot;);</span><br><span class="line"></span><br><span class="line">        // 写入响应体</span><br><span class="line">        try response.do();</span><br><span class="line">        if (response.request.method != .HEAD) &#123;</span><br><span class="line">            try response.writeAll(&quot;Zig &quot;);</span><br><span class="line">            try response.writeAll(&quot;Bits!\n&quot;);</span><br><span class="line">            try response.finish();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // 设置响应状态为 404（未找到）</span><br><span class="line">        response.status = .not_found;</span><br><span class="line">        try response.do();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们再来逐步分析一遍：</p><ol><li>首先，记录传入请求的详细信息。</li><li>读取请求体并合理分配内存（最大8KB）。</li><li>如有必要，设置 <code>Connection</code> 头。</li><li>如果请求目标以 <code>/get</code> 开头，则将 <code>&quot;Zig Bits!&quot;</code> 写入响应体。</li><li>如果目标包含 <code>?chunked</code>，则启用分块编码，否则使用固定内容长度。</li><li>如果请求未匹配到配置的路由，则返回404。</li></ol><p>需要注意的一点是 <code>finish()</code> 和 <code>do()</code> 方法的区别。<code>do()</code> 方法只是发送响应头，而 <code>finish()</code> 会发送分块消息的最后一个块，或者验证我们发送了约定数量的字节。因此，如果我们在发送数据时，总是应该调用 <code>finish()</code> 来完成响应。</p><p>现在请求已被处理，我们也有了服务器函数，下面看看如何运行服务器并记录错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 运行服务器</span><br><span class="line">runServer(&amp;server, allocator) catch |err| &#123;</span><br><span class="line">    // 处理服务器错误</span><br><span class="line">    log.err(&quot;server error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    if (@errorReturnTrace()) |trace| &#123;</span><br><span class="line">        std.debug.dumpStackTrace(trace.*);</span><br><span class="line">    &#125;</span><br><span class="line">    std.os.exit(1);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>最终的 HTTP 服务器代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">// Zig 版本: 0.11.0</span><br><span class="line"></span><br><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const http = std.http;</span><br><span class="line">const log = std.log.scoped(.server);</span><br><span class="line"></span><br><span class="line">const server_addr = &quot;127.0.0.1&quot;;</span><br><span class="line">const server_port = 8000;</span><br><span class="line"></span><br><span class="line">fn runServer(server: *http.Server, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    outer: while (true) &#123;</span><br><span class="line">        var response = try server.accept(.&#123;</span><br><span class="line">            .allocator = allocator,</span><br><span class="line">        &#125;);</span><br><span class="line">        defer response.deinit();</span><br><span class="line"></span><br><span class="line">        while (response.reset() != .closing) &#123;</span><br><span class="line">           </span><br><span class="line">            response.wait() catch |err| switch (err) &#123;</span><br><span class="line">                error.HttpHeadersInvalid =&gt; continue :outer,</span><br><span class="line">                error.EndOfStream =&gt; continue,</span><br><span class="line">                else =&gt; return err,</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">            try handleRequest(&amp;response, allocator);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fn handleRequest(response: *http.Server.Response, allocator: std.mem.Allocator) !void &#123;</span><br><span class="line">    log.info(&quot;&#123;s&#125; &#123;s&#125; &#123;s&#125;&quot;, .&#123; @tagName(response.request.method), @tagName(response.request.version), response.request.target &#125;);</span><br><span class="line">    const body = try response.reader().readAllAlloc(allocator, 8192);</span><br><span class="line">    defer allocator.free(body);</span><br><span class="line"></span><br><span class="line">    if (response.request.headers.contains(&quot;connection&quot;)) &#123;</span><br><span class="line">        try response.headers.append(&quot;connection&quot;, &quot;keep-alive&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (std.mem.startsWith(u8, response.request.target, &quot;/get&quot;)) &#123;</span><br><span class="line">        if (std.mem.indexOf(u8, response.request.target, &quot;?chunked&quot;) != null) &#123;</span><br><span class="line">            response.transfer_encoding = .chunked;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            response.transfer_encoding = .&#123; .content_length = 10 &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        try response.headers.append(&quot;content-type&quot;, &quot;text/plain&quot;);</span><br><span class="line"></span><br><span class="line">        try response.do();</span><br><span class="line">        if (response.request.method != .HEAD) &#123;</span><br><span class="line">            try response.writeAll(&quot;Zig &quot;);</span><br><span class="line">            try response.writeAll(&quot;Bits!\n&quot;);</span><br><span class="line">            try response.finish();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        response.status = .not_found;</span><br><span class="line">        try response.do();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    var gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;)&#123;&#125;;</span><br><span class="line">    defer std.debug.assert(gpa.deinit() == .ok);</span><br><span class="line">    const allocator = gpa.allocator();</span><br><span class="line"></span><br><span class="line">    var server = http.Server.init(allocator, .&#123; .reuse_address = true &#125;);</span><br><span class="line">    defer server.deinit();</span><br><span class="line"></span><br><span class="line">    log.info(&quot;Server is running at &#123;s&#125;:&#123;d&#125;&quot;, .&#123; server_addr, server_port &#125;);</span><br><span class="line"></span><br><span class="line">    const address = std.net.Address.parseIp(server_addr, server_port) catch unreachable;</span><br><span class="line">    try server.listen(address);</span><br><span class="line"></span><br><span class="line">    runServer(&amp;server, allocator) catch |err| &#123;</span><br><span class="line">        log.err(&quot;server error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">        if (@errorReturnTrace()) |trace| &#123;</span><br><span class="line">            std.debug.dumpStackTrace(trace.*);</span><br><span class="line">        &#125;</span><br><span class="line">        std.os.exit(1);</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>让我们看看运行情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ zig build run</span><br><span class="line"></span><br><span class="line">info(server): Server is running at 127.0.0.1:8000</span><br><span class="line">info(server): GET HTTP/1.1 /get</span><br><span class="line">info(server): GET HTTP/1.1 /get?chunked</span><br></pre></td></tr></table></figure><p>在另一个终端中执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl 127.0.0.1:8000/get</span><br><span class="line">Zig Bits!</span><br><span class="line"></span><br><span class="line">$ curl 127.0.0.1:8000/get?chunked</span><br><span class="line">Zig Bits!</span><br></pre></td></tr></table></figure><p>现在我们有了一个纯 Zig 编写的功能性 HTTP 服务器了。</p><ol start="4"><li>基准测试</li></ol><p>为了查看 Zig HTTP 客户端相对于其他编程语言实现的速度，这里使用 hyperfine 创建了一些基准测试。</p><p>这些基准测试可以在这个仓库中找到：<a href="https://github.com/orhun/zig-http-benchmarks">https://github.com/orhun/zig-http-benchmarks</a></p><p>将 HTTP 客户端与以下几种客户端进行比较：</p><ul><li>Rust HTTP 客户端（hyper、reqwest、ureq、attohttpc）</li><li>Go HTTP 客户端（net&#x2F;http）</li><li>Python HTTP 客户端（requests）</li><li>curl</li></ul><p>工作的方式是，运行 Zig HTTP 服务器并接受来自不同客户端的 N 个请求，然后让 hyperfine 完成它。</p><p>要运行基准测试，只需运行 <code>./bench.sh</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rust-ureq 运行结果:</span><br><span class="line">    比 rust-hyper 快 1.18 ± 0.22 倍</span><br><span class="line">    比 rust-reqwest 快 1.30 ± 0.27 倍</span><br><span class="line">    比 go-http-client 快 1.74 ± 0.38 倍</span><br><span class="line">    比 rust-attohttpc 快 1.92 ± 0.40 倍</span><br><span class="line">    比 zig-http-client 快 2.17 ± 0.63 倍</span><br><span class="line">    比 curl 快 4.25 ± 0.73 倍</span><br><span class="line">    比 python-http-client 快 10.31 ± 1.47 倍</span><br></pre></td></tr></table></figure><p>以下是具体的测试数据：</p><table><thead><tr><th>命令</th><th>平均时间 [ms]</th><th>最小时间 [ms]</th><th>最大时间 [ms]</th><th>相对速度</th></tr></thead><tbody><tr><td>curl</td><td>295.2 ± 29.3</td><td>248.6</td><td>367.9</td><td>4.25 ± 0.73</td></tr><tr><td>zig-http-client</td><td>150.9 ± 38.1</td><td>98.5</td><td>250.2</td><td>2.17 ± 0.63</td></tr><tr><td>rust-attohttpc</td><td>133.4 ± 20.6</td><td>101.1</td><td>174.7</td><td>1.92 ± 0.40</td></tr><tr><td>rust-hyper</td><td>82.1 ± 10.1</td><td>65.7</td><td>106.0</td><td>1.18 ± 0.22</td></tr><tr><td>rust-reqwest</td><td>90.0 ± 14.0</td><td>67.8</td><td>126.0</td><td>1.30 ± 0.27</td></tr><tr><td>rust-ureq</td><td>69.5 ± 9.6</td><td>55.3</td><td>92.9</td><td>1.00</td></tr><tr><td>go-http-client</td><td>120.8 ± 20.0</td><td>84.6</td><td>171.6</td><td>1.74 ± 0.38</td></tr><tr><td>python-http-client</td><td>716.5 ± 22.0</td><td>665.9</td><td>765.7</td><td>10.31 ± 1.47</td></tr></tbody></table><p><img src="https://raw.githubusercontent.com/orhun/zig-http-benchmarks/output/benchmarks-bw.png"></p><p>通过比较这两个服务器的性能指标，我们可以得出它们在处理相同请求的情况下的性能差异。通常来说，性能测试的结果可能会受到多种因素的影响，包括硬件配置、操作系统、编译优化等，因此需要在真实环境中进行测试以获得更准确的结果。</p><h2 id="0x4-项目实战"><a href="#0x4-项目实战" class="headerlink" title="0x4: 项目实战"></a>0x4: 项目实战</h2><p>在核心功能实现并且 Zig 代码经过了彻底的测试之后，我们可以开始做与项目管理相关的琐事，我们将分享在 Zig 项目管理方面的经验，以便能够正常的使用github等工具维护你的开源代码。</p><p>该章节将涵盖以下主题：</p><ul><li>添加库</li><li>运行测试</li><li>代码覆盖率</li><li>文档生成</li><li>CI&#x2F;CD</li></ul><ol><li>添加库</li></ol><p>对于新手来说，这是一个棘手的问题之一。目前，似乎还没有一个像 cargo add 这样简单和标准的方法来为你的 Zig 项目添加库。但是，有一些用于此目的的包管理器可供选择：</p><ul><li>gyro：一个带有索引、构建运行器和构建依赖项的 Zig 包管理器。</li><li>zigmod：Zig 编程语言的包管理器。</li><li>aquila：一个用于 Zig 项目的包索引。</li></ul><p>对于我们接下来的项目，选择采取了一个更简单直接的方法：Git 子模块</p><p>a. 在项目的根目录下创建 libs 目录。</p><p>b. 将库作为 Git 子模块添加：</p><p>要么运行 git submodule add <remote_url> libs&#x2F;<lib>，要么添加 .gitmodules 文件。例如，对于 zig-clap：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[submodule &quot;libs/zig-clap&quot;]</span><br><span class="line">  path = libs/zig-clap</span><br><span class="line">  url = https://github.com/Hejsil/zig-clap</span><br></pre></td></tr></table></figure><p>c. 然后你需要在 build.zig 中将该包添加到你的项目中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">const exe = b.addExecutable(&quot;exe_name&quot;, &quot;src/main.zig&quot;);</span><br><span class="line">// ...</span><br><span class="line">// ...</span><br><span class="line">exe.addPackagePath(&quot;clap&quot;, &quot;libs/zig-clap/clap.zig&quot;)</span><br></pre></td></tr></table></figure><p>d. 现在可以在我们的源文件中导入库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const clap = @import(&quot;clap&quot;);</span><br></pre></td></tr></table></figure><p>当然，现在还没有标准的方法来安装 Zig 库。人们常用的一些常见方法有：</p><ul><li>使用 git 子模块或将库复制粘贴到自己的项目中。</li><li>在 build.zig 文件中添加 exe.addPackagePath(“clap”, “path&#x2F;to&#x2F;clap.zig”); 以使用该库。</li><li>使用非官方的包管理器，如 zigmod 或 gyro。</li></ul><p>关于如何使用这些包管理器安装包，请查看这些包管理器的文档。</p><ol start="2"><li>运行测试</li></ol><p>在编写项目的测试时，我们需要为每个文件添加测试，并在 build.zig 中指定要测试的文件。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const exe_tests = b.addTest(&quot;src/main.zig&quot;);</span><br><span class="line">exe_tests.setTarget(target);</span><br><span class="line">exe_tests.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">const test_step = b.step(&quot;test&quot;, &quot;Run unit tests&quot;);</span><br><span class="line">test_step.dependOn(&amp;exe_tests.step);</span><br></pre></td></tr></table></figure><p>当运行 zig build test 时，它只会运行 main.zig 中的测试。我们希望运行项目中每个文件中的测试，所以我们做了下面的操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const test_step = b.step(&quot;test&quot;, &quot;Run tests&quot;);</span><br><span class="line"></span><br><span class="line">// loop through the modules and add them for testing</span><br><span class="line">for ([_][]const u8&#123; &quot;main&quot;, &quot;wav&quot;, &quot;file&quot;, &quot;gen&quot; &#125;) |module| &#123;</span><br><span class="line">    const test_module = b.fmt(&quot;src/&#123;s&#125;.zig&quot;, .&#123;module&#125;);</span><br><span class="line">    var exe_tests = b.addTest(test_module);</span><br><span class="line">    test_step.dependOn(&amp;exe_tests.step);</span><br><span class="line">&#125;</span><br><span class="line">sqlCopy code$ zig build test</span><br><span class="line"></span><br><span class="line">1/1 test.run... OK</span><br><span class="line">All 1 tests passed.</span><br><span class="line">1/2 test.encode WAV... OK</span><br><span class="line">2/2 test.stream out WAV... OK</span><br><span class="line">All 2 tests passed.</span><br><span class="line">1/1 test.read bytes from the file... OK</span><br><span class="line">All 1 tests passed.</span><br><span class="line">1/1 test.generate music... OK</span><br><span class="line">All 1 tests passed.</span><br></pre></td></tr></table></figure><p>这个方法运行结果符合预期，但还有另一种更好的方法，在是在 Reddit 帖子中找到的方法（其实我们看ziglang公共库也可以发现）。</p><p>我们需要有一个通用的文件来“引用”测试代码，以便通过一个单独的 “addTest” 语句在你的构建文件中运行它们。例如，在 src&#x2F;myLibrary.zig 中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pub const A = @import(&quot;A.zig&quot;);</span><br><span class="line">pub const B = @import(&quot;B.zig&quot;);</span><br><span class="line">pub const SomeDataType = @import(&quot;C.zig&quot;).SomeDataType;</span><br><span class="line"></span><br><span class="line">test &#123;</span><br><span class="line">  @import(&quot;std&quot;).testing.refAllDecls(@This());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在 build.zig 中，只需要简单地添加它作为 b.addTest(“src&#x2F;myLibrary.zig”)。</p><ol start="3"><li>代码覆盖率</li></ol><p>追踪项目的测试覆盖率。有助于更好地测试功能并消除潜在地错误。有时，甚至需要重构你的代码来为某个函数&#x2F;模块编写测试，这样的代码将会更好。</p><p>对于 Rust 项目，通常会按照以下步骤进行测试&#x2F;覆盖率检查：</p><ul><li>编写测试</li><li>使用 cargo-nextest 运行测试</li><li>使用工具生成代码覆盖率报告<ul><li>cargo-tarpaulin</li><li>cargo-llvm-cov</li></ul></li><li>将其上传到 Codecov.io</li></ul><p>对于 Zig，我们将采取以下步骤：</p><ul><li>编写测试</li><li>使用 zig build test 运行测试</li><li>使用 kcov 生成代码覆盖率报告</li><li>将其上传到 Codecov.io</li></ul><p>在测试通过后，第一步是生成代码覆盖率报告。我们只需要在 build.zig 中添加一个新的标志来生成覆盖率报告：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const coverage = b.option(bool, &quot;test-coverage&quot;, &quot;Generate test coverage&quot;) orelse false;</span><br><span class="line"></span><br><span class="line">const exe_tests = b.addTest(&quot;src/main.zig&quot;);</span><br><span class="line">exe_tests.setTarget(target);</span><br><span class="line">exe_tests.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">if (coverage) &#123;</span><br><span class="line">    exe_tests.setExecCmd(&amp;[_]?[]const u8&#123;</span><br><span class="line">        &quot;kcov&quot;,</span><br><span class="line">        &quot;kcov-output&quot;,</span><br><span class="line">        null,</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在当你运行 zig build test -Dtest-coverage 时，报告将存放在 kcov-output。</p><p>下一步是将这个报告上传到 Codecov。我们只需要编写了一个简单的 GitHub Actions 工作流程即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># contents of .github/workflows/ci.yml</span><br><span class="line"></span><br><span class="line">name: Continuous Integration</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  push:</span><br><span class="line">    branches:</span><br><span class="line">      - main</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  test:</span><br><span class="line">    name: Test</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          # include libraries</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Install kcov</span><br><span class="line">        run: |</span><br><span class="line">          sudo apt-get update</span><br><span class="line">          sudo apt-get install -y \</span><br><span class="line">            --no-install-recommends \</span><br><span class="line">            --allow-unauthenticated \</span><br><span class="line">            kcov</span><br><span class="line"></span><br><span class="line">      - name: Test</span><br><span class="line">        run: zig build test -Dtest-coverage</span><br><span class="line"></span><br><span class="line">      - name: Upload coverage to Codecov</span><br><span class="line">        uses: codecov/codecov-action@v3</span><br><span class="line">        with:</span><br><span class="line">          name: code-coverage-report</span><br><span class="line">          directory: kcov-output</span><br><span class="line">          fail_ci_if_error: true</span><br><span class="line">          verbose: true</span><br></pre></td></tr></table></figure><p><img src="https://blog.orhun.dev/zig-codecov.png" alt="codecov"></p><ol start="4"><li>文档生成</li></ol><p>在 Ziglearn 的第 3 章中，详细解释了文档生成过程：</p><p>Zig 编译器自带自动文档生成功能。你可以通过在 <code>zig build-&#123;exe, lib, obj&#125;</code> 或 <code>zig run</code> 命令中添加 <code>-femit-docs</code> 参数来调用这个功能。生成的文档会保存在 <code>./docs</code> 文件夹中，通常用于小型的静态网站。</p><p>需要注意的是这个文档生成功能目前还是实验性的，对于复杂的例子经常会失败。标准库的文档就是通过这个方式生成的。</p><p>因此，我们只需要激活 <code>emit_docs</code> 标志，就能自动生成文档。我们推荐在 <code>build.zig</code> 文件中添加一个标志，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const documentation = b.option(bool, &quot;docs&quot;, &quot;Generate documentation&quot;) orelse false;</span><br><span class="line"></span><br><span class="line">const exe = b.addExecutable(exe_name, &quot;src/main.zig&quot;);</span><br><span class="line">exe.setTarget(target);</span><br><span class="line">exe.setBuildMode(mode);</span><br><span class="line"></span><br><span class="line">if (documentation) &#123;</span><br><span class="line">    exe.emit_docs = .emit;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后，你可以通过 <code>zig build -Ddocs=true</code> 来生成文档。生成的静态网站将保存在 <code>docs/</code> 目录中，展示的样子如下：</p><p><img src="https://blog.orhun.dev/zig-docs.png" alt="docs"></p><p>但我们希望更进一步，把这个网站部署到 GitHub Pages 上。我们觉得在仓库中维护 <code>docs/</code> 文件夹不太合适，所以把它添加到了 <code>.gitignore</code> 中。我们想要的是在推送提交到主分支时，能够自动生成文档并部署它。</p><p><img src="https://blog.orhun.dev/github-pages-1.png" alt="GitHub Pages I"></p><p>要做到这一点，首先需要为 GitHub Pages 启用 GitHub Actions 功能：</p><blockquote><p> 仓库 -&gt; 设置 -&gt; 页面 -&gt; 构建和部署 -&gt; 源 -&gt; 选择 GitHub Actions 而不是以前从分支部署选项。</p></blockquote><p>之后，我们只需添加以下工作流程文件，即可将文档部署到 GitHub Pages：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># contents of .github/workflows/pages.yml</span><br><span class="line"></span><br><span class="line">name: GitHub Pages</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  # Runs on pushes targeting the default branch</span><br><span class="line">  push:</span><br><span class="line">    branches: [main]</span><br><span class="line"></span><br><span class="line">  # Allows you to run this workflow manually from the Actions tab</span><br><span class="line">  workflow_dispatch:</span><br><span class="line"></span><br><span class="line"># Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages</span><br><span class="line">permissions:</span><br><span class="line">  contents: read</span><br><span class="line">  pages: write</span><br><span class="line">  id-token: write</span><br><span class="line"></span><br><span class="line"># Allow one concurrent deployment</span><br><span class="line">concurrency:</span><br><span class="line">  group: &quot;pages&quot;</span><br><span class="line">  cancel-in-progress: true</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  deploy:</span><br><span class="line">    name: Deploy website</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    environment:</span><br><span class="line">      name: github-pages</span><br><span class="line">      url: $&#123;&#123; steps.deployment.outputs.page_url &#125;&#125;</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Generate documentation</span><br><span class="line">        run: zig build -Ddocs=true</span><br><span class="line"></span><br><span class="line">      - name: Setup Pages</span><br><span class="line">        uses: actions/configure-pages@v3</span><br><span class="line">      - name: Upload artifact</span><br><span class="line">        uses: actions/upload-pages-artifact@v1</span><br><span class="line">        with:</span><br><span class="line">          # upload documentation</span><br><span class="line">          path: docs</span><br><span class="line"></span><br><span class="line">      - name: Deploy to GitHub Pages</span><br><span class="line">        id: deployment</span><br><span class="line">        uses: actions/deploy-pages@v2</span><br></pre></td></tr></table></figure><p>你可以在设置 &gt; 页面 中查看部署情况：</p><p><img src="https://blog.orhun.dev/github-pages-2.png" alt="GitHub Pages II"></p><ol start="5"><li>CI&#x2F;CD</li></ol><p>最后，为我们的项目设置一个 CI&#x2F;CD 工作流程。包含两个工作流程文件：</p><ul><li>ci.yml：用于确保项目正常构建<ul><li>在提交时触发</li></ul></li><li>cd.yml：用于为不同平台分发预构建的二进制文件。<ul><li>在提交标签时触发</li></ul></li></ul><p><strong>持续集成</strong></p><p>以下是一个 GitHub Actions 工作流程文件，它会在每次推送或拉取请求到主分支时自动化项目的构建、测试和格式检查过程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">name: Continuous Integration</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  pull_request:</span><br><span class="line">  push:</span><br><span class="line">    branches:</span><br><span class="line">      - main</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  build:</span><br><span class="line">    name: &quot;Build with args: &#x27;$&#123;&#123; matrix.OPTIMIZE &#125;&#125;&#x27;&quot;</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    strategy:</span><br><span class="line">      fail-fast: false</span><br><span class="line">      matrix:</span><br><span class="line">        OPTIMIZE: [&quot;&quot;, &quot;-Drelease-safe&quot;, &quot;-Drelease-fast&quot;, &quot;-Drelease-small&quot;]</span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Build</span><br><span class="line">        run: zig build $&#123;&#123; matrix.OPTIMIZE &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Test</span><br><span class="line">        run: zig build test $&#123;&#123; matrix.OPTIMIZE &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Check formatting</span><br><span class="line">        run: zig fmt --check .</span><br></pre></td></tr></table></figure><p>这个二进制文件会针对 3 种不同的优化配置进行测试：</p><ul><li>ReleaseFast<ul><li>较快的运行时性能</li><li>禁用安全检查</li><li>编译速度慢</li><li>二进制文件较大</li></ul></li><li>ReleaseSafe<ul><li>中等的运行时性能</li><li>启用安全检查</li><li>编译速度慢</li><li>二进制文件较大</li></ul></li><li>ReleaseSmall<ul><li>中等的运行时性能</li><li>禁用安全检查</li><li>编译速度慢</li><li>二进制文件较小</li></ul></li></ul><p><strong>持续部署</strong></p><p>以下是一个 GitHub Actions 工作流程文件，它会在每次将新版本标签推送到存储库时自动构建一个特定目标的二进制文件，并将其发布到 GitHub：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">name: Continuous Deployment</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  push:</span><br><span class="line">    tags:</span><br><span class="line">      - &quot;v*.*.*&quot;</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  publish-github:</span><br><span class="line">    name: Publish on GitHub</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line">    strategy:</span><br><span class="line">      fail-fast: false</span><br><span class="line">      matrix:</span><br><span class="line">        TARGET:</span><br><span class="line">          [</span><br><span class="line">            x86_64-linux,</span><br><span class="line">            x86_64-macos,</span><br><span class="line">            x86_64-windows,</span><br><span class="line">            aarch64-linux,</span><br><span class="line">            aarch64-macos,</span><br><span class="line">            aarch64-windows,</span><br><span class="line">            arm-linux,</span><br><span class="line">            riscv64-linux,</span><br><span class="line">            i386-linux,</span><br><span class="line">          ]</span><br><span class="line"></span><br><span class="line">    steps:</span><br><span class="line">      - name: Checkout the repository</span><br><span class="line">        uses: actions/checkout@v3</span><br><span class="line">        with:</span><br><span class="line">          submodules: recursive</span><br><span class="line"></span><br><span class="line">      - name: Set the release version</span><br><span class="line">        run: echo &quot;RELEASE_VERSION=$&#123;GITHUB_REF:11&#125;&quot; &gt;&gt; $GITHUB_ENV</span><br><span class="line"></span><br><span class="line">      - name: Install Zig</span><br><span class="line">        uses: goto-bus-stop/setup-zig@v2</span><br><span class="line">        with:</span><br><span class="line">          version: 0.10.1</span><br><span class="line"></span><br><span class="line">      - name: Build</span><br><span class="line">        run: zig build -Drelease-safe -Dtarget=$&#123;&#123; matrix.TARGET &#125;&#125;</span><br><span class="line"></span><br><span class="line">      - name: Upload the binary</span><br><span class="line">        uses: svenstaro/upload-release-action@v2</span><br><span class="line">        with:</span><br><span class="line">          file: zig-out/bin/binary-$&#123;&#123; env.RELEASE_VERSION &#125;&#125;-$&#123;&#123; matrix.TARGET &#125;&#125;*</span><br><span class="line">          file_glob: true</span><br><span class="line">          overwrite: true</span><br><span class="line">          tag: $&#123;&#123; github.ref &#125;&#125;</span><br><span class="line">          repo_token: $&#123;&#123; secrets.GITHUB_TOKEN &#125;&#125;</span><br></pre></td></tr></table></figure><p>正如你在这里所看到的，通过只提供 -Dtarget 选项，轻松实现了 Zig 项目的交叉编译。</p><p>此工作流程中的 TARGET 变量由两部分组成：</p><ul><li>CPU 架构（例如 x86_64）</li><li>操作系统（例如 linux）</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本教程中，我们学习了如何使用 Zig 编程语言构建一个简单的 HTTP 服务器，并使用 std.http 模块来处理 HTTP 请求和发送 HTTP 响应。我们还通过与 Rust 编写的 HTTP 服务器进行基准测试，比较了它们在处理相同请求时的性能表现，通过项目实战，让我们的代码能够通过github实现CICD管理项目的生命周期。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://github.com/ziglang/zig/blob/master/lib/std/mem/Allocator.zig">https://github.com/ziglang/zig/blob/master/lib/std/mem/Allocator.zig</a></li><li><a href="https://blog.orhun.dev/zig-bits-01/">https://blog.orhun.dev/zig-bits-01/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;比肩Rust？万字Ziglang项目实战&quot;&gt;&lt;a href=&quot;#比肩Rust？万字Ziglang项目实战&quot; class=&quot;headerlink&quot; title=&quot;比肩Rust？万字Ziglang项目实战&quot;&gt;&lt;/a&gt;比肩Rust？万字Ziglang项目实战&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Ziglang 简明教程</title>
    <link href="https://zoues.com/posts/7ea0589/"/>
    <id>https://zoues.com/posts/7ea0589/</id>
    <published>2024-08-03T08:32:35.000Z</published>
    <updated>2024-08-03T09:59:57.746Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ziglang-简明教程"><a href="#Ziglang-简明教程" class="headerlink" title="Ziglang 简明教程"></a>Ziglang 简明教程</h1><blockquote><p>译：<a href="https://blog.logrocket.com/getting-started-zig-programming-language/">https://blog.logrocket.com/getting-started-zig-programming-language/</a></p></blockquote><p>在本文中，通过理论与示例相结合的方式来帮助初学者进行Zig语言的学习。</p><h2 id="What‘s-Ziglang"><a href="#What‘s-Ziglang" class="headerlink" title="What‘s Ziglang"></a>What‘s Ziglang</h2><p>Zig是一款开源的规模最小、功能齐全的系统编程语言，其被视作较C更友好的替代品。它具有类似Rust的极简语法，同时保持了C的简单性。</p><p>Zig的目标是通过一种新的、受Rust语法影响的C语法的现代化方法来解决C开发人员面临的问题。它提供了一个高效的C互操作解决方案，让C开发人员可以将他们的C代码迁移到Zig。</p><p>Zig不仅仅是一种语言——其具备是一套完整的、功能齐全的工具链，这也意味着您可以使用Zig来创建、开发、测试和构建程序&#x2F;库，而无需第三方自动化构建工具。Zig工具链还可以交叉编译C&#x2F;C++以及Rust项目，因此您可以有效地使用Zig工具链来构建您现有的C&#x2F;C++项目。Zig被设计为一种低级别、硬件友好的系统编程语言，但其高效、开发者友好的语法和功能使其更适合构建任何现代软件系统。</p><p>Zig项目最初由Andrew Kelley发起，现在由Zig软件基金会（ZSF）维护。</p><h3 id="Zig的突出特点"><a href="#Zig的突出特点" class="headerlink" title="Zig的突出特点"></a>Zig的突出特点</h3><p>Zig致力于成为一个更好的C语言替代品，其不仅适用于低级系统编程，还适用于开发通用软件系统，具有以下突出特点：</p><h4 id="设计简单"><a href="#设计简单" class="headerlink" title="设计简单"></a>设计简单</h4><p>现代化语言的设计目标是提供一套设计良好的语法，而不像汇编语言那样原子化。如果语言的抽象过于接近汇编语言，开发人员可能需要编写冗长的代码。另一方面，当语言被抽象成接近人类可读时，它可能与硬件相距甚远，可能不适合系统编程的需求。</p><p>Zig提供了轻量级的、类Rust的语法，其大多数C提供的能力都已具备，但是它不提供Rust和C++那些复杂的功能集和语法，而是提供了一个像Go那样简单性为先的开发路径。</p><h4 id="性能和安全性"><a href="#性能和安全性" class="headerlink" title="性能和安全性"></a>性能和安全性</h4><p>性能和安全性是选择的关键因素。语言的性能通常取决于其标准库、核心运行时功能的性能，以及编译器生成的二进制文件的质量。同时，安全设计实现边界检查、溢出处理和内存范围，并帮助开发人员减少关键安全漏洞。</p><p>Zig构建系统提供了四种构建模式，开发人员可以根据其性能和安全性要求使用。Zig还可以在编译时理解变量溢出。</p><p>此外，它可以生成带有运行时安全检查的优化二进制文件，就像Rust一样，也可以生成不带运行时安全检查的超轻量级二进制文件，就像C一样。Zig官方文档声称，由于其基于LLVM的优化和改进的未定义行为，Zig在理论上比C更快！</p><h4 id="完整的系统编程解决方案"><a href="#完整的系统编程解决方案" class="headerlink" title="完整的系统编程解决方案"></a>完整的系统编程解决方案</h4><p>大多数编程语言都有一个或多个标准编译器和标准库实现。例如，您可以使用以下编译C：</p><ul><li>GNU C</li><li>Apple Clang</li><li>带有libc、BSD-libc和Microsoft C运行时的MSVC编译器</li></ul><p>但是这两个组件对于现代系统编程需求来说还不够。程序员通常需要建立工具、包管理器和交叉编译工具等。</p><p>因此，在C生态系统中，像CMake、Ninja、Meson这样的构建工具以及类似Conan这样的包管理器逐渐流行，而像Go和Rust这样的现代语言官方内置了包管理器、构建工具及API、交叉编译支持和测试集成等。</p><p>与Go及Rust等现代语言一样，Zig内置了包管理器、构建系统及API、支持交叉编译和测试集成，这提高了Zig成为更好的C的机会，因为它解决了C（和C++）开发人员面临的关键系统编程问题。从语言设计的角度来看，Zig提供了C开发人员期望的现代语言的所有功能，因此C程序员可以逐步将他们的系统迁移到现代Zig，而无需重新编写他们遗留的代码库。</p><h3 id="为什么学习Zig"><a href="#为什么学习Zig" class="headerlink" title="为什么学习Zig"></a>为什么学习Zig</h3><p>虽然编程语言千千万，但只有其中的一小部分能够在开发者社区中流行开来。未来还会有更多的编程语言出现，它们具有各种各样的特性，旨在取代现有的语言。</p><p>我们不需要学习所有开发语言，但是，如果有那么一种语言具有理想中未来语言的意味，并且提供了强有力、有效和经过验证的论据，说明它可以作为现有语言的替代品，那么学习它的内部设计理念无疑比完全忽视它要好。Go 1.0在2012年发布，作为一种新的极简语言，现在已经成为主要技术公司的依赖。在1990年代，Python是一种新的实验性脚本语言，但现在数字世界依赖于它。</p><p>类似地，Zig最初出现于2016年，2017年发布了首个预发布版本，展示了它作为C的现代替代品的能力。Zig甚至提供了一个完整的系统编程工具链，经过几年的积极开发，并确立了一个充满希望的未来。鉴于它与C的相似性和互操作性，它还可以为您打开更广泛的开发机会，包括人工智能开发、游戏开发等领域。</p><p>学习Zig不仅为您的技能组合增加了一种有前途的与C相关的语言，而且由于其聪明、性能安全平衡的设计，还提高了您对系统编程的了解。</p><h3 id="谁在使用Zig？"><a href="#谁在使用Zig？" class="headerlink" title="谁在使用Zig？"></a>谁在使用Zig？</h3><p>以下流行的开源项目和技术公司使用Zig语言及其工具链：</p><ul><li>主要项目</li></ul><p><img src="https://pic.imgdb.cn/item/66adfe70d9c307b7e9a7d9e2.png" alt="image-20240803165637785"></p><ul><li><p>主要公司</p><p><img src="https://pic.imgdb.cn/item/66adfe89d9c307b7e9a7ee54.png" alt="image-20240803165725136"></p></li></ul><h2 id="学习Zig"><a href="#学习Zig" class="headerlink" title="学习Zig"></a>学习Zig</h2><p>现在您已经了解了Zig及其引入原因，让我们通过学习语言语法、概念和特性的方式开始使用Zig进行开发。</p><ol><li>设置开发环境</li></ol><p>与任何其他开源项目一样，您可以从官方网站下载Zig，或者从源代码构建，但最简单、最现代的方法是通过系统软件包管理器安装它。在Ubuntu上使用以下Snap命令（以sudo运行）安装了Zig开发工具链：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">snap install zig --beta --classic</span><br></pre></td></tr></table></figure><p>有关Zig安装方法的更多信息，请参阅官方安装指南。</p><p>安装Zig后，请在终端中输入zig验证您的安装状态</p><ol start="2"><li>Zig初体验</li></ol><p>我们已经安装了Zig工具链，现在让我们开始在Zig中编写程序。我们将编写一个（经典）的Hello, World!类型的程序，以了解基本的源代码结构和工具链基础知识。</p><p>创建一个名为hello.zig的新文件，并添加以下源代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello Zig!\n&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们在第一行导入了标准库，并将其引用加载到std常量中。然后，main函数（返回void）使用print函数在终端上打印调试消息。</p><p>使用以下命令运行上述代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig run hello.zig</span><br></pre></td></tr></table></figure><p>您将在终端中看到<code>Hello Zig！</code>。print函数提供了类似于C的printf函数的API，让我们通过第二个参数使用格式化字符串：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello &#123;s&#125;! &#123;d&#125;\n&quot;, .&#123;&quot;Zig&quot;, 2024&#125;); // Hello Zig! 2024</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>您还可以省略原子的格式类型，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std.debug.print(&quot;Hello &#123;&#125;\n&quot;, .&#123;2024&#125;); // Hello 2024</span><br></pre></td></tr></table></figure><ol start="3"><li>编译Zig二进制文件</li></ol><p>任何软件发布都需创建一个二进制文件。Zig提供四种模式构建配置基于性能和安全性需求来交叉编译二进制文件。</p><p>为您的Hello, World!程序创建一个二进制文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe --name hello-bin hello.zig</span><br></pre></td></tr></table></figure><p>上述命令将使用默认的Debug模式构建生成一个名为hello-bin的调试二进制文件，因此我们可以通过<code>./hello-bin</code>来执行。</p><p>就像 GNU C 一样，默认情况下，Zig 会为当前目标（CPU 和操作系统）生成二进制文件，因此上述命令在我的计算机上为我生成了 x86 Linux 二进制文件。此外，可以使用 -target 标志进行交叉编译二进制文件。</p><p><img src="https://pic.imgdb.cn/item/66adfea0d9c307b7e9a80225.png" alt="image-20240803170630942"></p><p>例如，以下命令会为 x64 Windows 系统交叉编译一个 .exe 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build-exe -target x86_64-windows --name hello-bin.exe hello.zig</span><br></pre></td></tr></table></figure><ol start="4"><li>使用构建器 API 编译 Zig</li></ol><p>Zig 的编译命令行界面与 GNU C 的命令行标志相同，因此我们可能需要经常重复使用编译命令。在 C 中，我们可以通过编写用于构建过程的 shell 脚本或 CMake来解决这个问题。</p><p>构建系统允许您将编译器标志存储在配置文件中，甚至添加自定义构建步骤。Zig 通过 std 包暴露了一个内置的构建器 API，作为独立的、第三方构建系统。它还提供了 zig build 命令来执行存储在 build.zig 文件中的构建步骤。</p><p>您可以根据自己的喜好使用以下命令搭建一个 Zig 构建项目：</p><ul><li>zig init-exe：初始化一个基于 Zig 构建器的可执行应用程序</li><li>zig init-lib：初始化一个基于 Zig 构建器的库</li></ul><p>之前的演示应用程序是一个可执行类型的应用程序，所以我们可以使用第一个命令来了解 Zig 构建系统：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir zig-exe-demo</span><br><span class="line">cd zig-exe-demo</span><br><span class="line"></span><br><span class="line">zig init-exe</span><br></pre></td></tr></table></figure><p>上述命令创建了一个新的可执行程序，其中包含 src&#x2F;main.zig 中的演示源代码。它向 build.zig 文件添加了几个构建步骤，我们可以使用 zig build 命令执行它们，而不是使用 zig run 或 zig build-exe。</p><p>例如，您可以通过执行以下命令来运行程序：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zig build run</span><br></pre></td></tr></table></figure><p>您可以使用 install 步骤创建二进制文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zig build install</span><br><span class="line"></span><br><span class="line">./zig-out/bin/zig-exe</span><br></pre></td></tr></table></figure><p>就像在其他构建系统（如 CMake）中所做的那样，您可以通过修改 build.zig 文件来添加更多的构建步骤或中间处理。通过zig build 运行所有的构建自动化步骤，这无疑简化了 Zig 开发过程。</p><p>现在，您知道了如何编写一个基本的 Zig 程序，使用 Zig 编译器进行编译，并使用 Zig 的内置构建系统简化开发过程。让我们开始学习语法和特性吧！</p><ol start="5"><li>Zig 基本类型</li></ol><p>与 C 类似，Zig 支持各种形式的整数、浮点数和指针。让我们学习一些您应该了解的基本类型。</p><p>以下代码片段定义了一些整数和浮点数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i8 = -100;     // 有符号 8 位整数</span><br><span class="line">    var y: u8 = 120;      // 无符号 8 位整数</span><br><span class="line">    var z: f32 = 100.324; // 32 位浮点数</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;x=&#123;&#125;\n&quot;, .&#123;x&#125;);        // x=-100</span><br><span class="line">    std.debug.print(&quot;y=&#123;&#125;\n&quot;, .&#123;y&#125;);        // y=120</span><br><span class="line">    std.debug.print(&quot;z=&#123;d:3.2&#125;\n&quot;, .&#123;z&#125;);   // z=100.32</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于上述标识符值永远不会更改，我们可以使用 const 关键字而不是 var：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">const x: i8 = -100;</span><br><span class="line">// ...</span><br><span class="line">// ...</span><br></pre></td></tr></table></figure><p>作为现代语言，布尔类型也得到了支持：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var initialized: bool = true;</span><br></pre></td></tr></table></figure><p>您可以将字符存储在无符号字节（8 位整数）中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const l1: u8 = &#x27;Z&#x27;;</span><br><span class="line">    const l2: u8 = &#x27;i&#x27;;</span><br><span class="line">    const l3: u8 = &#x27;g&#x27;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;c&#125;-&#123;c&#125;-&#123;c&#125;\n&quot;, .&#123;l1, l2, l3&#125;); // Z-i-g</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>您还可以定义变量而不写明其数据类型，如下所示。然后，Zig 将使用 comptime 类型将它们存储起来，保证编译时评估：</p><p>Zig 还支持原生 C 类型（即 c_char、c_int 等）。可以在官方文档的表格中查看所有支持的类型。Zig 中没有内置的字符串类型，因此我们必须使用字节数组。我们将在本教程的另一部分讨论数组。</p><ol start="6"><li>枚举</li></ol><p>Zig 提供了一个简单的语法来定义和访问枚举。看看以下示例源代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const LogType = enum &#123;</span><br><span class="line">        info,</span><br><span class="line">        err,</span><br><span class="line">        warn</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const ltInfo = LogType.info;</span><br><span class="line">    const ltErr = LogType.err;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;ltInfo&#125;); // main.main.LogType.info</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;ltErr&#125;);  // main.main.LogType.err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 允许覆盖枚举的序数值，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const LogType = enum(u32) &#123;</span><br><span class="line">    info = 200,</span><br><span class="line">    err = 500,</span><br><span class="line">    warn = 600</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol start="7"><li>数组和切片</li></ol><p>Zig 建议将数组用于编译时已知值（compile-time-known），切片用于运行时已知值（runtime-known）。例如，我们可以将英语元音存储在常量字符数组中，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const vowels = [5]u8&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;&#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;vowels&#125;); // aeiou</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;vowels.len&#125;); // 5</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，我们可以省略大小，因为它在编译时已知：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const vowels = [_]u8&#123;&#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;&#125;; // 注意 &quot;_&quot;</span><br></pre></td></tr></table></figure><p>您不需要使用这种方法来定义字符串，因为 Zig 允许您以 C 风格定义字符串，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const msg = &quot;Ziglang&quot;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg&#125;); // Zig</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(msg)&#125;); // *const [7:0]u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦将硬编码字符串存储在标识符中，Zig 将自动使用空终止数组引用（数组的指针）*const [7:0]u8 来存储元素。在这里，我们使用了 @TypeOf() 内置函数来获取变量的类型。您可以在官方文档中浏览所有支持的内置函数。</p><p>数组可以使用 ** 和 ++ 运算符进行重复或连接：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const msg1 = &quot;Zig&quot;;</span><br><span class="line">    const msg2 = &quot;lang&quot;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg1 ** 2&#125;); // ZigZig</span><br><span class="line">    std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;msg1 ++ msg2&#125;); // Ziglang</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 切片几乎与数组一样，但用于存储在运行时已知而不在编译时已知的值。看看下面的例子，它从数组中创建一个切片：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const nums = [_]u8&#123;2, 5, 6, 4&#125;;</span><br><span class="line">    var x: usize = 3;</span><br><span class="line">    const slice = nums[1..x];</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;any&#125;\n&quot;, .&#123;slice&#125;);        // &#123; 5, 6 &#125;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(slice)&#125;);  // []const u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，如果 x 是一个运行时已知的变量，slice 标识符就会变成一个切片。如果对 x 使用 const，则 slice 将变成一个数组指针（*const [2]u8），因为 x 在编译时已知。我们将在后面的章节中讨论指针。</p><ol start="8"><li>结构体和联合体</li></ol><p>结构体是用于存储多个值的有用数据结构，甚至可以用来实现面向对象编程（OOP）概念。</p><p>您可以创建结构体并使用以下语法访问其内部字段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const PrintConfig = struct &#123;</span><br><span class="line">        id: *const [4:0] u8,</span><br><span class="line">        width: u8,</span><br><span class="line">        height: u8,</span><br><span class="line">        zoom: f32</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const pc = PrintConfig &#123;</span><br><span class="line">        .id = &quot;BAX1&quot;,</span><br><span class="line">        .width = 200,</span><br><span class="line">        .height = 100,</span><br><span class="line">        .zoom = 0.234</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;ID: &#123;s&#125;\n&quot;, .&#123;pc.id&#125;);  // ID: BAX1</span><br><span class="line">    std.debug.print(&quot;Size: &#123;d&#125;x&#123;d&#125; (zoom: &#123;d:.2&#125;)\n&quot;,</span><br><span class="line">        .&#123;pc.width, pc.height, pc.zoom&#125;);  // Size: 200x100 (zoom: 0.23)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Zig 中，结构体也可以具有方法，所以当我们讨论函数时，下面将展示一个示例。</p><p>Zig 联合体类似于结构体，但一次只能有一个活动字段。看下面的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const ActionResult = union &#123;</span><br><span class="line">        code_int: u8,</span><br><span class="line">        code_float: f32</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    const ar1 = ActionResult &#123; .code_int = 200 &#125;;</span><br><span class="line">    const ar2 = ActionResult &#123; .code_float = 200.13 &#125;;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;code1 = &#123;d&#125;\n&quot;, .&#123;ar1.code_int&#125;);  // code1 = 200</span><br><span class="line">    std.debug.print(&quot;code2 = &#123;d:.2&#125;\n&quot;, .&#123;ar2.code_float&#125;);  // code2 = 200.13</span><br><span class="line">    // std.debug.print(&quot;code2 = &#123;d:.2&#125;\n&quot;, .&#123;ar2.code_int&#125;);  // 错误！</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="9"><li>使用控制结构</li></ol><p>每种编程语言通常都提供控制结构来处理程序的逻辑流程。Zig 支持所有常见的控制结构，如 if、switch、for 等。</p><p>看看以下 if…else 语句的示例代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var score: u8 = 100;</span><br><span class="line"></span><br><span class="line">    if(score &gt;= 90) &#123;</span><br><span class="line">        std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">        std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;&quot;*&quot; ** 10&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    else if(score &gt;= 50) &#123;</span><br><span class="line">        std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        std.debug.print(&quot;Try again...\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是 switch 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var score: u8 = 88;</span><br><span class="line"></span><br><span class="line">    switch(score) &#123;</span><br><span class="line">        90...100 =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">            std.debug.print(&quot;&#123;s&#125;\n&quot;, .&#123;&quot;*&quot; ** 10&#125;);</span><br><span class="line">        &#125;,</span><br><span class="line">        50...89 =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Congrats!\n&quot;, .&#123;&#125;);</span><br><span class="line">        &#125;,</span><br><span class="line">        else =&gt; &#123;</span><br><span class="line">            std.debug.print(&quot;Try again...\n&quot;, .&#123;&#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是while 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: u8 = 0;</span><br><span class="line">    while(x &lt; 11) &#123;</span><br><span class="line">        std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">        x += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是for 语句的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const A = [_]u8 &#123;2, 4, 6, 8&#125;;</span><br><span class="line"></span><br><span class="line">    for (A) |n| &#123;</span><br><span class="line">        std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;n&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="10"><li>函数</li></ol><p>函数通过允许我们使用可调用标识符来命名每个代码段来帮助我们创建可重用的代码段。我们已经使用了main 来启动我们的应用程序，让我们创建更多函数并进一步学习函数。</p><p>下面是一个简单的函数，它返回两个整数的总和：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn add(a: i8, b: i8) i8 &#123;</span><br><span class="line">    return a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const a: i8 = 10;</span><br><span class="line">    const b: i8 = -2;</span><br><span class="line">    const c = add(a, b);</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125; + &#123;d&#125; = &#123;d&#125;\n&quot;, .&#123;a, b, c&#125;); // 10 + -2 = 8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归也是 Zig 提供的一种编程功能，与许多其他通用语言一样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn fibonacci(n: u32) u32 &#123;</span><br><span class="line">    if(n == 0 or n == 1) return n;</span><br><span class="line">    return fibonacci(n - 1) + fibonacci(n - 2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;fibonacci(2)&#125;);   // 1</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;fibonacci(12)&#125;);  // 144</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与 Go 一样，Zig 允许您在结构体中创建方法，并将它们用作 OOP 方法，如下例所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Rectangle = struct &#123;</span><br><span class="line">    width: u32,</span><br><span class="line">    height: u32,</span><br><span class="line">    fn calcArea(self: *Rectangle) u32 &#123;</span><br><span class="line">        return self.width * self.height;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var rect = Rectangle &#123; .width = 200, .height = 25 &#125;;</span><br><span class="line">    var area = rect.calcArea();</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;area&#125;);   // 5000</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="11"><li>指针</li></ol><p>Zig 作为硬件友好的语言，其支持类似 C 的指针。看看下面的基本整数指针：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: u8 = 10;</span><br><span class="line">    var ptr_x = &amp;x;</span><br><span class="line">    ptr_x.* = 12;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;x&#125;);   // 12</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;ptr_x&#125;);   // u8@...mem_address</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_x)&#125;);   // *u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里，C&#x2F;C++ 开发人员需要注意，我们使用 ptr.* 语法对指针进行解引用，而不是像在 C&#x2F;C++ 中那样使用 *ptr。指向数组元素和指向整个数组的指针也能按预期工作，如下所示的代码片段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var A = [_]u8 &#123;2, 5, 6, 1, 1&#125;;</span><br><span class="line">    var ptr_x = &amp;A[1];</span><br><span class="line">    ptr_x.* = 12;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;A[1]&#125;);            // 12</span><br><span class="line">    std.debug.print(&quot;&#123;d&#125;\n&quot;, .&#123;ptr_x&#125;);           // u8@...mem_address</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_x)&#125;);   // *u8</span><br><span class="line"></span><br><span class="line">    var ptr_y = &amp;A;</span><br><span class="line">    ptr_y[2] = 11;</span><br><span class="line">    std.debug.print(&quot;&#123;any&#125;\n&quot;, .&#123;A&#125;);             // &#123; 2, 12, 11, 1, 1 &#125;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(ptr_y)&#125;);   // *[5]u8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="12"><li>高级语言特性</li></ol><p>以下是您应该了解的一些 Zig 高级语言特性的摘要：</p><ol><li>通过分配器和 defer 关键字</li><li>支持手动内存管理</li><li>使用简单的语法支持泛型</li><li>提供高效的关键字（async、suspend 和 resume）进行现代异步编程（后续版本已经撤回该特性）</li><li>提供自动类型转换和手动类型转换，使用内置的 @as Zig 的 C-interop 允许您调用 C API。使用 -lc 标志进行以下运行以链接到 libc：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const c = @cImport(&#123;</span><br><span class="line">    @cInclude(&quot;stdio.h&quot;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    const char_count = c.printf(&quot;Hello %s\n&quot;, &quot;C...&quot;); // Hello C...</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;@TypeOf(char_count)&#125;); // c_int</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;char_count&#125;); // 11</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过关注官方 Zig 新闻页面，及时了解最新功能和高级概念。</p><ol start="13"><li>Zig 中的标准库 API</li></ol><p>我们已经讨论了前面示例中的 Zig 语言语法和特性，但这些概念不足以开发通用程序 —— 我们经常需要使用复杂的数据结构、数学公式和操作系统级别的 API。Zig 通过 std 命名空间提供了一个功能齐全但又精简的标准库。</p><p>我们将编写一个简单的 CLI 程序来学习几个 Zig 标准库的特性。将以下代码添加到一个新的 Zig 文件中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const stdout = std.io.getStdOut().writer();</span><br><span class="line"></span><br><span class="line">fn print_help() !void &#123;</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot; , .&#123;&quot;-&quot; ** 25&#125;);</span><br><span class="line">    try stdout.print(&quot;0: 退出\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;1: 显示帮助\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;2: 打印 Node.js 版本\n&quot;, .&#123;&#125;);</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot; , .&#123;&quot;-&quot; ** 25&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn print_node_version() !void &#123;</span><br><span class="line">    const cmd_res = try std.ChildProcess.exec(.&#123;</span><br><span class="line">        .allocator = std.heap.page_allocator,</span><br><span class="line">        .argv = &amp;[_][]const u8&#123;</span><br><span class="line">            &quot;node&quot;,</span><br><span class="line">            &quot;--version&quot;,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;);</span><br><span class="line">    try stdout.print(&quot;&#123;s&#125;\n&quot;, .&#123;cmd_res.stdout&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn ask_action() !i64 &#123;</span><br><span class="line">    const stdin = std.io.getStdIn().reader();</span><br><span class="line">    var buf: [10]u8 = undefined;</span><br><span class="line"></span><br><span class="line">    try stdout.print(&quot;输入操作：&quot;, .&#123;&#125;);</span><br><span class="line"></span><br><span class="line">    if (try stdin.readUntilDelimiterOrEof(buf[0..], &#x27;\n&#x27;)) |user_input| &#123;</span><br><span class="line">        return std.fmt.parseInt(i64, user_input, 10);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        return @as(i64, 0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    try print_help();</span><br><span class="line">    while(true) &#123;</span><br><span class="line">        const action = ask_action() catch -1;</span><br><span class="line">        switch(action) &#123;</span><br><span class="line">            0 =&gt; &#123;</span><br><span class="line">                std.debug.print(&quot;再见！\n&quot;, .&#123;&#125;);</span><br><span class="line">                break;</span><br><span class="line">            &#125;,</span><br><span class="line">            1 =&gt; &#123;</span><br><span class="line">                try print_help();</span><br><span class="line">            &#125;,</span><br><span class="line">            2 =&gt; &#123;</span><br><span class="line">                try print_node_version();</span><br><span class="line">            &#125;,</span><br><span class="line">            else =&gt; &#123;</span><br><span class="line">                std.debug.print(&quot;无效的操作：&#123;d&#125;\n&quot;, .&#123;action&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个演示的 CLI 支持三种整数操作：</p><ul><li>0：退出程序</li><li>1：显示程序帮助</li><li>2：通过 Zig 子进程 API 打印当前 Node.js 版本</li></ul><p>在这里，我们使用了一些错误处理基础知识以及标准库的 io 命名空间和 ChildProcess 结构。您可以在官方标准库参考文档中查看所有可用的命名空间和结构。</p><h2 id="开源的-Zig-生态系统"><a href="#开源的-Zig-生态系统" class="headerlink" title="开源的 Zig 生态系统"></a>开源的 Zig 生态系统</h2><p>Zig 是一种新的语言，因此开源软件包的可用性和开发者资源仍在不断增长。请查看以下流行的开源 Zig 库：</p><ul><li>zigzap&#x2F;zap：用于构建 Web 后端的微型框架</li><li>JakubSzark&#x2F;zig-string：用于 Zig 的字符串库</li><li>kooparse&#x2F;zalgebra：游戏和实时图形的线性代数库</li><li>zigimg&#x2F;zigimg：用于读写不同图像格式的 Zig 库</li><li>ziglibs&#x2F;ini：用于 Zig 的简单 INI 解析器 您还可以从 awesome-zig 存储库中了解更多与 Zig 有关的开发内容。</li></ul><h2 id="Zig-vs-C-vs-Rust"><a href="#Zig-vs-C-vs-Rust" class="headerlink" title="Zig vs. C vs. Rust"></a>Zig vs. C vs. Rust</h2><table><thead><tr><th>比较因素</th><th>Zig</th><th>C</th><th>Rust</th></tr></thead><tbody><tr><td>语言影响</td><td>受Rust、C和Python影响</td><td>B</td><td>受函数式语言和C++影响</td></tr><tr><td>首次发布</td><td>2017年（0.1.0）</td><td>1972年</td><td>2012年（0.1.0）</td></tr><tr><td>语言语法复杂度</td><td>极简</td><td>极简</td><td>复杂</td></tr><tr><td>主要范式</td><td>过程式</td><td>过程式</td><td>函数式和过程式</td></tr><tr><td>内存管理</td><td>手动（通过分配器）</td><td>手动（通过malloc等）</td><td>手动（但改进了以避免安全问题）</td></tr><tr><td>性能和二进制文件大小</td><td>生成超轻量、速度更快的二进制文件，无专用运行时（可选地链接libc）</td><td>生成超轻量、速度更快的二进制文件，具有一个称为C运行时的最小运行时</td><td>生成更快的二进制文件，没有专用运行时，二进制大小问题可以通过技巧解决</td></tr><tr><td>第三方库生态系统</td><td>作为新语言仍在发展中</td><td>成熟的库生态系统，但没有标准化的库注册和集成方法（取决于构建工具）</td><td>在官方包注册中心Crates中有成熟的库生态系统</td></tr><tr><td>开发者资源</td><td>良好，但除了官方文档外只有少量资源</td><td>良好</td><td>良好</td></tr><tr><td>标准库功能</td><td>良好</td><td>仅具有低级API—需要第三方库或编写特定于平台的代码</td><td>良好</td></tr><tr><td>工具链</td><td>功能齐全</td><td>仅编译器、链接器和汇编器—需要单独的工具来运行测试、进行高级构建、使用包等</td><td>功能齐全</td></tr><tr><td>C语言互操作性</td><td>无需FFI（外部函数接口）即可直接导入</td><td>不适用</td><td>通过Rust FFI</td></tr></tbody></table><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本教程中，我们学习了Zig编程语言开发背后的概念、目标和设计技术。通过测试通用的、通用的编程知识来学习Zig语言，这些知识可以用来构建现代计算机程序。</p><p>Zig仍然是一种新语言，ZSF仍在定期实现和测试更多功能。学习Zig是一个很好的决定，因为它作为一种更好的C语言，有着光明的前景。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ziglang-简明教程&quot;&gt;&lt;a href=&quot;#Ziglang-简明教程&quot; class=&quot;headerlink&quot; title=&quot;Ziglang 简明教程&quot;&gt;&lt;/a&gt;Ziglang 简明教程&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;译：&lt;a href=&quot;https:</summary>
      
    
    
    
    
    <category term="ziglang" scheme="https://zoues.com/tags/ziglang/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Resource Allocation (DRA) for GPUs in Kubernetes</title>
    <link href="https://zoues.com/posts/11831eff/"/>
    <id>https://zoues.com/posts/11831eff/</id>
    <published>2024-07-20T05:29:19.000Z</published>
    <updated>2024-07-20T06:36:02.419Z</updated>
    
    <content type="html"><![CDATA[<p>Original: <a href="https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file">https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Dynamic Resource Allocation (DRA)</a> is an upcoming Kubernetes feature that puts resource scheduling in the hands of 3rd-party developers. From an end-user’s perspective, It moves away from the limited “countable” interface for requesting access to resources  (e.g. “<strong>nvidia.com&#x2F;gpu: 2</strong>“), providing an API more akin to that of persistent volumes. Under the hood it uses <a href="https://github.com/container-orchestrated-devices/container-device-interface">CDI</a> to do its device injection.</p><p>NVIDIA has been working with Intel for the past 2 years on this feature and we are excited to see it finally gain traction within the community. DRA has been <a href="https://github.com/kubernetes/kubernetes/pull/111023">merged</a> as an alpha feature for <a href="https://www.kubernetes.dev/resources/release/#kubernetes-126">Kubernetes 1.26</a> (released in December 2022). Its graduation to beta and GA will follow soon after.</p><p>In the context of GPUs, this unlocks a host of new features without the need for awkward solutions shoehorned on top of the existing device plugin API.</p><p>These features include:</p><ul><li>Controlled GPU Sharing (both within a pod and across pods)</li><li>Multiple GPU models per node (e.g. T4 and A100)</li><li>Specifying arbitrary constraints for a GPU (min&#x2F;max memory, device model, etc.)</li><li>Natural support for MPS</li><li>Dynamic allocation of MIG devices</li><li>Dynamic repurposing of a GPU from full to MIG mode</li><li>Dynamic repurposing of a GPU for use as Passthrough vs. vGPU</li><li>… the list goes on …</li></ul><p>A reference implementation of our <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> is already available and a demo showcasing a subset of the features listed above can be found <a href="https://drive.google.com/file/d/1iLg2FEAEilb1dcI27TnB19VYtbcvgKhS/view?usp=sharing">here</a>.</p><h2 id="User-Facing-API"><a href="#User-Facing-API" class="headerlink" title="User-Facing API"></a>User-Facing API</h2><p><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Dynamic Resource Allocation (DRA)</a> is a generalization of the <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a> API for generic resources. As such, it allows one to separate the <em>declaration</em> of a resource to be consumed, from its actual consumption. This allows one to move away from the limited  “countable” API  provided by device-plugins today, to something much more flexible in terms of controlling which resources are consumed (and where).</p><p>Using our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> as an example, the table below shows the difference between how one requests access to two <strong>gpus</strong> under the existing device plugin model vs. DRA.</p><table><tr><th>Existing device plugin</th><th>DRA resource driver for GPUs</th></tr><tr><td><pre>apiVersion: v1kind: Podmetadata:  name: podspec:  containers:  - name: ctr    image: nvidia/cuda    command: ["nvidia-smi", "-L"]    resources:      limits:        nvidia.com/gpu: 2</pre></td><td><pre>apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimTemplatemetadata:  name: gpu-templatespec:  spec:    resourceClassName: gpu.nvidia.com<p>––<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu0</li><li>name: gpu1<br>resourceClaims:</li></ul></li><li>name: gpu0<br>source:<br>resourceClaimTemplate: gpu-template</li><li>name: gpu1<br>source:<br>resourceClaimTemplate: gpu-template<br></pre></td></li></ul></tr></table><p>Moreover, <strong>ResourceClaims</strong> can be annotated with a set of parameters defined by the developer of the DRA resource driver for a given <strong>ResourceClass</strong>. These parameters allow users to attach additional constraints to their resource requests.</p><p>For example, our reference DRA resource driver for GPUs defines the following two claim parameter objects for use with claims against the <strong>gpu.nvidia.com</strong> resource class:</p><table><tr><th>GPU Sharing within a Pod</th><th>GPU Sharing across Pods</th></tr><tr><td><pre>---apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimTemplatemetadata:  name: gpu-templatespec:  spec:    resourceClassName: gpu.nvidia.com<p>––<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod<br>spec:<br>containers:</p><ul><li>name: ctr0<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu</li></ul></li><li>name: ctr1<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimTemplate: gpu-template<br></pre></td></li></ul><td><pre>–--apiVersion: resource.k8s.io/v1alpha1kind: ResourceClaimmetadata:  name: shared-gpuspec:  resourceClassName: gpu.nvidia.com<hr><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod0<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimName: shared-gpu</li></ul><hr><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>name: pod1<br>spec:<br>containers:</p><ul><li>name: ctr<br>image: nvidia&#x2F;cuda<br>command: [“nvidia-smi” “-L”]<br>resources:<br>claims:<ul><li>name: gpu<br>resourceClaims:</li></ul></li><li>name: gpu<br>source:<br>resourceClaimName: shared-gpu<br></pre></td></li></ul></tr></table><p>Moreover, <strong>ResourceClaims</strong> can be annotated with a set of parameters defined by the developer of the DRA resource driver for a given <strong>ResourceClass</strong>. These parameters allow users to attach additional constraints to their resource requests.</p><p>For example, our reference DRA resource driver for GPUs defines the following two claim parameter objects for use with claims against the <strong>gpu.nvidia.com</strong> resource class:</p><table><tr><th></th><th></th></tr><tr><td><pre>apiVersion: gpu.resource.nvidia.com/v1alpha1kind: GpuClaimParametersmetadata:  name: ...spec:  count: ...  migEnabled: ...</pre></td><td><pre>apiVersion: gpu.resource.nvidia.com/v1alpha1kind: MigDeviceClaimParametersmetadata:  name: ...spec:  profile: ...  gpuClaimName: ...</pre></td></tr></table><p>The <strong>GpuClaimParameters</strong> object gives users the ability to request more than one GPU from a single resource claim (via the <strong>count</strong> parameter) as well as specify whether they want to receive GPUs that have MIG mode enabled on them or not (via the <strong>migEnabled</strong> parameter) .</p><p>The <strong>MigDeviceClaimParameters</strong> object gives users the ability to specify the profile of a MIG device they would like access to (via the <strong>profile</strong> parameter) as well as an optional reference to the specific GPU they would like their MIG device to be allocated on (via the <strong>gpuClaimName</strong> parameter).</p><p><em><strong>Note:</strong> both of these claim parameter objects are reference implementations, and we plan to extend &#x2F; replace them before they are released. Any feedback on what you would like to see here is greatly appreciated.</em></p><p>An example of using <strong>GpuClaimParameters</strong> to request eight GPUs from a single resource claim can be seen below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: gpu.resource.nvidia.com/v1alpha1</span><br><span class="line">kind: GpuClaimParameters</span><br><span class="line">metadata:</span><br><span class="line">  name: eight-gpus</span><br><span class="line">spec:</span><br><span class="line">  count: 8</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: resource.k8s.io/v1alpha1</span><br><span class="line">kind: ResourceClaimTemplate</span><br><span class="line">metadata:</span><br><span class="line">  name: eight-gpus-template</span><br><span class="line">spec:</span><br><span class="line">  spec:</span><br><span class="line">    resourceClassName: gpu.nvidia.com</span><br><span class="line">    parametersRef:</span><br><span class="line">      apiGroup: gpu.resource.nvidia.com</span><br><span class="line">      kind: GpuClaimParameters</span><br><span class="line">      name: eight-gpus</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: ctr</span><br><span class="line">    image: nvidia/cuda</span><br><span class="line">    command: [&quot;nvidia-smi&quot; &quot;-L&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      claims:</span><br><span class="line">      - eight-gpus</span><br><span class="line">  resourceClaims:</span><br><span class="line">  - name: eight-gpus</span><br><span class="line">    source:</span><br><span class="line">      resourceClaimTemplate: eight-gpus-template</span><br></pre></td></tr></table></figure><p>A more complex example involving both a GpuClaimParameters object and a MigDeviceClaimParameters object can be seen here:</p><p>In this example, we create a single pod with two containers, each of which wants access to its own 3g.40gb MIG device.</p><p><img src="https://pic.imgdb.cn/item/669b5569d9c307b7e917a5e3.png"></p><p>To ensure that the two MIG devices ultimately come from the same underlying GPU, we first create a <strong>GpuClaimParameters</strong> object requesting access to a MIG enabled GPU. We call this claim parameters object <strong>mig-enabled-gpu</strong>.<br>We then create a <strong>ResourceClaimTemplate</strong> also called <strong>mig-enabled-gpu</strong>, which binds the <strong>gpu.nvidia.com</strong> resource class to the <strong>mig-enabled-gpu</strong> claim parameters object.<br>Next we create a <strong>MigDeviceClaimParameters</strong> object specifying the <strong>3g.40gb</strong> profile. This object also includes a forward reference to the (yet-to-be-created) resource claim of the MIG enabled GPU on which this MIG device should be created (<strong>shared-gpu</strong>). Note this is the name of the resource claim itself, <em>not</em> the claim parameters object we called <strong>mig-enabled-gpu</strong>. We call this new claim parameters object <strong>mig-3g.40gb</strong>.<br>We then create a <strong>ResourceClaimTemplate</strong> also called <strong>mig-3g.40gb</strong>, which binds the <strong>gpu.nvidia.com</strong> resource class to the <strong>mig-3g.40gb</strong> claim parameters object.<br>Next we create the actual resource claims themselves inside the pod spec. One resource claim called <strong>shared-gpu</strong>, which references the <strong>mig-enabled-gpu</strong> resource claim template, as well as two other resource claims, each referencing the <strong>mig-3g.40gb</strong> resource claim template. These two resource claims are called <strong>mig-3g-0</strong> and <strong>mig-3g-1</strong>, respectively.<br>Finally, we reference each of these resource claims in the <strong>resources.claims</strong> sections of our two containers. Both containers refer to the same underlying <strong>shared-gpu</strong> claim, with each container pointing to one of <strong>mig-3g-0</strong> or <strong>mig-3g-1</strong>, respectively.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">gpu.resource.nvidia.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">GpuClaimParameters</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">migEnabled:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">resource.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceClaimTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">resourceClassName:</span> <span class="string">gpu.nvidia.com</span></span><br><span class="line">    <span class="attr">parametersRef:</span></span><br><span class="line">      <span class="attr">apiGroup:</span> <span class="string">gpu.resource.nvidia.com</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">GpuClaimParameters</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">gpu.resource.nvidia.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MigDeviceClaimParameters</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">profile:</span> <span class="string">3g.40gb</span></span><br><span class="line">  <span class="attr">gpuClaimName:</span> <span class="string">shared-gpu</span></span><br><span class="line"></span><br><span class="line"><span class="string">–--</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">resource.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceClaimTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">resourceClassName:</span> <span class="string">gpu.nvidia.com</span></span><br><span class="line">    <span class="attr">parametersRef:</span></span><br><span class="line">      <span class="attr">apiGroup:</span> <span class="string">gpu.resource.nvidia.com</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">MigDeviceClaimParameters</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr0</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nvidia/cuda</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;nvidia-smi&quot;</span> <span class="string">&quot;-L&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">claims:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-0</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr1</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nvidia/cuda</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;nvidia-smi&quot;</span> <span class="string">&quot;-L&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">claims:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-1</span></span><br><span class="line">  <span class="attr">resourceClaims:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-gpu</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-enabled-gpu</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-0</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-3g.40gb</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mig-3g-1</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">resourceClaimTemplate:</span> <span class="string">mig-3g.40gb</span></span><br></pre></td></tr></table></figure><p>As mentioned previously, <strong>GpuClaimParameters</strong> and <strong>MigDeviceClaimParameters</strong> are just reference specifications, and we plan to iterate on them further before they get released. Any feedback on how you would like to see these evolve would be greatly appreciated.<br>In the following section, we discuss the details of the DRA resource driver architecture and how it interacts with Kubernetes to make the user-facing API described above possible.</p><h2 id="DRA-Resource-Driver-Architecture"><a href="#DRA-Resource-Driver-Architecture" class="headerlink" title="DRA Resource Driver Architecture"></a>DRA Resource Driver Architecture</h2><p>At a high-level, a DRA resource driver is responsible for:</p><ul><li>Defining a ResourceClass associated with a specific type of resource (e.g. gpu.nvidia.com)</li><li>Processing any class parameter objects associated with this ResourceClass</li><li>Watching for incoming ResourceClaims that reference this ResourceClass</li><li>Processing any claim parameter objects associated with this ResourceClaim</li><li>Coordinating with the Kubernetes scheduler to find a node where a given ResourceClaim should be allocated</li><li>Allocating the ResourceClaim on that node</li><li>Cleaning up any allocated ResourceClaims once they get deleted</li></ul><p>To accomplish this, DRA resource drivers consists of two separate-but-coordinating components:</p><ul><li>A centralized controller running with high-availability</li><li>A node-local kubelet plugin running as a daemonset</li></ul><p><img src="https://pic.imgdb.cn/item/669b567fd9c307b7e9197041.png"></p><p>The centralized controller serves to:</p><ol><li>Coordinate with the K8s scheduler to decide which nodes an incoming <strong>ResourceClaim</strong> can be serviced on</li><li>Perform the actual <strong>ResourceClaim</strong> allocation once the scheduler picks a node to allocate it on</li><li>Perform the deallocation of a <strong>ResourceClaim</strong> once it has been deleted</li></ol><p>The node-local <strong>kubelet</strong> plugin serves to:</p><ol><li>Advertise any node-local state required by the centralized controller to make its allocation decisions</li><li>Perform any node-local operations required as part of allocating a <strong>ResourceClaim</strong> on a node</li><li>Pass the set of devices associated with an allocated <strong>ResourceClaim</strong> to the kubelet so it can ultimately be forwarded to the underlying container runtime</li><li>Perform any node-local operations required as part of freeing a <strong>ResourceClaim</strong> on a node</li></ol><p>To help illustrate how these responsibilities are carried out by each component, the following section walks through the process of deploying a DRA resource driver and then allocating a <strong>ResourceClaim</strong> associated with a newly created pod.</p><h2 id="Allocating-a-ResourceClaim"><a href="#Allocating-a-ResourceClaim" class="headerlink" title="Allocating a ResourceClaim"></a>Allocating a ResourceClaim</h2><p>The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation">Kubernetes Enhancement Proposal for DRA</a>  is the definitive source for all of the details about how the internals of DRA work. It defines a number of modes of operation, including delayed vs. immediate allocation, shared vs. non-shared resource claims, etc. As the names suggest, immediate allocation occurs as soon as a resource claim is created (i.e. there is no need to wait for a consumer to begin the allocation process), whereas delayed allocation does not occur until a pod that references the resource claim is first created. Likewise, shared resource claims can have multiple pods consuming them, whereas non-shared resource claims can only have one.</p><p>In this section we walk through the process of allocating a shared resource claim with delayed allocation (which is the default). The steps are broken down into phases, with a diagram showing the steps from each phase in action.  The steps themselves are annotated as “1.x” from Phase 1, “2.x”  from Phase 2, etc.</p><p>Phase 1 - Setup:</p><ol><li>Admin registers a <strong>ResourceClass</strong> pointing to a specific DRA resource driver as its owner</li><li>Admin deploys DRA resource driver in the cluster</li><li>Each DRA kubelet plugin begins advertising its node-local state for the centralized controller to pick up on</li></ol><p>Phase 2 - Pod Creation:</p><ol><li>User creates a <strong>ResourceClaim</strong> referencing a registered ResourceClass</li><li>User submits a pod to the API server referencing the <strong>ResourceClaim</strong> in one of its containers</li></ol><p><img src="https://pic.imgdb.cn/item/669b5833d9c307b7e91b7264.png"></p><p>Phase 3 - Node Selection:</p><ol><li>Scheduler picks up a pod from the API server and begins to schedule it on a node</li><li>Scheduler sees the pod’s <strong>ResourceClaim</strong> and its <strong>ResourceClass</strong> pointing to a specific DRA resource driver</li><li>Scheduler provides a list of potential nodes where it is considering scheduling the pod that has a reference to the <strong>ResourceClaim</strong>.  It does this through a special <strong>PodScheduling</strong> object in the API server</li><li>DRA resource driver picks up the <strong>PodScheduling</strong> object</li><li>DRA resource driver narrows down the list of potential nodes to just those where the <strong>ResourceClaim</strong> could possibly be allocated. It writes this back to the <strong>PodScheduling</strong> object</li></ol><p><img src="https://pic.imgdb.cn/item/669b5901d9c307b7e91c91fd.png"></p><p>Phase 4 - Claim Allocation</p><ol><li>Scheduler considers other scheduling constraints in relation to each of the nodes in the narrowed down list</li><li>Scheduler picks a node and sets a field in the  <strong>ResourceClaim</strong> with a reference to it</li><li>DRA resource driver picks up the selected node from the <strong>ResourceClaim</strong></li><li>DRA resource driver allocates the <strong>ResourceClaim</strong> for use on the node</li><li>DRA resource driver  marks the allocation as complete in the <strong>ResourceClaim</strong> object</li><li>Scheduler picks up the allocation completion from the <strong>ResourceClaim</strong> object</li><li>Scheduler schedules the pod on the node</li><li>Scheduler writes the scheduled node back to the Pod object</li></ol><p><img src="https://pic.imgdb.cn/item/669b597cd9c307b7e91d0a1b.png"></p><p>Phase 5 - Container Start:</p><ol><li>Kubelet picks up the pod from the API server and begins creating its containers</li><li>Kubelet calls out to DRA’s kubelet plugin to get the list of CDI devices associated with the <strong>ResourceClaim</strong></li><li>Kubelet passes the CDI devices to the container-runtime via CRI</li><li>Container runtime starts the container with access to the devices associated with the <strong>ResourceClaim</strong></li></ol><p><img src="https://pic.imgdb.cn/item/669b59bfd9c307b7e91d50c5.png"></p><h2 id="Writing-your-own-DRA-resource-driver"><a href="#Writing-your-own-DRA-resource-driver" class="headerlink" title="Writing your own DRA resource driver"></a>Writing your own DRA resource driver</h2><p>As mentioned in the previous section, DRA resource drivers consist of two separate-but-coordinating components: a centralized controller and a daemonset of node-local kubelet plugins. Most of the work required by the centralized controller to coordinate with the scheduler can be handled by boilerplate code. Only the business logic required to actually allocate ResourceClaims against the ResourceClasses owned by the resource driver needs to be customized.  As such, the following package is provide by Kubernetes to include APIs for invoking this boilerplate code as well as a Driver interface that one can implement to provide their custom business logic:</p><pre><code>k8s.io/dynamic-resource-allocation/controller</code></pre><p>Likewise, boilerplate code can be used to register the kubelet plugin with the kubelet, as well as start a gRPC server to implement DRA’s kubelet plugin API. The following package is provided for this purpose:</p><p><code>k8s.io/dynamic-resource-allocation/kubeletplugin</code></p><p>The set of functions defined by the controller’s Driver interface can be seen below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">type Driver interface &#123;</span><br><span class="line">GetClassParameters()</span><br><span class="line">GetClaimParameters()</span><br><span class="line">Allocate()</span><br><span class="line">Deallocate()</span><br><span class="line">UnsuitableNodes() </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Likewise, the set of functions defined by the gRPC API for the node-local kubelet plugin are:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">service Node &#123;</span><br><span class="line">  rpc NodePrepareResource (NodePrepareResourceRequest)</span><br><span class="line">    returns (NodePrepareResourceResponse) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  rpc NodeUnprepareResource (NodeUnprepareResourceRequest)</span><br><span class="line">    returns (NodeUnprepareResourceResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Examples of implementing these can be found in our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a>.</p><h2 id="Status-Future-Work"><a href="#Status-Future-Work" class="headerlink" title="Status &amp; Future Work"></a>Status &amp; Future Work</h2><p>mentioned previously, the alpha release of DRA has been <a href="https://github.com/kubernetes/kubernetes/pull/111023">merged</a> as part of <a href="https://www.kubernetes.dev/resources/release/#kubernetes-126">Kubernetes 1.26</a> in December 2022. Its graduation to beta and GA will follow soon after. Our reference <a href="https://gitlab.com/nvidia/cloud-native/k8s-dra-driver">DRA resource driver for GPUs</a> is feature-complete in terms of supporting all of the APIs required by DRA, but there is still quite a bit of room for improvement. We plan to continue iterating on this resource driver with an official release to coincide with the beta release of DRA itself. These are still the early days of DRA, and we are excited to see what other technologies this new feature helps unlock.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file">https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Original: &lt;a href=&quot;https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-file&quot;&gt;https://github.com/NVIDIA/k8s-dra-driver?tab=readme-ov-fi</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Container Interference Detection and Mitigation:Part 1</title>
    <link href="https://zoues.com/posts/48bcc63c/"/>
    <id>https://zoues.com/posts/48bcc63c/</id>
    <published>2024-07-19T23:48:30.000Z</published>
    <updated>2024-07-20T11:26:17.419Z</updated>
    
    <content type="html"><![CDATA[<p>From the perspective of resource allocation, the cluster appears relatively balanced. However, in terms of actual load, many hotspots have already emerged. This can lead to competition for shared resources between applications, causing performance interference in core business processes. As a result, application response times often exhibit long-tail behavior, leading to a decline in service quality and an increased likelihood of failures. This inter-application resource contention and performance interference (such as the “noisy neighbor” phenomenon) make application scheduling and resource management extremely complex. Therefore, it is crucial to consider how to reduce performance interference between applications to ensure business stability.</p><p><img src="https://pic.imgdb.cn/item/6691e507d9c307b7e9943c62.png"></p><p>The “noisy neighbor” problem is a common phenomenon in cloud infrastructure, referring to a situation where the resources required by an application are heavily consumed by other applications on the same compute node, leading to degraded performance for the application, such as increased latency. This issue is particularly troublesome for business-critical applications but is often difficult to identify.</p><p>The root cause lies in the lack of strict isolation policies during resource sharing. From a resource perspective, this includes CPU, memory (L1&#x2F;L2&#x2F;L3 cache), network, and block I&#x2F;O. From a hardware topology perspective, it involves CPU caches, memory bandwidth, and more. CPU cores are tightly coupled with other shared resources such as the last level cache (LLC) and memory bandwidth. If there are no effective isolation strategies in place when allocating shared resources, problems arise. For instance, sibling hyperthreads share the same physical CPU core, workloads on different physical CPU cores share the same last level cache, memory controller, and I&#x2F;O bus bandwidth, and even workloads on different physical CPU sockets share CPU interconnect bandwidth, the same storage devices, or I&#x2F;O bus. In such cases, the “noisy neighbor” often requires more resources and takes more time to complete the same task. This can lead to critical applications not getting the resources they need, causing performance degradation and stalls.</p><p>Kubernetes offers a CPU manager and device plugin manager for hardware resource allocation, such as CPUs and devices (SR-IOV, GPUs). Currently, Kubernetes also provides a topology manager to achieve NUMA topology awareness, coordinate resources, and ensure optimal performance for critical workloads. However, these features do not directly address the “noisy neighbor” problem.</p><p>To address this issue, the following optimization strategies can be considered:</p><ol><li><strong>Resource Requests and Limits</strong>: Set reasonable resource requests and limits for each Pod in Kubernetes to ensure that applications can obtain sufficient resources during contention.</li><li><strong>Affinity and Anti-affinity Rules</strong>: Use Kubernetes affinity and anti-affinity rules to deploy critical applications separately from other high resource-consuming applications to reduce resource contention.</li><li><strong>Isolation Policies</strong>: Utilize Kubernetes CPU isolation policies to ensure that critical applications have exclusive access to specific CPU cores, reducing competition for shared resources.</li><li><strong>Priority and Preemption</strong>: Set the priority of applications to ensure that critical applications can obtain resources first and preempt resources from lower-priority applications if necessary.</li><li><strong>NUMA-aware Scheduling</strong>: Through Kubernetes topology manager, ensure that critical applications are allocated resources within NUMA nodes to maximize the efficiency of local resource use and reduce latency from cross-node resource access.</li><li><strong>Dedicated Hardware</strong>: For high-performance applications, consider using dedicated hardware (such as GPUs, FPGAs) or isolated physical nodes to avoid sharing resources with other applications.</li><li><strong>Monitoring and Adjustment</strong>: Continuously monitor the resource usage and performance of applications, and adjust resource allocation strategies promptly to ensure the stable operation of critical applications.</li></ol><p>By implementing these optimization strategies, the “noisy neighbor” problem can be largely mitigated, ensuring the performance and stability of critical applications. Isolation policies, priority and preemption, and NUMA-aware scheduling are resource QoS optimization strategies for application scenarios. So, how do we measure performance interference?</p><blockquote><p>NOTE: Mixed deployment, here “mixed” essentially means “distinguishing priorities”. Narrowly, it can be simply understood as “online + offline” (offline) mixed deployment, broadly, it can be extended to a wider range of applications: multi-priority business mixed deployment.</p></blockquote><hr><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><h3 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h3><h4 id="a-CPI"><a href="#a-CPI" class="headerlink" title="a. CPI"></a>a. CPI</h4><p>CPI stands for Cycles Per Instruction, which means the <strong>number of cycles per instruction</strong>. Additionally, in some contexts, you may also encounter IPC, which stands for Instructions Per Cycle, meaning the <strong>number of instructions per cycle</strong>.</p><blockquote><p>The smaller the CPI value, the higher the instruction execution efficiency of the computer system.</p></blockquote><p>The relationship between CPI and IPC is: CPI &#x3D; 1 &#x2F; IPC</p><p><strong>In the context of single CPU program execution performance</strong>, it can actually be expressed as:</p><p><img src="https://pic.imgdb.cn/item/6691e5acd9c307b7e994e785.png" alt="img"></p><p>Due to the limitations of silicon material and manufacturing processes, increasing the processor’s clock speed has reached a bottleneck. Therefore, improving program performance mainly depends on two variables: Instruction Count and CPI.</p><p>Typically, by examining the CPI value, we can roughly determine whether a compute-intensive task is CPU-bound or memory-bound:</p><ul><li>CPI less than 1: The program is usually CPU-bound.</li><li>CPI greater than 1: The program is usually memory-bound.</li></ul><p>This is just a rough estimation method. Different types of tasks may have different CPI value ranges, so it is necessary to judge based on actual circumstances. Additionally, other factors such as memory size, bandwidth, and CPU cache must be considered for a more comprehensive assessment of the task type.</p><p>A key criterion for evaluating the efficiency of a compute-intensive task is the CPU utilization during the program’s execution. Many people believe that high CPU utilization indicates that the program’s code is running intensely. In fact, high CPU utilization can also mean that the CPU is <strong>busy-waiting</strong> for some resources (not iowait), such as encountering bottlenecks when accessing memory.</p><p>Some compute-intensive tasks normally have a low CPI and good performance. The CPU utilization is very high. However, as system load increases and other tasks compete for system resources, the CPI of these compute tasks can significantly rise, leading to a drop in performance. At this time, CPU utilization might still appear high, but this high utilization actually reflects CPU busy-waiting and the effects of pipeline stalls.</p><p>Brendan Gregg pointed out in his blog “CPU Utilization is Wrong” that CPU utilization metrics need to be analyzed in conjunction with CPI&#x2F;IPC metrics.</p><p>By using <code>perf record</code> to generate a CPI flame graph, one can show the association between the program’s call stack and CPU occupancy, and also reveal which parts of the CPU occupancy represent actual effective runtime and which parts are due to CPU busy-waiting caused by some stalls.</p><p>This tool can generally help identify system resource bottlenecks and provide ways to mitigate these bottlenecks; for example, interference from cache thrashing between applications can be resolved by binding applications to different CPUs.</p><p>Application developers can improve program performance by optimizing relevant functions. For instance, by optimizing code to reduce cache misses, thereby lowering the application’s CPI and reducing performance issues caused by memory access stalls.</p><h4 id="b-LLC"><a href="#b-LLC" class="headerlink" title="b. LLC"></a>b. LLC</h4><p>Older CPUs had two levels of cache (L1 and L2), while newer CPUs have three levels of cache (L1, L2, and L3), as shown in the following diagram:</p><img src="https://pic2.zhimg.com/80/v2-f4551e163f574dfe4a98dc1847272e05_720w.webp" alt="img" style="zoom:50%;" /><ul><li><p>L1 cache is divided into two types: instruction cache and data cache. L2 and L3 caches do not differentiate between instructions and data.</p></li><li><p>L1 and L2 caches are located within each CPU core, while the L3 cache is shared among all CPU cores.</p></li><li><p>The closer the cache (L1, L2, L3) is to the CPU, the smaller and faster it is; the farther it is from the CPU, the slower it becomes.</p><p><img src="https://frankdenneman.nl/wp-content/uploads/2019/10/03-Rome-Chiplet.png" alt="img"></p></li></ul><p>The LLC (Last Level Cache) cache resources are crucial for job performance, and interference on these resources cannot be ignored. When multiple applications share the cache, they may experience mutual replacement issues. When LLC is ineffective, the execution time of memory access instructions can increase from 15ns to 70ns. Assuming a CPU clock speed of 3 GHz, this results in each memory access instruction consuming an additional 200 or more cycles to complete. To mitigate such interference, resource partitioning techniques can be employed to physically separate multiple jobs’ access to shared resources. The diagram above shows a classic multi-core architecture where multiple CPUs share the LLC. Jobs running on different CPUs may contend for LLC resources. The Intel Cascade Lake microarchitecture and AMD’s Rome chiplet reduce inter-core competition for LLC by providing each core with its own independent LLC. However, multiple applications running on the same CPU can still experience cache replacement issues when running in a mixed deployment, necessitating application-level cache partitioning techniques. To achieve application-level cache partitioning, Intel introduced RDT (Resource Director Technology), which includes CAT (Cache Allocation Technology). CAT allows for the allocation of private cache space to processes or cgroups to avoid cache replacement.</p><p>The LLC cache hit rate and miss rate are typically calculated as follows:</p><ul><li>LLC Cache Hit Rate &#x3D; L3_CACHE_HITS &#x2F; L3_CACHE_REFERENCES</li><li>LLC Cache Miss Rate &#x3D; L3_CACHE_MISSES &#x2F; L3_CACHE_REFERENCES</li></ul><p>Here, L3_CACHE_REFERENCES represents the event counter for all LLC cache accesses, L3_CACHE_HITS represents the event counter for LLC cache hits, and L3_CACHE_MISSES represents the event counter for LLC cache misses.</p><p>It is important to note that these event counters are not simple accumulative counters; they need to be processed and normalized to obtain accurate results. Detailed processing methods can be found in the documentation for tools like <code>pcm-exporter</code> or other similar resources.</p><p>In a containerized environment, a streaming application running inside a container that continuously reads and writes data can consume a large amount of LLC space. This can lead to data required by an LS application running in another container on the same machine being evicted from the LLC, resulting in decreased performance for the LS application.<br><img src="https://lynnapan.github.io/images/cache/8.png" alt="img"></p><h3 id="Metric-Collection"><a href="#Metric-Collection" class="headerlink" title="Metric Collection"></a>Metric Collection</h3><p>Several implementation paths:</p><ol><li><p><strong>Using cgroup <code>perf_event</code> to obtain CPI metrics for all applications on the host:</strong></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unix.IoctlSetInt(p.fd, unix.PERF_EVENT_IOC_ENABLE, <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Using node <code>perf</code> to obtain CPI metrics for each CPU:</strong></p><p>This approach involves using Go’s implementation, which requires locking the OS and then utilizing <code>perf_event_open</code>.</p></li><li><p><strong>Using cAdvisor with the <code>libpfm</code> tool:</strong></p><p>This requires using cgo, with <code>libpfm</code> relying on <code>perf_event_open</code> at a lower level.</p></li></ol><h4 id="a-Node-Exporter"><a href="#a-Node-Exporter" class="headerlink" title="a. Node Exporter"></a>a. Node Exporter</h4><p>Currently, node exporter only supports CPU-level metrics. For more details, refer to the <a href="https://github.com/prometheus/node_exporter/pull/1274">implementation</a>.</p><h4 id="b-cgroup-perf-event"><a href="#b-cgroup-perf-event" class="headerlink" title="b. cgroup perf_event"></a>b. cgroup <code>perf_event</code></h4><p>This approach aims to support cgroup-level metrics, allowing monitoring of all threads belonging to a specific group and threads running on a specific CPU. For more information, see <a href="https://lwn.net/Articles/421574/">LWN.net</a>. cAdvisor supports cgroup <code>perf_event</code>, and its <a href="https://github.com/google/cadvisor/blob/a52ec5d60cf70b22f8b6d204780aec7a222cf6bb/manager/manager.go">implementation</a> can be reviewed for details.</p><p>Other metric collection methods:</p><ul><li><a href="https://github.com/intel/pcm">Intel PCM</a></li></ul><h3 id="Intel-RDT"><a href="#Intel-RDT" class="headerlink" title="Intel RDT"></a>Intel RDT</h3><p>Previously, solutions involved controlling virtual machine logical resources (cgroups), but this approach was too coarse-grained and couldn’t manage sensitive and scarce processor caches effectively. To address this, Intel introduced RDT (Resource Director Technology), which provides more precise control. For more details, refer to the <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">related introduction</a>.</p><p>RDT, which stands for Resource Director Technology, offers two main capabilities: monitoring and allocation. It enables users to directly monitor and allocate L2 and L3 caches (LLC) and memory bandwidth for each CPU core (or each logical core after Hyper-Threading) using a set of CPU instructions.</p><p>The Intel RDT implementation architecture was introduced in Linux Kernel 4.10, which provided L3 Cache Allocation Technology (CAT), L3 Code and Data Prioritization (CDP), and L2 CAT through the <code>resctrl</code> filesystem. Linux Kernel 4.12 further introduced support for Memory Bandwidth Allocation (MBA) for memory bandwidth management.</p><p>Intel RDT provides a series of allocation (resource control) capabilities, including Cache Allocation Technology (CAT), Code and Data Prioritization (CDP), and Memory Bandwidth Allocation (MBA).</p><p>The Intel Xeon E5-xxxx v4 series (Broadwell) offers L3 cache configuration and CAT mechanisms, with some communication-related features introduced in the E5-xxxx v3 series (Haswell). Some Intel processor series (e.g., Intel Atom processors) may support control of L2 caches. Additionally, the MBA functionality provides memory bandwidth management at the processor core level.</p><p>To use resource allocation technologies in Linux, the <code>resctrl</code> interface needs to be introduced in both the kernel and user space. From Linux Kernel 4.10 onwards, L3 CAT, L3 CDP, and L2 CAT are available with the <code>resctrl</code> architecture. Starting with Linux Kernel 4.12, MBA technology was introduced and is under development. For kernel usage instructions, see the <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">kernel documentation</a>.</p><h5 id="RDT-Technology-Architecture"><a href="#RDT-Technology-Architecture" class="headerlink" title="RDT Technology Architecture"></a>RDT Technology Architecture</h5><p>The core goal of Cache Allocation Technology (CAT) is to allocate resources based on Class of Service (COS or CLOS). Applications or individual threads can be tagged according to a series of service levels provided by the processor. This allows the allocation and restriction of cache usage based on the service classification of applications and threads. Each CLOS can use Capacity Bitmasks (CBMs) to indicate and specify the degree of overlap or isolation in service classification.</p><p>For each logical processor, there is a register (known as <code>IA32_PQR_ASSOC</code> MSR or PQR) that allows the operating system (OS) or Virtual Machine Monitor (VMM) to specify the CLOS when scheduling applications, threads, or virtual machines (VMs).</p><p>RDT is divided into five functional modules:</p><ul><li><strong>Cache Monitoring Technology (CMT)</strong>: Cache monitoring</li><li><strong>Cache Allocation Technology (CAT)</strong>: Cache allocation</li><li><strong>Memory Bandwidth Monitoring (MBM)</strong>: Memory bandwidth monitoring</li><li><strong>Memory Bandwidth Allocation (MBA)</strong>: Memory bandwidth allocation</li><li><strong>Code and Data Prioritization (CDP)</strong>: Code and data prioritization</li></ul><blockquote><p>RDT technology targets caches and memory bandwidth, divided into monitoring and control, forming four functional modules. Adding Code and Data Prioritization (control technology) results in a total of five functional modules.</p></blockquote><img src="https://pic.imgdb.cn/item/6691e693d9c307b7e995d1f8.png" alt="英特尔® RDT 内核架构" style="zoom:50%;" /><p>After enabling RDT control, you can create user directories in the root directory (e.g., “CG1” and “CG2”, as shown in Figure 4: Intel® RDT Hierarchy in the resctrl Filesystem) and specify different levels of control for each shared resource. The RDT control groups include the following files:</p><ul><li><strong>“tasks”</strong>: Reading this file displays a list of all tasks in the group. Writing task IDs to this file adds tasks to the group.</li><li><strong>“cpus”</strong>: Reading this file displays the bitmask of logical CPUs owned by the group. Writing a bitmask to this file adds CPUs to the group or removes CPUs from the group.</li><li><strong>“schemata”</strong>: A list of all resources accessible by the group.</li></ul><p>After enabling RDT monitoring features, the root directory and other top-level directories will include a “mon_groups” directory, where user directories (e.g., “M1” and “M2”, as shown in Figure 4: Intel® RDT Hierarchy in the resctrl Filesystem) can be created to monitor task groups. The “Mon_data” directory contains a set of files organized by resource domain and RDT events. Each directory within “Mon_data” has files for each event (e.g., “llc_occupancy”, “mbm_total_bytes”, and “mbm_local_bytes”). These files provide counters for the current values of events for all tasks in the group.<br><img src="https://pic.imgdb.cn/item/6691e6c8d9c307b7e9960b0d.png" alt="英特尔® RDT 在 resctrl 文件系统中的监测和控制示意图" style="zoom:50%;" /></p><h2 id="Theoretical-Foundation"><a href="#Theoretical-Foundation" class="headerlink" title="Theoretical Foundation"></a>Theoretical Foundation</h2><h3 id="CPI2-CPU-Performance-Isolation-for-Shared-Compute-Clusters"><a href="#CPI2-CPU-Performance-Isolation-for-Shared-Compute-Clusters" class="headerlink" title="CPI2: CPU Performance Isolation for Shared Compute Clusters"></a>CPI2: CPU Performance Isolation for Shared Compute Clusters</h3><h4 id="Performance-Metrics"><a href="#Performance-Metrics" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h4><p>CPI</p><h4 id="Core-Methodology"><a href="#Core-Methodology" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>Google’s approach relies entirely on statistical analysis based on historical data, without requiring separate stress testing, making the method straightforward. Google demonstrated the correlation between CPI and RT (Response Time) using historical data, finding a correlation of 0.97 for <strong>leaf-level links</strong> primarily for <strong>CPU-bound</strong> applications. Other services, such as some I&#x2F;O-bound services and intermediate node services, still showed a correlation above 0.7. Thus, CPI was confirmed as a valid proxy for performance.</p><p>Online jobs are typically long-running tasks, and their CPI trends on the same CPU model usually follow a predictable pattern. Therefore, traditional <strong>sliding window prediction</strong> methods are used to forecast the CPI for the next period.</p><img src="https://justadogistaken.github.io/images/cpi-1.png" alt="/images/cpi-1.png" style="zoom:50%;" /><p>Furthermore, the distribution of CPI data remains relatively consistent day-to-day, allowing the use of statistical methods to calculate the average CPI (CPIavg) and standard deviation (stddev) from the previous day. A threshold is set at CPIavg + 2 * stddev; if the CPI exceeds this value, QoS (Quality of Service) is considered to be impacted. Additionally, to avoid false positives, the rule requires that this threshold be exceeded three times within a 5-minute window before determining that QoS is affected.<br><img src="https://justadogistaken.github.io/images/cpi-2.png" alt="/images/cpi-2.png" style="zoom:30%;" /></p><h4 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h4><p>Experimental data indicate that the linear correlation coefficient between CPI and interference is 0.97.</p><h3 id="LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management"><a href="#LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management" class="headerlink" title="LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management"></a>LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management</h3><h4 id="Performance-Metrics-1"><a href="#Performance-Metrics-1" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h4><p>CPI</p><h4 id="Core-Methodology-1"><a href="#Core-Methodology-1" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>The primary contribution of this article from Alibaba is in how to adjust LLC (Last Level Cache) more finely. Intel RDT provides two technologies—CAT (Cache Allocation Technology) and MBA (Memory Bandwidth Allocation)—which are similar to cgroups in usage. Due to the slow integration into container runtimes, Intel open-sourced <code>intel-resource-manager</code> to support these technologies. CAT isolates LLC size, while MBA isolates L2-L3 memory bandwidth. However, the adjustment granularity for these two dimensions is 10%, which is too coarse. Alibaba combined CAT and MBA to achieve finer-grained adjustments.</p><p>In this approach, CPI is used for interference detection, but Alibaba calculated it through stress testing. They established a linear relationship <code>RT = k * CPI + l</code> to relate RT (Response Time) to CPI. Real-time CPI data is then used to estimate RT values and assess whether application QoS exceeds SLA (Service Level Agreement).</p><h4 id="Conclusions-1"><a href="#Conclusions-1" class="headerlink" title="Conclusions"></a>Conclusions</h4><p>CPI is linearly related to interference. By calculating RT from CPI, adjustments to LLC and MBA can be made to manage resource isolation effectively.</p><h3 id="PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services"><a href="#PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services" class="headerlink" title="PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services"></a>PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services</h3><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>Latency (ms)</p><h4 id="Core-Methodology-2"><a href="#Core-Methodology-2" class="headerlink" title="Core Methodology"></a>Core Methodology</h4><p>Stress testing is used to determine the optimal latency target QoS for single applications (by applying pressure until the latency-pressure curve shows an inflection point). Resources are categorized as &lt;cpu core, cache way, cpu frequency, memory space, disk bandwidth&gt;, with corresponding adjustment granularity of &lt;1 core, 1 way, 100 MHz, 1 GB, 1 GB&#x2F;s&gt;. QoS is checked every 500 ms, and if deviations from target QoS are detected, resources are adjusted. For each application, different resource adjustments are attempted each round (similar to guessing the resource causing interference) until the QoS for all applications on the machine is ensured. The article provides a good approach by treating resources as interchangeable. For example, when interference is detected, resources are not uniformly scaled up or down; instead, resources are exchanged, such as reallocating CPU from I&#x2F;O-intensive applications to CPU-intensive ones, ensuring that each application has the appropriate resources.</p><p>Due to the author’s limited time, perspective, and understanding, there may be errors or omissions in this article. Feedback and corrections from readers and industry experts are welcomed. The troubleshooting information above has been updated to reflect community content.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1759977">Caelus—全场景在离线混部解决方案</a></li><li>Google Borg 2015：<a href="https://research.google/pubs/pub43438/">https://research.google/pubs/pub43438/</a></li><li>Google Borg 2019：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387517">https://dl.acm.org/doi/pdf/10.1145/3342195.3387517</a></li><li>Google Autopilot：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387524">https://dl.acm.org/doi/pdf/10.1145/3342195.3387524</a></li><li>百度千寻：<a href="https://www.infoq.cn/article/aEut*ZAIffp0q4MSKDSg">百度大规模战略性混部系统演进</a></li><li>阿里伏羲：<a href="https://yq.aliyun.com/articles/651202">https://yq.aliyun.com/articles/651202</a></li><li>阿里k8s混部：<a href="https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf">https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf</a></li><li>CPI论文: <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf</a></li><li>Heracles论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf</a></li><li>Bubble-up论文：<a href="https://ieeexplore.ieee.org/document/7851476">https://ieeexplore.ieee.org/document/7851476</a></li><li><a href="http://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/08/29/perf-arch">Linux kernel perf architecture (terenceli.github.io)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/noisy-neighbors-problem-in-kubernetes.html">resolving noisy neighbors (intel.com)</a></li><li><a href="https://facebookmicrosites.github.io/cgroup2/docs/overview.html">Maximizing Resource Utilization with cgroup2</a></li><li><a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html">字节跳动：混布环境下集群的性能评估与优化</a></li><li><a href="https://www.cnblogs.com/tencent-cloud-native/p/14754230.html">混部之殇-论云原生资源隔离技术之CPU隔离(一) </a></li><li><a href="https://patents.google.com/patent/CN106776005A/zh">CN106776005A - 一种面向容器化应用的资源管理系统及方法</a></li><li><a href="https://www.aboutyun.com/thread-27867-1-1.html">阿里K8s之动态解决容器资源按需分配</a></li><li><a href="https://justadogistaken.github.io/posts/handle-interference/">混部场景下的单机服务质量保障 </a></li><li>[Cache高速缓存和缓存隔离](<a href="https://lynnapan.github.io/2017/04/18/understand">https://lynnapan.github.io/2017/04/18/understand</a> Cache&#x2F;)</li><li><a href="https://github.com/hodgesds/perf-utils">hodgesds&#x2F;perf-utils</a></li><li><a href="https://cloud.tencent.com/developer/article/1517979">用CPI火焰图分析Linux性能问题</a></li><li><a href="https://www.openeuler.org/zh/blog/rubik/index.html">openEuler资源利用率提升之道</a></li><li><a href="http://jos.org.cn/html/2020/10/6066.htm">在离线混部作业调度与资源管理技术研究综述</a></li><li><a href="https://qiankunli.github.io/2021/11/22/hybrid_deployment.html">从混部到统一调度</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;From the perspective of resource allocation, the cluster appears relatively balanced. However, in terms of actual load, many hotspots hav</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>容器干扰检测与治理（上篇）</title>
    <link href="https://zoues.com/posts/e9953e7c/"/>
    <id>https://zoues.com/posts/e9953e7c/</id>
    <published>2024-07-12T23:48:30.000Z</published>
    <updated>2024-07-13T02:32:15.895Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>从资源分配视图来看，集群分配相对平衡的，但从实际负载情况来看，已经出现较多的热点，此时容易发生应用间竞争共享资源, 导致核心业务出现性能干扰，应用的响应时间往往会出现长尾现象，导致应用服务质量下降，且会增加其故障的可能性，这种应用间的资源竞争及性能干扰（如noisy neighbor现象）使得应用调度与资源管理变得十分复杂，因此考虑如何降低应用间的性能干扰, 以保障业务的稳定性。</p><p><img src="https://pic.imgdb.cn/item/6691e507d9c307b7e9943c62.png"></p><p>“noisy neighbor”问题是云基础设施中的一种常见现象，指的是当应用程序所需的资源被同一计算节点中的其他应用程序大量占用时，导致应用程序性能降低，如延迟时间增加。这对业务敏感型应用来说，尤其棘手，但通常难以识别。</p><p>问题的根源在于资源共享时缺乏严格的隔离策略。从资源层面来看，包括CPU、内存（L1&#x2F;L2&#x2F;L3缓存）、网络和块I&#x2F;O等。从硬件拓扑层面来看，则涉及到CPU缓存、内存带宽等。CPU核心紧密连接着其他可共享资源，如最后一级缓存（LLC）和内存带宽。在分配共享资源时，如果没有有效的隔离策略，问题就会出现。例如，同级超线程共享相同的物理CPU核心，不同物理CPU核心上的工作负载共享相同的最后一级缓存、内存控制器和I&#x2F;O总线带宽，甚至不同物理CPU插槽上的工作负载也共享CPU互连带宽、相同的存储设备或I&#x2F;O总线。在这种情况下，”noisy neighbor”往往需要更多的资源，并且需要消耗更多时间才能完成同一任务。这会导致核心应用无法获得所需资源，导致性能下降和stall情况的出现。</p><p>Kubernetes提供了CPU管理器和设备插件管理器，用于硬件资源分配，例如CPU和设备（SR-IOV、GPU）。目前，Kubernetes还提供了拓扑管理器，以实现NUMA拓扑感知，协调资源并保证关键工作负载的最佳性能。然而，这些功能并未直接解决”noisy neighbor”问题。</p><p>要解决这个问题，可以考虑以下优化策略：</p><ol><li><strong>资源请求和限制</strong>：在Kubernetes中为每个Pod设置合理的资源请求和限制，以确保应用程序在资源竞争时能获得足够的资源。</li><li><strong>亲和性和反亲和性规则</strong>：使用Kubernetes的亲和性和反亲和性规则，将关键应用与其他高资源消耗的应用分开部署，减少资源竞争。</li><li><strong>隔离策略</strong>：利用Kubernetes的CPU隔离策略，确保关键应用独占特定的CPU核心，减少共享资源的争夺。</li><li><strong>优先级和抢占</strong>：设置应用程序的优先级，确保关键应用可以优先获得资源，并在必要时抢占低优先级应用的资源。</li><li><strong>NUMA感知调度</strong>：通过Kubernetes的拓扑管理器，确保关键应用在NUMA节点内分配资源，最大化本地资源的使用效率，减少跨节点资源访问的延迟。</li><li><strong>使用专用硬件</strong>：对于需要高性能的应用，考虑使用专用硬件（如GPU、FPGA）或独立的物理节点，避免与其他应用共享资源。</li><li><strong>监控和调整</strong>：持续监控应用程序的资源使用情况和性能表现，及时调整资源分配策略，确保关键应用的稳定运行。</li></ol><p>通过实施这些优化策略，可以在很大程度上缓解”noisy neighbor”问题，保障关键应用程序的性能和稳定性,其中隔离策略、优先级和抢占以及numa感知调度都是针对应用场景的资源QoS优化策略，那么我们如何衡量性能干扰？</p><blockquote><p>NOTE: 混部(混合部署)，这里的“混”，本质上就是“区分优先级”。狭义上，可以简单的理解为“在线+离线”(在离线)混部，广义上，可以扩展到更广的应用范围：多优先级业务混合部署</p></blockquote><hr><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><h4 id="a-CPI"><a href="#a-CPI" class="headerlink" title="a. CPI"></a>a. CPI</h4><p>CPI 即 Cycle Per Instruction 的缩写，它的含义就是<strong>每指令周期数</strong>。此外，在一些场合，也可以经常看到 IPC，即 Instruction Per Cycle，含义为<strong>每周期指令数</strong>。</p><blockquote><p>CPI 值越小，表示计算机系统的指令执行效率越高。</p></blockquote><p>CPI 和 IPC 的关系为: CPI &#x3D; 1 &#x2F; IPC</p><p><strong>如果具体到单 CPU 的程序执行性能场景</strong>，实际上可以表示为：</p><p><img src="https://pic.imgdb.cn/item/6691e5acd9c307b7e994e785.png" alt="img"></p><p>由于受到硅材料和制造工艺的限制，处理器主频的提高已经面临瓶颈，因此，程序性能的提高，主要的变量在 Instruction Count 和 CPI 这两个方面</p><p>通常情况下，通过 CPI 的取值，我们可以大致判断一个计算密集型任务，到底是 CPU 密集型的还是 Memory 密集型的：</p><ul><li>CPI 小于 1，程序通常是 CPU 密集型的；</li><li>CPI 大于 1，程序通常是 Memory 密集型的;</li></ul><p>这只是一个大致的判断方法，不同的任务类型可能会有不同的CPI取值范围，因此需要结合实际情况进行判断。同时，还需要考虑到其它因素，比如内存大小、带宽、CPU的缓存等，才能更全面地判断任务类型。</p><p>判断一个计算密集型任务运行效率的重要依据就是看程序运行时的 CPU 利用率。很多人认为 CPU 利用率高就是程序的代码在疯狂运行。实际上，CPU 利用率高，也有可能是 CPU 正在<strong>忙等</strong>一些资源(非iowait)，如访问内存遇到了瓶颈。</p><p>一些计算密集型任务，在正常情况下，CPI 很低，性能原本很好。CPU 利用率很高。但是随着系统负载的增加，其它任务对系统资源的争抢，导致这些计算任务的 CPI 大幅上升，性能下降。而此时，很可能 CPU 利用率上看，还是很高的，但是这种 CPU 利用率的高，实际上体现的是 CPU 的忙等，及流水线的停顿带来的效应。</p><p>Brendan Gregg 曾在 CPU Utilization is Wrong 这篇博客中指出，CPU 利用率指标需要结合 CPI&#x2F;IPC 指标一起来分析。</p><p>通过perf record，生成CPI 火焰图，其可以展示了程序的 Call Stack 与 CPU 占用率的关联性，而且还揭示了这些 CPU 占用率里，哪些部分是真正的有效的运行时间，哪些部分实际上是 CPU 因某些停顿造成的忙等。</p><p>一般可以通过此工具发现系统存在的资源瓶颈，并且通过一些方式来缓解资源的瓶颈；例如，应用间的 Cache 颠簸干扰，可以通过将应用绑到不同的 CPU 上解决。</p><p>而应用开发者则可以通过优化相关函数，来提高程序的性能。例如，通过优化代码减少 Cache Miss，从而降低应用的 CPI 来减少处理器因访存停顿造成的性能问题。</p><h4 id="b-LLC"><a href="#b-LLC" class="headerlink" title="b. LLC"></a>b. LLC</h4><p>旧式的 CPU 会有两级内存（L1 和 L2），新的CPU会有三级内存（L1，L2，L3 ），如下图所示：</p><img src="https://pic2.zhimg.com/80/v2-f4551e163f574dfe4a98dc1847272e05_720w.webp" alt="img" style="zoom:50%;" /><p>其中：</p><ul><li><p>L1 缓存分成两种，一种是指令缓存，一种是数据缓存。L2 缓存和 L3 缓存不分指令和数据。</p></li><li><p>L1 和 L2 缓存在每一个 CPU 核中，L3 则是所有 CPU 核心共享的内存。</p></li><li><p>L1、L2、L3 的越离CPU近就越小，速度也越快，越离 CPU 远，速度也越慢。</p><p><img src="https://frankdenneman.nl/wp-content/uploads/2019/10/03-Rome-Chiplet.png" alt="img"></p></li></ul><p>LLC(last level cache)缓存资源作为影响作业性能的重要资源, 其上的干扰同样不可忽略.多个应用在共享缓存时可能出现相互替换的现象, LLC失效时原本访存指令的执行时间将从15ns上升至70ns, 假设CPU主频为3Ghz, 则一条访存指令需要多消耗200多个周期才能完成，消除此类干扰的方法是使用资源划分技术, 在物理上划分多个作业对共享资源的使用，上图所示为经典的多核体系结构, 多个CPU共享了LLC, 运行于不同CPU上的作业会在LLC上发生竞争, Intel Cascade Lake微架构与amd的Rome chiplet, 通过为每个核设置独立的LLC以减少核间对于LLC的资源竞争; 但是, 同一CPU上的多个应用在混部运行时仍然会出现缓存相互替换问题, 因此需要应用级别的缓存划分技术.为了实现应用级别的缓存划分, Intel提出了RDT技术，其中, CAT(cache allocation technology)可为进程或者CGroup分配私有的缓存空间, 避免缓存相互替换;</p><p>LLC 缓存的命中率和缺失率的计算方式通常如下：</p><ul><li>LLC 缓存命中率 &#x3D; L3_CACHE_HITS &#x2F; L3_CACHE_REFERENCES</li><li>LLC 缓存缺失率 &#x3D; L3_CACHE_MISSES &#x2F; L3_CACHE_REFERENCES</li></ul><p>其中，L3_CACHE_REFERENCES 表示所有访问 LLC 缓存的事件计数器，L3_CACHE_HITS 表示 LLC 缓存的命中事件计数器，L3_CACHE_MISSES 表示 LLC 缓存的缺失事件计数器。</p><p>需要注意的是，这里的事件计数器并不是简单的累加计数器，而是需要进行一定的处理和归一化，才能得到准确的结果。具体处理方法可以参考 <code>pcm-exporter</code> 的文档或者其他类似的文档。</p><p>在container环境下，一个运行在container里面的streaming应用不停的读写数据导致大量的LLC占用，会导致同机器上另外一个container里运行的LS需要的数据被evict出LLC，从而导致LS应用性能下降。</p><p><img src="https://lynnapan.github.io/images/cache/8.png" alt="img"></p><h3 id="指标采集"><a href="#指标采集" class="headerlink" title="指标采集"></a>指标采集</h3><p>几种实现路径：</p><ol><li><p>通过cgroup perf_event获取主机所以的应用的CPI指标</p><p><code>unix.IoctlSetInt(p.fd, unix.PERF_EVENT_IOC_ENABLE, 0)</code></p></li><li><p>通过node perf获取各cpu的CPI指标，这里使用的go的实现，需要LockOS，然后使用perf_event_open</p></li><li><p>cadvisor使用libpfm工具，这里要cgo，其中libpfm底层用的perf_event_open</p></li></ol><h4 id="a-node-exporter"><a href="#a-node-exporter" class="headerlink" title="a. node exporter"></a>a. node exporter</h4><p>目前node exporter只支持cpu级别的metrics，具体<a href="https://github.com/prometheus/node_exporter/pull/1274">实现</a></p><h4 id="b-cgroup-perf-event"><a href="#b-cgroup-perf-event" class="headerlink" title="b. cgroup perf_event"></a>b. cgroup perf_event</h4><p>期望支持cgroup级别的metrics，即可以监测属于某个特定的 group 的所有线程以及运行在特定 CPU 上的线程,<a href="https://lwn.net/Articles/421574/">LWN.net]</a>，其中cadvisor支持cgroup perf_event，具体<a href="https://github.com/google/cadvisor/blob/a52ec5d60cf70b22f8b6d204780aec7a222cf6bb/manager/manager.go">实现</a></p><p>其他指标采集方式：</p><ul><li><a href="https://github.com/intel/pcm">https://github.com/intel/pcm</a></li></ul><h3 id="Intel-RDT"><a href="#Intel-RDT" class="headerlink" title="Intel RDT"></a>Intel RDT</h3><p>以往解决方法是通过控制虚拟机逻辑资源(cgroup)但是调整粒度太粗，并且无法控制处理器缓存这样敏感而且稀缺的资源。为此Intel推出了RDT技术, <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">相关介绍</a></p><p>RDT技术，全称为Resource Director Technology，提供了两种能力：监控和分配。该技术旨在通过一系列的CPU指令从而允许用户直接对每个CPU核心（附加了HT技术后为每个逻辑核心）的L2缓存、L3缓存（LLC）以及内存带宽进行监控和分配。</p><p>Linux Kernel 4.10引入了Intel RDT实现架构，基于 <code>resctrl</code> 文件系统提供了 L3 CAT (Cache Allocation Technology)，L3 CDP(Code and Data Prioritization)，以及L2 CAT。并且Linux Kernel 4.12进一步实现支持了MBA(Memory Bandwidth Allocation)内存带宽分配技术。</p><p>Intel RDT提供了一系列分配(资源控制)能力，包括缓存分配技术(Cache Allocation Technology, CAT)，代码和数据优先级(Code and Data Prioritization, CDP) 以及 内存带宽分配(Memory Bandwidth Allocation, MBA)。</p><p>Intel志强处理器 E5-xxxx v4系列(即Broadwell)提供了L3缓存的配置以及CAT机制，其中部分通讯相关功能在 E5-xxxx v3系列(即Haswell)引入。一些Intel处理器系列(例如Intel Atom处理器系列)可能支持对L2缓存的控制。此外，MBA共功能提供了相应的处理器核心级别的内存带宽管理。</p><p>为了能够在Linux中使用资源分配技术，需要在内核和用户空间引入 <code>resctl</code> 接口。从Linux Kernel 4.10开始，可以使用 L3 CAT, L3 CDP 和 L2 CAT 以及 <code>resctrl</code> 架构。从Linux Kernel 4.12开始，开始引入并正在开发MBA技术, <a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html">内核使用说明</a></p><h5 id="RDT技术架构"><a href="#RDT技术架构" class="headerlink" title="RDT技术架构"></a>RDT技术架构</h5><p>缓存分配技术CAT(Cache Allocation Technology)的核心目标是基于服务级别(Class of Service, COS 或 CLOS)来实现资源分配。应用程序或者独立线程可以按照处理器提供的一系列服务级别来标记。这样就会按照应用程序和线程的服务分类来限制和分配其使用的缓存。每个CLOS可以使用能力掩码(capacity bitmasks, CBMs)来标志并在服务分类中指定覆盖(overlap)或隔离(isolation)的程度。</p><p>对于每个逻辑处理器，都有一个寄存器(被称为 <code>IA32_PQR_ASSOC</code> MSR或PQR)来允许操作系统(OS)或虚拟机管理器(VMM)在应用程序、线程、虚拟机(VM)调度(scheduled)的时候指定它的CLOS。</p><p>RDT分为5个功能模块：</p><ul><li>Cache Monitoring Technology (CMT) 缓存检测技术</li><li>Cache Allocation Technology (CAT) 缓存分配技术</li><li>Memory Bandwidth Monitoring (MBM) 内存带宽监测</li><li>Memory Bandwidth Allocation (MBA) 内存带宽分配</li><li>Code and Data Prioritization (CDP) 代码和数据优先级</li></ul><blockquote><p>RDT技术针对的是缓存和内存带宽，分别又分为监控和控制，就形成了4个功能模块，再加上代码和数据优先级(控制技术)，合起来形成5个功能模块。</p></blockquote><img src="https://pic.imgdb.cn/item/6691e693d9c307b7e995d1f8.png" alt="英特尔® RDT 内核架构" style="zoom:50%;" /><p>启用 RDT 控制后，可在根目录中创建用户目录（“CG1” 和 “CG2”，见图 4：英特尔® RDT 在 resctrl 文件系统中的分层结构），为每个共享资源指定不同的控制力度。RDT 控制组包含以下文件：“tasks”：读取该文件会显示该群组所有任务的列表。将任务 ID 写入文件会添加任务到群组。“cpus”：读取该文件会显示该群组拥有的逻辑 CPU 的位掩码。将掩码写入文件会添加 CPU 到群组或从群组中移除 CPU。“schemata”：该群组可访问的所有资源的列表。</p><p>启用 RDT 监控功能后，根目录和其他顶层目录会包含 “mon_groups” 目录，在此目录中可以创建用户目录（“M1” 和 “M2”，见图 4：英特尔® RDT 在 resctrl 文件系统中的分层结构），以监控任务群组。“Mon_data” 目录包含一组按照资源域和 RDT 事件组织的文件。这些目录中，每个目录针对每个事件都有一个文件（“llc_occupancy”、“mbm_ total_bytes” 和 “mbm_local_bytes”）。这些文件为群组中的所有任务提供了事件当前值的计数器。</p><img src="https://pic.imgdb.cn/item/6691e6c8d9c307b7e9960b0d.png" alt="英特尔® RDT 在 resctrl 文件系统中的监测和控制示意图" style="zoom:50%;" /><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="CPI2-CPU-performance-isolation-for-shared-compute-clusters"><a href="#CPI2-CPU-performance-isolation-for-shared-compute-clusters" class="headerlink" title="CPI2 : CPU performance isolation for shared compute clusters"></a>CPI2 : CPU performance isolation for shared compute clusters</h3><h4 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h4><p>CPI</p><h4 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h4><p>Google的方法，完全基于历史数据做统计分析，不需要单独做压测，方法简单。Google基于历史数据，对CPI与RT的相关性做论证，得出对于<strong>链路处于叶子</strong>，且主要为<strong>CPU型</strong>的应用，相关性有0.97；其他一些服务，如部分IO型，中间节点服务，仍然0.7+的相关性。所以确定CPI可以作为性能的proxy。在线作业通常为常驻作业，这类作业在同一CPU型号的CPI数据走向通常呈现一定规律，是可预测的。所以用传统的<strong>滑动窗口预测</strong>方法，对下一周期的CPI进行预测。</p><img src="https://justadogistaken.github.io/images/cpi-1.png" alt="/images/cpi-1.png" style="zoom:50%;" /><p>并且每天CPI的数据分布都相差不大，所以可以直接用统计的方法，算前一天CPI的平均值CPIavg，及标准差stddev，设置 CPIavg + 2 * stddev为阈值，超过该值，认定QoS受到影响。同时，为了避免误判，规则为5min中内发现3次超过，才确定QoS受到影响。</p><img src="https://justadogistaken.github.io/images/cpi-2.png" alt="/images/cpi-2.png" style="zoom:30%;" /><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>实验数据表明,CPI与干扰相关性线性系数为0.97</p><h3 id="LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management"><a href="#LIBRA-Clearing-the-Cloud-Through-Dynamic-Memory-Bandwidth-Management" class="headerlink" title="LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management"></a>LIBRA: Clearing the Cloud Through Dynamic Memory Bandwidth Management</h3><h4 id="性能指标-1"><a href="#性能指标-1" class="headerlink" title="性能指标"></a>性能指标</h4><p>CPI</p><h4 id="核心方法-1"><a href="#核心方法-1" class="headerlink" title="核心方法"></a>核心方法</h4><p>阿里这篇文章的主要贡献在于如何更细粒度的调整LLC；Intel RDT提供了CAT和MBA两种技术（用法与cgroups相似，由于推入container runtime太慢了，intel专门开源了intel-resource-manager支持他家的黑科技），前者是对LLC size的隔离，后者是对L2-L3内存带宽的隔离；由于两个维度单独调整的粒度都是10%，粒度太粗；阿里通过CAT和MBA结合，实现更细粒度的调整。其中用CPI做干扰检测，但是阿里是用压测的方式计算出；RT与CPI的相关性，构建<code>RT=k*CPI+l</code> like线性方程；从而用实时的CPI，计算出大致的RT值，判断应用QoS是否超过SLA。</p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>CPI与干扰线性相关，根据CPI计算RT，据此调整LLC、MBA等资源隔离</p><h3 id="PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services"><a href="#PARTIES-QoS-Aware-Resource-Partitioning-for-Multiple-Interactive-Services" class="headerlink" title="PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services"></a>PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services</h3><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>Latency(ms)</p><h4 id="核心方法-2"><a href="#核心方法-2" class="headerlink" title="核心方法"></a>核心方法</h4><p>压测得出单应用最合适的Latency-targetQoS（加压直至Latency与压力曲线出现拐点），资源维度为&lt;cpu core， cache way， cpu frequency， mem space， disk bandwidth&gt;，对应的调整粒度为&lt;1 core, 1 way, 100MHz, 1GB, 1GB&#x2F;s&gt;* 500ms检测一次QoS，如果发现与targetQoS偏离过大，则开始调整资源，对每个应用每轮尝试不同的资源up&#x2F;down（等于猜受干扰资源），直至保证了机器所有应用的QoS。文章提供了很好的思路，资源是可交换的，即发现干扰时，不用统一扩容或缩容资源，文章的背景是所有在线应用部署在一个集群里，一台机器各维度资源有限，那就通过交换，比如把io密集型应用的cpu让给cpu密集型的。从而保证每个应用具有合适的资源。</p><p>由于笔者时间、视野、认知有限，本文难免出现错误、疏漏等问题，期待各位读者朋友、业界专家指正交流，上述排障信息已修改为社区内容。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1759977">Caelus—全场景在离线混部解决方案</a></li><li>Google Borg 2015：<a href="https://research.google/pubs/pub43438/">https://research.google/pubs/pub43438/</a></li><li>Google Borg 2019：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387517">https://dl.acm.org/doi/pdf/10.1145/3342195.3387517</a></li><li>Google Autopilot：<a href="https://dl.acm.org/doi/pdf/10.1145/3342195.3387524">https://dl.acm.org/doi/pdf/10.1145/3342195.3387524</a></li><li>百度千寻：<a href="https://www.infoq.cn/article/aEut*ZAIffp0q4MSKDSg">百度大规模战略性混部系统演进</a></li><li>阿里伏羲：<a href="https://yq.aliyun.com/articles/651202">https://yq.aliyun.com/articles/651202</a></li><li>阿里k8s混部：<a href="https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf">https://static.sched.com/hosted_files/kccncosschn19chi/70/ColocationOnK8s.pdf</a></li><li>CPI论文: <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/40737.pdf</a></li><li>Heracles论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43792.pdf</a></li><li>Bubble-up论文：<a href="https://ieeexplore.ieee.org/document/7851476">https://ieeexplore.ieee.org/document/7851476</a></li><li><a href="http://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/08/29/perf-arch">Linux kernel perf architecture (terenceli.github.io)</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/noisy-neighbors-problem-in-kubernetes.html">resolving noisy neighbors (intel.com)</a></li><li><a href="https://facebookmicrosites.github.io/cgroup2/docs/overview.html">Maximizing Resource Utilization with cgroup2</a></li><li><a href="https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/bytedance-performance-evaluation-optimization.html">字节跳动：混布环境下集群的性能评估与优化</a></li><li><a href="https://www.cnblogs.com/tencent-cloud-native/p/14754230.html">混部之殇-论云原生资源隔离技术之CPU隔离(一) </a></li><li><a href="https://patents.google.com/patent/CN106776005A/zh">CN106776005A - 一种面向容器化应用的资源管理系统及方法</a></li><li><a href="https://www.aboutyun.com/thread-27867-1-1.html">阿里K8s之动态解决容器资源按需分配</a></li><li><a href="https://justadogistaken.github.io/posts/handle-interference/">混部场景下的单机服务质量保障 </a></li><li>[Cache高速缓存和缓存隔离](<a href="https://lynnapan.github.io/2017/04/18/understand">https://lynnapan.github.io/2017/04/18/understand</a> Cache&#x2F;)</li><li><a href="https://github.com/hodgesds/perf-utils">hodgesds&#x2F;perf-utils</a></li><li><a href="https://cloud.tencent.com/developer/article/1517979">用CPI火焰图分析Linux性能问题</a></li><li><a href="https://www.openeuler.org/zh/blog/rubik/index.html">openEuler资源利用率提升之道</a></li><li><a href="http://jos.org.cn/html/2020/10/6066.htm">在离线混部作业调度与资源管理技术研究综述</a></li><li><a href="https://qiankunli.github.io/2021/11/22/hybrid_deployment.html">从混部到统一调度</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</title>
    <link href="https://zoues.com/posts/186b05db/"/>
    <id>https://zoues.com/posts/186b05db/</id>
    <published>2024-06-08T13:10:08.000Z</published>
    <updated>2024-07-13T00:50:10.065Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>针对该问题的信息做了部分加工处理,thanks <strong><a href="https://github.com/polarathene">polarathene</a></strong></p></blockquote><p>我们在Fedora系统上将containerd.io从1.4.13版本升级到了1.5.10之后，发现多个项目中所有MySQL 容器实例消耗内存暴涨超过20GB，而在此之前它们仅消耗不到300MB。同事直接上了重启大招，但重启后问题依旧存在。最后选择回滚到1.4.13版本，该现象也随之消失。</p><p>值得注意的是，在Ubuntu 18.04.6系统上运行相同版本的containerd和runc时，MySQL 容器实例一切工作正常。只有在Fedora 35系统(配置相同的containerd与runc版本)，出现了内存消耗异常的情况。下面是出现异常的容器组件版本信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">go1.16.15</span><br><span class="line">containerd: 1.5.11</span><br><span class="line">runc: 1.0.3</span><br></pre></td></tr></table></figure><p>在Fedora 35上，执行以下命令执行会引发系统崩溃：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm mysql:5.7.36</span><br><span class="line">docker run -it --rm mysql:5.5.62</span><br></pre></td></tr></table></figure><p>但是mysql 8.0.29版本在Fedora 35上却运行正常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm mysql:8.0.29</span><br></pre></td></tr></table></figure><p>OOM相关信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2023-06-06T17:23:24.094275-04:00 laptop kernel: oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=user.slice,mems_allowed=0,global_oom,task_memcg=/system.slice/docker-xxx.scope,task=mysqld,pid=38421,uid=0</span><br><span class="line">2023-06-06T17:23:24.094288-04:00 laptop kernel: Out of memory: Killed process 38421 (mysqld) total-vm:16829404kB, anon-rss:12304300kB, file-rss:108kB, shmem-rss:0kB, UID:0 pgtables:28428kB oom_score_adj:0</span><br><span class="line">2022-06-06T17:23:24.094313-04:00 laptop systemd[1]: docker-xxx.scope: A process of this unit has been killed by the OOM killer.</span><br><span class="line">2022-06-06T17:23:24.856029-04:00 laptop systemd[1]: docker-xxx.scope: Deactivated successfully.</span><br></pre></td></tr></table></figure><p>原先在空闲状态下，<code>mysql</code>容器使用内存大约在200MB左右；但在某些操作系统上，如RedHat、Arch Linux或Fedora，一旦为容器设置了非常高的打开文件数（<code>nofile</code>）限制，则可能会导致<code>mysql</code>容器异常地占用大量内存。</p><pre><code>cat /proc/$(pgrep dockerd)/limits | grep &quot;Max open files&quot;cat /proc/$(pgrep containerd)/limits | grep &quot;Max open files&quot;</code></pre><p>如果输出值为1073741816或更高，那么您可能也会遇到类似异常。</p><p>在相关社区，我们发现了类似的案例:</p><h4 id="xinetd-slowly"><a href="#xinetd-slowly" class="headerlink" title="xinetd slowly"></a>xinetd slowly</h4><p>xinetd服务启动极其缓慢，我们查看了dockerd的系统设置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/$(pidof dockerd)/limits | grep &quot;Max open files&quot;</span><br><span class="line">Max open files            1048576              1048576              files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show docker | grep LimitNOFILE</span><br><span class="line">LimitNOFILE=1048576</span><br></pre></td></tr></table></figure><p>但是，在容器内部，则是一个非常巨大的数字——1073741816</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm ubuntu bash -c &quot;cat /proc/self/limits&quot; | grep  &quot;Max open files&quot;</span><br><span class="line">Max open files            1073741816           1073741816           files</span><br></pre></td></tr></table></figure><p><code>xinetd</code>程序在初始化时使用<code>setrlimit(2)</code>设置文件描述符的数量，这会消耗大量的时间及CPU资源去关闭1073741816个文件描述符。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@1b3165886528# strace xinetd</span><br><span class="line">execve(&quot;/usr/sbin/xinetd&quot;, [&quot;xinetd&quot;], 0x7ffd3c2882e0 /* 9 vars */) = 0</span><br><span class="line">brk(NULL)                               = 0x557690d7a000</span><br><span class="line">arch_prctl(0x3001 /* ARCH_??? */, 0x7ffee17ce6f0) = -1 EINVAL (Invalid argument)</span><br><span class="line">mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb14255c000</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">close(12024371)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024372)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024373)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024374)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024375)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024376)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024377)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024378)                         = -1 EBADF (Bad file descriptor)</span><br></pre></td></tr></table></figure><h4 id="yum-hang"><a href="#yum-hang" class="headerlink" title="yum hang"></a>yum hang</h4><p>从docker社区获取Rocky Linux 9对应的Docker版本，在容器中执行yum操作时速度非常缓慢。</p><p>在CentOS 7和Rocky Linux 9宿主机上，我们都进行了以下操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><p>在CentOS 7宿主机上，耗时在2分钟左右； 而在Rocky Linux 9上，一个小时也未能完成。</p><p>复现步骤如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><h4 id="rpm-slow"><a href="#rpm-slow" class="headerlink" title="rpm slow"></a>rpm slow</h4><p>在宿主机上执行下述命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion</span><br></pre></td></tr></table></figure><p>消耗的各类时间如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m11.248s</span><br><span class="line">user    0m7.316s</span><br><span class="line">sys     0m1.932s</span><br></pre></td></tr></table></figure><p>在容器中执行测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --net=none --log-driver=none -v &quot;/workspace:/workspace&quot; -v &quot;/disks:/disks&quot; opensuse bash -c &quot;time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion&quot;</span><br></pre></td></tr></table></figure><p>消耗的各类时间激增：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m31.089s</span><br><span class="line">user    0m14.876s</span><br><span class="line">sys     0m12.524s</span><br></pre></td></tr></table></figure><p>我们找到了RPM的触发问题的根因，其属于RPM内部POSIX lua库 <a href="https://github.com/rpm-software-management/rpm/commit/7a7c31f">rpm-software-management&#x2F;rpm@<code>7a7c31f</code></a>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">static int Pexec(lua_State *L) /** exec(path,[args]) */</span><br><span class="line">&#123;</span><br><span class="line">/* ... */</span><br><span class="line">open_max = sysconf(_SC_OPEN_MAX);</span><br><span class="line">if (open_max == -1) &#123;</span><br><span class="line">    open_max = 1024;</span><br><span class="line">&#125;</span><br><span class="line">for (fdno = 3; fdno &lt; open_max; fdno++) &#123;</span><br><span class="line">    flag = fcntl(fdno, F_GETFD);</span><br><span class="line">    if (flag == -1 || (flag &amp; FD_CLOEXEC))</span><br><span class="line">continue;</span><br><span class="line">    fcntl(fdno, F_SETFD, FD_CLOEXEC);</span><br><span class="line">&#125;</span><br><span class="line">/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类似的，如果设置的最大打开文件数限制过高，那么<code>luaext/Pexec()</code>和<code>lib/doScriptExec()</code>在尝试为所有这些文件描述符设置<code>FD_CLOEXEC</code>标志时，会花费过多的时间，从而导致执行如<code>rpm</code>或<code>dnf</code>等命令的时间显著增加。</p><h4 id="PtyProcess-spawn-slowdown-in-close-loop"><a href="#PtyProcess-spawn-slowdown-in-close-loop" class="headerlink" title="PtyProcess.spawn slowdown in close() loop"></a>PtyProcess.spawn slowdown in close() loop</h4><p>ptyprocess存在问题的相关代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Do not allow child to inherit open file descriptors from parent, </span><br><span class="line"> # with the exception of the exec_err_pipe_write of the pipe </span><br><span class="line"> # and pass_fds. </span><br><span class="line"> # Impose ceiling on max_fd: AIX bugfix for users with unlimited </span><br><span class="line"> # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange() </span><br><span class="line"> # occasionally raises out of range error </span><br><span class="line"> max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0]) </span><br><span class="line"> spass_fds = sorted(set(pass_fds) | &#123;exec_err_pipe_write&#125;) </span><br><span class="line"> for pair in zip([2] + spass_fds, spass_fds + [max_fd]): </span><br><span class="line">     os.closerange(pair[0]+1, pair[1]) </span><br></pre></td></tr></table></figure><p>当处理文件描述符时，为了提高效率，应避免遍历所有可能的文件描述符来关闭它们，尤其是在Linux系统上，因为这会通过<code>close()</code>系统调用消耗大量时间。尤其是当打开文件描述符的限制（可以通过<code>ulimit -n</code>、<code>RLIMIT_NOFILE</code>或<code>SC_OPEN_MAX</code>查看）被设置得非常高时，这种遍历方式将导致数百万次不必要的系统调用，显著增加了处理时间。</p><p>一个更为高效的解决方案是仅关闭那些实际上已打开的文件描述符。在Python 3中，<code>subprocess</code>模块已经实现了这一功能，而对于使用Python 2的用户，<code>subprocess32</code>的兼容库可以作为回退选项。通过利用这些库或类似的技术，我们可以显著减少不必要的系统调用，从而提高程序的运行效率。</p><h3 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h3><h4 id="1-RLIMIT-NOFILE"><a href="#1-RLIMIT-NOFILE" class="headerlink" title="1. RLIMIT_NOFILE"></a>1. RLIMIT_NOFILE</h4><blockquote><p><a href="https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94">https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94</a></p></blockquote><p>当前Linux内核对于用户空间进程的RLIMIT_NOFILE资源限制默认设置为1024（软限制）和4096（硬限制）。以前，systemd在派生进程时会直接传递这些未修改的限制。在systemd240版本中，systemd传递的硬限制增加到了512K，其覆盖了内核的默认值，并大大增加了非特权用户空间进程可以同时分配的文件描述符数量。</p><p>注意，为了兼容性考虑，软限制仍保持在1024，传统的UNIX select()调用无法处理大于或等于1024的文件描述符（FD_SET宏不管是否越界以及越界的后果，fd_set也并非严格限制在1024，FD_SET超过1024的值，会造成越界），因此如果全局提升了软限制，那么在使用select()时可能出现异常（在现代编程中，程序不应该再使用select()，而应该选择poll()&#x2F;epoll，但遗憾的是这个调用仍然大规模存在）。</p><p>在较新的内核中，分配大量文件描述符在内存和性能上比以前消耗少得多。Systemd社区中有用户称在实际应用中他们使用了约30万个文件描述符，因此Systemd认为512K作为新的默认值是足够高的。但是需要注意的是，也有报告称使用非常高的硬限制（例如1G）是有问题的，因此，超高硬限制会触发部分应用程序中过大的内存分配。</p><h4 id="2-File-Descriptor-Limits"><a href="#2-File-Descriptor-Limits" class="headerlink" title="2. File Descriptor Limits"></a>2. File Descriptor Limits</h4><p>最初，文件描述符（fd）主要用于引用打开的文件和目录等资源。如今，它们被用来引用Linux用户空间中几乎所有类型的运行时资源，包括打开的设备、内存（<a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd_create(2)</code></a>）、定时器（<a href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"><code>timefd_create(2)</code></a>）甚至进程（通过新的<a href="https://man7.org/linux/man-pages/man2/pidfd_open.2.html"><code>pidfd_open(2)</code></a>系统调用）。文件描述符的广泛应用使得“万物皆文件描述符”成为UNIX的座右铭。</p><p>由于文件描述符的普及，现代软件往往需要同时处理更多的文件描述符。与Linux上的大多数运行时资源一样，文件描述符也有其限制：一旦达到通过<a href="https://man7.org/linux/man-pages/man2/getrlimit.2.html"><code>RLIMIT_NOFILE</code></a>配置的限制，任何进一步的分配尝试都会被拒绝，并返回<code>EMFILE</code>错误，除非关闭一些已经打开的文件描述符。</p><p>以前文件描述符的限制普遍较低。当Linux内核首次调用用户空间时，<code>RLIMIT_NOFILE</code>的默认值设置为软限制1024和硬限制4096。软限制是实际生效的限制，可以通过程序自身调整到硬限制，但超过硬限制则需要更高权限。1024个文件描述符的限制使得文件描述符成为一种稀缺资源，导致开发者在使用时非常谨慎。这也引发了一些次要描述符的使用，例如inotify观察描述符，以及代码中频繁的文件描述符关闭操作（例如<code>ftw()</code>&#x2F;<code>nftw()</code>），以避免达到限制。</p><p>一些操作系统级别的API在设计时只考虑了较低的文件描述符限制，例如BSD&#x2F;POSIX的<a href="https://man7.org/linux/man-pages/man2/select.2.html"><code>select(2)</code></a>系统调用，它只能处理数字范围在0到1023内的文件描述符。如果文件描述符超出这个范围，<code>select()</code>将越界出现异常。</p><p>Linux中的文件描述符以整数形式暴露，并且通常分配为最低未使用的整数，随着文件描述符用于引用各种资源（例如eBPF程序、cgroup等），确实需要提高这个限制。</p><p>在2019年的systemd v240版本中，采取了一些措施：</p><ul><li>在启动时，自动将两个系统控制参数<code>fs.nr_open</code>和<code>fs.file-max</code>设置为最大值，使其实际上无效，从而简化了配置。</li><li>将<code>RLIMIT_NOFILE</code>的硬限制大幅提高到512K。</li><li>保持<code>RLIMIT_NOFILE</code>的软限制为1024，以避免破坏使用<code>select()</code>的程序。但每个程序可以自行将软限制提高到硬限制，无需特权。</li></ul><p>通过这种方法，文件描述符变得不再稀缺，配置也更简便。程序可以在启动时自行提高软限制，但要确保避免使用<code>select()</code>。</p><p>具体建议如下：</p><ol><li>**不要再使用<code>select()</code>**。使用<code>poll()</code>、<code>epoll</code>、<code>io_uring</code>等更现代的API。</li><li><strong>如果程序需要大量文件描述符</strong>，在启动时将<code>RLIMIT_NOFILE</code>的软限制提高到硬限制，但确保避免使用<code>select()</code>。</li><li><strong>如果程序会fork出其他程序</strong>，在fork之前将<code>RLIMIT_NOFILE</code>的软限制重置为1024，因为子进程可能无法处理高于1024的文件描述符。</li></ol><p>这些建议能帮助你在处理大量文件描述符时避免常见问题。</p><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p><strong>supervisord</strong></p><ul><li>在2011年，<a href="https://github.com/Supervisor/supervisor/issues/26"><code>supervisord</code>报告了一个与<code>select()</code>相关的问题</a>，并在2014年得到修复。这表明<code>supervisord</code>早期版本可能使用了<code>select()</code>，但后续版本已更新。</li></ul><p><strong>Nginx</strong></p><ul><li>Nginx允许用户通过配置提高文件描述符的软限制。2015年的<a href="https://github.com/nginx/nginx/issues/311">bug报告</a>指出了Nginx在某些情况下使用<code>select()</code>并受限于1024个文件描述符的问题。目前，提供了多种方法来处理高并发场景。</li></ul><p><strong>Redis</strong></p><ul><li>Redis文档建议使用高达2^16的文件描述符数量，具体取决于实际工作负载。<ul><li><a href="https://github.com/redis/redis-py/issues/419#issuecomment-41134427">2013年12月，<code>redis-py</code>的<code>select()</code>问题</a>，在2014年6月修复。</li><li>2015年<a href="https://github.com/redis/hiredis/issues/385#issue-123387718"><code>redis/hiredis</code>的问题</a>，用户依赖<code>select()</code>。</li><li><a href="https://blog.pjam.me/posts/select-syscall-in-rust/">2020年11月的文章</a>提到Redis仍将<code>select()</code>作为后备方案，参考了<a href="https://github.com/redis/redis/blob/e12f2decc1cf7742878d516d89d38af178119b17/src/ae_select.c"><code>ae_select.c</code></a>文件。</li></ul></li></ul><p><strong>Apache HTTP Server</strong></p><ul><li>2002年的<a href="https://github.com/apache/httpd/commit/d7bff9e33d304ff95e2b888fd1f0e3b56a62e041">commit</a>显示了Apache HTTP Server早期使用<code>select()</code>。尽管Apache后续增加了对其他I&#x2F;O多路复用机制的支持，但在处理较低并发连接时，仍可能使用<code>select()</code>。</li></ul><p><strong>PostgreSQL</strong></p><ul><li>PostgreSQL没有硬限制，以避免对其他运行的软件产生负面影响。在容器化环境中，这个问题不太严重，因为可以为容器设置适当的限制。PostgreSQL提供了一个配置选项<a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS"><code>max_files_per_process</code></a>，限制每个进程可以打开的最大文件数。</li><li>PostgreSQL的源代码中仍然有使用<code>select()</code>的地方。</li></ul><p><strong>MongoDB</strong></p><ul><li>2014年，<a href="https://narkive.com/CrxObBlI.2">MongoDB仍在使用<code>select()</code></a>。在<a href="https://github.com/mongodb/mongo/blob/91fc5673cab5d1267fd805f1375577df9072ea1b/src/mongo/util/net/listen.cpp#L252-L270">3.7.5版本</a>中，<code>select()</code>仍在<code>listen.cpp</code>中使用，但在<a href="https://github.com/mongodb/mongo/commit/55aac9ac800531ad021f18f56d69c69ac5619245">3.7.6版本</a>（<strong>2018年4月</strong>）中被移除。不过，<a href="https://github.com/mongodb/mongo/blob/8de341d0d2011b51eb1d140fb4820424d29fe510/src/mongo/transport/asio/asio_utils.cpp#L131">MongoDB的源代码</a>中仍然存在<code>select()</code>的调用。</li></ul><h3 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h3><p>虽然 cgroup 控制器在现代资源管理中起着重要作用，但 <code>ulimit</code> 作为一种传统的资源管理机制，依然不可或缺。</p><p>在容器中，默认的 <code>ulimit</code> 设置是从 <code>containerd</code> 继承的（而非 <code>dockerd</code>），这些设置在 <code>containerd.service</code> 的 systemd 单元文件中被配置为无限制：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep ^Limit /lib/systemd/system/containerd.service</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>虽然这些设置满足 <code>containerd</code> 自身的需求，但对于其运行的容器来说，这样的配置显得过于宽松。相比之下，主机系统上的用户（包括 root 用户）的 <code>ulimit</code> 设置则相当保守（以下是来自 Ubuntu 18.04 的示例）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sh复制代码$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 62435</span><br><span class="line">max locked memory       (kbytes, -l) 16384</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 62435</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure><p>这种宽松的容器设置可能会引发一系列问题，例如容器滥用系统资源，甚至导致 DoS 攻击。尽管 cgroup 限制通常用于防止这些问题，但将 <code>ulimit</code> 设置为更合理的值也是必要的。</p><p>特别是 <code>RLIMIT_NOFILE</code>（打开文件的数量限制）被设置为 2^30（即 1073741816），这会导致一些程序运行缓慢，因为这些程序会遍历所有可能打开的文件描述符，并在每次 fork&#x2F;exec 之前关闭这些文件描述符（或设置 CLOEXEC 位）。以下是一些具体情况：</p><ul><li><strong>rpm</strong>：在<a href="https://github.com/moby/moby/issues/23137">安装 RPM 以创建新的 Docker 镜像时性能缓慢 #23137</a> 和 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1537564">Red Hat Bugzilla #1537564</a> 中有报告，修复方案为：<a href="https://github.com/rpm-software-management/rpm/pull/444">优化并统一在文件描述符上设置 CLOEXEC 的 rpm-software-management&#x2F;rpm#444</a>（在 Fedora 28 中修复）。</li><li><strong>python2</strong>：在 <a href="https://github.com/docker/for-linux/issues/502">Docker 18.09 上 PTY 进程的创建速度大大降低 #502</a> 中有报告，建议的修复方案为：<a href="https://github.com/python/cpython/pull/11584">subprocess.Popen: 在 Linux 上优化 close_fds python&#x2F;cpython#11584</a>（由于 python2 已经冻结，所以此修复方案不会被采用）。</li><li><strong>python 的 pexpect&#x2F;ptyprocess 库</strong>：在 <a href="https://github.com/pexpect/ptyprocess/issues/50">PtyProcess.spawn（以及因此 pexpect）在 close() 循环中速度降低 #50</a> 中有报告。</li></ul><p>逐一解决这些问题既复杂且收益低，其中一些软件已经过时，另外有一些软件难以修复。上述列表并不全面，可能还有更多类似的问题尚未觉察到。</p><h4 id="探究资源消耗"><a href="#探究资源消耗" class="headerlink" title="探究资源消耗"></a>探究资源消耗</h4><p><code>2^16</code>（65k）个<code>busybox</code>容器的预估资源使用情况如下所示：</p><ul><li>在 <code>containerd</code> 中，共需 688k 个任务和 206 GB（192 GiB）的内存（每个容器约需 10.5 个任务和 3 MiB 的内存）。</li><li>至少需要将 <code>containerd.service</code> 的 <code>LimitNOFILE</code> 设置为 262144。</li><li>打开的文件数达到 249 万（其中<code>fs.file-nr</code> 必须低于 <code>fs.file-max</code> 限制），每个容器大约需要 38 个文件描述符。</li><li>容器的 cgroup 需要 25 GiB 的内存（每个容器大约需要 400 KiB）。</li></ul><p>因此<code>LimitNOFILE=524288</code>（自 v240 版本以来，systemd 的默认值）对于大多数系统作为默认值已经足够，其能满足 <code>docker.service</code> 和 <code>containerd.service</code> 支持 65k 个容器的资源需求。</p><p>从GO 1.19开始将隐式地将 <code>fork</code> &#x2F; <code>exec</code> 进程的软限制恢复到默认值。在此之前，Docker 守护进程可以通过配置 <code>default-ulimit</code> 设置来强制容器使用 <code>1024</code> 的软限制。</p><ol><li>测试详情</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fedora 37 VM 6.1.9 kernel x86_64 (16 GB memory)</span><br><span class="line">Docker v23, containerd 1.6.18, systemd v251</span><br><span class="line"></span><br><span class="line"># Additionally verified with builds before Go 1.19 to test soft limit lower than the hard limit:</span><br><span class="line">dnf install docker-ce-3:20.10.23 docker-ce-cli-1:20.10.23 containerd.io-1.6.8</span><br></pre></td></tr></table></figure><p>在Fedora 37 VM上大约有 1800 个文件描述符被打开（<code>sysctl fs.file-nr</code>）。通过 shell 循环运行 <code>busybox</code> 容器直到失败，并调整 <code>docker.service</code> 和 <code>containerd.service</code> 的 <code>LimitNOFILE</code> 来收集测试数据：</p><ul><li><code>docker.service</code> - <code>6:1</code> 的比例（使用 <code>--network=host</code> 时是 <code>5:1</code>），在 <code>LimitNOFILE=5120</code> 下大约能运行 853 个容器（使用主机网络时为 1024）。</li><li><code>containerd.service</code> - <code>4:1</code> 的比例（未验证 <code>--network=host</code> 是否会降低了比例），<code>LimitNOFILE=1024</code> 能支持 256 个容器，前提是 <code>docker.service</code> 的 <code>LimitNOFILE</code> 也足够高（如 <code>LimitNOFILE=2048</code>）。</li></ul><p>每个容器的资源使用模式：</p><ul><li>每个容器的 systemd <code>.scope</code> 有 1 个任务和大约 400 KiB 的内存（<code>alpine</code> 和 <code>debian</code> 稍少）。</li><li>每个容器增加了 10.5 个任务和 3 MiB 的内存。</li><li>每个正在运行的容器大约打开了 38 个文件。</li></ul><p>在 <code>docker.service</code> 中设置 <code>LimitNOFILE=768</code>，然后执行 <code>systemctl daemon-reload &amp;&amp; systemctl restart docker</code>。通过 <code>cat /proc/$(pidof dockerd)/limits</code> 确认该限制是否已应用。</p><p>运行以下命令列出：</p><ul><li>正在运行的容器数量。</li><li>打开的文件数量。</li><li><code>containerd</code> 和 <code>dockerd</code> 守护进程分别使用的任务和内存数量。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Useful to run before the loop to compare against output after the loop is done</span><br><span class="line">(pgrep containerd-shim | wc -l) &amp;&amp; sysctl fs.file-nr \</span><br><span class="line">  &amp;&amp; (echo &#x27;Containerd service:&#x27; &amp;&amp; systemctl status containerd | grep -E &#x27;Tasks|Memory&#x27;) \</span><br><span class="line">  &amp;&amp; (echo &#x27;Docker service:&#x27; &amp;&amp; systemctl status docker | grep -E &#x27;Tasks|Memory&#x27;)</span><br></pre></td></tr></table></figure><p>运行以下循环时，最后几个容器将失败，大约创建 123 个容器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># When `docker.service` limit is the bottleneck, you may need to `CTRL + C` to exit the loop</span><br><span class="line"># if it stalls while waiting for new FDs once exhausted and outputting errors:</span><br><span class="line">for i in $(seq 1 130); do docker run --rm -d busybox sleep 180; done</span><br></pre></td></tr></table></figure><p>可以添加额外的选项：</p><ul><li><code>--network host</code>：避免每次 <code>docker run</code> 时向默认的 Docker 桥接器创建新的 veth 接口（参见 <code>ip link</code>）。</li><li><code>--ulimit &quot;nofile=1023456789&quot;</code>：不会影响内存使用，但在基于 Debian 的发行版中，值高于 <code>fs.nr_open</code>（1048576）将失败，请使用该值或更低的值。</li><li><code>--cgroup-parent=LimitTests.slice</code>：类似 <code>docker stats</code> 但与其他容器隔离，<code>systemd-cgtop</code> 报告内存使用时包括磁盘缓存（可使用 <code>sync &amp;&amp; sysctl vm.drop_caches=3</code> 清除）。</li></ul><p>为更好了解所有创建容器的资源使用情况，创建一个用于测试的临时 slice：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/LimitTests.slice</span><br><span class="line">systemd-cgtop --order=memory LimitTests.slice</span><br></pre></td></tr></table></figure><p>显示整个 slice 和每个容器的内存使用情况，一个 <code>busybox</code> 容器大约使用 400 KiB 的内存。</p><ol start="2"><li>限制对子进程的影响</li></ol><p>我原以为子进程会继承父进程的文件描述符（FD）限制。然而实际却是，每个进程继承限制但有独立的计数。</p><ul><li>可以通过以下命令观察 <code>dockerd</code> 和 <code>containerd</code> 进程打开的文件描述符数量：<code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>。</li><li>这不适用于负责容器的 <code>containerd-shim</code> 进程，所以 <code>ls -1 /proc/$(pgrep --newest --exact containerd-shim)/fd | wc -l</code> 不会有用。</li></ul><p>为了验证这一点，可以运行以下测试容器：<code>docker run --rm -it --ulimit &quot;nofile=1024:1048576&quot; alpine bash</code>。然后尝试以下操作：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹并添加许多文件：</span></span><br><span class="line"><span class="built_in">mkdir</span> /tmp/test &amp;&amp; <span class="built_in">cd</span> /tmp/test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建空文件：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 3 2048); <span class="keyword">do</span> <span class="built_in">touch</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开文件并指定文件描述符：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 1000 1030); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>&quot;</span>; <span class="built_in">eval</span> <span class="string">&quot;exec <span class="variable">$&#123;x&#125;</span>&lt; <span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"><span class="comment"># 因为软限制在 1024，所以会失败。提高限制：</span></span><br><span class="line"><span class="built_in">ulimit</span> -Sn 2048</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在前面的循环将成功。</span></span><br><span class="line"><span class="comment"># 你可以覆盖整个初始软限制范围（不包括 FDs 0-2：stdin、stdout、stderr）：</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(<span class="built_in">seq</span> 3 1024); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;x&#125;</span>&quot;</span>; <span class="built_in">eval</span> <span class="string">&quot;exec <span class="variable">$&#123;x&#125;</span>&lt; <span class="variable">$&#123;x&#125;</span>.tmp&quot;</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多个容器进程/子进程打开尽可能多的文件：</span></span><br><span class="line"><span class="comment"># 可以在新 shell 进程中运行相同的循环 `ash -c &#x27;for ... done&#x27;`</span></span><br><span class="line"><span class="comment"># 或通过另一个终端的 `docker exec` 进入容器并在 `/tmp/test` 再次运行循环。</span></span><br><span class="line"><span class="comment"># 每个进程可以根据其当前软限制打开文件，`dockerd`、`containerd` 或容器的 PID 1 的限制无关。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"><span class="comment">### 提示 ###</span></span><br><span class="line"><span class="comment">############</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以观察当前应用的限制：</span></span><br><span class="line"><span class="built_in">cat</span> /proc/self/limits</span><br><span class="line"><span class="comment"># 如果未达到软限制（由于管道），这将报告已使用的限制：</span></span><br><span class="line"><span class="built_in">ls</span> -1 /proc/self/fd | <span class="built_in">wc</span> -l</span><br><span class="line"><span class="comment"># 否则，若这是唯一运行的 `ash` 进程，可以查询其 PID 获取信息：</span></span><br><span class="line"><span class="built_in">ls</span> -1 /proc/$(pgrep --newest --exact ash)/fd | <span class="built_in">wc</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 容器中的进程数：</span></span><br><span class="line"><span class="comment"># `docker stats` 列出容器的 PIDs 数量，</span></span><br><span class="line"><span class="comment"># `systemd-cgtop` 的 Tasks 列也报告相同值。</span></span><br><span class="line"><span class="comment"># 或者如果知道 cgroup 名称，如 `docker-&lt;CONTAINER_ID&gt;.scope`：</span></span><br><span class="line"><span class="comment"># （注意：路径可能因 `--cgroup-parent` 不同）</span></span><br><span class="line"><span class="built_in">cat</span> /sys/fs/cgroup/system.slice/docker-&lt;CONTAINER_ID&gt;.scope/pids.current</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出进程及其 PIDs：</span></span><br><span class="line"><span class="comment"># 对于单个容器，可以可视化进程树：</span></span><br><span class="line">pstree --arguments --show-pids $(pgrep --newest --exact containerd-shim)</span><br><span class="line"><span class="comment"># 或者如果知道 cgroup 名称，如 `docker-&lt;CONTAINER_ID&gt;.scope`：</span></span><br><span class="line">systemd-cgls --unit docker-&lt;CONTAINER_ID&gt;.scope</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察内存监控中的磁盘缓存，通过创建 1GB 文件：</span></span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=bigfile bs=1M count=1000</span><br><span class="line">free -h</span><br><span class="line"><span class="comment"># `systemd-cgtop` 会将此容器的内存使用量增加 1GB，</span></span><br><span class="line"><span class="comment"># 而 `docker stats` 仅增加约 30MiB（按比例）。</span></span><br><span class="line"><span class="comment"># 在容器外清除缓存后再次观察内存使用情况：</span></span><br><span class="line"><span class="built_in">sync</span> &amp;&amp; sysctl vm.drop_caches=3</span><br></pre></td></tr></table></figure><ol start="3"><li>结果观察</li></ol><ul><li>每个进程将这些文件描述符添加到 <code>fs.file-nr</code> 返回的打开文件计数中，并在该进程关闭时释放它们。</li><li>重新运行同一进程的循环不会变化，因为文件已经被计算为该进程打开的。</li><li>这涉及到内存成本：<ul><li>每个通过 <code>touch</code> 创建的文件大约占用 <code>2048</code> 字节（仅在打开前占用磁盘缓存）。</li></ul></li><li>每个打开的文件（每个文件描述符引用都会使 <code>fs.file-nr</code> 增加）大约需要 <code>512</code> 字节的内存。<ul><li>以这种方式创建 512k 个文件大约会占用 1.1 GiB 的内存（当至少有一个文件描述符打开时，使用 <code>sysctl vm.drop_caches=3</code> 也不会释放），每个进程打开等量的文件描述符还会额外使用 250 MiB（262 MB）。</li></ul></li></ul><ol start="4"><li>错误处理</li></ol><p>这些问题主要与系统服务的文件描述符限制有关，不同服务的限制耗尽会导致不同错误。</p><p>有时这会导致任何<code>docker</code>命令（如<code>docker ps</code>）挂起（守护进程耗尽限制）。常见现象包括：</p><ul><li>容器未运行（*<code>pgrep containerd-shim</code>没有输出，但<code>docker ps</code>列出的容器超出预期的退出时间*）。</li><li>容器在<code>containerd-shim</code>进程中占用内存，即使执行了<code>systemctl stop docker containerd</code>。有时需要<code>pkill containerd-shim</code>来清理，并且<code>systemctl start docker containerd</code>会在<code>journalctl</code>中记录错误，处理已死的shims的清理（<em>根据容器数量，这可能会超时，需要再次启动<code>containerd</code>服务</em>）。</li><li>即使排除了所有这些因素，仍然有额外的几百MB内存使用。由于它似乎不属于任何进程，推测是内核内存。我尝试运行的最大容器数量大约是1600个左右。</li></ul><h4 id="docker-service超出限制"><a href="#docker-service超出限制" class="headerlink" title="docker.service超出限制"></a><code>docker.service</code>超出限制</h4><p>每次<code>docker run</code>时，系统会输出不同的错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0000] Error waiting for container: container caff476371b6897ef35a95e26429f100d0d929120ff1abecc8a16aa674d692bf: driver &quot;overlay2&quot; failed to remove root filesystem: open /var/lib/docker/overlay2/35f26ec862bb91d7c3214f76f8660938145bbb36eda114f67e711aad2be89578-init/diff/etc: too many open files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: time=&quot;2023-03-12T02:26:20Z&quot; level=fatal msg=&quot;failed to create a netlink handle: could not get current namespace while creating netlink socket: too many open files&quot;: unknown.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to initialize logging driver: open /var/lib/docker/containers/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f-json.log: too many open files.</span><br></pre></td></tr></table></figure><h4 id="containerd-service限制超出"><a href="#containerd-service限制超出" class="headerlink" title="containerd.service限制超出"></a><code>containerd.service</code>限制超出</h4><p>我也观察到一些类似的错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to start shim: start failed: : pipe2: too many open files: unknown.</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>2023年8月</strong>：在<code>docker.service</code>中<a href="https://github.com/moby/moby/pull/42373">移除了<code>LimitNOFILE=infinity</code></a>。</li><li><strong>2021年5月</strong>：<code>LimitNOFILE=infinity</code> 和 <code>LimitNPROC=infinity</code> <a href="https://github.com/moby/moby/pull/42373">重新添加回<code>docker.service</code></a>，以与Docker CE的配置同步。<ul><li>这个PR是一个合并提交，源自<a href="https://github.com/moby/moby/commit/80039b4699e36ceb0eb81109cd1686aaa805c5ec">2018年9月的提交</a>。</li></ul></li><li><strong>2016年7月</strong>：<a href="https://github.com/moby/moby/pull/24555"><code>LimitNOFILE=infinity</code>更改为<code>LimitNOFILE=1048576</code></a>。<ul><li>讨论引用了<a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible">2009年StackOverflow上的回答</a>，关于特定发行版&#x2F;内核中<code>infinity</code>被限制为<code>2^20</code>。今天的一些系统上，这个上限可以是1024倍更高（*<code>2^30 == 1073741816</code>，超过10亿*）。</li></ul></li><li><strong>2016年7月</strong>：<a href="https://github.com/moby/moby/pull/24307"><code>LimitNOFILE</code>和<code>LimitNPROC</code>从<code>1048576</code>更改为<code>infinity</code></a>。<ul><li>这个PR不久后撤回了对<code>LimitNOFILE</code>的更改。</li></ul></li><li><strong>2014年3月</strong>：<a href="https://github.com/moby/moby/pull/4455#issuecomment-36679884">原始<code>LimitNOFILE</code> + <code>LimitNPROC</code>以<code>1048576</code>添加</a>。<ul><li>链接的PR评论提到这个<code>2^20</code>的值已经高于Docker所需。</li></ul></li></ul><p><strong>当前状态：</strong></p><ul><li>在Docker v25之前，<code>LimitNOFILE=infinity</code>仍然是默认设置，除非将其回退。</li><li><code>containerd</code> 已经<a href="https://github.com/containerd/containerd/pull/8924">合并了相应的更改</a>，从他们的systemd服务文件中移除了<code>LimitNOFILE</code>设置。</li></ul><h4 id="Systemd-240"><a href="#Systemd-240" class="headerlink" title="Systemd &lt; 240"></a>Systemd &lt; 240</h4><p>在某些systemd版本中，设置<code>LimitNOFILE</code>为无穷大可能导致它被限制为65536。请检查服务配置：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># ulimit -n -u</span></span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">max user processes              (-u) 499403</span><br></pre></td></tr></table></figure><p>containerd的systemd服务配置如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /usr/lib/systemd/system/containerd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target local-fs.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line">Type=notify</span><br><span class="line">Delegate=<span class="built_in">yes</span></span><br><span class="line">KillMode=process</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">OOMScoreAdjust=-999</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>查看配置对docker和containerd进程的影响：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># cat /proc/$(pidof dockerd)/limits</span></span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max open files            1048576              1048576              files     </span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]<span class="comment"># cat /proc/$(pidof containerd)/limits</span></span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max open files            1048576              1048576              files     </span><br></pre></td></tr></table></figure><p>这个补丁使systemd查看<code>/proc/sys/fs/nr_open</code>来找到内核中编译的当前最大打开文件数，并尝试将<code>RLIMIT_NOFILE</code>的最大值设置为此值。这样做的好处是所选的限制值不太随意，并且改善了在设置了rlimit的容器中systemd的行为。</p><p>详细讨论见：<a href="https://github.com/systemd/systemd/issues/6559">systemd GitHub issue</a>。</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li><a href="https://github.com/moby/moby/issues/45838">https://github.com/moby/moby/issues/45838</a></li><li><a href="https://github.com/moby/moby/issues/23137">https://github.com/moby/moby/issues/23137</a></li><li><a href="https://0pointer.net/blog/file-descriptor-limits.html">https://0pointer.net/blog/file-descriptor-limits.html</a></li><li><a href="https://www.codenong.com/cs105896693/">https://www.codenong.com/cs105896693/</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li><li><a href="https://github.com/cri-o/cri-o/issues/7703">https://github.com/cri-o/cri-o/issues/7703</a></li><li><a href="https://github.com/envoyproxy/envoy/issues/31502">https://github.com/envoyproxy/envoy/issues/31502</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="container" scheme="https://zoues.com/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>Why does RLIMIT_NOFILE slow down your containerized application</title>
    <link href="https://zoues.com/posts/5cdbe8ce/"/>
    <id>https://zoues.com/posts/5cdbe8ce/</id>
    <published>2024-04-30T05:32:59.000Z</published>
    <updated>2024-04-30T06:11:34.615Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>thanks <strong><a href="https://github.com/polarathene">polarathene</a></strong></p></blockquote><h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>The max open files limit <code>NOFILE</code> of dockerd is 1048576, which is defined in dockerd’s systemd unit file.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/$(pidof dockerd)/limits | grep &quot;Max open files&quot;</span><br><span class="line">Max open files            1048576              1048576              files</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show docker | grep LimitNOFILE</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNOFILESoft=1048576</span><br></pre></td></tr></table></figure><p>However, inside the container, the value of the limit is a very large number — 1073741816:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm ubuntu bash -c &quot;cat /proc/self/limits&quot; | grep  &quot;Max open files&quot;</span><br><span class="line">Max open files            1073741816           1073741816           files</span><br></pre></td></tr></table></figure><p>It may cause the program iterate all available fds until the limit is reached; for example, the <code>xinetd</code> program sets the number of file descriptors using <code>setrlimit(2)</code> at initialization, which causes unnecessary waste of CPU resources and time on closing 1073741816 fds.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@1b3165886528# strace xinetd</span><br><span class="line">execve(&quot;/usr/sbin/xinetd&quot;, [&quot;xinetd&quot;], 0x7ffd3c2882e0 /* 9 vars */) = 0</span><br><span class="line">brk(NULL)                               = 0x557690d7a000</span><br><span class="line">arch_prctl(0x3001 /* ARCH_??? */, 0x7ffee17ce6f0) = -1 EINVAL (Invalid argument)</span><br><span class="line">mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb14255c000</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">close(12024371)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024372)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024373)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024374)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024375)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024376)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024377)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024378)                         = -1 EBADF (Bad file descriptor)</span><br></pre></td></tr></table></figure><p>we found similar cases:</p><h3 id="yum-hang"><a href="#yum-hang" class="headerlink" title="yum hang"></a>yum hang</h3><p>I noticed that newest version of docker, on rockylinux-9, taken from <a href="https://download.docker.com/linux/centos/$releasever/$basearch/stable">https://download.docker.com/linux/centos/$releasever/$basearch/stable</a> are a bit slow especially for operations done by yum</p><p>On both centos-7 and rocky-9 hosts I did:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><p>On centos7 host it takes ~2 minutes<br>On rocky-9 host after an hour it did not complete the process, I can leave it under tmux to discover how much time it takes</p><p>reproduce steps:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure><h3 id="rpm-slow"><a href="#rpm-slow" class="headerlink" title="rpm slow"></a>rpm slow</h3><p>run below comamnd on host:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion</span><br></pre></td></tr></table></figure><p>spend time：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m11.248s</span><br><span class="line">user    0m7.316s</span><br><span class="line">sys     0m1.932s</span><br></pre></td></tr></table></figure><p>when test it in container</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --net=none --log-driver=none -v &quot;/workspace:/workspace&quot; -v &quot;/disks:/disks&quot; opensuse bash -c &quot;time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion&quot;</span><br></pre></td></tr></table></figure><p>spend time：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m31.089s</span><br><span class="line">user    0m14.876s</span><br><span class="line">sys     0m12.524s</span><br></pre></td></tr></table></figure><p>Here’s the relevant section of code from RPM. It’s part of the POSIX lua library that’s inside RPM, and was added by <a href="https://github.com/rpm-software-management/rpm/commit/7a7c31f551ff167f8718aea6d5048f6288d60205">rpm-software-management&#x2F;rpm@<code>7a7c31f</code></a>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">static int Pexec(lua_State *L) /** exec(path,[args]) */</span><br><span class="line">&#123;</span><br><span class="line">/* ... */</span><br><span class="line">open_max = sysconf(_SC_OPEN_MAX);</span><br><span class="line">if (open_max == -1) &#123;</span><br><span class="line">    open_max = 1024;</span><br><span class="line">&#125;</span><br><span class="line">for (fdno = 3; fdno &lt; open_max; fdno++) &#123;</span><br><span class="line">    flag = fcntl(fdno, F_GETFD);</span><br><span class="line">    if (flag == -1 || (flag &amp; FD_CLOEXEC))</span><br><span class="line">continue;</span><br><span class="line">    fcntl(fdno, F_SETFD, FD_CLOEXEC);</span><br><span class="line">&#125;</span><br><span class="line">/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So the reason for doing <code>F_GETFD</code> is because they are setting all of the FDs to <code>CLOEXEC</code> before doing the requested <code>exec(2)</code>. There’s a <a href="https://bugzilla.redhat.com/show_bug.cgi?id=919801">redhat bugzilla entry</a> in the commit message, which says that this was an SELinux issue where Fedora (or RHEL) have an SELinux setup where you cannot execute a process if it will inherit FDs it shouldn’t have access to?</p><p>I guess if this is an SELinux issue it should be handled by only applying this fix when SELinux is used (though there are arguably security reasons why you might want to <code>CLOEXEC</code> every file descriptor).</p><h3 id="PtyProcess-spawn-slowdown-in-close-loop"><a href="#PtyProcess-spawn-slowdown-in-close-loop" class="headerlink" title="PtyProcess.spawn slowdown in close() loop"></a>PtyProcess.spawn slowdown in close() loop</h3><p>The following code in ptyprocess</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Do not allow child to inherit open file descriptors from parent, </span><br><span class="line"> # with the exception of the exec_err_pipe_write of the pipe </span><br><span class="line"> # and pass_fds. </span><br><span class="line"> # Impose ceiling on max_fd: AIX bugfix for users with unlimited </span><br><span class="line"> # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange() </span><br><span class="line"> # occasionally raises out of range error </span><br><span class="line"> max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0]) </span><br><span class="line"> spass_fds = sorted(set(pass_fds) | &#123;exec_err_pipe_write&#125;) </span><br><span class="line"> for pair in zip([2] + spass_fds, spass_fds + [max_fd]): </span><br><span class="line">     os.closerange(pair[0]+1, pair[1]) </span><br></pre></td></tr></table></figure><p>is looping through all possible file descriptors in order to close those (note that <code>closerange()</code> implemented as a loop at least on Linux). In case the limit of open fds (aka <code>ulimit -n</code>, aka <code>RLIMIT_NOFILE</code>, aka <code>SC_OPEN_MAX</code>) is set too high (for example, with recent docker it is 1024*1024), this loop takes considerable time (as it results in about a million <code>close()</code> syscalls).</p><p>The solution (at least for Linux and Darwin) is to obtain the list of actually opened fds, and only close those. This is implemented in <code>subprocess</code> module in Python3, and there is a backport of it to Python2 called subprocess32.</p><h3 id="MySQL-has-been-known-to-allocate-excessive-memory"><a href="#MySQL-has-been-known-to-allocate-excessive-memory" class="headerlink" title="MySQL has been known to allocate excessive memory"></a><a href="https://github.com/overhangio/tutor/pull/810/files#diff-a7c720c9cc5fa1b93c92c33e3e75a0b0880d80915d867d57d48464ff4f472b0aR115-R117">MySQL has been known to allocate excessive memory</a></h3><p>In idle mode, the “mysql” container should use ~200MB memory; ~200-300MB for the the “lms” and “cms” containers.</p><p>On some operating systems, such as RedHat, Arch Linux or Fedora, a very high limit of the number of open files (<code>nofile</code>) per container may cause the “mysql”, “lms” and “cms” containers to use a lot of memory: up to 8-16GB. To check whether you might impacted, run::</p><pre><code>cat /proc/$(pgrep dockerd)/limits | grep &quot;Max open files&quot;</code></pre><p>If the output is 1073741816 or higher, then it is likely that you are affected by <code>this mysql issue &lt;https://github.com/docker-library/mysql/issues/579&gt;</code><strong>. To learn more about the root cause, read <code>this containerd issue comment &lt;https://github.com/containerd/containerd/pull/7566#issuecomment-1285417325&gt;</code></strong>. Basically, the OS is hard-coding a very high limit for the allowed number of open files, and this is causing some containers to fail. To resolve the problem, you should configure the Docker daemon to enforce a lower value, as described <code>here &lt;https://github.com/docker-library/mysql/issues/579#issuecomment-1432576518&gt;</code>__. Edit <code>/etc/docker/daemon.json</code> and add the following contents::</p><pre><code>&#123;    &quot;default-ulimits&quot;: &#123;        &quot;nofile&quot;: &#123;            &quot;Name&quot;: &quot;nofile&quot;,            &quot;Hard&quot;: 1048576,            &quot;Soft&quot;: 1048576        &#125;    &#125;&#125;</code></pre><p>Check your configuration is valid with:</p><pre><code>dockerd --validate</code></pre><p>Then restart the Docker service:</p><pre><code>sudo systemctl restart docker.service</code></pre><h2 id="Technical-Background-Introduction"><a href="#Technical-Background-Introduction" class="headerlink" title="Technical Background Introduction"></a>Technical Background Introduction</h2><h3 id="1-RLIMIT-NOFILE"><a href="#1-RLIMIT-NOFILE" class="headerlink" title="1. RLIMIT_NOFILE"></a>1. RLIMIT_NOFILE</h3><blockquote><p><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></p></blockquote><p>Don’t use. Be careful when raising the soft limit above 1024, since <a href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a> cannot function with file descriptors above 1023 on Linux. Nowadays, the hard limit defaults to 524288, a very high value compared to historical defaults. Typically applications should increase their soft limit to the hard limit on their own, if they are OK with working with file descriptors above 1023, i.e. do not use <a href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a>. Note that file descriptors are nowadays accounted like any other form of memory, thus there should not be any need to lower the hard limit. Use <code>MemoryMax=</code> to control overall service memory use, including file descriptor memory.</p><blockquote><p><a href="https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94">https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94</a></p></blockquote><p>The Linux kernel’s current default RLIMIT_NOFILE resource limit for userspace processes is set to 1024 (soft) and 4096 (hard). Previously, systemd passed this on unmodified to all processes it forked off. With this systemd release the hard limit systemd passes on is increased to 512K, overriding the kernel’s defaults and substantially increasing the number of simultaneous file descriptors unprivileged userspace processes can allocate. Note that the soft limit remains at 1024 for compatibility reasons: the traditional UNIX select() call cannot deal with file descriptors &gt;&#x3D; 1024 and increasing the soft limit globally might thus result in programs unexpectedly allocating a high file descriptor and thus failing abnormally when attempting to use it with select() (of course, programs shouldn’t use select() anymore, and prefer poll()&#x2F;epoll, but the call unfortunately remains undeservedly popular at this time). This change reflects the fact that file descriptor<br>handling in the Linux kernel has been optimized in more recent kernels and allocating large numbers of them should be much cheaper both in memory and in performance than it used to be. Programs that<br>want to take benefit of the increased limit have to “opt-in” into high file descriptors explicitly by raising their soft limit. Of course, when they do that they must acknowledge that they cannot use select() anymore (and neither can any shared library they use — or any shared library used by any shared library they use and so on). Which default hard limit is most appropriate is of course hard to decide. However, given reports that ~300K file descriptors are used in real-life applications we believe 512K is sufficiently high as new default for now. Note that there are also reports that using very high hard limits (e.g. 1G) is problematic: some software allocates large arrays with one element for each potential file descriptor  (Java, …) — a high hard limit thus triggers excessively large memory allocations in these applications. Hopefully, the new default of 512K is a good middle ground: higher than what real-life applications currently need, and low enough for avoid triggering excessively large allocations in problematic software. (And yes, somebody should fix<br>Java.)</p><p>systemd v240 release in 2018Q4. Both Docker and Containerd projects have recently removed the line from their configs to rely on the <code>1024:524288</code> default systemd v240 provides (<em>unless the system has been configured explicitly to some other value, which the system administrator may do so when they know they need higher limits</em>).</p><h3 id="2-File-Descriptor-Limits"><a href="#2-File-Descriptor-Limits" class="headerlink" title="2.  File Descriptor Limits"></a>2.  File Descriptor Limits</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.)  to exceed this limit yield the error EMFILE.  (Historically, this limit was named RLIMIT_OFILE on BSD.)</span><br><span class="line">Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have &quot;in</span><br><span class="line">flight&quot; to other processes, by being passed across UNIX domain sockets.  This limit applies to the sendmsg(2) system call.  For further details, see unix(7).</span><br></pre></td></tr></table></figure><p>The primary way to reference, allocate and pin runtime OS resources on Linux today are file descriptors (“fds”). Originally they were used to reference open files and directories and maybe a bit more, but today they may be used to reference almost any kind of runtime resource in Linux userspace, including open devices, memory (<a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd_create(2)</code></a>), timers (<a href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"><code>timefd_create(2)</code></a>) and even processes (with the new <a href="https://man7.org/linux/man-pages/man2/pidfd_open.2.html"><code>pidfd_open(2)</code></a> system call). In a way, the philosophically skewed UNIX concept of “everything is a file” through the proliferation of fds actually acquires a bit of sensible meaning: “everything <em>has</em> a file <em>descriptor</em>“ is certainly a much better motto to adopt.</p><p>Because of this proliferation of fds, non-trivial modern programs tend to have to deal with substantially more fds at the same time than they traditionally did. Today, you’ll often encounter real-life programs that have a few thousand fds open at the same time.</p><p>Like on most runtime resources on Linux limits are enforced on file descriptors: once you hit the resource limit configured via <a href="https://man7.org/linux/man-pages/man2/getrlimit.2.html"><code>RLIMIT_NOFILE</code></a> any attempt to allocate more is refused with the <code>EMFILE</code> error — until you close a couple of those you already have open.</p><p>Because fds weren’t such a universal concept traditionally, the limit of <code>RLIMIT_NOFILE</code> used to be quite low. Specifically, when the Linux kernel first invokes userspace it still sets <code>RLIMIT_NOFILE</code> to a low value of 1024 (soft) and 4096 (hard). (Quick explanation: the <em>soft</em> limit is what matters and causes the <code>EMFILE</code> issues, the <em>hard</em> limit is a secondary limit that processes may bump their soft limit to — if they like — without requiring further privileges to do so. Bumping the limit further would require privileges however.). A limit of 1024 fds made fds a <em>scarce</em> resource: APIs tried to be careful with using fds, since you simply couldn’t have that many of them at the same time. This resulted in some questionable coding decisions and concepts at various places: often secondary descriptors that are very similar to fds — but were not actually fds — were introduced (e.g. inotify watch descriptors), simply to avoid for them the low limits enforced on true fds. Or code tried to aggressively close fds when not absolutely needing them (e.g. <code>ftw()</code>&#x2F;<code>nftw()</code>), losing the nice + stable “pinning” effect of open fds.</p><p>Worse though is that certain OS level APIs were designed having only the low limits in mind. The worst offender being the BSD&#x2F;POSIX <a href="https://man7.org/linux/man-pages/man2/select.2.html"><code>select(2)</code></a> system call: it only works with fds in the numeric range of 0…1023 (aka <code>FD_SETSIZE</code>-1). If you have an fd outside of this range, tough luck: select() won’t work, and only if you are lucky you’ll detect that and can handle it somehow.</p><p>Linux fds are exposed as simple integers, and for most calls it is guaranteed that the lowest unused integer is allocated for new fds. Thus, as long as the <code>RLIMIT_NOFILE</code> soft limit is set to 1024 everything remains compatible with <code>select()</code>: the resulting fds will also be below 1024. Yay. If we’d bump the soft limit above this threshold though and at some point in time an fd higher than the threshold is allocated, this fd would not be compatible with <code>select()</code> anymore.</p><p>Because of that, indiscriminately increasing the soft <code>RLIMIT_NOFILE</code> resource limit today for every userspace process is problematic: as long as there’s userspace code still using <code>select()</code> doing so will risk triggering hard-to-handle, hard-to-debug errors all over the place.</p><p>However, given the nowadays ubiquitous use of fds for all kinds of resources (did you know, an eBPF program is an fd? and a cgroup too? and attaching an eBPF program to cgroup is another fd? …), we’d really like to raise the limit anyway.</p><p>So before we continue thinking about this problem, let’s make the problem more complex (…uh, I mean… “more exciting”) first. Having just one hard and one soft per-process limit on fds is boring. Let’s add more limits on fds to the mix. Specifically on Linux there are two system-wide sysctls: <code>fs.nr_open</code> and <code>fs.file-max</code>. (Don’t ask me why one uses a dash and the other an underscore, or why there are two of them…) On today’s kernels they kinda lost their relevance. They had some originally, because fds weren’t accounted by any other counter. But today, the kernel tracks fds mostly as small pieces of memory allocated on userspace requests — because that’s ultimately what they are —, and thus charges them to the memory accounting done anyway.</p><p>So now, we have four limits (actually: five if you count the memory accounting) on the same kind of resource, and all of them make a resource artificially scarce that we don’t want to be scarce. So what to do?</p><p>Back in systemd v240 already (i.e. 2019) we decided to do something about it. Specifically:</p><ul><li>Automatically at boot we’ll now bump the two sysctls to their maximum, making them effectively ineffective. This one was easy. We got rid of two pretty much redundant knobs. Nice!</li><li>The <code>RLIMIT_NOFILE</code> hard limit is bumped substantially to 512K. Yay, cheap fds! <em>You</em> may have an fd, and <em>you</em>, and <em>you</em> as well, <em>everyone</em> may have an fd!</li><li>But … we left the soft <code>RLIMIT_NOFILE</code> limit at 1024. We weren’t quite ready to break all programs still using <code>select()</code> in 2019 yet. But it’s not as bad as it might sound I think: given the hard limit is bumped every program can easily opt-in to a larger number of fds, by setting the soft limit to the hard limit early on — without requiring privileges.</li></ul><p>So effectively, with this approach fds should be much less scarce (at least for programs that opt into that), and the limits should be much easier to configure, since there are only two knobs now one really needs to care about:</p><ul><li>Configure the <code>RLIMIT_NOFILE</code> hard limit to the maximum number of fds you actually want to allow a process.</li><li>In the program code then either bump the soft to the hard limit, or not. If you do, you basically declare “I understood the problem, I promise to not use <code>select()</code>, drown me fds please!”. If you don’t then effectively everything remains as it always was.</li></ul><p>Apparently this approach worked, since the negative feedback on change was even scarcer than fds traditionally were (ha, fun!). We got reports from pretty much only two projects that were bitten by the change (one being a JVM implementation): they already bumped their soft limit automatically to their hard limit during program initialization, and then allocated an array with one entry per possible fd. With the new high limit this resulted in one massive allocation that traditionally was just a few K, and this caused memory checks to be hit.</p><p>Anyway, here’s the take away of this blog story:</p><ul><li>Don’t use <code>select()</code> anymore in 2021. Use <code>poll()</code>, <code>epoll</code>, <code>iouring</code>, …, but for heaven’s sake don’t use <code>select()</code>. It might have been all the rage in the 1990s but it doesn’t scale and is simply not designed for today’s programs. I wished the man page of <code>select()</code> would make clearer how icky it is and that there are plenty of more preferably APIs.</li><li>If you hack on a program that potentially uses a lot of fds, add <a href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L373">some simple code</a> somewhere to its start-up that bumps the <code>RLIMIT_NOFILE</code> soft limit to the hard limit. But if you do this, you have to make sure your code (and any code that you link to from it) refrains from using <code>select()</code>. (Note: there’s at least one glibc NSS plugin using <code>select()</code> internally. Given that NSS modules can end up being loaded into pretty much <em>any</em> process such modules should probably be considered just buggy.)</li><li>If said program you hack on forks off foreign programs, make sure to reset the <code>RLIMIT_NOFILE</code> soft limit <a href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L394">back to 1024</a> for them. Just because your program might be fine with fds &gt;&#x3D; 1024 it doesn’t mean that those foreign programs might. And unfortunately <code>RLIMIT_NOFILE</code> is inherited down the process tree unless explicitly set.</li></ul><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><ul><li><a href="https://github.com/Supervisor/supervisor/issues/26"><code>supervisord</code> with <code>select()</code></a> (<em>2011 reported, <strong>2014</strong> fixed</em>).</li><li>Nginx will <a href="https://nginx.org/en/docs/ngx_core_module.html#worker_rlimit_nofile">raise soft limit when you tell it to</a> via config (<em><a href="https://forum.nginx.org/read.php?2,258259,258259">bug report in <strong>2015</strong>, where <code>select()</code> was used</a> by nginx limiting it to <code>1024</code></em>). Alternatively if you’re using the nginx container, you can raise the soft limit at the container level.</li><li>Redis docs advising <code>2^16</code> in example, will be dependent upon your workload.<ul><li><a href="https://github.com/redis/redis-py/issues/419#issuecomment-41134427"><code>redis-py</code> Dec 2013 issue with <code>select()</code></a>, fixed in June <strong>2014</strong></li><li><code>redis/hiredis</code> 2015 issue where a <a href="https://github.com/redis/hiredis/issues/385#issue-123387718">user was relying on <code>select()</code></a>.</li><li><a href="https://blog.pjam.me/posts/select-syscall-in-rust/">Nov 2020 article on <code>select()</code></a>, references Redis still carries <code>select()</code> as a fallback (<a href="https://github.com/redis/redis/blob/e12f2decc1cf7742878d516d89d38af178119b17/src/ae_select.c"><code>ae_select.c</code></a>)</li></ul></li><li>httpd has <code>select()</code> (see <a href="https://github.com/apache/httpd/commit/8286cb80fd160a4259ddf10c3ea016777d34d083">this 2002 commit</a>, which is <a href="https://github.com/apache/httpd/blob/bc0e56cdd3eebbe0fae3f9f5770b09236e8a4a17/modules/http/http_core.c#L241">still present today</a> in 2024_)</li><li>Postgres hasthis 2010 response for why they don’t use the hard limit to not negatively impact other software running. Less of an issue in a container, especially when you can set the limits. The limits work a little bit differently now , that the issue shouldn’t be applicable anymore (the global FD limit for a system is notably higher than the hard limit tends to be per process).<ul><li><a href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS">Docs on <code>max_files_per_process</code></a>. Similar to nginx, there is a specific setting here, and it’s also per process this software manages. Both of these are doing the correct thing by not having a monolithic process sharing the same FD limit. The soft limit applied is per process, if you have 1024 processes with the soft limit of 1024, you also have <code>2^20</code> FDs available across them…not only 1024.</li><li>Postgres still has source with usage of <code>select()</code> <a href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pg_basebackup/receivelog.c#L904">here</a>, <a href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pgbench/pgbench.c#L7862">here</a> and various other locations if you want to look through it.</li></ul></li><li><a href="https://narkive.com/CrxObBlI.2">MongoDB using <code>select()</code> in 2014</a>, in the <a href="https://github.com/mongodb/mongo/blob/91fc5673cab5d1267fd805f1375577df9072ea1b/src/mongo/util/net/listen.cpp#L252-L270">3.7.5 release it was still using it for <code>listen.cpp</code></a>, but that was <a href="https://github.com/mongodb/mongo/commit/55aac9ac800531ad021f18f56d69c69ac5619245">dropped in the 3.7.6 release</a> (April <strong>2018</strong>). A <a href="https://github.com/mongodb/mongo/blob/8de341d0d2011b51eb1d140fb4820424d29fe510/src/mongo/transport/asio/asio_utils.cpp#L131"><code>select()</code> call still exists though</a>.</li></ul><h2 id="Dig-deeping-into"><a href="#Dig-deeping-into" class="headerlink" title="Dig deeping into"></a>Dig deeping into</h2><p>ulimit, being an archaic resource management mechanism, is not completely obsoleted by cgroup controllers, and it is still an essential part of system administration.</p><p>Default ulimits for a new container are derived from those of <del>dockerd</del> containerd itself. They are set in <code>containerd.service</code> systemd unit file to <code>unlimited</code> values:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep ^Limit /lib/systemd/system/containerd.service</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>This is required for containerd itself, but is way too generous for containers it runs. For comparison, ulimits for a user (including root) on the host system are pretty modest (this is an example from Ubuntu 18.04):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 62435</span><br><span class="line">max locked memory       (kbytes, -l) 16384</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 62435</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure><p>This can create a number of problems, such as container abusing system resources (e.g. DoS attacks). In general, cgroup limits should be used to prevent those, yet I think ulimits should be set to a saner values.</p><p>In particular, <code>RLIMIT_NOFILE</code>, a number of open files limit, which is set to 2^20 (aka 1048576), causes a slowdown in a number of programs, as they use the upper limit value to iterate over all potentially opened file descriptors, closing those (or setting CLOEXEC bit) before every fork&#x2F;exec. I am aware of the following cases:</p><ul><li>rpm, reported in <a href="https://github.com/moby/moby/issues/23137">Slow performance when installing RPMs to create new Docker images #23137</a>, <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1537564">https://bugzilla.redhat.com/show_bug.cgi?id=1537564</a>, fix: <a href="https://github.com/rpm-software-management/rpm/pull/444">Optimize and unite setting CLOEXEC on fds rpm-software-management&#x2F;rpm#444</a> (fixed in Fedora 28).</li><li>python2, reported in <a href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09 docker&#x2F;for-linux#502</a>, proposed fix: [<a href="https://github.com/python/cpython/pull/11584">2.7] bpo-35757: subprocess.Popen: optimize close_fds for Linux python&#x2F;cpython#11584</a> (WONTFIX as python2 is frozen)</li><li>python’s pexpect&#x2F;ptyprocess library, reported in <a href="https://github.com/pexpect/ptyprocess/issues/50">PtyProcess.spawn (and thus pexpect) slowdown in close() loop pexpect&#x2F;ptyprocess#50</a>.</li></ul><p>Attacking those one by one proved complicated and not very fruitful, as some software is obsoleted, some is hard to fix, etc. In addition, the above list is not a concise one, so there might be more cases like this we’re not aware of.</p><h3 id="Investigated-limits-impact-and-costs"><a href="#Investigated-limits-impact-and-costs" class="headerlink" title="Investigated limits impact and costs"></a>Investigated limits impact and costs</h3><p><code>2^16</code> (65k) <code>busybox</code> containers estimated resource usage:</p><ul><li>688k tasks + 206 GB (192 GiB) memory in <code>containerd</code> (<em>10.5 tasks + 3MiB per container</em>)</li><li>Requiring at minimum <code>LimitNOFILE=262144</code> (<code>containerd.service</code>) + <code>LimitNOFILE=393216</code> (<code>docker.service</code>) - based on <code>4:1</code> + <code>6:1</code> service FDs needed per container ratio.</li><li>2.49 million open files (<em><code>fs.file-nr</code> must be below <code>fs.file-max</code> limit</em>) - approx 38 FDs per container.</li><li>25 GiB memory for the containers cgroup (<em>approx 400KiB per container</em>).</li></ul><p><code>LimitNOFILE=524288</code> (<em>systemd default since v240</em>) should be more than enough for most systems as a sane default. This should be more than enough for both <code>docker.service</code> and <code>containerd.service</code> resource needs, capable of supporting 65k containers.</p><p>Containers that do need higher limits can explicitly declare that (<em>via <code>--ulimit</code> or equivalent</em>), as the upper bound is not impacted by <code>containerd.service</code>. The same can be done for lowering limits if necessary, both should rarely be necessary for most containers.</p><p>While <code>docker.service</code> and <code>containerd.service</code> need the higher soft limit (<em>enforced implicitly since Go 1.19</em>), it would be unlikely required for containers. An upcoming release of Go (with backports to 1.19) will implicitly restore the soft limit to <code>fork</code> &#x2F; <code>exec</code> processes AFAIK. Until then, the Docker daemon can be configured with <code>default-ulimit</code> setting to enforce a <code>1024</code> soft limit on containers.</p><h4 id="System-details"><a href="#System-details" class="headerlink" title="System details"></a>System details</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fedora 37 VM 6.1.9 kernel x86_64 (16 GB memory)</span><br><span class="line">Docker v23, containerd 1.6.18, systemd v251</span><br><span class="line"></span><br><span class="line"># Additionally verified with builds before Go 1.19 to test soft limit lower than the hard limit:</span><br><span class="line">dnf install docker-ce-3:20.10.23 docker-ce-cli-1:20.10.23 containerd.io-1.6.8</span><br></pre></td></tr></table></figure><h4 id="Observations-in-service-files-for-LimitNOFILE"><a href="#Observations-in-service-files-for-LimitNOFILE" class="headerlink" title="Observations in .service files for LimitNOFILE"></a>Observations in <code>.service</code> files for <code>LimitNOFILE</code></h4><p>On a fresh install (<em>via VM on Vultr</em>) there was approx 1800 file descriptors open (<code>sysctl fs.file-nr</code>). I used a shell loop to run <code>busybox</code> containers until failure and adjusted the <code>LimitNOFILE</code> for <code>docker.service</code> and <code>containerd.service</code> to collect metrics for insights.</p><p>I noticed a consistent ratio of number of FDs needed per container:</p><ul><li><code>docker.service</code> - <code>6:1</code> ratio (<em><code>5:1</code> with <code>--network=host</code></em>), approx 853 containers with <code>LimitNOFILE=5120</code> (<em>1024 with host network</em>).</li><li><code>containerd.service</code> - <code>4:1</code> ratio (<em>I did not verify if <code>--network=host</code> reduced this</em>), <code>LimitNOFILE=1024</code> should be capable of 256 containers, provided <code>docker.service</code> is also high enough (eg: <code>LimitNOFILE=2048</code>_).</li></ul><p>In <code>containerd.service</code> there was also a clear pattern in resources per container, where the <code>LimitNOFILE</code> value, image used (<em><code>busybox</code>, <code>alpine</code>, <code>debian</code></em>), and number of containers remained constant:</p><ul><li><p>Each containers systemd <code>.scope</code> has 1 task and approx 400KiB memory (little bit less for <code>alpine</code> and <code>debian</code>).</p></li><li><p>10.5 tasks + 3MiB memory added per container to <code>systemctl status containerd</code> report.</p></li><li><p>Approx 38 open files per container running (<em><code>fs.file-nr</code> after, minus the before value, divided by number of containers</em>).</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mailserver/docker-mailserver:edge</span><br></pre></td></tr></table></figure><p>  was also tested to compare to the</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sleep 180</span><br></pre></td></tr></table></figure><p>  containers:</p><pre><code>- 33 tasks per container `.scope` and 85MiB memory reported via `systemd-cgtop` (*10GiB needed min to run 120 of these containers*).- In `containerd` per container average resources were 11 tasks + 3.4MiB memory (*approx 400MiB usage for 120 of these containers*). Roughly consistent with the lighter images resource usage in `containerd`.- Files opened per container also increased to 291 (*approx 35k files open for 120 of these containers*).- If you want to reproduce for this image, `docker run` should include these extra options: `--hostname example.test --env SMTP_ONLY=1` (*hostname required to init, `SMTP_ONLY=1` skips needing an account configured*).</code></pre><p>Operations like <code>docker stats</code> need to open as many file descriptors as total containers running, otherwise it’ll hang waiting. You can observe if the daemon has reached the limit with <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</p><h4 id="Reproduction"><a href="#Reproduction" class="headerlink" title="Reproduction"></a>Reproduction</h4><p>Set <code>LimitNOFILE=768</code> in <code>docker.service</code>, then <code>systemctl daemon-reload &amp;&amp; systemctl restart docker</code>. You can confirm the limit is applied to the daemon process with <code>cat /proc/$(pidof dockerd)/limits</code>.</p><p>Running the following should list:</p><ul><li>How many containers are running.</li><li>Number of open files.</li><li>How many tasks and memory both the <code>containerd</code> and <code>dockerd</code> daemons are using.</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Useful to run before the loop to compare against output after the loop is done</span><br><span class="line">(pgrep containerd-shim | wc -l) &amp;&amp; sysctl fs.file-nr \</span><br><span class="line">  &amp;&amp; (echo &#x27;Containerd service:&#x27; &amp;&amp; systemctl status containerd | grep -E &#x27;Tasks|Memory&#x27;) \</span><br><span class="line">  &amp;&amp; (echo &#x27;Docker service:&#x27; &amp;&amp; systemctl status docker | grep -E &#x27;Tasks|Memory&#x27;)</span><br></pre></td></tr></table></figure><p>Running this loop below should fail on the last few containers, about 123 should be created:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># When `docker.service` limit is the bottleneck, you may need to `CTRL + C` to exit the loop</span><br><span class="line"># if it stalls while waiting for new FDs once exhausted and outputting errors:</span><br><span class="line">for i in $(seq 1 130); do docker run --rm -d busybox sleep 180; done</span><br></pre></td></tr></table></figure><p>You can add additional options:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--network host</span><br></pre></td></tr></table></figure><pre><code>- Avoids creating a new veth interface (see `ip link`) to the default Docker bridge each `docker run`.- Without this `docker run` [may fail after `1023` interfaces are present on a single bridge](https://github.com/moby/moby/issues/44973#issuecomment-1543747718)?- Creation will be a bit faster, and the FD to container ratio for `dockerd` is lowered to `5:1`.</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit &quot;nofile=1023456789&quot;</span><br></pre></td></tr></table></figure><pre><code>- Useful to observe that it does not affect memory usage on it&#39;s own.- Also shows `dockerd` + `containerd` limits don&#39;t affect how high this can go.- For Debian based distros this would fail as it&#39;s higher than `fs.nr_open` (`1 048 576`), use that or a lower value.</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--cgroup-parent=LimitTests.slice</span><br></pre></td></tr></table></figure><pre><code>- Similar to `docker stats` but isolated from other running containers. `systemd-cgtop` does include disk cache (*file-backed memory pages*) in it&#39;s reported memory usage however (*use `sync &amp;&amp; sysctl vm.drop_caches=3` to clear that*).- This can be useful if you want a better overview of resource usage across all the containers created:    - Create a temporary slice for testing with: `mkdir /sys/fs/cgroup/LimitTests.slice`.    - Run `systemd-cgtop --order=memory LimitTests.slice` to only view the containers running in this cgroup slice sorted by memory usage.    - Memory usage shown for the entire slice and per container. A `busybox` container uses roughly 400KB per container.</code></pre><h3 id="Limits-impact-across-process-children"><a href="#Limits-impact-across-process-children" class="headerlink" title="Limits impact across process children"></a>Limits impact across process children</h3><p>I had a misconception that child processes contributed to the parents open files limit. However as my notes in this section detail, it’s only inheriting the limits applied, each process seems to have it’s own individual count.</p><p>Although, I’m probably missing something here as I have read of processes passing down FDs to children, which is also why daemons have a common hygiene practice to close the FD range available I think? This is lower level than I’m familiar with 😅</p><ul><li>You can also observe the number of file descriptors open for the <code>dockerd</code> and <code>containerd</code> processes like this: <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</li><li>This isn’t applicable to the <code>containerd-shim</code> process that is responsible for the container, so <code>ls -1 /proc/$(pgrep --newest --exact containerd-shim)/fd | wc -l</code> won’t be useful there.</li></ul><p>To confirm this, run a container to test with: <code>docker run --rm -it --ulimit &quot;nofile=1024:1048576&quot; alpine ash</code>. Then try the following:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Create a folder to add many files to:</span><br><span class="line">mkdir /tmp/test; cd /tmp/test</span><br><span class="line"></span><br><span class="line"># Create empty files:</span><br><span class="line">for x in $(seq 3 2048); do touch &quot;$&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Open files to specific file descriptors:</span><br><span class="line">for x in $(seq 1000 1030); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"># Fails after 1024 because soft limit is capped there. Raise it:</span><br><span class="line">ulimit -Sn 2048</span><br><span class="line"></span><br><span class="line"># Now the loop before will be successful.</span><br><span class="line"># You could cover the whole original soft limit range (excluding FDs 0-2: stdin, stdout, stderr):</span><br><span class="line">for x in $(seq 3 1024); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Multiple container processes / children opening as many files:</span><br><span class="line"># You can run the same loop in a new shell process with `ash -c &#x27;for ... done&#x27;`</span><br><span class="line"># Or `docker exec` into the container from another terminal and run the loop at `/tmp/test` again.</span><br><span class="line"># Each can open files up to their current soft limit, it doesn&#x27;t matter what limits are set on `dockerd`, `containerd` or the containers PID 1.</span><br><span class="line"></span><br><span class="line">############</span><br><span class="line">### Tips ###</span><br><span class="line">############</span><br><span class="line"></span><br><span class="line"># You can observe the current limit applied:</span><br><span class="line">cat /proc/self/limits</span><br><span class="line"># And if you have not exhausted your FDs soft limit (due to the pipe),</span><br><span class="line"># this will report how much of the limit is used:</span><br><span class="line">ls -1 /proc/self/fd | wc -l</span><br><span class="line"># Otherwise, outside of the container if this is your only `ash` process running,</span><br><span class="line"># you can query it&#x27;s PID to get this information:</span><br><span class="line">ls -1 /proc/$(pgrep --newest --exact ash)/fd | wc -l</span><br><span class="line"></span><br><span class="line"># Process count in container:</span><br><span class="line"># `docker stats` should list a containers PIDs count for number of processes,</span><br><span class="line"># `systemd-cgtop` should report the same value in it&#x27;s Tasks column.</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line"># (NOTE: Path may differ if you used `--cgroup-parent`)</span><br><span class="line">cat /sys/fs/cgroup/system.slice/docker-&lt;CONTAINER_ID&gt;.scope/pids.current</span><br><span class="line"></span><br><span class="line"># List the processes with their PIDs:</span><br><span class="line"># For a single container, you can visualize the process tree:</span><br><span class="line">pstree --arguments --show-pids $(pgrep --newest --exact containerd-shim)</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line">systemd-cgls --unit docker-&lt;CONTAINER_ID&gt;.scope</span><br><span class="line"></span><br><span class="line"># You can also observe disk cache in memory monitoring by creating a 1GB file:</span><br><span class="line">dd if=/dev/zero of=bigfile bs=1M count=1000</span><br><span class="line">free -h</span><br><span class="line"># `systemd-cgtop` will include 1GB more in memory usage for the container,</span><br><span class="line"># while `docker stats` appears to account only 30MiB (scales proportionally).</span><br><span class="line"># Now clear the cache outside of the container and observe memory usage again:</span><br><span class="line">sync &amp;&amp; sysctl vm.drop_caches=3</span><br></pre></td></tr></table></figure><p>You will notice that:</p><ul><li><p>Each process adds those FDs to the open file count returned from <code>fs.file-nr</code>, and frees them when that process is closed.</p></li><li><p>You can re-run the loops for the same process and observe no change, the files are already counted as open for that process.</p></li><li><p>There is a memory cost involved:</p><ul><li>Each file <code>touch</code> costs about <code>2048</code> bytes (<em>disk-cache only until opened</em>).</li><li>Each file open (<em>1 or more FD references each increment <code>fs.file-nr</code></em>) costs about <code>512</code> bytes per FD open for it.</li><li>Creating 512k files this way uses approx 1.1GiB memory (<em>not released with <code>sysctl vm.drop_caches=3</code> while opened by at least one FD</em>), while each process opening the equivalent amount of file descriptors additionally uses 250MiB (262MB).</li></ul></li></ul><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>Nothing useful here, other than depending on which service limit was exhausted first resulted in slightly different errors.</p><p>Sometimes this made any <code>docker</code> command like <code>docker ps</code> hang (daemon exhausted limit). I had also observed:</p><ul><li>Containers not running (<em>no <code>pgrep containerd-shim</code> output, but <code>docker ps</code> listed containers running well beyond when they should have exited</em>).</li><li>Containers running with <code>containerd-shim</code> process (using memory), despite <code>systemctl stop docker containerd</code>. Sometimes this needed <code>pkill containerd-shim</code> to cleanup and <code>systemctl start docker containerd</code> would log a bunch of errors in <code>journalctl</code> handling cleanup of dead shims (<em>depending on the number of containers, this may time out and need to start the <code>containerd</code> service again</em>).</li><li>Even with all that out of the way, there was some memory usage of several hundred MB from the baseline that lingered. As it didn’t seem to belong to any process, I assume it was kernel memory. I think the largest number of containers I experimented running with was around 1600-ish.</li></ul><h4 id="docker-service-limit-exceeded"><a href="#docker-service-limit-exceeded" class="headerlink" title="docker.service limit exceeded"></a><code>docker.service</code> limit exceeded</h4><p>This failure output more errors per <code>docker run</code> but the errors varied:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0000] Error waiting for container: container caff476371b6897ef35a95e26429f100d0d929120ff1abecc8a16aa674d692bf: driver &quot;overlay2&quot; failed to remove root filesystem: openfdat /var/lib/docker/overlay2/35f26ec862bb91d7c3214f76f8660938145bbb36eda114f67e711aad2be89578-init/diff/etc: too many open files </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: time=&quot;2023-03-12T02:26:20Z&quot; level=fatal msg=&quot;failed to create a netlink handle: could not get current namespace while creating netlink socket: too many open files&quot;: unknown.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to initialize logging driver: open /var/lib/docker/containers/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f-json.log: too many open files.</span><br></pre></td></tr></table></figure><h4 id="containerd-service-limit-exceeded"><a href="#containerd-service-limit-exceeded" class="headerlink" title="containerd.service limit exceeded"></a><code>containerd.service</code> limit exceeded</h4><p>I think I’ve seen some others, but it’s usually this one:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to start shim: start failed: : pipe2: too many open files: unknown.</span><br></pre></td></tr></table></figure><h2 id="Scope-and-Explanation"><a href="#Scope-and-Explanation" class="headerlink" title="Scope and Explanation"></a>Scope and Explanation</h2><ul><li><p>Aug 2023: <code>LimitNOFILE=infinity</code>  <a href="https://github.com/moby/moby/pull/42373">remove from <code>docker.service</code></a></p></li><li><p>May 2021: <code>LimitNOFILE=infinity</code> + <code>LimitNPROC=infinity</code> <a href="https://github.com/moby/moby/pull/42373">brought back into <code>docker.service</code></a> to sync with Docker CE’s equivalent config.</p><ul><li>This PR was a merge commit of <a href="https://github.com/moby/moby/commit/80039b4699e36ceb0eb81109cd1686aaa805c5ec">this one from Sep 2018</a> (<em>commit history interleaved ranges from 2017-2021</em>).</li><li>As the PR main diff shows, <code>LimitNPROC=infinity</code> was already added, and <code>LimitNOFILE=1048576</code> was changed to <code>infinity</code> by the PR merge (<em>initially confused since I’m using <code>git blame</code> on the master branch</em>).</li></ul></li><li><p>July 2016: <a href="https://github.com/moby/moby/pull/24555"><code>LimitNOFILE=infinity</code> changed to <code>LimitNOFILE=1048576</code></a> (<em>this number is <code>2^20</code></em>).</p><ul><li>Discussion references a <a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible">2009 StackOverflow answer</a> about <code>infinity</code> being capped to <code>2^20</code> in a specific distro release &#x2F; kernel. On some systems today, that ceiling can be 1024 times higher (<em><code>2^30 == 1073741816</code>, over 1 billion</em>).</li></ul></li><li><p>July 2016: <a href="https://github.com/moby/moby/pull/24307"><code>LimitNOFILE</code> and <code>LimitNPROC</code> changed from <code>1048576</code> to <code>infinity</code></a></p><ul><li>This PR reverted the <code>LimitNOFILE</code> change shortly after as described above.</li></ul></li><li><p>March 2014: <a href="https://github.com/moby/moby/pull/4455#issuecomment-36679884">Original <code>LimitNOFILE</code> + <code>LimitNPROC</code> added with <code>1048576</code></a>.</p><ul><li><p>Linked PR comment mentions that this 2^20 value is already higher than Docker needs:</p></li><li><p>It appears it was later changed to <code>infinity</code> to improve CI times where a smaller limit was applied (<em>like <a href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible#comment76259289_1213069">this comment about Ubuntu 14.04 adjusting any limit exceeding <code>2^20</code> to <code>2^10</code>?</a></em>).</p></li><li><p>PR also [referenced relevant systemd docs](<a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process">https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process</a> Properties) (<em>which may have changed since 2014</em>)</p></li></ul></li></ul><hr><p><strong>Current Status:</strong></p><ul><li><code>LimitNOFILE=infinity</code> is still the case until Docker v25, unless the team is backporting it to any releases built with Go 1.19+</li><li><code>containerd</code> has <a href="https://github.com/containerd/containerd/pull/8924">merged the equivalent change to remove <code>LimitNOFILE</code></a> from their systemd service file.</li></ul><h3 id="Systemd-240"><a href="#Systemd-240" class="headerlink" title="Systemd &lt; 240"></a>Systemd &lt; 240</h3><p>Why is LimitNOFILE not set to infinity when configured in the service?</p><p>After setting LimitNOFILE to infinity in the service, when checking the limit of the process ID (pid), it is observed that the open file limit is 65536 instead of infinity.</p><p>Please review the service configuration.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# ulimit -n -u</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">max user processes              (-u) 499403</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>containerd systemd configuration:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/lib/systemd/system/containerd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target local-fs.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"># Having non-zero Limit*s causes performance problems due to accounting overhead</span><br><span class="line"># in the kernel. We recommend using cgroups to do container-local accounting.</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line"># Comment TasksMax if your systemd version does not supports it.</span><br><span class="line"># Only systemd 226 and above support this version.</span><br><span class="line">TasksMax=infinity</span><br><span class="line">OOMScoreAdjust=-999</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Viewing the configuration effect</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# cat /proc/$(pidof dockerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us        </span><br><span class="line">[root@XXX ~]# cat /proc/$(pidof containerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">This has systemd look at /proc/sys/fs/nr_open to find the current maximum of</span><br><span class="line">open files compiled into the kernel and tries to set the RLIMIT_NOFILE max to</span><br><span class="line">it. This has the advantage the value chosen as limit is less arbitrary and also</span><br><span class="line">improves the behavior of systemd in containers that have an rlimit set: When</span><br><span class="line">systemd currently starts in a container that has RLIMIT_NOFILE set to e.g.</span><br><span class="line">100000 systemd will lower it to 65536. With this patch systemd will try to set</span><br><span class="line">the nofile limit to the allowed kernel maximum. If this fails, it will compute</span><br><span class="line">the minimum of the current set value (the limit that is set on the container)</span><br><span class="line">and the maximum value as soft limit and the currently set maximum value as the</span><br><span class="line">maximum value. This way it retains the limit set on the container.</span><br></pre></td></tr></table></figure><p>see: <a href="https://github.com/systemd/systemd/issues/6559">https://github.com/systemd/systemd/issues/6559</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://github.com/moby/moby/issues/45838">https://github.com/moby/moby/issues/45838</a></li><li><a href="https://github.com/moby/moby/issues/23137">https://github.com/moby/moby/issues/23137</a></li><li><a href="https://0pointer.net/blog/file-descriptor-limits.html">https://0pointer.net/blog/file-descriptor-limits.html</a></li><li><a href="https://www.codenong.com/cs105896693/">https://www.codenong.com/cs105896693/</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li><li><a href="https://github.com/cri-o/cri-o/issues/7703">https://github.com/cri-o/cri-o/issues/7703</a></li><li><a href="https://github.com/envoyproxy/envoy/issues/31502">https://github.com/envoyproxy/envoy/issues/31502</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;thanks &lt;strong&gt;&lt;a href=&quot;https://github.com/polarathene&quot;&gt;polarathene&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Description&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="container" scheme="https://zoues.com/categories/container/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="container" scheme="https://zoues.com/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>Interpreting common exit codes in Kubernetes You Need to Know</title>
    <link href="https://zoues.com/posts/45fc99a8/"/>
    <id>https://zoues.com/posts/45fc99a8/</id>
    <published>2024-03-06T12:23:48.000Z</published>
    <updated>2024-03-06T10:21:52.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Interpreting-common-exit-codes-in-Kubernetes"><a href="#Interpreting-common-exit-codes-in-Kubernetes" class="headerlink" title="Interpreting common exit codes in Kubernetes"></a>Interpreting common exit codes in Kubernetes</h2><p>In this article, we will delve into Kubernetes exit codes 127 and 137, explaining what they are, common causes in K8s and Docker, and how to fix them!</p><p>We will cover:</p><ol><li>History of exit codes</li><li>Exit code 127</li><li>Exit code 137</li></ol><h2 id="The-History-of-Exit-Codes"><a href="#The-History-of-Exit-Codes" class="headerlink" title="The History of Exit Codes"></a>The History of Exit Codes</h2><p>The history of process exit codes can be traced back to the early days of the Unix operating system. In Unix systems, a process exit code is an integer value passed to its parent process upon termination, used to indicate the termination status of the process. This integer value typically falls between 0 and 255, where 0 indicates successful termination, and other values are usually used to represent different errors or exceptional conditions.</p><p>Process exit codes were initially designed to provide a simple mechanism for parent processes to understand the outcome of their child processes’ execution. This allows parent processes to take appropriate actions based on the exit code of the child process, such as handling error conditions or continuing with other operations.</p><p>In Unix systems, specific exit code values often have specific meanings, such as:</p><ul><li>0: Indicates successful execution without errors.</li><li>1: Typically signifies a general error.</li><li>2: Indicates a syntax error in the command.</li><li>127: Indicates command not found.</li></ul><p>Over time, with the development of Unix operating systems and different implementations, the meanings of process exit codes may vary, but the basic concept remains unchanged.</p><p>In Linux systems, the usage of process exit codes is similar to Unix systems. Linux inherits Unix’s process management mechanism and extends and improves upon it. Therefore, process exit codes remain an important concept in Linux, used to aid in understanding and diagnosing the execution status of processes.</p><p>The history of process exit codes can be traced back to the early days of Unix systems and is an important concept in both Unix and Linux operating systems, providing a simple yet effective mechanism for inter-process communication. When an application or command terminates or fails to execute due to a fatal error, it produces exit codes in the 128 series (<code>128+n</code>), where <code>n</code> represents the signal number. <code>n</code> includes all types of termination codes, such as <code>SIGTERM</code>, <code>SIGKILL</code>, etc.</p><h2 id="Exit-Code-127"><a href="#Exit-Code-127" class="headerlink" title="Exit Code 127"></a>Exit Code 127</h2><p>Exit code 127 is not a Kubernetes-specific error code, but rather a standard exit code used in Linux and similar Unix-like operating systems. However, it is frequently encountered in Kubernetes and typically indicates that a command or binary executed within a container could not be found.</p><p>Some standard exit codes include:</p><table><thead><tr><th>Exit Code</th><th>Explanation</th></tr></thead><tbody><tr><td>0</td><td>Command executed successfully</td></tr><tr><td>1</td><td>General error</td></tr><tr><td>2</td><td>Misuse of shell builtins</td></tr><tr><td>126</td><td>Permission denied, command invoked cannot execute</td></tr><tr><td>127</td><td>Command not found, incorrect PATH</td></tr><tr><td>128+n</td><td>Command terminated by signal, fatal error encountered</td></tr><tr><td>&gt;255</td><td>Exit codes beyond 255 will be re-calculated (mod 256)</td></tr></tbody></table><p>Let’s take a look at some common reasons for exit code 127:</p><ol><li><p>Command or binary not installed</p><p>The executable specified in the <code>command</code> field of a Kubernetes container is not installed in the container’s file system. Ensure that the required binary or command is available.</p></li><li><p>Incorrect path or command</p><p>The command specified in the Pod definition is incorrect or does not exist in the specified path. This is one of the most common errors, often caused by incorrect input in the Dockerfile or pod spec’s <code>entrypoint</code> or <code>command</code>.</p></li><li><p>Missing dependencies</p><p>The application or script running inside the container lacks necessary dependencies. Ensure that all required dependencies are included in the container image.</p></li><li><p>Shell interpreter</p><p>If a script is specified as a command, ensure that the script is valid (e.g., <code>#!/bin/bash</code>) and available in the container.</p></li><li><p>Shell script syntax error</p><p>If the shell script exits with code 127, check if there are syntax errors in the script or issues preventing its execution.</p></li><li><p>Insufficient permissions</p><p>The user running the command inside the container may not have the necessary permissions to execute the specified command. Ensure that the container runs with appropriate privileges.</p></li><li><p>Image compatibility issues</p><p>Ensure that the container image used is compatible with the host architecture and operating system. Mismatched images may result in commands not being found, such as running an x86 image on an ARM machine.</p></li><li><p>Volume mounts</p><p>If the command relies on files mounted from a volume, check if the volume mount is configured correctly and the required files are accessible.</p></li><li><p>Environment variables</p><p>Some commands may depend on specific environment variables. Ensure that the necessary environment variables are set correctly.</p></li><li><p>Kubernetes RBAC policies</p><p>If RBAC is enabled, ensure that the necessary permissions are granted to execute the specified command.</p></li></ol><h3 id="How-to-Troubleshoot"><a href="#How-to-Troubleshoot" class="headerlink" title="How to Troubleshoot"></a>How to Troubleshoot</h3><p>To diagnose the issue, you can use the following commands to check the logs of the Pod:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f </span><br></pre></td></tr></table></figure><p>You can also inspect the Pod’s status, which provides detailed information about the Pod, including its current state, recent events, and any error messages.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod </span><br></pre></td></tr></table></figure><p>Additionally, you can attach a debugging container to the Pod, which includes a shell (e.g., BusyBox). This allows you to enter the container and manually check the availability of environment, paths, and commands.</p><p>Example of debugging with BusyBox:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    command: [&quot;/bin/sleep&quot;, &quot;36000&quot;]</span><br><span class="line">  - name: debug</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br></pre></td></tr></table></figure><p>If you are using a higher version of Kubernetes, you can also utilize Ephemeral Containers, which are temporary containers. This is a new feature introduced as alpha in Kubernetes v1.16, and enabling the feature of Ephemeral Containers is straightforward. Just configure <code>--feature-gates=EphemeralContainers=true</code> in the kube-api and kubelet services, then restart.</p><p>By carefully examining the logs and investigating in the above directions, you should be able to determine the cause of the exit code 127 issue.</p><h3 id="How-to-Fix"><a href="#How-to-Fix" class="headerlink" title="How to Fix"></a>How to Fix</h3><p>Now that we know the common causes of exit code 127 and how to troubleshoot them, let’s see how to fix them.</p><ol><li>Command or binary not installed</li></ol><p>If the required command or binary is missing, it may need to be installed in the container image. Modify the Dockerfile or the build process to install the necessary software.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add &lt;package&gt; </span><br></pre></td></tr></table></figure><ol><li>Incorrect path or command</li></ol><p>When specifying commands in the Pod definition, consider using the absolute path of the binary. This helps ensure that the binary is found by the runtime, regardless of the current working directory.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    command: [&quot;/usr/bin/command&quot;]</span><br></pre></td></tr></table></figure><ol><li>Missing dependencies</li></ol><p>The reason for the command not running may be that additional software needs to be installed in the container image. If the command requires additional setup or installation steps, you can use init containers to perform these tasks before the main container starts.</p><p>Example (installing a package using init container):</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">  - name: install-package</span><br><span class="line">    image: alpine:latest</span><br><span class="line">    command: [&quot;apk&quot;, &quot;--no-cache&quot;, &quot;add&quot;, &quot;&lt;package-name&gt;&quot;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-data</span><br><span class="line">      mountPath: /data</span><br></pre></td></tr></table></figure><ol><li>Shell interpreter</li></ol><p>If a script is specified as a command, ensure that the script is valid (e.g., <code>#!/bin/bash</code>) and available in the container.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br></pre></td></tr></table></figure><ol><li>Volume mounts</li></ol><p>Check the Pod’s configuration to ensure that volumes are correctly mounted. Verify that the volume names, mount paths, and subPaths are correct.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: test</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">containers:</span><br><span class="line">  - name: test</span><br><span class="line">    image: test</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: test</span><br><span class="line">      mountPath: /path in container</span><br></pre></td></tr></table></figure><p>Additionally, confirm that the volume specified in the Pod definition exists and is accessible. If it’s a persistent volume (PV), check its status. If it’s an emptyDir or other types of volumes, verify that they are created and mounted correctly. If subPaths are used in the volume mount, ensure that the specified subPaths exist in the source directory or file.</p><p>Example:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumeMounts:</span><br><span class="line">  - name: test</span><br><span class="line">    mountPath: /path in container</span><br><span class="line">    subPath: my-file.txt</span><br></pre></td></tr></table></figure><h2 id="Exit-Code-137"><a href="#Exit-Code-137" class="headerlink" title="Exit Code 137"></a>Exit Code 137</h2><p>In Kubernetes, the exit code 137 indicates that the process was terminated forcibly. In Unix and Linux systems, when a process is terminated due to a signal, the exit code is determined by adding the signal number to 128. Since the signal number for “SIGKILL” is 9, adding 128 to 9 results in exit code 137.</p><p>When a container exceeds its memory limit in a Kubernetes cluster, it may be terminated by the Kubernetes system with an “OOMKilled” error, indicating that the process was terminated due to insufficient memory. The exit code for this error is 137, where OOM stands for “out-of-memory”.</p><p>If the Pod state shows as “OOMKilled”, you can check it using the following command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pods PODNAME</span><br></pre></td></tr></table></figure><h3 id="OOMKiller"><a href="#OOMKiller" class="headerlink" title="OOMKiller"></a>OOMKiller</h3><p>OOMKiller is a mechanism in the Linux kernel responsible for preventing the system from running out of memory by terminating processes that consume too much memory. When the system runs out of memory, the kernel invokes OOMKiller to select a process to terminate in order to free up memory and keep the system running.</p><p>There are two different OOM Killers in the kernel; one is the global OOM Killer, and the other is the OOM Killer based on cgroup memory control, which can be either cgroup v1 or cgroup v2.</p><p>In summary, when the kernel encounters issues allocating physical memory pages, the global OOM Killer is triggered. When the kernel attempts to allocate memory pages (whether for kernel use or for processes needing pages) and initially fails, it tries various ways to reclaim and consolidate memory. If this attempt is successful or at least makes some progress, the kernel will continue retrying allocation (from the code I can see); if it fails to free pages or make progress, it will often trigger the OOM Killer in many cases.</p><p>Once the OOMKiller selects a process to terminate, it sends a signal to that process requesting it to gracefully terminate. If the process does not respond to the signal, the kernel forcibly terminates the process and releases its memory.</p><p>Note: Pods terminated due to memory issues may not necessarily be evicted from the node; if their restart policy is set to “Always”, they will attempt to restart the Pod.</p><p>At the system level, the Linux kernel maintains an oom_score for each process running on the host. The likelihood of a process being terminated depends on how high the score is.</p><p>The oom_score_adj value allows users to customize OOM processes and define when processes should be terminated. Kubernetes uses the oom_score_adj value when defining the Quality of Service (QoS) of Pods.</p><p>Kubernetes defines three types of QoS for Pods, each with a corresponding oom_score_adj value:</p><ul><li>Guaranteed: -997</li><li>BestEffort: 1000</li><li>Burstable: <em>min(max(2, 1000 — (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</em></li></ul><p>Where Pods of Guaranteed QoS have an oom_score_adj value of -997, so they are the last ones to be terminated when the node runs out of memory. BestEffort Pod configurations have an oom_score_adj value of 1000, so they are the first ones to be terminated.</p><p>To check the QoS of a Pod, you can use the following command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o jsonpath=&#x27;&#123;.status.qosClass&#125;&#x27;</span><br></pre></td></tr></table></figure><p>Here’s the calculation policy for defining Pods of <code>Guaranteed</code> QoS type:</p><ul><li>Each container in the Pod must have memory limits and memory requests.</li><li>For each container in the Pod, the memory limit must be equal to the memory request.</li><li>Each container in the Pod must have CPU limits and CPU requests.</li><li>For each container in the Pod, the CPU limit must be equal to the CPU request.</li></ul><p>Exit code 137 typically has two scenarios:</p><p>First and foremost, the most common cause is related to resource constraints. In this scenario, Kubernetes typically exceeds the memory allocation limit of the container. When this happens, it terminates the container to ensure the stability of the node.</p><p>The other scenario involves manual intervention - a user or a script may send a “SIGKILL” signal to the container process, resulting in this exit code. OOMKilled (exit code 137)</p><h3 id="How-to-Troubleshoot-1"><a href="#How-to-Troubleshoot-1" class="headerlink" title="How to Troubleshoot"></a>How to Troubleshoot</h3><ol><li>Check Pod logs</li></ol><p>The first step in diagnosing OOMKilled errors is to check the Pod logs for any error messages indicating memory issues. The events section of the describe command will provide further confirmation and the time&#x2F;date of the error occurrence.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br><span class="line">State:          Running</span><br><span class="line">       Started:      Fri, 12 May 2023 11:14:13 +0200</span><br><span class="line">       Last State:   Terminated</span><br><span class="line">       Reason:       OOMKilled</span><br><span class="line">       Exit Code:    137</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><p>You can also query the Pod logs:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/pods/&lt;podname&gt;</span><br></pre></td></tr></table></figure><p>Of course, you can also do it via (standard output)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f &lt;podname&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>Monitor memory usage</li></ol><p>Monitor memory usage in Pods and containers using monitoring systems like Prometheus or Grafana. This can help us identify which containers are consuming too much memory and triggering OOMKilled errors, and you can use dmesg on the container host to check the scene of the oomkiller.</p><ol start="3"><li>Use memory profilers</li></ol><p>Use memory profilers like pprof to identify memory leaks or inefficient code that may be causing excessive memory usage.</p><h3 id="How-to-Fix-1"><a href="#How-to-Fix-1" class="headerlink" title="How to Fix"></a>How to Fix</h3><p>Below are common causes of OOMKilled Kubernetes errors and their solutions.</p><ol><li>Container memory limit reached</li></ol><p>This may be due to improper setting of the memory limit value specified in the container. The solution is to increase the value of the memory limit or investigate the root cause of the increased load and correct it. Common causes of this situation include large file uploads, as uploading large files can consume a significant amount of memory resources, especially when multiple containers are running in a single Pod, and sudden increases in traffic volume.</p><ol start="2"><li>Application memory leak, container memory usage reaches the upper limit</li></ol><p>Debug the application to locate the cause of the memory leak.</p><ol start="3"><li>Total memory used by all Pods exceeds available node memory</li></ol><p>Increase the available memory of the node by increasing the memory of the node, or migrate Pods to nodes with more memory. Alternatively, adjust the memory limits of Pods running on the node to comply with memory constraints.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://spacelift.io/blog/oomkilled-exit-code-137">https://spacelift.io/blog/oomkilled-exit-code-137</a></p><p><a href="https://spacelift.io/blog/exit-code-127">https://spacelift.io/blog/exit-code-127</a></p><p><a href="https://cloud.tencent.com/developer/news/1152344">https://cloud.tencent.com/developer/news/1152344</a></p><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen">https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Interpreting-common-exit-codes-in-Kubernetes&quot;&gt;&lt;a href=&quot;#Interpreting-common-exit-codes-in-Kubernetes&quot; class=&quot;headerlink&quot; title=&quot;Inte</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>解读Kubernetes常见错误码</title>
    <link href="https://zoues.com/posts/b421b57/"/>
    <id>https://zoues.com/posts/b421b57/</id>
    <published>2024-03-02T12:23:48.000Z</published>
    <updated>2024-07-13T00:50:26.165Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>在这篇文章中，我们将深入分析Kubernetes中的退出码127与137，解释它们是什么，K8s和Docker中常见的原因是什么，以及如何修复它们！</p><p>我们将涵盖以下内容：</p><ol><li><p>退出码的历史 </p></li><li><p>退出码 127</p></li><li><p>退出码 137</p></li></ol><h2 id="退出码的历史"><a href="#退出码的历史" class="headerlink" title="退出码的历史"></a>退出码的历史</h2><p>进程退出码的历史可以追溯到Unix操作系统的早期。在Unix系统中，进程退出码是进程终止时向其父进程传递的一个整数值，用于表示进程的终止状态。这个整数值通常在0到255之间，其中0表示进程成功终止，其他值通常用来表示不同的错误或异常情况。</p><p>进程退出码最初被设计用于提供一种简单的机制，使父进程能够了解子进程的执行结果。这使得父进程能够根据子进程的退出码来采取适当的行动，比如处理错误情况或继续执行其他操作。</p><p>在Unix系统中，特定的退出码值通常具有特定的含义，例如：</p><ul><li>0：表示成功执行，没有错误。</li><li>1：通常表示通用的错误。</li><li>2：表示命令的语法错误。</li><li>127：表示命令未找到。</li></ul><p>随着时间的推移，Unix操作系统的发展和不同的实现，进程退出码的含义可能有所不同，但基本的概念保持不变。</p><p>在Linux系统中，进程退出码的使用与Unix系统类似。Linux继承了Unix的进程管理机制，并在其基础上进行了扩展和改进。因此，Linux中的进程退出码仍然是一个重要的概念，用于帮助理解和诊断进程的执行状态。</p><p>进程退出码的历史可以追溯到早期的Unix系统，是Unix和Linux操作系统中的一个重要概念，为进程间通信提供了一种简单而有效的机制。当应用程序或命令因致命错误而终止或执行失败时，将产生 128 系列退出码（<code>128+n</code>），其中 <code>n</code> 为信号编号。<code>n</code> 包括所有类型的终止代码，如 <code>SIGTERM</code>、<code>SIGKILL</code> 等。</p><h2 id="退出码127"><a href="#退出码127" class="headerlink" title="退出码127"></a>退出码127</h2><p>退出码 127 不是特定于 Kubernetes 的错误代码，而是 Linux 和类 Unix 操作系统中使用的标准退出码。当然，我们在Kubernetes中经常看到它，并且通常表示容器内执行的命令或二进制文件找不到。</p><p>一些标准的退出码包括：</p><table><thead><tr><th>退出码</th><th>解释</th></tr></thead><tbody><tr><td>0</td><td>命令成功执行</td></tr><tr><td>1</td><td>通用错误</td></tr><tr><td>2</td><td>命令（参数）使用不当</td></tr><tr><td>126</td><td>权限被拒绝、无法执行</td></tr><tr><td>127</td><td>未找到命令行、PATH错误</td></tr><tr><td>128+n</td><td>命令被信号从外部终止、遇到致命错误</td></tr><tr><td>&gt;255</td><td>退出码超过255范围的，会重新计算（mod 256）</td></tr></tbody></table><p>让我们看一下退出码 127 的一些常见原因：</p><ol><li><p>命令或二进制文件未安装 </p><p>Kubernetes 容器的 command 字段中指定的可执行文件未安装在容器的文件系统中。需要确保所需的二进制文件或命令可用。</p></li><li><p>路径或命令不正确</p><p>Pod 定义中指定的命令不正确或在指定的路径中不存在。这是错误的最常见原因之一，通常是由于 Dockerfile 或 pod  spec中的entrypoint或command输入不正确造成的。</p></li><li><p>缺少依赖</p><p>在容器内运行的应用程序或脚本未安装相关依赖。需要确保所有必需的依赖项包含在容器映像中。</p></li><li><p>shell 解释器</p><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p></li><li><p>shell 脚本语法错误</p><p>如果 shell 脚本退出码是127，请检查脚本是否存有语法错误或可能阻止其执行的问题。</p></li><li><p>权限不足</p><p>在容器内运行命令的用户可能没有执行指定命令所需的必要权限。确保容器以适当的特权运行。</p></li><li><p>镜像兼容性问题</p><p>确保使用的容器镜像与宿主机架构和操作系统兼容。不匹配的映像可能导致命令找不到，比如x86的镜像运行在arm的机器上</p></li><li><p>卷挂载</p><p>如果命令是卷挂载的文件，请检查卷挂载是否配置正确，且所需的文件可以被访问到。</p></li><li><p>环境变量</p><p>一些命令可能依赖于特定的环境变量。确保必需的环境变量设置正确。</p></li><li><p>Kubernetes RBAC 策略</p><p> 如果启用了RBAC，需要确保具有执行指定命令所需的权限。</p></li></ol><h3 id="如何排查"><a href="#如何排查" class="headerlink" title="如何排查"></a>如何排查</h3><p>要排除问题，可以使用以下命令检查 Pod 的日志：</p><p><code>kubectl logs -f &lt;pod-name&gt; </code></p><p>还可以检查 Pod 状态，该状态提供有关 Pod 的详细信息，包括其当前状态、最近事件和任何错误消息。</p><p><code>kubectl describe pod &lt;pod-name&gt; </code></p><p>还可以为把调试容器attach到Pod 中，该容器包括一个 shell（例如 BusyBox）。这允许您进入容器并手动检查环境、路径和命令的可用性。</p><p>使用 BusyBox 进行调试的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/bin/sleep&quot;, &quot;infinity&quot;]</span><br><span class="line">  - name: debug-container</span><br><span class="line">    image: busybox:latest</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br><span class="line">    tty: true</span><br><span class="line">    stdin: true</span><br></pre></td></tr></table></figure><p>如果是高版本K8s，也可以使用Ephemeral Containers，它就是一个临时容器。这是一个自Kubernetes v1.16中作为alpha引入的新功能，启用临时容器的特性也非常简单，在kubernetes v1.16之后的版本中将启动参数<code>--feature-gates=EphemeralContainers=true</code>配置到kube-api和kubelet服务上重启即可。</p><p>通过仔细查看日志并排查上述几个方向，应该能够确定退出码 127 问题的原因。</p><h3 id="如何修复"><a href="#如何修复" class="headerlink" title="如何修复"></a>如何修复</h3><p> 我们知道了退出码 127 的常见原因以及排查方式，现在让我们看看如何修复它们。</p><ol><li>命令或二进制文件未安装</li></ol><p>如果所需的命令或二进制文件丢失，则可能需要在容器镜像中安装。修改 Dockerfile 或构建过程安装所需软件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add &lt;package-name&gt; </span><br></pre></td></tr></table></figure><ol start="2"><li>路径或命令不正确</li></ol><p>在 Pod 定义中指定命令时，考虑使用二进制文件的绝对路径。这有助于确保不受当前工作目录的影响， runtime可以找到二进制文件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/usr/local/bin/my-command&quot;]</span><br></pre></td></tr></table></figure><ol start="3"><li>缺少依赖项</li></ol><p>导致命令无法运行的原因可能是容器镜像需要安装额外的软件。如果命令需要额外的设置或安装步骤，可以使用init容器在主容器启动之前执行这些任务。</p><p>示例（使用init容器安装软件包）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">  - name: install-package</span><br><span class="line">    image: alpine:latest</span><br><span class="line">    command: [&quot;apk&quot;, &quot;--no-cache&quot;, &quot;add&quot;, &quot;&lt;package-name&gt;&quot;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-data</span><br><span class="line">      mountPath: /data</span><br></pre></td></tr></table></figure><ol start="4"><li>shell解释器</li></ol><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br></pre></td></tr></table></figure><ol start="5"><li>卷挂载</li></ol><p>检查Pod的配置，确保卷已正确挂载。验证卷名称、挂载路径和 subPaths是否正确。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: my-volume</span><br><span class="line">      mountPath: /path/in/container</span><br></pre></td></tr></table></figure><p>同时我们需要确认Pod 定义指定的卷存在且可用。如果是持久卷（PV），需要检查其状态。如果是 emptyDir 或其他类型的卷，需要验证其是否正确创建和挂载。如果在卷挂载中使用了 subPaths，需要确保源目录或文件中存在指定的 subPaths。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumeMounts:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    mountPath: /path/in/container</span><br><span class="line">    subPath: my-file.txt</span><br></pre></td></tr></table></figure><h2 id="退出码137"><a href="#退出码137" class="headerlink" title="退出码137"></a>退出码137</h2><p>在Kubernetes中，137退出码表示进程被强制终止。在Unix和Linux系统中，当进程由于信号而终止时，退出码由信号编号加上128确定。信号编号为9，意味着“SIGKILL”，因此将9加上128，得到137退出码。</p><p>当Kubernetes集群中容器超出其内存限制时，它可能会被Kubernetes系统终止，并显示“OOMKilled”错误，这表示进程因内存不足而被终止。此错误的退出码为137OOM代表“内存耗尽（out-of-memory）”。</p><p>如果Pod状态将显示为“OOMKilled”，你可以使用以下命令查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br></pre></td></tr></table></figure><h3 id="OOMKiller"><a href="#OOMKiller" class="headerlink" title="OOMKiller"></a>OOMKiller</h3><p>OOMKiller是Linux内核中的一种机制，它负责通过终止消耗过多内存的进程来防止系统耗尽内存。当系统内存耗尽时，内核会调用OOMKiller来选择一个要终止的进程，以释放内存并保持系统运行。</p><p>内核中有两种不同的OOM Killer；一种是全局的OOM Killer，另一种是基于cgroup内存控制器的OOM Killer，可以是cgroup v1或cgroup v2。</p><p>简单来说是，当内核在分配物理内存页面时遇到问题时，全局的OOM Killer 会触发。当内核尝试分配内存页面（无论是用于内核使用还是用于需要页面的进程），并且最初失败时，它将尝试各种方式来回收和整理内存。如果这种尝试成功或者至少取得了一些进展，内核将继续重试分配（从代码中我可以看到）；如果无法释放页面或者取得进展，它将在许多情况下触发OOM Killer。</p><p>一旦OOMKiller选择要终止的进程，它会向该进程发送信号，要求其优雅地终止。如果进程不响应信号，则内核会强制终止该进程并释放其内存。</p><p>注意：由于内存问题而被终止的Pod不一定会被节点驱逐，如果其设置的重启策略设置为“Always”，它将尝试重新启动Pod。</p><p>在系统层面，Linux内核为运行在主机上的每个进程维护一个oom_score。进程被终止的机率取决于分数有多高。</p><p>oom_score_adj值允许用户自定义OOM进程，并定义何时应终止进程。Kubernetes在定义Pod的Quality of Service（QoS）时使用oom_score_adj值。</p><p>K8s针对Pod定义了三种QoS，每个类型具有对应的oom_score_adj值：</p><ul><li>Guaranteed: -997</li><li>BestEffort: 1000</li><li>Burstable: <em>min(max(2, 1000 — (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</em></li></ul><p>其中Pod为Guaranteed QoS，则其oom_score_adj的值是-997，因此它们在节点内存不足时最后一个被终止。BestEffort Pod配置的是1000，所以它们第一个被被终止。</p><p>要查看Pod的QoS，可以通过下述命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o jsonpath=&#x27;&#123;.status.qosClass&#125;&#x27;</span><br></pre></td></tr></table></figure><p>下面是定义Pod<code>Guaranteed</code> QoS 类型的计算策略：</p><ul><li>Pod 中的每个容器必须有内存 limit 和内存 request。</li><li>对于 Pod 中的每个容器，内存 limit 必须等于内存 request。</li><li>Pod 中的每个容器必须有 CPU limit 和 CPU request。</li><li>对于 Pod 中的每个容器，CPU limit 必须等于 CPU request。</li></ul><p>退出码137通常有两种情况：</p><ul><li><p>首先，也是最常见的原因是与资源限制相关。在这种情况下，通常情况下，Kubernetes超出了容器的分配内存限制，当发生这种情况时，它将终止容器以确保节点的稳定性。 </p></li><li><p>另一种情况是手动干预 - 用户或脚本可能会向容器进程发送“SIGKILL”信号，导致此退出码。 OOMKilled（退出码137）</p></li></ul><h3 id="如何排查-1"><a href="#如何排查-1" class="headerlink" title="如何排查"></a>如何排查</h3><ol><li>检查Pod日志</li></ol><p>诊断OOMKilled错误的第一步是检查Pod日志，查看是否有任何指示内存问题的错误消息。描述命令的事件部分将提供进一步的确认以及发生错误的时间&#x2F;日期。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br><span class="line">State:          Running</span><br><span class="line">       Started:      Fri, 12 May 2023 11:14:13 +0200</span><br><span class="line">       Last State:   Terminated</span><br><span class="line">       Reason:       OOMKilled</span><br><span class="line">       Exit Code:    137</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><p>您还可以查询Pod日志：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/pods/&lt;podname&gt;</span><br></pre></td></tr></table></figure><p>当然也可以通过(标准输出)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f &lt;podname&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>监视内存使用情况</li></ol><p>使用监视系统（如Prometheus或Grafana）监视Pod和容器中的内存使用情况。这可以帮助我们确定哪些容器消耗了过多的内存从而触发了OOMKilled错误，也可以在容器宿主机使用dmesg查看当时oomkiller的现场</p><ol start="3"><li>使用内存分析器</li></ol><p>使用内存分析器（如pprof）来识别可能导致过多内存使用的内存泄漏或低效代码。</p><h3 id="如何修复-1"><a href="#如何修复-1" class="headerlink" title="如何修复"></a>如何修复</h3><p>以下是OOMKilled Kubernetes错误的常见原因及其解决方法。</p><ol><li>容器内存限制已达到</li></ol><p>这可能是由于在容器指定的内存限制值设置不当导致的。解决方法是增加内存限制的值，或者调查导致负载增加的根本原因并进行纠正。导致这种情况的常见原因包括大文件上传，因为上传大文件可能会消耗大量内存资源，特别是当多个容器在一个Pod内运行时，以及突然增加的流量量。</p><ol start="2"><li>因为应用程序内存泄漏,容器内存使用达到上限</li></ol><p>需要调试应用程序来定位内存泄漏的原因，</p><ol start="3"><li>所有Pod使用的总内存大于节点可用内存</li></ol><p>通过增加节点可用内存来增加节点内存，或者将Pod迁移到内存更多的节点。当然也可以调整运行在节点上的Pod的内存限制，使其符合内存限制，注意你还应该注意内存请求设置，它指定了Pod应该使用的最小内存量。如果设置得太高，可能不是有效利用可用内存，关于资源配置相关的建议，可以参看VPA组件</p><p>在调整内存请求和限制时，当节点过载时，Kubernetes按照以下优先级顺序终止Pod：</p><ul><li><p>没有请求或限制的Pod。 </p></li><li><p>具有请求但没有限制的Pod。</p></li><li><p>使用超过其内存请求值的内存 - 指定的最小内存值 - 但低于其内存限制的Pod。 </p></li><li><p>使用超过其内存限制的Pod。</p></li></ul><h3 id="如何预防"><a href="#如何预防" class="headerlink" title="如何预防"></a>如何预防</h3><p>有几种方法可以防止OOMKilled的发生：</p><ol><li>设置适当的内存限制</li></ol><p>通过压测及监控来确定应用程序的内存使用，通过上述方式配置容器允许使用的最大内存量。过度保守可能会导致因资源利用率低效而造成资金的浪费，同时低估会导致频繁出现OOMKilled现象。</p><ol start="2"><li>HPA</li></ol><p>最佳做法是利用K8s提供的HPA机制，当应用程序的内存使用升高时自动增加Pod副本数量。</p><ol start="3"><li>节点资源分配</li></ol><p>确保节点具有足够的资源来处理业务。</p><ol start="4"><li>优化应用程序内存使用</li></ol><p>监视应用程序并进行适当优化，以减少内存消耗。</p><ol start="5"><li>避免应用程序中的内存泄漏</li></ol><p>从应用程序来看，需要长期检查并修复内存泄漏。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><p><a href="https://spacelift.io/blog/oomkilled-exit-code-137">https://spacelift.io/blog/oomkilled-exit-code-137</a></p></li><li><p><a href="https://spacelift.io/blog/exit-code-127">https://spacelift.io/blog/exit-code-127</a></p></li><li><p><a href="https://cloud.tencent.com/developer/news/1152344">https://cloud.tencent.com/developer/news/1152344</a></p></li><li><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen">https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Exploring the Root Cause of Kubernetes 1.28 Scheduler OOM</title>
    <link href="https://zoues.com/posts/e4e37b07/"/>
    <id>https://zoues.com/posts/e4e37b07/</id>
    <published>2024-03-01T09:13:10.000Z</published>
    <updated>2024-03-01T09:17:38.524Z</updated>
    
    <content type="html"><![CDATA[<p>Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a look together. In the context of fluctuating business tidal changes affecting both pods and nodes in the cluster, memory usage shows a continuous upward trend until reaching Out Of Memory (OOM) conditions.</p><h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><blockquote><p>The following data is all publicly available information from the community.</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>The triggering scenarios include the following two types (there are also other reproduction methods in the community):</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=60</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>Our findings in the community revealed multiple instances of similar memory anomaly scenarios, with various methods of reproduction. The conclusion regarding the above issue is as follows:</p><blockquote><p>The Kubernetes community defaulted to enabling the scheduling feature SchedulerQueueingHints in version 1.28, which led to memory anomalies in the scheduler component. To temporarily address memory-related issues, the community adjusted this feature to default to disabled in version 1.28.5. However, as the problem has not been completely resolved, it is advisable to exercise caution when enabling this feature.</p></blockquote><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><p>This section introduces the following content:</p><ul><li>Introduction to Kubernetes scheduler related data structures</li><li>Introduction to Kubernetes scheduler QueueingHint</li><li>Doubly linked lists in Golang</li></ul><h3 id="K8s-Scheduler-Introduction"><a href="#K8s-Scheduler-Introduction" class="headerlink" title="K8s-Scheduler Introduction"></a>K8s-Scheduler Introduction</h3><p>The PriorityQueue is an interface implementation of the SchedulingQueue. It holds the highest priority pod ready for scheduling at its head. PriorityQueue contains the following important fields:</p><ol><li>activeQ: This queue holds pods ready for scheduling. Newly added pods are placed into this queue. When the scheduling queue needs to perform scheduling, it fetches pods from this queue. activeQ is implemented using a heap.</li><li>backoffQ: This queue holds pods that have been determined to be unschedulable for various reasons (such as not meeting node requirements). These pods will be moved to activeQ to attempt scheduling again after a certain backoff period. backoffQ is also implemented using a heap.</li><li>unschedulablePods: This map data structure holds pods that cannot be scheduled for various reasons. Instead of directly placing them in backoffQ, they are recorded here. When conditions are met, they will be moved to activeQ or backoffQ. The scheduling queue periodically cleans up pods in unschedulablePods.</li><li>inFlightEvents: This is used to store events received by the scheduling queue (with the entry value as clusterEvent) and pods that are currently being processed (with the entry value as *v1.Pod). It’s based on a doubly linked list implemented in Go.</li><li>inFlightPods: This holds the UIDs of all pods that have been popped but have not yet had Done called on them. In other words, it keeps track of all pods that are currently being processed (i.e., in scheduling, in admit, or in binding phases).</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p> For a comprehensive introduction to Kubernetes, please refer to <a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">An In-depth Introduction to Kubernetes Scheduling: Framework</a>. Updates on the latest Kubernetes scheduler will be provided subsequently.</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>Kubernetes scheduler introduced the <code>QueueingHint</code> feature to provide recommendations for re-queueing pods from each plugin. This aims to reduce unnecessary scheduling retries, thereby enhancing scheduling throughput. Additionally, it skips backoff under appropriate circumstances to further improve pod scheduling efficiency.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Currently, each plugin can define when to retry scheduling pods that have been rejected by the plugin through the <code>EventsToRegister</code> mechanism.</p><p>For example, the <code>NodeAffinity</code> plugin may retry scheduling pods when nodes are added or updated because newly added or updated nodes may have labels that match the node affinity on the pod. However, in practice, a large number of node update events occur in the cluster, which does not guarantee successful scheduling of pods previously rejected by <code>NodeAffinity</code>.</p><p>To address this issue, the scheduler introduced more refined callback functions to filter out irrelevant events, thereby only retrying pods that are likely to be successfully scheduled in the next scheduling cycle.</p><p>Furthermore, the Dynamic Resource Allocation (DRA) scheduling plugin sometimes needs to reject pods to wait for status updates from device drivers. Therefore, certain pods may require several scheduling cycles to be scheduled. For this scenario, the wait time for fallback is longer compared to waiting for device driver status updates. Hence, there is a need to allow plugins to skip fallback in specific cases to improve scheduling performance.</p><h3 id="Implementation-Goals"><a href="#Implementation-Goals" class="headerlink" title="Implementation Goals"></a>Implementation Goals</h3><p>To improve scheduling throughput, the community proposed the following enhancements:</p><ol><li>Introduction of QueueingHint<ul><li>Introducing <code>QueueingHint</code> into the <code>EventsToRegister</code> mechanism, allowing plugins to provide recommendations for re-queuing pods.</li></ul></li><li>Enhancement of Pod Tracking and Re-queueing Mechanism<ul><li>Optimizing the implementation for tracking pods currently being processed in the scheduling queue.</li><li>Implementing a mechanism to re-queue rejected pods to appropriate queues.</li><li>Optimizing the backoff strategy for rejected pods, allowing plugins to skip backoff in specific cases to improve scheduling throughput.</li></ul></li></ol><h3 id="Potential-Risks"><a href="#Potential-Risks" class="headerlink" title="Potential Risks"></a>Potential Risks</h3><blockquote><h4 id="1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods"><a href="#1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods" class="headerlink" title="1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods"></a>1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods</h4></blockquote><p>If a plugin is configured with QueueingHint but misses some events that could make pods schedulable, pods rejected by that plugin may remain stuck in unschedulablePods for a long time.</p><p>Although the scheduling queue periodically cleans up pods in unschedulablePods (default is 5 minutes, configurable).</p><blockquote><h4 id="2-Increase-in-Memory-Usage"><a href="#2-Increase-in-Memory-Usage" class="headerlink" title="2. Increase in Memory Usage"></a>2. Increase in Memory Usage</h4></blockquote><p>As the scheduling queue needs to retain events occurring during scheduling, the memory usage of kube-scheduler will increase. Therefore, the busier the cluster, the more memory it may require.</p><p>Although it’s not possible to completely eliminate memory growth, releasing cached events as soon as possible can slow down the rate of memory growth.</p><blockquote><h4 id="3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension"><a href="#3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension" class="headerlink" title="3. Significant Changes in EventsToRegister in EnqueueExtension"></a>3. Significant Changes in <code>EventsToRegister</code> in <code>EnqueueExtension</code></h4></blockquote><p>Developers of custom scheduler plugins need to perform compatibility upgrades, as the return type of <code>EventsToRegister</code> in <code>EnqueueExtension</code> has changed from <code>ClusterEvent</code> to <code>ClusterEventWithHint</code>. <code>ClusterEventWithHint</code> allows each plugin to filter out more useless events through a callback function called <code>QueueingHintFn</code>.</p><p>To simplify the migration work, an empty <code>QueueingHintFn</code> is considered to always return <code>Queue</code>. Thus, if they only want to maintain the existing behavior, they only need to change <code>ClusterEvent</code> to <code>ClusterEventWithHint</code> without registering any <code>QueueingHintFn</code>.</p><h3 id="Design-of-QueueingHints"><a href="#Design-of-QueueingHints" class="headerlink" title="Design of QueueingHints"></a>Design of QueueingHints</h3><p>The return type of the <code>EventsToRegister</code> method has been changed to <code>[]ClusterEventWithHint</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Each <code>ClusterEventWithHint</code> structure consists of a <code>ClusterEvent</code> and a <code>QueueingHintFn</code>. When an event occurs, the <code>QueueingHintFn</code> is executed to determine whether the event can make the pod eligible for scheduling.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The type <code>QueueingHintFn</code> is a function with a return type of <code>(QueueingHint, error)</code>. Here, <code>QueueingHint</code> is an enumeration type with possible values of <code>QueueSkip</code> and <code>Queue</code>. The invocation of <code>QueueingHintFn</code> occurs before moving pods from <code>unschedulableQ</code> to <code>backoffQ</code> or <code>activeQ</code>. If an error is returned, the <code>QueueingHint</code> provided by the caller will be treated as <code>QueueAfterBackoff</code>, which ensures that the pod does not remain indefinitely in the <code>unschedulableQ</code> queue, regardless of the returned result.</p><h3 id="When-to-Skip-Not-Skip-Backoff"><a href="#When-to-Skip-Not-Skip-Backoff" class="headerlink" title="When to Skip&#x2F;Not Skip Backoff"></a>When to Skip&#x2F;Not Skip Backoff</h3><p>The <code>backoffQ</code> prevents pods that are “long-term unschedulable” from blocking the queue, maintaining a lightweight queue with high throughput.</p><p>The longer a pod is rejected during the scheduling cycle, the longer it waits in the <code>backoffQ</code>.</p><p>For example, when <code>NodeAffinity</code> rejects a pod, and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod needs to wait for backoff before retrying scheduling.</p><p>However, certain plugins are designed to experience some failures during the scheduling cycle itself. For instance, the built-in DRA (Dynamic Resource Allocation) plugin, at the Reserve extension, informs the resource driver of the scheduling result and rejects the pod once to wait for a response from the resource driver. For such rejection scenarios, it should not be considered wasted scheduling cycles. Although a specific scheduling cycle fails, the scheduling result based on that cycle can facilitate pod scheduling. Therefore, pods rejected for this reason do not need to be penalized (backoff).</p><p>To support this scenario, we introduce a new state, <code>Pending</code>. When the DRA plugin rejects a pod using <code>Pending</code> and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod skips backoff and is rescheduled.</p><h3 id="How-QueueingHint-Works"><a href="#How-QueueingHint-Works" class="headerlink" title="How QueueingHint Works"></a>How QueueingHint Works</h3><p>When Kubernetes cluster events occur, the scheduling queue executes the <code>QueueingHintFn</code> of those plugins that rejected pods in the previous scheduling cycle.</p><p>The following scenarios describe how they are executed and how pods are moved:</p><blockquote><p>Pod Rejected by One or More Plugins</p></blockquote><p>Suppose there are three nodes. When a pod enters the scheduling cycle, one node rejects the pod due to insufficient resources, and the other two nodes reject it due to mismatching node affinity.</p><p>In this scenario, the pod is rejected by the <code>NodeResourceFit</code> and <code>NodeAffinity</code> plugins and is eventually placed in <code>unschedulableQ</code>.</p><p>Subsequently, whenever cluster events registered in these plugins occur, the scheduling queue notifies them via <code>QueueingHint</code>. If any <code>QueueingHintFn</code> from <code>NodeResourceFit</code> or <code>NodeAffinity</code> returns <code>Queue</code>, the pod is moved to <code>activeQ</code> or <code>backoffQ</code>. (For example, when a <code>NodeAdded</code> event occurs, <code>QueueingHint</code> from <code>NodeResourceFit</code> returns <code>Queue</code> because the pod may be schedulable to the new node.)</p><p>Whether it moves to <code>activeQ</code> or <code>backoffQ</code> depends on how long the pod has been in <code>unschedulableQ</code>. If the time spent in <code>unschedulableQ</code> exceeds the expected pod backoff delay time, it is moved directly to <code>activeQ</code>. Otherwise, it is moved to <code>backoffQ</code>.</p><blockquote><p>Pod Rejected due to Pending State</p></blockquote><p>When the DRA plugin returns <code>Pending</code> for a pod during the Reserve extension phase, the scheduling queue adds the DRA plugin to the pod’s <code>pendingPlugins</code> dictionary and returns the pod.</p><p>When a call to the <code>QueueingHint</code> of the DRA plugin returns <code>Queue</code> in subsequent invocations, the scheduling queue places the pod directly into <code>activeQ</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Tracking-Pods-Being-Processed-in-the-Scheduling-Queue"><a href="#Tracking-Pods-Being-Processed-in-the-Scheduling-Queue" class="headerlink" title="Tracking Pods Being Processed in the Scheduling Queue"></a>Tracking Pods Being Processed in the Scheduling Queue</h4><p>By introducing <code>QueueingHint</code>, we can only retry scheduling when specific events occur. But what if these events happen during pod scheduling?</p><p>The scheduler takes a snapshot of the cluster data and schedules pods based on the snapshot. The snapshot is updated each time a scheduling cycle is initiated, meaning the same snapshot is used within the same scheduling cycle.</p><p>Consider a scenario where, during the scheduling of a pod, it is rejected due to no nodes meeting the pod’s node affinity, but a new node matching the pod’s node affinity is added during the scheduling process.</p><p>As mentioned earlier, this new node is not considered a candidate node within the current scheduling cycle. Therefore, the pod is still rejected by the node affinity plugin. The issue arises if the scheduling queue puts the pod into <code>unschedulableQ</code>, as the pod would still need to wait for another event even though a node matching the pod’s node affinity requirement is available.</p><p>To prevent scenarios where pods miss events during scheduling, the scheduling queue records events occurring during pod scheduling and determines the pod’s queueing position based on these events and <code>QueueingHint</code>.</p><p>Therefore, the scheduling queue caches events from the time a pod leaves the scheduling queue until it returns to the scheduling queue or is scheduled. When the cached events are no longer needed, they are discarded.</p><h2 id="Golang-Doubly-Linked-List"><a href="#Golang-Doubly-Linked-List" class="headerlink" title="Golang Doubly Linked List"></a>Golang Doubly Linked List</h2><p><code>*list.List</code> is a data structure in Go’s standard library <code>container/list</code> package, representing a doubly linked list. In Go, doubly linked lists are a common data structure used to provide efficient performance for operations like element insertion, deletion, and traversal.</p><p>Here’s a brief overview of the <code>*list.List</code> structure:</p><ul><li><strong>Definition</strong>: <code>*list.List</code> is a pointer to a doubly linked list. It contains pointers to the head and tail of the list, as well as information about the length of the list.</li><li><strong>Features</strong>: Each node in the doubly linked list contains pointers to the previous and next nodes, making operations like inserting and deleting elements in the list efficient.</li><li><strong>Usage</strong>: <code>*list.List</code> is commonly used in scenarios where frequent insertion and deletion operations are required, especially when the number of elements is not fixed or the order may change frequently.</li></ul><p>Here’s a demonstration of how to use <code>*list.List</code> in Go:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">goCopy codepackage main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // Create a new doubly linked list</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // Add elements to the end of the list</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // Iterate over the list and print elements</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>PushBack</code> method adds a new element to the end of the list and returns a pointer to the new element (<code>*list.Element</code>). This pointer can be used for further operations on the element, such as removal or modification.</p><p>The <code>*list.Element</code> structure contains pointers to the previous and next elements in the list, as well as a field for storing the element’s value. By returning a <code>*list.Element</code> pointer, we can conveniently access the newly added element when needed for further operations. To remove an element from the doubly linked list, you can use the <code>list.Remove()</code> method. This method takes a list element as input and removes the element from the list.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>outputs：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>In this example, we remove the second element (with a value of 2) from the linked list.</p><h2 id="A-brief-analysis"><a href="#A-brief-analysis" class="headerlink" title="A brief analysis"></a>A brief analysis</h2><p>Let’s dive straight into analyzing the memory usage using pprof. Here’s a partial list of pprof profiles:</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>Here, we can observe that memory usage is primarily concentrated in protobuf’s <code>Decode</code>. Without delving into specific pprof analysis, we can consider three potential factors:</p><ul><li>Whether grpc-go has memory issues</li><li>Whether there are issues with Go itself</li><li>Whether Kubernetes has memory issues</li></ul><p>Regarding the first assumption, we can check related issues in grpc-go. However, recent reports do not indicate any memory anomalies. As for issues with Go itself, it doesn’t seem likely, although we did find a related Transparent Huge Pages (THP) issue, which we can briefly discuss later. Thus, the most probable cause would be an issue within Kubernetes itself. However, considering that <code>(*FieldsV1).Unmarshal</code> hasn’t been modified for 5 years, it’s highly unlikely to be the source of the problem. Therefore, let’s delve deeper into analyzing pprof.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>In the continuously growing list of pods, I noticed some unreleased data that seemed to align with the results of my previous analysis using pprof. Interestingly, pods are the only continuously changing objects. Therefore, I attempted another troubleshooting method to verify if the community had resolved this issue. I used minikube to launch Kubernetes version 1.18.5 locally for investigation. Fortunately, I couldn’t reproduce the issue, indicating that the problem might have been fixed after version 1.18.5.</p><p>To narrow down the investigation further, I asked my colleagues to inspect the commit history between these three minor versions. Eventually, we found a PR that closed the <code>SchedulerQueueingHints</code> feature. As mentioned in the technical background, the <code>SchedulerQueueingHints</code> feature could potentially lead to memory growth issues.</p><p>By examining the <code>PriorityQueue</code> structure, it was evident that the logic handling the feature was controlled by <code>isSchedulingQueueHintEnabled</code>. If the <code>QueueingHint</code> feature is enabled, when executing the <code>Pop</code> method to schedule pods, the UID of the corresponding pod in <code>inFlightPods</code> needs to be populated with the same linked list as <code>inFlightEvents</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So, when are the linked list fields removed? We can observe that the only time they are removed is when the pod completes its scheduling cycle, that is, when the <code>Done</code> method is called.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, it can be observed that the later the timing of <code>Done</code>, the more pronounced the memory growth, and if pod events are ignored or missed, abnormal memory growth in the linked list can also occur. Some fixes for the above scenario can be seen:</p><ul><li>A PR, such as #120586, which emphasizes calling <code>Done()</code> as soon as possible.</li><li>The <code>QueueingHint</code> of the NodeAffinity&#x2F;NodeUnschedulable plugins missed relevant Node events, as seen in PR#122284.</li></ul><p>Despite these modifications, such issues are far from over.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://openai.com/">https://openai.com/</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>探索Kubernetes 1.28调度器OOM的根源</title>
    <link href="https://zoues.com/posts/e46bd846/"/>
    <id>https://zoues.com/posts/e46bd846/</id>
    <published>2024-02-25T12:25:08.000Z</published>
    <updated>2024-07-13T00:51:35.754Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>年前，同事升级K8s调度器至1.28.3，观察到内存异常现象，帮忙一起看看，在集群pod及node随业务潮汐变动的情况下，内存呈现不断上升的趋势，直至OOM</p><blockquote><p>下面数据均为社区公开信息</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>触发场景有以下两种(社区还有其他复现方式)：</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=50</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>我们在社区的发现多起类似内存异常场景，复现方式不尽相同，关于上述问题的结论是：</p><blockquote><blockquote><p>Kubernetes社区在1.28版本中默认开启了调度特性SchedulerQueueingHints，导致调度组件内存异常。为了临时解决内存等问题，社区在1.28.5中将该特性调整为默认关闭。因为问题并未完全修复，所以建议审慎开启该特性。</p></blockquote></blockquote><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>介绍K8s调度器相关结构体</li><li>介绍K8s调度器QueueingHint</li><li>golang的双向链表</li></ul><h3 id="调度器简介"><a href="#调度器简介" class="headerlink" title="调度器简介"></a>调度器简介</h3><p>PriorityQueue是SchedulingQueue的接口实现。它的头部存放着优先级最高的待调度Pod。PriorityQueue包含以下重要字段：</p><ol><li>activeQ：存放准备好调度的Pod。新添加的Pod会被放入该队列。调度队列需要执行调度时，会从该队列中获取Pod。activeQ由堆来实现。</li><li>backoffQ：存放因各种原因（比如未满足节点要求）而被判定为无法调度的Pod。这些Pod会在一段退避时间后，被移到activeQ以尝试再次调度。backoffQ也由堆来实现。</li><li>unschedulablePods：存放因各种原因无法调度的Pod，是一个map数据结构。这些Pod被认定为无法调度，不会直接放入backoffQ，而是被记录在这里。待条件满足时，它们将被移到activeQ或者backoffQ中，调度队列会定期清理unschedulablePods 中的 Pod。</li><li>inFlightEvents：用于保存调度队列接收到的事件（entry的值是clusterEvent），以及正在处理中的Pod（entry的值是*v1.Pod），基于golang内部实现的双向链表</li><li>inFlightPods：保存了所有已经Pop，但尚未调用Done的Pod的UID，换句话说，所有当前正在处理中的Pod（正在调度、在admit中或在绑定周期中）。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>关于K8s完整介绍，参看<a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">kuberneter调度由浅入深：框架</a>，后续会更新最新的K8s调度器梳理</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>K8s调度器引入了<code>QueueingHint</code>特性，通过从每个插件获取有关Pod重新入队的建议，以减少不必要的调度重试，从而提升调度吞吐量。同时，在适当情况下跳过退避，进一步提高Pod调度效率。</p><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><p>当前，每个插件可以通过EventsToRegister定义何时重试调度被插件拒绝的Pod。</p><p>比如，NodeAffinity会在节点添加或更新时重试调度Pod，因为新添加或更新的节点可能具有与Pod上的NodeAffinity匹配的标签。然而，实际上，在集群中会发生大量节点更新事件，这并不能保证之前被NodeAffinity拒绝的Pod能够成功调度。</p><p>为了解决这个问题，调度器引入了更精细的回调函数，以过滤掉无关的事件，从而在下一个调度周期中仅重试可能成功调度的Pod。</p><p>另外，DRA（动态资源分配）调度插件有时需要拒绝Pod以等待来自设备驱动程序的状态更新。因此，某些Pod可能需要经过几个调度周期才能完成调度。针对这种情况，与等待设备驱动程序状态更新相比，回退等待的时间更长。因此，希望能够使插件在特定情况下跳过回退以改善调度性能。</p><h4 id="实现目标"><a href="#实现目标" class="headerlink" title="实现目标"></a>实现目标</h4><p>为了提高调度吞吐量，社区提出以下改进：</p><ol><li><strong>引入QueueingHint</strong><ul><li>将 <code>QueueingHint</code> 引入到 <code>EventsToRegister</code> 机制中，允许插件提供针对Pods重新入队的建议</li></ul></li><li><strong>增强 Pod 跟踪和重新入队机制</strong>：<ul><li>优化追踪调度队列内正在处理的 Pods实现</li><li>实现一种机制，将被拒绝的 Pods 重新入队到适当的队列</li><li>优化被拒绝的Pods的退避策略，能够使插件在特定情况下跳过回退，从而提高调度吞吐量。</li></ul></li></ol><h4 id="潜在风险"><a href="#潜在风险" class="headerlink" title="潜在风险"></a>潜在风险</h4><p><em><strong>1. 实现中的错误可能导致 Pod 在 unschedulablePods 中长时间无法被调度</strong></em></p><p>如果一个插件配置了 QueueingHint，但它错过了一些可以让 Pod 可调度的事件， 被该插件拒绝的 Pod 可能会长期困在 unschedulablePods 中。</p><p>虽然调度队列会定期清理unschedulablePods 中的 Pod。（默认为 5 分钟，可配）</p><p><em><strong>2. 内存使用量的增加</strong></em></p><p>因为调度队列需要保留调度过程中发生的事件，kube-scheduler的内存使用量会增加。 所以集群越繁忙，它可能需要的内存就越多。</p><p>虽然无法完全消除内存增长，但如果能够尽快释放缓存的事件，就可以延缓内存增长的速度。</p><p><em><strong>3.<code>EnqueueExtension</code> 中 <code>EventsToRegister</code> 中的重大变更</strong></em></p><p>自定义调度器插件的开发者需要进行兼容性升级， <code>EnqueueExtension</code> 中的 <code>EventsToRegister</code> 将返回值从 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code>。<code>ClusterEventWithHint</code> 允许每个插件通过名为 <code>QueueingHintFn</code> 的回调函数过滤更多无用的事件。</p><p>社区为了简化迁移工作，空的 <code>QueueingHintFn</code> 被视为始终返回 <code>Queue</code>。 因此，如果他们只想保持现有行为，他们只需要将 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code> 并不需要注册任何 <code>QueueingHintFn</code>。</p><h4 id="QueueingHints设计"><a href="#QueueingHints设计" class="headerlink" title="QueueingHints设计"></a>QueueingHints设计</h4><p>EventsToRegister 方法的返回类型已更改为 []ClusterEventWithHint</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// EnqueueExtensions 是一个可选接口，插件可以实现在内部调度队列中移动无法调度的 Pod。可以导</span><br><span class="line">// 致Pod无法调度（例如，Filter 插件）的插件可以实现此接口。</span><br><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 ClusterEventWithHint结构体包含一个 ClusterEvent 和一个 QueueingHintFn，当事件发生时执行 QueueingHintFn，并确定事件是否可以让 Pod满足调度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>类型 QueueingHintFn 是一个函数，其返回类型为 (QueueingHint, error)。其中，QueueingHint 是一个枚举类型，可能的值有 QueueSkip 和 Queue。QueueingHintFn 调用时机位于将 Pod 从 unschedulableQ 移动到 backoffQ 或 activeQ 之前，如果返回错误，将把调用方返回的 QueueingHint 处理为 <code>QueueAfterBackoff</code>，这种处理无论返回的结果是什么，都可以防止 Pod 永远待在unschedulableQ 队列中。</p><p><em><strong>何时跳过&#x2F;不跳过 backoff</strong></em></p><p>BackoffQ 通过防止“长期无法调度”的 Pod 阻塞队列以保持高吞吐量的轻量级队列。</p><p>Pod 在调度周期中被拒绝的次数越多，Pod 需要等待的时间就越长，即在BackoffQ 待得时间就越长。</p><p>例如，当 NodeAffinity 拒绝了 Pod，后来在其 QueueingHintFn 中返回 Queue 时，Pod 需要等待 backoff 后才能重试调度。</p><p>但是，某些插件的设计本身就需要在调度周期中经历一些失败。比如内置插件DRA（动态资源分配），在 Reserve extension处，它告诉资源驱动程序调度结果，并拒绝 Pod 一次以等待资源驱动程序的响应。针对这种拒绝情况，不能将其视作调度周期的浪费，尽管特定调度周期失败了，但基于该周期的调度结果可以促进 Pod 的调度。因此，由于这种原因被拒绝的 Pod 不需要受到惩罚（backoff）。</p><p>为了支持这种情况，我们引入了一个新的状态 Pending。当 DRA 插件使用 Pending 拒绝 Pod，并且后续在其 QueueingHintFn 中返回 Queue 时，Pod 跳过 backoff，Pod 被重新调度。</p><p><em><strong>QueueingHint 如何工作</strong></em></p><p>当K8s集群事件发生时，调度队列将执行在之前调度周期中拒绝 Pod 的那些插件的 QueueingHintFn。</p><p>通过下述几个场景，描述一下它们如何被执行以及如何移动 Pod。</p><p>a. Pod被一个或多个插件拒绝</p><p>假设有三个节点。当 Pod 进入调度周期时，一个节点由于资源不足拒绝了Pod，其他两个节因为Pod 的 NodeAffinity不匹配拒绝了Pod。</p><p>在这种情况下，Pod 被 NodeResourceFit 和 NodeAffinity 插件拒绝，最终被放到 unschedulableQ 中。</p><p>此后，每当注册在这些插件中的集群事件发生时，调度队列通过 QueueingHint 通知它们。如果来自 NodeResourceFit 或 NodeAffinity 的任何一个的 QueueingHintFn 返回 Queue，则将 Pod 移动到 activeQ或者backoffQ中。 （例如，当 NodeAdded 事件发生时，NodeResourceFit 的 QueueingHint 返回 Queue，因为 Pod 可能可调度到该新节点。）</p><p>它是移动到 activeQ 还是 backoffQ，这取决于此 Pod 在unschedulableQ 中停留的时间有多长。如果在unschedulableQ 停留的时间超过了预期的 Pod 的 backoff 延迟时间，则它将直接移动到 activeQ。否则，它将移动到 backoffQ。</p><p>b. Pod因 Pending 状态而被拒绝 </p><p>当 DRA 插件在 Reserve extension 阶段针对Pod返回 Pending时，调度队列将 DRA 插件添加到 Pod 的pendingPlugins 字典中的同时，Pod 返回调度队列。</p><p>当 DRA 插件的 QueueingHint 之后的调用中返回 Queue 时，调度队列将此 Pod 直接放入 activeQ。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>c. 跟踪调度队列中正在处理的 Pod</p><p>通过引入 QueueingHint，我们只能在特定事件发生时重试调度。但是，如果这些事件发生在Pod 的调度期间呢？</p><p>调度器对集群数据进行快照，并根据快照调度 Pod。每次启动调度周期时都会更新快照，换句话说，相同的快照在相同的调度周期中使用。</p><p>考虑到这样一个情景，比如，在调度一个 Pod 时，由于没有任何节点符合 Pod 的节点亲和性(NodeAffinity)，因此被拒绝，但是在调度过程中加入了一个新的节点，它与 Pod 的节点亲和性匹配。</p><p>如前所述，这个新节点在本次调度周期内不被视为候选节点，因此 Pod 仍然被节点亲和性插件拒绝。问题在于，如果调度队列将 Pod 放入unschedulableQ中，那么即使已经有一个节点匹配了 Pod 的节点亲和性要求，该 Pod 仍需要等待另一个事件。</p><p>为了避免类似Pod 在调度过程中错过事件的场景，调度队列会记录 Pod 调度期间发生的事件，并根据这些事件和QueueingHint来决定Pod 入队的位置。</p><p>因此，调度队列会缓存自 Pod 离开调度队列直到 Pod 返回调度队列或被调度的所有事件。当不再需要缓存的事件时，缓存的事件将被丢弃。</p><h3 id="Golang双向链表"><a href="#Golang双向链表" class="headerlink" title="Golang双向链表"></a>Golang双向链表</h3><p><code>*list.List</code> 是 Go 语言标准库 <code>container/list</code> 包中的一种数据结构，表示一个双向链表。在 Go 中，双向链表是一种常见的数据结构，用于在元素的插入、删除和遍历等操作上提供高效性能。</p><p>以下是 <code>*list.List</code> 结构的简要介绍：</p><ul><li><strong>定义</strong>：<code>*list.List</code> 是一个指向双向链表的指针，它包含了链表的头部和尾部指针，以及链表的长度信息。</li><li><strong>特性</strong>：双向链表中的每个节点都包含指向前一个节点和后一个节点的指针，这使得在链表中插入和删除元素的操作效率很高。</li><li><strong>用途</strong>：<code>*list.List</code> 常用于需要频繁插入和删除操作的场景，尤其是当元素的数量不固定或顺序可能经常变化时。</li></ul><p>演示了如何在 Go 中使用 <code>*list.List</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // 创建一个新的双向链表</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // 在链表尾部添加元素</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // 遍历链表并打印元素</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>PushBack</code> 方法会向链表的尾部添加一个新元素，并返回表示新元素的 <code>*list.Element</code> 指针。这个指针可以用于后续对该元素的操作，例如删除或修改。</p><p><code>*list.Element</code> 结构体包含了指向链表中前一个和后一个元素的指针，以及一个存储元素值的字段。通过返回 <code>*list.Element</code> 指针，我们可以方便地在需要时访问到新添加的元素，以便进行进一步的操作。要从双向链表中删除元素，你可以使用<code>list.Remove()</code>方法。这个方法需要传入一个链表元素，然后会将该元素从链表中移除。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">// 创建一个新的双向链表</span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">// 在链表尾部添加元素</span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">// 找到要删除的元素</span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">// 从链表中移除该元素</span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">// 打印剩余的元素</span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码会输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>在这个例子中，我们移除了链表中第二个元素（值为2）。</p><h2 id="浅析一番"><a href="#浅析一番" class="headerlink" title="浅析一番"></a>浅析一番</h2><p>直接上pprof来分析一下内存使用情况,部分pprof列表，如下所示：</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>这里可以发现，内存主要集中在protobuf的Decode，在不具体分析pprof的前提下，我们的思路有三点：</p><ul><li>grpc-go是否有内存问题</li><li>go本身是否问题</li><li>K8s内存问题</li></ul><p>针对第一个的假设，可以捞一下grpc-go的相关issue，可以发现近期未见相关内存异常的报告，go本身的问题，看起来也不太像，但倒是找到一个THP的相关问题，以后可以简单介绍一下，那么只剩一个结果，就是K8s本身存在问题，但其中<code>(*FieldsV1).Unmarshal</code>5年没动了，大概率不会存在问题，那么我们深入分析一下pprof吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>在持续增长的 Pod 列表中，发现了一些未释放的数据似乎与先前使用 pprof 分析的结果吻合，仅发现 Pod 是持续变更的对象。因此，我尝试了另一种排查方法，验证社区是否已解决此问题。我使用 minikube 在本地启动了 Kubernetes 1.18.5 版本进行排查。幸运的是，我未能复现这一现象，表明问题可能在 1.18.5 版本后已修复。</p><p>为了进一步缩小排查范围，我让同事检查了这三个小版本之间的提交记录。最终发现了一个关闭了 SchedulerQueueingHints 特性的 PR。正如在技术背景中提到的，SchedulerQueueingHints 特性可能导致内存增长问题。</p><p>通过PriorityQueue结构体可以发现其通过isSchedulingQueueHintEnabled来控制特性的逻辑处理，如果开启了<code>QueueingHint</code> 特性，那么在执行Pop方法来调度Pod时，需要为inFlightPods对应pod的UID填充相同inFlightEvents的链表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么链表字段何时移除？我们可以观察到移除的唯一时间点在pod完成调度周期时，也就是调用Done方法时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以发现如何done的时机越晚，内存的增长将越明显，并且如果Pod的事件被忽视或者遗漏，链表的内存同样会出现异常增加的现象，可以看到针对上述场景的一些修复：</p><ul><li>出现了call Done() as soon as possible这样的PR，参看PR#120586</li><li>NodeAffinity&#x2F;NodeUnschedulable插件的QueueingHint 遗漏相关Node事件，参看PR#122284</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>揭开K8s适配CgroupV2内存虚高的迷局</title>
    <link href="https://zoues.com/posts/3f237e52/"/>
    <id>https://zoues.com/posts/3f237e52/</id>
    <published>2024-01-27T12:40:08.000Z</published>
    <updated>2024-07-13T00:50:34.272Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><p>在Almalinux替换CentOS的过程中，我们通过kubectl top nodes命令观察到了两个相同规格的节点（只有cgroup版本不同）。在分别调度两个相同的Pod后，我们预期它们的内存使用量应该相近。然而，我们发现使用了cgroupv2的节点的内存使用量比使用了cgroupv1的节点多了约280Mi。</p><p>初步分析表明，可能是cAdvisor在统计cgroupv1和v2的内存使用量时存在逻辑上的不一致。</p><p>理论上，无论使用cgroupv1还是cgroupv2，两个相同配置的节点的内存使用量应该相近。实际上，在比较&#x2F;proc&#x2F;meminfo时，我们发现了总内存使用量近似的情况。那么问题出在哪里呢？</p><p>我们发现，这个问题只影响了节点级别的内存统计数据，而不影响Pod级别的统计数据。</p><p>问题的根本原因是cAdvisor调用了runc的接口，其计算root cgroup的内存数据方面存在差异。在cgroupv2中，root cgroup不存在memory.current这个文件，但在cgroupv1中root cgroup是存在memory.usage_in_bytes文件的。这导致了在统计cgroupv2内存使用量时出现了不一致的情况。</p><p>这个问题可能需要在cAdvisor或runc的逻辑中进行修复，以确保在cgroupv1和cgroupv2中的内存统计一致性。下面我们基于社区issue展开介绍。</p><p>v1.28.3 commit:a8a1abc25cad87333840cd7d54be2efaf31a3177</p><blockquote><p>NOTE: Containerd:1.6.21，K8s:1.28, Kernel:5.15.0<br>(同步以前的文章)</p></blockquote><hr><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为false）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">klet.containerRuntime,</span><br><span class="line">klet.statusManager,</span><br><span class="line">hostStatsProvider)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">kubeDeps.RemoteRuntimeService,</span><br><span class="line">kubeDeps.RemoteImageService,</span><br><span class="line">hostStatsProvider,</span><br><span class="line">utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在<code>/stats/</code>暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p>图片</p><p>从 Kubernetes 1.12 版本开始，kubelet 直接从 cAdvisor 暴露了多个接口。包括以下接口：</p><ul><li>cAdvisor 的 Prometheus 指标位于 <code>/metrics/cadvisor</code>。</li><li>cAdvisor v1 Json API 位于 <code>/stats/</code>、<code>/stats/container</code>、<code>/stats/&#123;podName&#125;/&#123;containerName&#125;</code> 和 <code>/stats/&#123;namespace&#125;/&#123;podName&#125;/&#123;uid&#125;/&#123;containerName&#125;</code>。</li><li>cAdvisor 的机器信息位于 &#x2F;spec。</li></ul><p>此外，kubelet还暴露了<code>summary API</code>，其中cAdvisor 是该接口指标来源之一。在社区的监控架构文档中描述了“核心”指标和“监控”指标的定义。这个文档中规定了一组核心指标及其用途，并且目标是通过拆分监控架构来实现以下两个目标：</p><ul><li><p>减小核心指标的统计收集性能影响，允许更频繁地收集这些指标。</p></li><li><p>使监控方案可替代且可扩展。</p></li></ul><p>因此移除cadvisor的接口，成了一项长期目标，目前进度如下(进度状态的标记略为滞后)：</p><ul><li><p>[1.13] 引入 Kubelet 的 pod-resources gRPC 端点；KEP: 支持设备监控社区#2454</p></li><li><p>[1.14] 引入 Kubelet 资源指标 API</p></li><li><p>[1.15] 通过添加和弃用 <code>--enable-cadvisor-json-endpoints</code> 标志，废弃“直接” cAdvisor API 端点</p></li><li><p>[1.18] 默认将 –enable-cadvisor-json-endpoints 标志设置为禁用</p></li><li><p>[1.21] 移除 <code>--enable-cadvisor-json-endpoints</code> 标志</p></li><li><p>[1.21] 将监控服务器过渡到 Kubelet 资源指标 API（需要3个版本的差异）</p></li><li><p>[TBD] 为 kubelet 监控端点提出外部替代方案</p></li><li><p>[TBD] 通过添加和废弃 <code>--enable-container-monitoring-endpoints</code> 标志，废弃摘要 API 和 cAdvisor Prometheus 端点</p></li><li><p>[TBD+2] 移除“直接”的 cAdvisor API 端点</p></li><li><p>[TBD+2] 默认将 –enable-container-monitoring-endpoints 标志设置为禁用</p></li><li><p>[TBD+4] 移除摘要 API、cAdvisor Prometheus 指标和移除 –enable-container-monitoring-endpoints 标志。</p></li></ul><p>当前版本的cadvisor接口已经做了部分废弃，例如<code>/spec及/stats/*</code>等<br><a href="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></a></p><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>kubelet 使用 cadvisor 来获取节点级别的统计信息（无论是使用 cri 还是通过cadvisor 来统计提供程序来获取 pod 的统计信息）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/provider.go</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// NewCRIStatsProvider returns a Provider that provides the node stats</span><br><span class="line">// from cAdvisor and the container stats from CRI.</span><br><span class="line">func NewCRIStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">runtimeService internalapi.RuntimeService,</span><br><span class="line">imageService internalapi.ImageManagerService,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">podAndContainerStatsFromCRI bool,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCRIStatsProvider(cadvisor, resourceAnalyzer,</span><br><span class="line">runtimeService, imageService, hostStatsProvider, podAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// NewCadvisorStatsProvider returns a containerStatsProvider that provides both</span><br><span class="line">// the node and the container stats from cAdvisor.</span><br><span class="line">func NewCadvisorStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">imageService kubecontainer.ImageService,</span><br><span class="line">statusProvider status.PodStatusProvider,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCadvisorStatsProvider(cadvisor, resourceAnalyzer, imageService, statusProvider, hostStatsProvider))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下述两种方式获取节点的内存使用情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl top node</span><br><span class="line">kubectl get --raw /api/v1/nodes/foo/proxy/stats/summary | jq -C .node.memory</span><br></pre></td></tr></table></figure><p>结果显示cgroupv2节点的内存使用量比相同节点配置但使用 cgroupv1的高一些。kubectl top node 获取节点信息的逻辑在：<a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40">https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40</a></p><p>kubelet使用 cadvisor 来获取 cgroup 统计信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/server/stats/summary.go</span><br><span class="line"></span><br><span class="line">rootStats, err := sp.provider.GetCgroupCPUAndMemoryStats(&quot;/&quot;, false)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return nil, fmt.Errorf(&quot;failed to get root cgroup stats: %v&quot;, err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里GetCgroupCPUAndMemoryStats调用以下cadvisor逻辑</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/helper.go</span><br><span class="line"></span><br><span class="line">infoMap, err := cadvisor.ContainerInfoV2(containerName, cadvisorapiv2.RequestOptions&#123;</span><br><span class="line">IdType:    cadvisorapiv2.TypeName,</span><br><span class="line">Count:     2, // 2 samples are needed to compute &quot;instantaneous&quot; CPU</span><br><span class="line">Recursive: false,</span><br><span class="line">MaxAge:    maxAge,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>cadvisor 基于 cgroup v1&#x2F;v2 获取不同 cgroup manager接口实现，然后调用GetStats()获取监控信息。</p><p>这些实现在计算root cgroup 的内存使用方面存在差异。</p><ul><li><p>v1 使用来自 memory.usage_in_bytes 的内存使用情况：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224</a></p></li><li><p>v2 使用 &#x2F;proc&#x2F;meminfo 并计算使用情况为总内存 - 空闲内存：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217</a></p></li></ul><p>usage_in_bytes 大致等于 RSS + Cache。workingset是 usage - 非活动文件。</p><p>在 cadvisor 中，在workingset中排除了非活动文件：<a href="https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844">https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844</a>“</p><p>因此可以判断在cgroupv2计算内存使用使用了total-free，这里面包含了inactive_anon，而内核以及cgroupv1计算内存使用量时不会计入 inactive_anon：<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720</a></p><p>通过下面的测试中，inactive_anon 解释数据看到了差异。</p><p>下述分别为cgroupv1及cgroupv2的两个集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   98m          2%     1512Mi          12%</span><br><span class="line">node2   99m          2%     1454Mi          11%</span><br><span class="line">node3   94m          2%     1448Mi          11%</span><br></pre></td></tr></table></figure><p>其中cgroupv1节点的root cgroup内存使用如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /sys/fs/cgroup/memory/memory.usage_in_bytes</span><br><span class="line">6236864512</span><br><span class="line">~ # cat /sys/fs/cgroup/memory/memory.stat</span><br><span class="line">cache 44662784</span><br><span class="line">rss 3260416</span><br><span class="line">rss_huge 2097152</span><br><span class="line">shmem 65536</span><br><span class="line">mapped_file 11083776</span><br><span class="line">dirty 135168</span><br><span class="line">writeback 0</span><br><span class="line">pgpgin 114774</span><br><span class="line">pgpgout 103506</span><br><span class="line">pgfault 165891</span><br><span class="line">pgmajfault 99</span><br><span class="line">inactive_anon 135168</span><br><span class="line">active_anon 3645440</span><br><span class="line">inactive_file 5406720</span><br><span class="line">active_file 39333888</span><br><span class="line">unevictable 0</span><br><span class="line">hierarchical_memory_limit 9223372036854771712</span><br><span class="line">total_cache 5471584256</span><br><span class="line">total_rss 767148032</span><br><span class="line">total_rss_huge 559939584</span><br><span class="line">total_shmem 1921024</span><br><span class="line">total_mapped_file 605687808</span><br><span class="line">total_dirty 270336</span><br><span class="line">total_writeback 0</span><br><span class="line">total_pgpgin 51679194</span><br><span class="line">total_pgpgout 50291069</span><br><span class="line">total_pgfault 97383769</span><br><span class="line">total_pgmajfault 5610</span><br><span class="line">total_inactive_anon 1081344</span><br><span class="line">total_active_anon 772235264</span><br><span class="line">total_inactive_file 4648124416</span><br><span class="line">total_active_file 820551680</span><br><span class="line">total_unevictable 0</span><br></pre></td></tr></table></figure><p>meminfo文件如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /proc/meminfo</span><br><span class="line">MemTotal:       16393244 kB</span><br><span class="line">MemFree:         9744148 kB</span><br><span class="line">MemAvailable:   15020900 kB</span><br><span class="line">Buffers:          132344 kB</span><br><span class="line">Cached:          5207356 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:          1557252 kB</span><br><span class="line">Inactive:        4526668 kB</span><br><span class="line">Active(anon):     745916 kB</span><br><span class="line">Inactive(anon):      792 kB</span><br><span class="line">Active(file):     811336 kB</span><br><span class="line">Inactive(file):  4525876 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               636 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        618992 kB</span><br><span class="line">Mapped:           624384 kB</span><br><span class="line">Shmem:              2496 kB</span><br><span class="line">KReclaimable:     285824 kB</span><br><span class="line">Slab:             423600 kB</span><br><span class="line">SReclaimable:     285824 kB</span><br><span class="line">SUnreclaim:       137776 kB</span><br><span class="line">KernelStack:        8400 kB</span><br><span class="line">PageTables:         9060 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8196620 kB</span><br><span class="line">Committed_AS:    2800016 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       40992 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             4432 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    270336 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">CmaTotal:              0 kB</span><br><span class="line">CmaFree:               0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      302344 kB</span><br><span class="line">DirectMap2M:     3891200 kB</span><br><span class="line">DirectMap1G:    14680064 kB</span><br></pre></td></tr></table></figure><p>当前的计算</p><p>memory.current - memory.stat.total_inactive_file &#x3D; 6236864512 - 4648124416 &#x3D; 1515 Mi -&gt; kubelet 报告的结果</p><p>cgroupv2 集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   113m         2%     2196Mi          17%</span><br><span class="line">node2   112m         2%     2171Mi          17%</span><br><span class="line">node3   113m         2%     2180Mi          17%</span><br></pre></td></tr></table></figure><p>其中一节点的meminfo文件如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">MemTotal:       16374584 kB</span><br><span class="line">MemFree:         9505980 kB</span><br><span class="line">MemAvailable:   14912544 kB</span><br><span class="line">Buffers:          155164 kB</span><br><span class="line">Cached:          5335576 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:           872420 kB</span><br><span class="line">Inactive:        5399340 kB</span><br><span class="line">Active(anon):       2568 kB</span><br><span class="line">Inactive(anon):   791340 kB</span><br><span class="line">Active(file):     869852 kB</span><br><span class="line">Inactive(file):  4608000 kB</span><br><span class="line">Unevictable:       30740 kB</span><br><span class="line">Mlocked:           27668 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               148 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        716552 kB</span><br><span class="line">Mapped:           608424 kB</span><br><span class="line">Shmem:              6320 kB</span><br><span class="line">KReclaimable:     274360 kB</span><br><span class="line">Slab:             355976 kB</span><br><span class="line">SReclaimable:     274360 kB</span><br><span class="line">SUnreclaim:        81616 kB</span><br><span class="line">KernelStack:        8064 kB</span><br><span class="line">PageTables:         7692 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8187292 kB</span><br><span class="line">Committed_AS:    2605012 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       48092 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             3472 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    409600 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      271624 kB</span><br><span class="line">DirectMap2M:     8116224 kB</span><br><span class="line">DirectMap1G:    10485760 kB</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">usage = total - free = 16374584 - 9505980</span><br><span class="line"></span><br><span class="line">workingset = 总内存 - 空闲内存 - 非活动文件 = 16374584 - 9505980 - 4608000 = 2207 Mi（kubelet 报告的结果）</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如上所述，在Linux kernel及runc cgroupv1计算内存使用为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mem_cgroup_usage =NR_FILE_PAGES + NR_ANON_MAPPED + nr_swap_pages (如果swap启用的话)</span><br><span class="line"></span><br><span class="line">// - rss (NR_ANON_MAPPED)</span><br><span class="line">// - cache (NR_FILE_PAGES)</span><br></pre></td></tr></table></figure><p>但是runc在cgroupv2计算使用了total-free，因此在相似负载下，同一台机器上v1和v2版本的节点级别报告确实会相差约250-750Mi，为了让cgroup v2的内存使用计算更接近 cgroupv1，  cgroup v2调整计算内存使用量方式为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.MemoryStats.Usage.Usage = stats.MemoryStats.Stats[&quot;anon&quot;] + stats.MemoryStats.Stats[&quot;file&quot;]</span><br></pre></td></tr></table></figure><p>当然，我们同时还需要处理cadvisor的woringset的处理逻辑</p><p>由于笔者时间、视野、认知有限，本文难免出现错误、疏漏等问题，期待各位读者朋友、业界专家指正交流，上述排障信息已修改为社区内容。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680">https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680</a><br>2.<a href="https://github.com/kubernetes/kubernetes/issues/68522">https://github.com/kubernetes/kubernetes/issues/68522</a><br>3.<a href="https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/">https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一条K8s命令行引发的血案</title>
    <link href="https://zoues.com/posts/5a8a6c8d/"/>
    <id>https://zoues.com/posts/5a8a6c8d/</id>
    <published>2024-01-27T01:25:08.000Z</published>
    <updated>2024-07-13T00:50:17.283Z</updated>
    
    <content type="html"><![CDATA[<p>为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了<a href="/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html">&lt;&lt;Kubernetes经典案例30篇&gt;&gt;</a>，该系列涵盖了不同的使用场景，从runc到containerd，从K8s到Istio等微服务架构，全面展示了Kubernetes在实际应用中的最佳实践。通过这些案例，读者可以掌握如何应对复杂的技术难题，并提升Kubernetes集群的性能和稳定性。</p><ol><li><a href="/posts/652176ee/">Containerd CVE-2020–15257细节说明</a></li><li><a href="/posts/1df3dc63/">OpenAI关于Kubernetes集群近万节点的生产实践</a></li><li><a href="/posts/5a8a6c8d/">一条K8s命令行引发的血案</a></li><li><a href="/posts/3f237e52/">揭开K8s适配CgroupV2内存虚高的迷局</a></li><li><a href="/posts/e46bd846/">探索Kubernetes 1.28调度器OOM的根源</a></li><li><a href="/posts/b421b57/">解读Kubernetes常见错误码</a></li><li><a href="/posts/186b05db/">RLIMIT_NOFILE设置陷阱：容器应用高频异常的隐形元凶</a></li><li><a href="/posts/e9953e7c/">容器干扰检测与治理（上篇）</a></li></ol><h2 id="一条K8s命令行引发的血案"><a href="#一条K8s命令行引发的血案" class="headerlink" title="一条K8s命令行引发的血案"></a>一条K8s命令行引发的血案</h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>因为Centos EOL的缘故，去年内部忙着换OS，打算趁此机会从cgroup v1切到cgroup v2，然而，在低版本K8s适配cgroupv2的过程中，遇到了一些问题，前期kubelet在cgroup v1的环境下，使用<code>-enable_load_reader</code>暴露容器的cpu load等相关监控数据，但在cgroup v2环境下，使用该配置会导致kubelet发生panic</p><p>下述为关键性信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container.go:422] Could not initialize cpu load reader for &quot;/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podXXX.slice&quot;: failed to create a netlink based cpuload reader: failed to get netlink family id for task stats: binary.Read: invalid type int32</span><br></pre></td></tr></table></figure><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>容器指标如何生成</li><li>K8s如何集成容器监控</li><li>cpu load如何计算等</li></ul><h3 id="cadvisor"><a href="#cadvisor" class="headerlink" title="cadvisor"></a>cadvisor</h3><p>cAdvisor是一款强大的Docker容器监控工具，专为容器场景设计，方便监控资源使用和性能分析。它用于收集、汇总、处理和输出容器的相关信息。cAdvisor支持Docker容器，同时支持其他类型的容器运行时。</p><p>Kubelet内置了对cAdvisor的支持，用户可以直接通过Kubelet组件获取有关节点上容器的监控指标。</p><blockquote><p>K8s 1.19使用的cAdvisor版本为0.39.3，而这里的简要介绍使用的是版本0.48.1。</p></blockquote><p>以下是主要功能代码，其中包含了一些注释以提高可读性。代码路径为：&#x2F;cadvisor&#x2F;cmd&#x2F;cadvisor.go。</p><p>cAdvisor主要完成以下几项任务：</p><ul><li>对外提供外部使用的API，包括一般的API接口和Prometheus接口。</li><li>支持第三方数据存储，包括BigQuery、Elasticsearch、InfluxDB、Kafka、Prometheus、Redis、StatsD和标准输出。</li><li>收集与容器、进程、机器、Go运行时以及自定义业务相关的监控。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func init() &#123;</span><br><span class="line">optstr := container.AllMetrics.String()</span><br><span class="line">flag.Var(&amp;ignoreMetrics, &quot;disable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be disabled. Options are %s.&quot;, optstr))</span><br><span class="line">flag.Var(&amp;enableMetrics, &quot;enable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be enabled. If set, overrides &#x27;disable_metrics&#x27;. Options are %s.&quot;, optstr))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上述代码可以看到，cadvisor支持是否开启相关指标的能力，其中<code>AllMetrics</code>主要是下述指标:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/container/factory.go#L72</span><br><span class="line"></span><br><span class="line">var AllMetrics = MetricSet&#123;</span><br><span class="line">CpuUsageMetrics:                struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessSchedulerMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerCpuUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryNumaMetrics:              struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CpuLoadMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskIOMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskUsageMetrics:               struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkTcpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkAdvancedTcpUsageMetrics: struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUdpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">AppMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">HugetlbUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerfMetrics:                    struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ReferencedMemoryMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUTopologyMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ResctrlMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUSetMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">OOMMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">var includedMetrics container.MetricSet</span><br><span class="line">if len(enableMetrics) &gt; 0 &#123;</span><br><span class="line">includedMetrics = enableMetrics</span><br><span class="line">&#125; else &#123;</span><br><span class="line">includedMetrics = container.AllMetrics.Difference(ignoreMetrics)</span><br><span class="line">&#125;</span><br><span class="line">// 上述处理需要开启的指标</span><br><span class="line">klog.V(1).Infof(&quot;enabled metrics: %s&quot;, includedMetrics.String())</span><br><span class="line">setMaxProcs()</span><br><span class="line">// 内存方式存在监控指标</span><br><span class="line">memoryStorage, err := NewMemoryStorage()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to initialize storage driver: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 这是cadvisor核心逻辑，kubelet内部就是直接调用的manager.New</span><br><span class="line">resourceManager, err := manager.New(memoryStorage, sysFs, manager.HousekeepingConfigFlags, includedMetrics, &amp;collectorHTTPClient, strings.Split(*rawCgroupPrefixWhiteList, &quot;,&quot;), strings.Split(*envMetadataWhiteList, &quot;,&quot;), *perfEvents, *resctrlInterval)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to create a manager: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 注册对外的HTTP接口.</span><br><span class="line">err = cadvisorhttp.RegisterHandlers(mux, resourceManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm, *urlBasePrefix)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to register HTTP handlers: %v&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line">// 这里是容器标签的处理，kubelet 1.28切换到CRI之后需要修改kubelet</span><br><span class="line">containerLabelFunc := metrics.DefaultContainerLabels</span><br><span class="line">if !*storeContainerLabels &#123;</span><br><span class="line">whitelistedLabels := strings.Split(*whitelistedContainerLabels, &quot;,&quot;)</span><br><span class="line">// Trim spacing in labels</span><br><span class="line">for i := range whitelistedLabels &#123;</span><br><span class="line">whitelistedLabels[i] = strings.TrimSpace(whitelistedLabels[i])</span><br><span class="line">&#125;</span><br><span class="line">containerLabelFunc = metrics.BaseContainerLabels(whitelistedLabels)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中cpu load是否生成指标，同时也由命令行<code>enable_load_reader</code>控制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/42bb3d13a0cf9ab80c880a16c4ebb4f36e51b0c9/manager/container.go#L455</span><br><span class="line"></span><br><span class="line">if *enableLoadReader &#123;</span><br><span class="line">// Create cpu load reader.</span><br><span class="line">loadReader, err := cpuload.New()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Warningf(&quot;Could not initialize cpu load reader for %q: %s&quot;, ref.Name, err)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cont.loadReader = loadReader</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为<strong>false</strong>）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      klet.containerRuntime,</span><br><span class="line">      klet.statusManager,</span><br><span class="line">      hostStatsProvider)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      kubeDeps.RemoteRuntimeService,</span><br><span class="line">      kubeDeps.RemoteImageService,</span><br><span class="line">      hostStatsProvider,</span><br><span class="line">      utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在&#x2F;stats&#x2F;暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p><img src="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></p><p>最终可以看到cadvisor组件如何在kubelet完成初始化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L80</span><br><span class="line"></span><br><span class="line">func New(imageFsInfoProvider ImageFsInfoProvider, rootPath string, cgroupRoots []string, usingLegacyStats, localStorageCapacityIsolation bool) (Interface, error) &#123;</span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line">// 这里就是kubelet默认暴露的监控指标类型</span><br><span class="line">includedMetrics := cadvisormetrics.MetricSet&#123;</span><br><span class="line">...</span><br><span class="line">cadvisormetrics.CpuLoadMetrics:      struct&#123;&#125;&#123;&#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">// 创建cAdvisor container manager.</span><br><span class="line">m, err := manager.New(memory.New(statsCacheDuration, nil), sysFs, housekeepingConfig, includedMetrics, http.DefaultClient, cgroupRoots, nil /* containerEnvMetadataWhiteList */, &quot;&quot; /* perfEventsFile */, time.Duration(0) /*resctrlInterval*/)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里就是直接调用的cadvisor的manager.New的函数接口，更详细的信息可参看：<a href="https://zoues.com/posts/3f237e52/">https://zoues.com/posts/3f237e52/</a></p><h3 id="CPU-Load指标"><a href="#CPU-Load指标" class="headerlink" title="CPU Load指标"></a>CPU Load指标</h3><p>CPU使用率反映的是当前cpu的繁忙程度，CPU平均负载（load average）是指某段时间内占用cpu时间的进程和等待cpu时间的进程数，这里等待cpu时间的进程是指等待被唤醒的进程，不包括处于wait状态进程。</p><p>在对设备做相关诊断时，需要结合cpu使用率、平均负载以及任务状态来进行判断，比如CPU使用率低但负载高，可能是IO瓶颈等，对此不作深入介绍。</p><p>在cadvisor中对外暴露的指标名称为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container_cpu_load_average_10s</span><br></pre></td></tr></table></figure><p>那么我们来看看是如何被计算出来的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L632</span><br><span class="line"></span><br><span class="line">// Calculate new smoothed load average using the new sample of runnable threads.</span><br><span class="line">// The decay used ensures that the load will stabilize on a new constant value within</span><br><span class="line">// 10 seconds.</span><br><span class="line">func (cd *containerData) updateLoad(newLoad uint64) &#123;</span><br><span class="line">if cd.loadAvg &lt; 0 &#123;</span><br><span class="line">cd.loadAvg = float64(newLoad) // initialize to the first seen sample for faster stabilization.</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>公式计算：<code>cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</code></p><p>大体意思是取的上一次采集计算出来的值cd.loadAvg乘以计算因子cd.loadDecay，然后加上当前采集</p><p>到的newLoad值乘以(1.0-cd.loadDecay)最后得出当前的cd.loadAvg值</p><p>其中<code>cont.loadDecay</code>计算逻辑如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L453</span><br><span class="line"></span><br><span class="line">cont.loadDecay = math.Exp(float64(-cont.housekeepingInterval.Seconds() / 10))</span><br></pre></td></tr></table></figure><p>这里是跟<code>housekeepingInterval</code>相关的固定值，衰变窗口</p><blockquote><p>关于容器cpu load的详细介绍可以看引用链接</p></blockquote><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>cpu load的cd.loadAvg前值通过如下方式获取：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L650</span><br><span class="line"></span><br><span class="line">if cd.loadReader != nil &#123;</span><br><span class="line">// TODO(vmarmol): Cache this path.</span><br><span class="line">path, err := cd.handler.GetCgroupPath(&quot;cpu&quot;)</span><br><span class="line">if err == nil &#123;</span><br><span class="line">loadStats, err := cd.loadReader.GetCpuLoad(cd.info.Name, path)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return fmt.Errorf(&quot;failed to get load stat for %q - path %q, error %s&quot;, cd.info.Name, path, err)</span><br><span class="line">&#125;</span><br><span class="line">stats.TaskStats = loadStats</span><br><span class="line">cd.updateLoad(loadStats.NrRunning)</span><br><span class="line">// convert to &#x27;milliLoad&#x27; to avoid floats and preserve precision.</span><br><span class="line">stats.Cpu.LoadAverage = int32(cd.loadAvg * 1000)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>深入探究可以发现使用了netlink来获取系统指标，关键调用路径:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">updateStats-&gt;GetCpuLoad-&gt;getLoadStats-&gt;prepareCmdMessage-&gt;prepareMessage</span><br></pre></td></tr></table></figure><p>经过上述分析可知， cAdvisor通过发送CGROUPSTATS_CMD_GET请求来获取CPU负载信息，通过netlink消息进行通信：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cadvisor/utils/cpuload/netlink/netlink.go</span><br></pre></td></tr></table></figure><p>在<code>v0.48.1</code>分支的第128到132行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func prepareCmdMessage(id uint16, cfd uintptr) (msg netlinkMessage) &#123; </span><br><span class="line">buf := bytes.NewBuffer([]byte&#123;&#125;) </span><br><span class="line">addAttribute(buf, unix.CGROUPSTATS_CMD_ATTR_FD, uint32(cfd), 4) </span><br><span class="line">return prepareMessage(id, unix.CGROUPSTATS_CMD_GET, buf.Bytes()) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终内核在<code>cgroupstats_user_cmd</code>中处理获取请求：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/* user-&gt;kernel request/get-response */</span><br></pre></td></tr></table></figure><p><a href="https://github.com/torvalds/linux/blob/master/kernel/taskstats.c#L407"><code>kernel/taskstats.c#L407</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">static int cgroupstats_user_cmd(struct sk_buff *skb, struct genl_info *info)</span><br><span class="line">&#123;</span><br><span class="line">int rc = 0;</span><br><span class="line">struct sk_buff *rep_skb;</span><br><span class="line">struct cgroupstats *stats;</span><br><span class="line">struct nlattr *na;</span><br><span class="line">size_t size;</span><br><span class="line">u32 fd;</span><br><span class="line">struct fd f;</span><br><span class="line"></span><br><span class="line">na = info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD];</span><br><span class="line">if (!na)</span><br><span class="line">return -EINVAL;</span><br><span class="line"></span><br><span class="line">fd = nla_get_u32(info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD]);</span><br><span class="line">f = fdget(fd);</span><br><span class="line">if (!f.file)</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">size = nla_total_size(sizeof(struct cgroupstats));</span><br><span class="line"></span><br><span class="line">rc = prepare_reply(info, CGROUPSTATS_CMD_NEW, &amp;rep_skb,</span><br><span class="line">size);</span><br><span class="line">if (rc &lt; 0)</span><br><span class="line">goto err;</span><br><span class="line"></span><br><span class="line">na = nla_reserve(rep_skb, CGROUPSTATS_TYPE_CGROUP_STATS,</span><br><span class="line">sizeof(struct cgroupstats));</span><br><span class="line">if (na == NULL) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">rc = -EMSGSIZE;</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stats = nla_data(na);</span><br><span class="line">memset(stats, 0, sizeof(*stats));</span><br><span class="line"></span><br><span class="line">rc = cgroupstats_build(stats, f.file-&gt;f_path.dentry);</span><br><span class="line">if (rc &lt; 0) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rc = send_reply(rep_skb, info);</span><br><span class="line"></span><br><span class="line">err:</span><br><span class="line">fdput(f);</span><br><span class="line">return rc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并在<code>cgroupstats_build</code>函数中构建cgroup stats结果：</p><p><a href="https://github.com/torvalds/linux/blob/5c1ee569660d4a205dced9cb4d0306b907fb7599/kernel/cgroup/cgroup-v1.c#L699"><code>kernel/cgroup/cgroup-v1.c#L699</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * cgroupstats_build - build and fill cgroupstats</span><br><span class="line"> * @stats: cgroupstats to fill information into</span><br><span class="line"> * @dentry: A dentry entry belonging to the cgroup for which stats have</span><br><span class="line"> * been requested.</span><br><span class="line"> *</span><br><span class="line"> * Build and fill cgroupstats so that taskstats can export it to user</span><br><span class="line"> * space.</span><br><span class="line"> *</span><br><span class="line"> * Return: %0 on success or a negative errno code on failure</span><br><span class="line"> */</span><br><span class="line">int cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)</span><br><span class="line">&#123;</span><br><span class="line">……</span><br><span class="line">/* it should be kernfs_node belonging to cgroupfs and is a directory */</span><br><span class="line">if (dentry-&gt;d_sb-&gt;s_type != &amp;cgroup_fs_type || !kn ||</span><br><span class="line">    kernfs_type(kn) != KERNFS_DIR)</span><br><span class="line">return -EINVAL;  // 导致返回EINVAL错误码</span><br></pre></td></tr></table></figure><p>这里可以发现<code>cgroup_fs_type</code>是cgroup v1的类型，而没有处理cgroup v2。因此，<code>cgroupstats_build</code>函数在路径类型判断语句上返回EINVAL。</p><p>在内核社区也有相关问题的说明：<a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">kernel community issue</a></p><p>那么我们看看tejun(meta，cgroupv2 owner)如何解释的：</p><blockquote><p>The exclusion of cgroupstats from v2 interface was intentional due to the duplication and inconsistencies with other statistics. If you need these numbers, please justify and add them to the appropriate cgroupfs stat file.</p></blockquote><p>简单翻译：对v2接口中排除cgroupstats的操作是有意的，因为它与其他统计数据存在重复和不一致之处。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>那么他的建议是什么？</p><p>他建议我们使用psi，而不是通过CGROUPSTATS_CMD_GET netlink api获取CPU统计信息，直接从<code>cpu.pressure</code>、<code>memory.pressure</code>以及<code>io.pressure</code>文件中获取，后续我们会介绍psi在容器领域的相关进展，当前Containerd已经支持PSI相关监控.</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://github.com/containerd/cgroups/pull/308">https://github.com/containerd/cgroups/pull/308</a></li><li><a href="https://cloud.tencent.com/developer/article/2329489">https://cloud.tencent.com/developer/article/2329489</a></li><li><a href="https://github.com/google/cadvisor/issues/3137">https://github.com/google/cadvisor/issues/3137</a></li><li><a href="https://www.cnblogs.com/vinsent/p/15830271.html">https://www.cnblogs.com/vinsent/p/15830271.html</a></li><li><a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;为了帮助读者深入了解Kubernetes在各种应用场景下所面临的挑战和解决方案，以及如何进行性能优化。我们推出了&lt;a href=&quot;/Kubernetes%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B30%E7%AF%87/index.html&quot;&gt;&amp;</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Rust vs. Zig：究竟谁更胜一筹？性能、安全性等全面对决！</title>
    <link href="https://zoues.com/posts/423bdf96/"/>
    <id>https://zoues.com/posts/423bdf96/</id>
    <published>2024-01-21T12:23:48.000Z</published>
    <updated>2024-02-03T06:32:45.374Z</updated>
    
    <content type="html"><![CDATA[<p>Rust和Zig，这两种语言都旨在编写高效、性能优异的代码，然而它们在实现这一目标时采用了不同的方式。</p><p>值得注意的是，Rust和Zig根植于截然不同的理念，这可能影响开发者选择时的取舍。为了更深入地了解它们在相互比较中的表现，我们将进一步探讨它们各自的特点。</p><h2 id="什么是Rust？"><a href="#什么是Rust？" class="headerlink" title="什么是Rust？"></a>什么是Rust？</h2><p>Rust是一种以效率、性能和内存安全著称的通用型编程语言。它引入了一种新的编程方式，使开发者仍然能够使用面向对象以及函数式编程。</p><p>使用Rust进行编码需要一种不同往常的思维方式，这部分主要围绕着语言规则中的所有权和借用展开。</p><p>虽然这种思维方式能够让开发者更容易编写出安全高效的代码，但与C和C++等语言相比，特别是对于新手来说，充满挑战性。</p><p>Rust消除了C和C++跨平台的限制，允许将代码编译为目标系统运行的可执行文件。这意味着可以在不做重大修改的情况下将代码编译为多系统版本。</p><p>让我们看一个Rust版的Hello world：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fn main() &#123;</span><br><span class="line">    let text: &amp;str = &quot;World&quot;;</span><br><span class="line">    println!(&quot;Hello, &#123;&#125;!&quot;, text);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类似于其他编译型编程语言，在Rust中，每个可执行程序同样也都从main函数开始。如果运行上述示例，将在你的终端上输出“Hello, World!”。</p><h3 id="Rust优势与短板"><a href="#Rust优势与短板" class="headerlink" title="Rust优势与短板"></a>Rust优势与短板</h3><p>在Rust中，一些特性对开发者是有益的，而另一些则让开发变得更具挑战性。在这一章节，我们将分别介绍一下Rust的优势与劣势。</p><p>Rust的一些<strong>优势</strong>包括以下几点：</p><ul><li><p>并发和并行：Rust内置对并行编程的支持，以及安全高效的多线程特性</p></li><li><p>性能：由于Rust代码不需要运行时，同时它不需要额外的垃圾回收器功耗，从而可以使用更少的资源并提高性能，</p></li><li><p>内存安全且无垃圾回收：由于所有权和借用等规则，Rust在没有垃圾回收器的情况下管理内存，从而实现更高效和可预测的性能</p></li><li><p>跨平台兼容性：Rust支持跨平台开发，意味着可以在多个系统上编译代码而不需要太多的修改代码</p></li><li><p>强大的生态系统：Rust拥有强大的工具和库生态系统。它的包管理器Cargo显著简化了依赖管理和与外部库集成的难度</p></li></ul><p>Rust的一些<strong>劣势</strong>包括以下几点：</p><ul><li><p>学习曲线：Rust的语法对新开发者可能有些棘手。其语法融合了函数式和系统编程，受所有权和借用规则的影响很大。此外，新开发者还必须学习所有权系统、生命周期和借用规则等概念，需要付出一定的努力 ，下图是流传甚广的一张学习曲线图(来源于极客邦)</p><p><img src="https://pic.imgdb.cn/item/65ad10c7871b83018ac90c50.png" alt="Rust 编程第一课，实战驱动，快速上手 Rust"></p></li><li><p>编译耗时：Rust的安全需求导致较长的编译时间。Rust会彻底检查你的代码以防止运行时可能出现的问题，这意味着它的编译时间会比大多数语言更长</p></li><li><p>有限的资源：尽管Cargo是一个有用的包管理器，提供了许多可用的工具和库，但从整体来看，Rust的生态系统相较大多数语言来说都不够成熟。在一些专业领域，Rust的资源可能较少，迫使开发者更多地从零开始编写代码</p></li><li><p>繁琐的开发过程：由于强调安全和准确性，使用严格的规则和明确性，开发者通常在Rust中需要编写更多的代码，虽然可能会有高质量的输出，但往往会使开发过程变得更长，对小项目影响显著</p></li><li><p>互操作性：将Rust代码整合到其他语言编写的代码中可能有些困难。</p></li></ul><p>虽然Rust有其劣势，但它仍然是开发者的热门选择。在2023年Stack Overflow开发者调查中，Rust荣获最受喜爱的语言的桂冠，超过80%的受访者表示明年仍然想要使用它。</p><h3 id="Rust的常见使用场景"><a href="#Rust的常见使用场景" class="headerlink" title="Rust的常见使用场景"></a>Rust的常见使用场景</h3><p>既然你已经了解了Rust的功能，让我们看看它已经在哪些场景落地。</p><ul><li><p>在系统编程中，Rust对于构建操作系统、数据库系统、设备驱动程序和嵌入式系统等场景非常有用。</p></li><li><p>前后端Web开发者也使用Rust，与像Rocket或Actix这样的流行框架一起进行后端开发，以及使用WebAssembly或Tauri进行前端开发。</p></li></ul><p>Rust还被用于网络服务，如网络协议、代理、负载均衡器、VPN软件等。</p><p>一些Rust的更专业用例包括：</p><ul><li>游戏开发，使用像Amethyst和Bevy这样的游戏引擎</li><li>在区块链和加密货币领域，用于开发智能合约和项目中的区块链网络，如Solana 在物联网（IoT）中，用于编程微控制器和传感器等设备</li></ul><h2 id="什么是Zig？"><a href="#什么是Zig？" class="headerlink" title="什么是Zig？"></a>什么是Zig？</h2><p>虽然Zig更类似于传统的编程语言，如C和C++，但它像Rust一样注重内存安全和效率。然而，与Rust不同的是，Zig与现有的C和C++代码整合良好，无需像FFI这样的外部机制来简化互操作性。</p><p>与Rust、C和C++一样，Zig不使用垃圾收集器。为了实现类似Rust的内存安全性，Zig提供了促进内存安全的机制，例如：</p><ul><li>严格的编译时检查</li><li>用于处理潜在空值的可选类型</li><li>带有Error类型的明确错误处理</li><li>内置分配器的增强内存分配</li></ul><p>这些机制不会像Rust中那样严重影响编码习惯。让我们看一个Zig中的Hello world例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello, world&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对开发者来说，许多编程语言背后都有一种理念和设计哲学。例如，Rust注重内存安全性、效率、无垃圾收集和性能。</p><p>那么Zig呢？它的哲学包括：</p><ul><li>与C和C++代码轻松整合</li><li>生成不依赖系统依赖项的独立二进制文件</li><li>轻松的跨平台开发</li><li>快速的编译时间</li></ul><p>接下来，我们将看一看Zig的优势和劣势，之后再看它的用例。</p><h3 id="Zig的优势与短板"><a href="#Zig的优势与短板" class="headerlink" title="Zig的优势与短板"></a>Zig的优势与短板</h3><p>与我们在Rust中所做的一样，让我们从优势开始，然后再看劣势。</p><p>Zig为开发者提供的一些好处包括：</p><ul><li><p>控制和低级能力：Zig非常适合系统编程和需要直接管理系统资源的情场景</p></li><li><p>安全功能：内置分配器等功能使开发者能够轻松防止错误，提高代码可靠性，并减少错误和漏洞</p></li><li><p>性能优化：Zig是一个为高效执行和性能调优而优化代码的工具。它提供手动内存管理、编译时检查以及直接访问CPU指令的功能，以实现更高性能的应用程序</p></li><li><p>简单和可读性：Zig具有与C类似的简单语法和语言设计。这使得阅读、编写和维护代码变得简单</p></li><li><p>最小的外部依赖：Zig最小化了构建和运行程序所需的外部依赖，简化了开发，增强了可移植性，并减轻了跨平台依赖管理的负担</p></li><li><p>元编程能力：Zig的编译时元编程通过减少样板代码的需求和启用代码优化来提高代码的灵活性和生产力</p></li></ul><p>Zig的一些劣势包括：</p><ul><li>有限的生态系统：因为它仍处于早期阶段，Zig语言的生态系统比成熟语言更小</li><li>学习曲线：对于不熟悉低级编程概念的开发者来说，理解Zig可能需要一些时间 （相较Rust来说，所需的时间很短）</li><li>成熟度和工具：Zig是一种新语言，还有改进的空间。但请注意，仍然有一个强大而活跃的社区支持它</li><li>互操作性挑战：Zig提供了用于兼容性的C接口，但与其他语言集成可能需要额外的工作，比如管理数据转换和语言之间的通信</li><li>文档可用性：Zig是一种相对较新的语言，因此文档有限，社区正在努力提高文档的可用性</li></ul><h3 id="Zig的常见使用场景"><a href="#Zig的常见使用场景" class="headerlink" title="Zig的常见使用场景"></a>Zig的常见使用场景</h3><p>让我们深入一些Zig的实际用例，看看它在实际场景中是如何落地的！</p><p>开发者可以在系统编程中使用Zig来构建操作系统、设备驱动程序和嵌入式系统。其还在命令行工具中也有很多应用场景，可用于创建高效和快速的命令行界面，构建系统脚本，或优化现有工具的性能。</p><p>在编译器和语言开发中，Zig以其元编程能力和对简易性的追求而闻名。比较著名的开源项目是Bun，其是一个使用Zig开发的JavaScript运行时。</p><p>与Rust一样，Zig也有一些更为专业的使用场景：</p><ul><li>游戏开发，因支持高性能游戏引擎、能够实时模拟</li><li>在嵌入式系统和物联网中，用于编程微控制器、传感器和其他资源受限设备</li><li>在密码应用中，用于实现加密算法、数字签名、安全通信协议和其他安全敏感组件</li></ul><h2 id="Rust-vs-Zig-相似之处与差异"><a href="#Rust-vs-Zig-相似之处与差异" class="headerlink" title="Rust vs. Zig: 相似之处与差异"></a>Rust vs. Zig: 相似之处与差异</h2><p>前面我们已经分别看过Rust和Zig，现在是时候将它们放在一起进行比较了。比较不同的编程语言总是很有趣，特别是当它们有着相似的目标时。</p><p>让我们从它们的共同之处开始：</p><ul><li>内存安全性：Rust和Zig都优先考虑内存安全性，并通过严格的编译器检查、静态类型和适用于每种语言的特殊规则来防止常见的编程错误。</li><li>低级控制：两者都提供对系统资源更多的控制，使它们非常适合低级任务和系统编程。</li><li>性能优化：这两种编程语言都以高度优化的代码而闻名，具有手动内存管理、直接CPU访问和编译时评估的特性。</li><li>社区和可用性：Rust和Zig都是开源项目，拥有积极的社区、文档和工具支持。</li><li>无未定义行为：这两种编程语言都有严格的编译器检查和其他功能，可以防止未定义的行为。通过在编译时捕获问题，提高了程序的稳定性和安全性。</li></ul><p>与此同时，您可以使用下面的比较了解Rust和Zig之间的差异：</p><table><thead><tr><th>特征</th><th>Rust 使用其严格的所有权和借用规则来确保开发者编写的任何代码都是安全的。</th><th>Zig 使用跟踪和控制内存分配和释放的机制来防止开发者编写的任何代码都是不安全的。</th></tr></thead><tbody><tr><td>语法</td><td>Rust 通过显式注解强调所有权和生命周期，可能导致代码更长。</td><td>Zig 遵循类似于C的语法。</td></tr><tr><td>生态系统</td><td>Rust 提供了强大的生态系统，包括库、工具和社区支持。</td><td>Zig 是一个较年轻的语言，生态系统相对较小。</td></tr><tr><td>互操作性</td><td>Rust 具有良好的FFI兼容性。它在从C调用Rust函数方面表现良好，但从Rust调用C函数可能会有难度。</td><td>Zig 具有更出色的FFI。它在从C调用Zig函数和从C调用Zig函数方面表现良好。</td></tr><tr><td>错误处理</td><td>Rust 使用Result和Option类型进行显式错误处理。</td><td>Zig 使用错误类型、错误联合和延迟语句进行错误处理。</td></tr><tr><td>包管理器</td><td>Rust 使用Cargo包管理器处理包和依赖关系。</td><td>Zig 使用其内置的包管理器处理包和依赖关系。</td></tr></tbody></table><p>除了它们的相似之处和差异之外，我们还可以通过性能、流行度以及它们的程序员薪酬来比较Rust和Zig。让我们更仔细地看一看。</p><h3 id="Rust-vs-Zig-性能"><a href="#Rust-vs-Zig-性能" class="headerlink" title="Rust vs. Zig: 性能"></a>Rust vs. Zig: 性能</h3><p>客观来看，在Rust和Zig之间，并没有绝对性能更好的语言。Rust在特定应用中可能会胜过Zig，而Zig在其他方面可能会超越Rust。</p><p>让我们通过从编程语言和编译器基准测试中进行比较，仔细研究每种语言的性能：</p><p><img src="https://pic.imgdb.cn/item/65ad10df871b83018ac973e8.png" alt="Screenshot Taken From Programming Languages And Compiler Benchmark Project Showing Rust Vs Zig Performance For Two Example Programs"></p><p>这个基准测试项目包含用多种编程语言编写，并同时运行的程序。以表格形式呈现它们的运行结果，可以看到每种编程语言在任务中的表现到底如何。</p><p>在上面的图片中，我们使用Rust和Zig编写的mandelbrot和nbody程序，从性能<strong>由好到差</strong>进行排列。</p><p>你会注意到在某些情况下，Zig的性能优于Rust，而在其他情况下，Rust的性能优于Zig。两者都是高性能的语言，因此在项目中选择任一选项都应该能够满足你的需求。</p><h3 id="Rust-vs-Zig：流行度"><a href="#Rust-vs-Zig：流行度" class="headerlink" title="Rust vs. Zig：流行度"></a>Rust vs. Zig：流行度</h3><p>在选择要学习的编程语言时，流行度可能是一个重要因素。选择一种流行的语言不仅增加了你找到资源和支持的机会，还意味着你更有可能找到合作的开发者。</p><p>StackOverflow最新的开发者调查提供了一些有趣的观察视角。正如前面提到的，Rust是今年最受钦佩的语言，有84.66％的受访者表示他们明年想再次使用它，而Zig只有71.33％。</p><p>Rust在受欢迎语言列表中排名第14位，而Zig在总共列出的51种语言中排名第41位。</p><p>可能是因为它仍处于早期阶段，因此Zig在这两种情况下才获得较低的流行度。无论如何，考虑你选择工作的语言的流行度是至关重要的。</p><h3 id="Rust-vs-Zig：薪酬"><a href="#Rust-vs-Zig：薪酬" class="headerlink" title="Rust vs. Zig：薪酬"></a>Rust vs. Zig：薪酬</h3><p>StackOverflow的开发者调查还包含了受访者报告的最高薪酬的信息。如果你对进入软件开发市场感兴趣，这张图表可能对你很有帮助。</p><p>有趣的是，尽管Zig是一个新的选择，但实际上是今年最高薪酬的语言，而Rust在列表中排名第14位。如果你出于专业原因想要学习Rust或Zig，这些信息可能会有所帮助：</p><p><img src="https://pic.imgdb.cn/item/65ad10f2871b83018ac9cd1e.png" alt="Red Bar Chart With Dark Grey Background And White Labels Comparing Reported Pay For Developers By Language Ordered From Highest Pay To Lowest Pay"></p><p>尽管这张图表非常有帮助，但它只提供了局部的一些信息。当确定一个开发者的薪酬时，还有其他因素需要考虑，比如他们的经验水平和他们所在公司。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>关于Rust和Zig，很难说哪一个是明显的赢家。每种语言都有其优点和缺点。在深入使用任何一种语言之前，进行研究是至关重要的。这就是为什么我希望这篇文章能帮助你找到正确的选择。</p><p><a href="https://blog.logrocket.com/comparing-rust-vs-zig-performance-safety-more/">https://blog.logrocket.com/comparing-rust-vs-zig-performance-safety-more/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Rust和Zig，这两种语言都旨在编写高效、性能优异的代码，然而它们在实现这一目标时采用了不同的方式。&lt;/p&gt;
&lt;p&gt;值得注意的是，Rust和Zig根植于截然不同的理念，这可能影响开发者选择时的取舍。为了更深入地了解它们在相互比较中的表现，我们将进一步探讨它们各自的特点。&lt;</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Optimizing the construction of the VM ecosystem with KubeVirt</title>
    <link href="https://zoues.com/posts/a78d5062/"/>
    <id>https://zoues.com/posts/a78d5062/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-02-03T06:32:45.374Z</updated>
    
    <content type="html"><![CDATA[<p>Two months ago, we were thrilled to share insights in the article “Best Practices for Migrating VM Clusters to KubeVirt 1.0.” As previously mentioned, we have selected AlmaLinux and Kubernetes 1.28 as the foundation for virtualization, employing cgroup v2 for resource isolation. Before moving to the production phase, we encountered additional challenges, particularly related to Kubernetes, containerd, and specific issues within KubeVirt. Therefore, in this second article, our goal is to share practical experiences and insights gained before the deployment of KubeVirt into a production environment.</p><h3 id="Latest-Developments"><a href="#Latest-Developments" class="headerlink" title="Latest Developments"></a>Latest Developments</h3><p>KubeVirt containerizes the trusted virtualization layer of QEMU and libvirt, enabling the management of VMs as standard Kubernetes resources. This approach offers users a more flexible, scalable, and contemporary solution for virtual machine management. As the project progresses, we’ve identified specific misconceptions, configuration errors, and opportunities to enhance KubeVirt functionality, especially in the context of utilizing Kubernetes 1.28 and containerd. The details are outlined below:</p><h4 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h4><ul><li>kubelet ready-only port</li></ul><p>To address security concerns, we have taken measures to mitigate potential malicious attacks on pods and containers. Specifically, we have discontinued the default opening of the insecure read-only port 10255 for the kubelet in K8s clusters running Kubernetes 1.26 or later. Instead, the authentication port 10250 is now opened and utilized by the kubelet.</p><ul><li>service account token expiration</li></ul><p>To enhance data security, Kubernetes 1.21 defaults to enabling the BoundServiceAccountTokenVolume feature. This feature specifies the validity period of service account tokens, automatically renews them before expiration, and invalidates tokens after associated pods are deleted. If using client-go version 11.0.0 or later, or 0.15.0 or later, the kubelet automatically reloads service account tokens from disks to facilitate token renewal.</p><ul><li>securing controller-manager and scheduler metrics</li></ul><p>Secure serving on port 10257 to kube-controller-manager (configurable via –secure-port) is now enabled. Delegated authentication and authorization are to be configured using the same flags as for aggregated API servers. Without configuration, the secure port will only allow access to &#x2F;healthz. (#64149, @sttts) Courtesy of SIG API Machinery, SIG Auth, SIG Cloud Provider, SIG Scheduling, and SIG Testing</p><p>Added secure port 10259 to the kube-scheduler (enabled by default) and deprecate old insecure port 10251. Without further flags self-signed certs are created on startup in memory. (#69663, @sttts)</p><h4 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h4><ul><li>private registry</li></ul><p>Modify your config.toml file (usually located at &#x2F;etc&#x2F;containerd&#x2F;config.toml) as shown below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">version = 2</span><br><span class="line"></span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">config_path = &quot;/etc/containerd/certs.d&quot;</span><br></pre></td></tr></table></figure><p>In containerd registry configuration, a registry host namespace refers to the path of the hosts.toml file specified by the registry host name or IP address, along with an optional port identifier. When submitting a pull request for an image, the typical format is as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pull [registry_host_name|IP address][:port][/v2][/org_path]&lt;image_name&gt;[:tag|@DIGEST]</span><br></pre></td></tr></table></figure><p>The registry host namespace part is <code>[registry_host_name|IP address][:port]</code>. For example, the directory structure for docker.io looks like this:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code$ tree /etc/containerd/certs.d</span><br><span class="line">/etc/containerd/certs.d</span><br><span class="line">└── docker.io</span><br><span class="line">└── hosts.toml</span><br></pre></td></tr></table></figure><p>Alternatively, you can use the _default registry host namespace as a fallback if no other namespace matches.</p><ul><li>systemd cgroup</li></ul><p>While containerd and Kubernetes default to using the legacy cgroupfs driver for managing cgroups, it is recommended to utilize the systemd driver on systemd-based hosts to adhere to the “single-writer” rule of cgroups.</p><p>To configure containerd to use the systemd driver, add the following option in <code>/etc/containerd/config.toml</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">version = 2</span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</span><br><span class="line">SystemdCgroup = true</span><br></pre></td></tr></table></figure><p>Additionally, apart from configuring containerd, you need to set the KubeletConfiguration to use the “systemd” cgroup driver. The KubeletConfiguration is typically found at <code>/var/lib/kubelet/config.yaml</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: &quot;systemd&quot;</span><br></pre></td></tr></table></figure><ul><li>[community issue]containerd startup hangs when &#x2F;etc is ready-only</li></ul><p>We observed that, following the update from containerd v1.6.21 to v1.6.22, the systemd service failed to start successfully. Upon closer inspection during debugging, it was revealed that containerd did not fully initialize (lacking the “containerd successfully booted in …” message) and did not send the sd notification READY&#x3D;1 event.</p><ul><li>migration docker to containerd</li></ul><p>you have to configure the KubeletConfiguration to use the “containerd” endpoint. The KubeletConfiguration is typically located at &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">containerRuntimeEndpoint: &quot;unix:///run/containerd/containerd.sock&quot;</span><br></pre></td></tr></table></figure><p>Because <code>/var/lib/docker</code> is mounted on a separate disk, switching to containerd requires navigating to the root directory of containerd.</p><h3 id="kubevirt"><a href="#kubevirt" class="headerlink" title="kubevirt"></a>kubevirt</h3><ul><li>containerDisk data persistent<br>The containerDisk feature provides the ability to store and distribute VM disks in the container image registry. containerDisks can be assigned to VMs in the disks section of the VirtualMachineInstance spec.containerDisks are ephemeral storage devices that can be assigned to any number of active VirtualMachineInstances.We can persist data locally through incremental backups.</li><li>hostdisk support qcow2 format</li><li>hostdisk support hostpath capacity expansion</li></ul><h3 id="Storage-Solution"><a href="#Storage-Solution" class="headerlink" title="Storage Solution"></a>Storage Solution</h3><h4 id="VM-Image-storage-Soultion"><a href="#VM-Image-storage-Soultion" class="headerlink" title="VM Image storage Soultion"></a>VM Image storage Soultion</h4><p>In KubeVirt, the original virtual machine image file is incorporated into the &#x2F;disk path of the Docker base image and subsequently pushed to the image repository for utilization in virtual machine creatio.</p><p>Example: we could Inject a local VirtualMachineInstance disk into a container image</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; END &gt; Dockerfile</span><br><span class="line">FROM scratch</span><br><span class="line">ADD --chown=107:107 almalinux.qcow2 /disk/</span><br><span class="line">END</span><br><span class="line"></span><br><span class="line">docker build -t kubevirt/alamlinux:latest .</span><br></pre></td></tr></table></figure><p>When initiating a virtual machine, a Virtual Machine Instance (VMI) Custom Resource Definition (CRD) is created, capturing the specified virtual machine image’s name. Subsequent to VMI creation, the virt-controller generates a corresponding virt-launcher pod for the VMI. This pod comprises threee containers: compute container hosting the compute process for virt-launcher, named container-disk, responsible for managing the storage of the virtual machine image and guest-console-log container. The imageName of the container-disk container corresponds to the virtual machine image name recorded in the VMI. Once the virt-launcher pod is created, kubelet retrieves the container-disk image and initiates the container-disk container. During startup, the container-disk consistently monitors the disk_0.sock file under the -copy-path, with the sock file mapped to the path &#x2F;var&#x2F;run&#x2F;kubevirt&#x2F;container-disk&#x2F;{vmi-uuid}&#x2F; on the host machine through hostPath.</p><p>To facilitate the retrieval of necessary information during virtual machine creation, the virt-handler pod utilizes HostPid, enabling visibility of the host machine’s pid and mount details within the virt-handler container. During the virtual machine creation process, virt-handler identifies the pid of the container-disk process by referencing the disk_0.sock file of the VMI. It proceeds to determine the disk number of the container-disk container’s root disk using &#x2F;proc&#x2F;{pid}&#x2F;mountInfo. Subsequently, by cross-referencing the disk number of the container-disk root disk with the mount information of the host machine , it pinpoints the physical location of the container-disk root disk. Finally, it constructs the path for the virtual machine image file (&#x2F;disk&#x2F;disk.qcow2), retrieves the actual storage location (sourceFile) of the original virtual machine image on the host machine, and mounts the sourceFile to the targetFile for subsequent use as a backingFile during virtual machine creation.</p><h4 id="Host-Disk-Storage"><a href="#Host-Disk-Storage" class="headerlink" title="Host Disk Storage"></a>Host Disk Storage</h4><p>A hostDisk volume type provides the ability to create or use a disk image located somewhere on a node. It works similar to a hostPath in Kubernetes and provides two usage types:</p><p>DiskOrCreate if a disk image does not exist at a given location then create one<br>Disk a disk image must exist at a given location<br>need to enable the HostDisk feature gate.</p><p>Currently, hostdisk feature has some limitations. The expansion of hostdisk is only supported in the manner of using Persistent Volume Claims (PVC), and the disk format is limited to raw files.</p><p>Details regarding the above will be elaborated in the Feature Expansion section.</p><h3 id="Feature-Expansion"><a href="#Feature-Expansion" class="headerlink" title="Feature Expansion"></a>Feature Expansion</h3><h4 id="Support-VM-static-expansion"><a href="#Support-VM-static-expansion" class="headerlink" title="Support VM static expansion"></a>Support VM static expansion</h4><p>The CPU&#x2F;Mem is also provided with a synchronous interface when the CPU&#x2F;Mem disk is stopped and expanded. The CPU hotplug feature was introduced in KubeVirt v1. 0, making it possible to configure the VM workload to allow for adding or removing virtual CPUs while the VM is running,While the current version supports online expansion, we still opt for static expansion, primarily due to the temporary nature of VMs. The challenge here is that when resources are insufficient, the VM will not start.</p><h4 id="hostdisk-support-qcow2-and-online-expand"><a href="#hostdisk-support-qcow2-and-online-expand" class="headerlink" title="hostdisk support qcow2 and online expand"></a>hostdisk support qcow2 and online expand</h4><p>The current hostdisk has some limitations. The expansion of hostdisk is only supported in the manner of using Persistent Volume Claims (PVC), and the disk is limited to raw format,To implement this feature, we made minor adjustments to all components.</p><h4 id="cold-migration"><a href="#cold-migration" class="headerlink" title="cold migration"></a>cold migration</h4><p>We refrain from employing live migration capabilities due to their complexity and several limitations in our specific scenario. Instead, with data locally persisted and VMs scheduled in a fixed manner, we utilize cold migration through the rsync command.</p><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><p>In addition to the enhanced features mentioned earlier, we have integrated support for both static and dynamic addition or removal of host disks for virtual machines, password reset capabilities, pass-through of physical machine disks, and addressed various user requirements to deliver a more versatile and comprehensive usage experience.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>KubeVirt simplifies running virtual machines on Kubernetes, making it as easy as managing containers. It provides a cloud-native approach to managing virtual machines. KubeVirt addresses the challenge of unifying the management of virtual machines and containers, effectively harnessing the strengths of both. However, there is still a long way to go in practice.</p><p><a href="https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132">https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132</a></p><p><a href="https://segmentfault.com/a/1190000040926384/en">https://segmentfault.com/a/1190000040926384/en</a></p><p><a href="https://www.alibabacloud.com/help/en/ack/product-overview/solution-to-serviceaccount-token-expiration-after-upgrading-122-version">https://www.alibabacloud.com/help/en/ack/product-overview/solution-to-serviceaccount-token-expiration-after-upgrading-122-version</a></p><p><a href="https://github.com/containerd/containerd/issues/9139">https://github.com/containerd/containerd/issues/9139</a></p><p><a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md">https://github.com/containerd/containerd/blob/main/docs/cri/config.md</a></p><p><a href="https://www.cncf.io/blog/2023/09/22/best-practices-for-transitioning-vm-clusters-to-kubevirt-1-0/https://kubevirt.io/user-guide/virtualmachines/disksand_volumes/#hostdisk">https://www.cncf.io/blog/2023/09/22/best-practices-for-transitioning-vm-clusters-to-kubevirt-1-0/https://kubevirt.io/user-guide/virtualmachines/disksand_volumes/#hostdisk</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Two months ago, we were thrilled to share insights in the article “Best Practices for Migrating VM Clusters to KubeVirt 1.0.” As previous</summary>
      
    
    
    
    <category term="kubevirt" scheme="https://zoues.com/categories/kubevirt/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="kubevirt" scheme="https://zoues.com/tags/kubevirt/"/>
    
  </entry>
  
  <entry>
    <title>Best practices for transitioning VM clusters to KubeVirt 1.0</title>
    <link href="https://zoues.com/posts/912f3650/"/>
    <id>https://zoues.com/posts/912f3650/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-02-03T06:32:45.375Z</updated>
    
    <content type="html"><![CDATA[<p>The KubeVirt community is thrilled to announce the highly-anticipated release of KubeVirt v1.0! This momentous release signifies the remarkable achievements and widespread adoption within the community, marking a significant milestone for all stakeholders involved. This project became part of CNCF as a sandbox project in September 2019 and attained incubation status in April 2022. KubeVirt has evolved into a production-ready virtual machine management project that seamlessly operates as a native Kubernetes API. We have also chosen KubeVirt as our ultimate solution for virtual machine orchestration. Currently, we are utilizing AlmaLinux as the virtualization foundation and cgroup v2 as the resource isolation mechanism. Throughout the process of implementing KubeVirt, we encountered certain challenges. Therefore, we aim to share some of the practical experiences and insights we’ve gained from working with KubeVirt in this article.</p><h3 id="Why-KubeVirt"><a href="#Why-KubeVirt" class="headerlink" title="Why KubeVirt?"></a>Why KubeVirt?</h3><p>While OpenStack has seen widespread adoption, its architecture is relatively complex. By utilizing KubeVirt, virtual machine management is streamlined, offering an improved integration experience. With KubeVirt’s inclusion in the CNCF sandbox project and its integration with the CNCF ecosystem, Kubernetes API has been extended with custom resource definitions (CRDs) to enable native VM operation within Kubernetes.</p><p>KubeVirt containerizes the trusted virtualization layer of QEMU and libvirt, allowing VMs to be handled just like any other Kubernetes resource. This approach provides users with a more flexible, scalable, and modern virtual machine management solution, offering the following key advantages:</p><ul><li>Simplified Architecture and Management: Compared to OpenStack, KubeVirt offers a simplified architecture and management requirements. OpenStack can be unwieldy and costly to maintain, while KubeVirt leverages Kubernetes for the automated lifecycle management of VMs. It eliminates separate processes for VMs and containers, facilitating the integration of workflows for both virtualization and containerization. This simplifies the underlying infrastructure stack and reduces management costs.</li><li>Modern, Scalable, Kubernetes-Based Solution: KubeVirt is a modern, scalable, Kubernetes-based virtual machine management solution. By standardizing automated testing and deployment of all applications using Kubernetes, and unifying metadata within Kubernetes, it reduces the risk of deployment errors and enables faster iteration. This minimizes the operational workload for DevOps teams and accelerates day-to-day operations.</li><li>Tight Integration with the Kubernetes Ecosystem: KubeVirt seamlessly integrates with the Kubernetes ecosystem, offering improved scalability and performance. When VMs are migrated to Kubernetes, it can lead to cost reductions for software and application use and minimize performance overhead at the virtualization layer.</li><li>Ideal for Lightweight, Flexible, and Modern VM Management: KubeVirt is well-suited for scenarios requiring lightweight, flexible, and modern virtual machine management. Users can run their virtual workloads alongside container workloads, managing them in the same manner. They can also leverage familiar cloud-native tools such as Tekton, Istio, ArgoCD, and more, which are already favored by cloud-native users.</li></ul><h3 id="What‘s-new-in-kubeVirt-1-0-？"><a href="#What‘s-new-in-kubeVirt-1-0-？" class="headerlink" title="What‘s new in kubeVirt 1.0 ？"></a>What‘s new in kubeVirt 1.0 ？</h3><p>In POC phase, two RPM-based packages and six container images were used, providing an extension for virtual machine management within Kubernetes:</p><ol><li>kubevirt-virtctl: This package can be installed on any machine with administrator access to the cluster. It contains the virtctl tool, which simplifies virtual machine management using kubectl. While kubectl can be used for this purpose, managing VMs can be complex due to their stateful nature. The virtctl tool abstracts this complexity, enabling operations like starting, stopping, pausing, unpausing, and migrating VMs. It also provides access to the virtual machine’s serial console and graphics server.</li><li>kubevirt-manifests: This package contains manifests for installing KubeVirt. Key files include kubevirt-cr.yaml, representing the KubeVirt Custom Resource definition, and kubevirt-operator.yaml, which deploys the KubeVirt operator responsible for managing the KubeVirt service within the cluster.<br>The container images are as follows:</li></ol><ul><li>virt-api: Provides a Kubernetes API extension for virtual machine resources.</li><li>virt-controller: Watches for new or updated objects created via virt-api and ensures object states match the requested state.</li><li>virt-handler: A DaemonSet and node component that keeps cluster-level virtual machine objects in sync with libvirtd domains running in virt-launcher. It can also perform node-centric operations like configuring networking and storage.</li><li>virt-launcher: A node component that runs libvirt and QEMU to provide the virtual machine environment.</li><li>virt-operator: Implements the Kubernetes operator pattern for managing the KubeVirt application.</li><li>libguestfs-tools: Provides utilities for accessing and modifying VM disk images.<br>The v1.0 release signifies significant growth for the KubeVirt community, progressing from an idea to a production-ready Virtual Machine Management solution over the past six years. This release emphasizes maintaining APIs while expanding the project. The release cadence has shifted to align with Kubernetes practices, enabling better stability, compatibility, and support.</li></ul><p>The project has embraced Kubernetes community practices, including SIGs for test and review responsibilities, a SIG release repo for release-related tasks, and regular SIG meetings covering areas like scale, performance, and storage.</p><p>Notable features in v1.0 include memory over-commit support, persistent vTPM for easier BitLocker usage on Windows, initial CPU Hotplug support, hot plug, and hot unplug (in alpha), and further developments in API stabilization and SR-IOV interface support.</p><p>The focus is on aligning KubeVirt with Kubernetes and fostering community collaboration to enhance virtual machine management within the Kubernetes ecosystem.</p><h3 id="What-issues-in-kubeVirt-1-0？"><a href="#What-issues-in-kubeVirt-1-0？" class="headerlink" title="What issues in kubeVirt 1.0？"></a>What issues in kubeVirt 1.0？</h3><h4 id="CgroupV2-support"><a href="#CgroupV2-support" class="headerlink" title="CgroupV2 support"></a>CgroupV2 support</h4><p>When using cgroup v2, starting a VM with a non-hotpluggable volume can be problematic because cgroup v2 doesn’t provide information about the currently allowed devices for a container. KubeVirt addresses this issue by tracking device rules internally using a global variable:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/kubevirt/7/pkg/virt-handler/cgroup/cgroup_v2_manager.go#L10</span><br><span class="line"></span><br><span class="line">var rulesPerPid = make(map[string][]*devices.Rule)</span><br></pre></td></tr></table></figure><p>However, this approach has some drawbacks:</p><p>The variable won’t survive a crash or restart of the virt-handler pod, resulting in data loss.<br>The state is stored in a dynamic structure (a map), and stale data is not removed, causing memory consumption to continuously increase.<br>A potential solution is to store the state in a file, for example, <code>/var/run/kubevirt-private/devices.list</code>. This file should be updated each time a device is added or removed. Additionally, it should be removed when the corresponding VM is destroyed, or periodic cleanup can be performed. The file can follow the same data format as devices.list on cgroup v1 hosts, allowing the same code to parse the current state for both v1 and v2.</p><p>However, managing the file introduces the challenge of performing transactions, i.e., applying actual device rules and writing the state to the file atomically.</p><p>You can find more details and discussions about this issue in GitHub issue #7710.</p><h4 id="Cilium-Support"><a href="#Cilium-Support" class="headerlink" title="Cilium Support"></a>Cilium Support</h4><ul><li>cilium multi-homing<br>In Kubernetes, each pod typically has only one network interface (aside from a loopback interface). Cilium-native multi-homing aims to enable the attachment of additional network interfaces to pods. This functionality is similar to what the Multus CNI offers, which allows the attachment of multiple network interfaces to pods. However, Cilium-native multi-homing distinguishes itself by relying exclusively on the Cilium CNI as the sole CNI installed.</li></ul><p>This feature should provide robust support for all existing Cilium datapath capabilities, including network policies, observability, datapath acceleration, and service discovery. Furthermore, it aims to offer a straightforward developer experience that aligns with the simplicity and usability that Cilium already provides today.</p><ul><li>multus<br>When utilizing Cilium version 1.14.0 alongside multus-cni, there seems to be an issue where the secondary interface does not become visible. Here’s a list of files you can find under the &#x2F;etc&#x2F;cni directory after installing multus in a Cilium 1.14 environment:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l</span><br><span class="line">/etc/cni/net.d/05-cilium.conflist</span><br><span class="line">/etc/cni/net.d/00-multus.conf.cilium_bak</span><br><span class="line">/etc/cni/net.d/100-crio-bridge.conflist.cilium_bak</span><br><span class="line">/etc/cni/net.d/200-loopback.conflist.cilium_bak</span><br><span class="line">/etc/cni/net.d/multus.d/multus.kubeconfig</span><br></pre></td></tr></table></figure>The issue with multus installation in Cilium 1.14 has been resolved by setting cni.exclusive to false.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Make Cilium take ownership over the `/etc/cni/net.d` directory on the</span><br><span class="line"># node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.</span><br><span class="line"># This ensures no Pods can be scheduled using other CNI plugins during Cilium</span><br><span class="line"># agent downtime.</span><br><span class="line">exclusive: false</span><br></pre></td></tr></table></figure></li></ul><h4 id="Harbor-limit"><a href="#Harbor-limit" class="headerlink" title="Harbor limit"></a>Harbor limit</h4><p>We encountered an issue when attempting to push a container with a single layer size which contain win.qcow2 image exceeding 10.25GB to our Harbor instance hosted on an EC2 instance. Our Harbor version is v2.1.2, and we are using S3 as the storage backend.</p><p>Our system has successfully handled containers with total sizes exceeding 15GB in the past. However, this specific container with a single layer size of 13.5GB repeatedly fails to push. On the client side, we receive limited feedback:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sep 10 22:29:19 backend-ci dockerd[934]:</span><br><span class="line">time=&quot;2023-09-10T22:29:19.628869277+02:00&quot; level=error msg=&quot;Upload failed, retrying: blob upload unknown&quot;</span><br><span class="line">Although the push activity completes successfully, the client-side error only appears afterward. In the registry.log, we’ve noticed the following error:</span><br><span class="line"></span><br><span class="line">registry[885]: time=&quot;2023-09-10T08:47:25.330317861Z&quot; level=error msg=&quot;upload resumed at wrong offest: 10485760000 != 12341008872&quot;</span><br></pre></td></tr></table></figure><p>We would greatly appreciate any insights or advice on this matter. Perhaps others have encountered similar issues with very large layers, especially when using S3 as a storage backend, where pushing layers larger than 10GB is not supported. We’ve also come across potential fixes proposed in this GitHub pull request:</p><p><a href="https://github.com/goharbor/harbor/pull/16322">https://github.com/goharbor/harbor/pull/16322</a></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>KubeVirt simplifies running virtual machines on Kubernetes, making it as easy as managing containers. It provides a cloud-native approach to managing virtual machines. KubeVirt addresses the challenge of unifying the management of virtual machines and containers, effectively harnessing the strengths of both. However, there is still a long way to go in practice. Nevertheless, the release of version 1.0 is significant for the community and users. We look forward to the widespread adoption of KubeVirt and its full support for cgroupv2.</p><ul><li><p><a href="https://documentation.suse.com/container/kubevirt/html/SLE-kubevirt/index.html">https://documentation.suse.com/container/kubevirt/html/SLE-kubevirt/index.html</a></p></li><li><p><a href="https://kubevirt.io/2023/KubeVirt-v1-has-landed.html">https://kubevirt.io/2023/KubeVirt-v1-has-landed.html</a></p></li><li><p>CFP: Cilium-native multi-homing · Issue #20129</p></li><li><p><a href="https://github.com/goharbor/harbor/issues/15719">https://github.com/goharbor/harbor/issues/15719</a></p></li><li><p><a href="https://github.com/kubevirt/kubevirt/issues/398">https://github.com/kubevirt/kubevirt/issues/398</a></p></li><li><p><a href="https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132">https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The KubeVirt community is thrilled to announce the highly-anticipated release of KubeVirt v1.0! This momentous release signifies the rema</summary>
      
    
    
    
    <category term="kubevirt" scheme="https://zoues.com/categories/kubevirt/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="kubevirt" scheme="https://zoues.com/tags/kubevirt/"/>
    
  </entry>
  
  <entry>
    <title>Embracing Cgroup V2:Best Practices for Migrating Kubernetes Clusters to AlmaLinux</title>
    <link href="https://zoues.com/posts/58fc8d19/"/>
    <id>https://zoues.com/posts/58fc8d19/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-02-03T06:32:45.375Z</updated>
    
    <content type="html"><![CDATA[<p>With the announcement of CentOS discontinuation by the CentOS community , along with the set dates for service termination, we have put the switch to a new container operating system on our agenda. Based on factors such as migration cost, smoothness of transition, and maintenance difficulty, we have chosen AlmaLinux from the RHEL series as an alternative solution.</p><p>NOTE: AlmaLinux is just one of the replacement options available within the RHEL ecosystem. Our choice is based on our specific production needs and does not necessarily apply to everyone.</p><p>AlmaLinux 9 defaults to using cgroup v2, and this configuration affects some underlying components. Therefore, certain adaptations and compatibility work need to be done. This article presents the best practices for migrating Kubernetes cluster nodes from CentOS to AlmaLinux which involves removing dockershim and utilizing cgroup v2 for node resource management.</p><h2 id="Why-Cgroup-v2"><a href="#Why-Cgroup-v2" class="headerlink" title="Why Cgroup v2?"></a>Why Cgroup v2?</h2><p>Effective resource management is a critical aspect of Kubernetes. This involves managing the finite resources in your nodes, such as CPU, memory, and storage.</p><p><em>cgroups</em> are a Linux kernel capability that establish resource management functionality like limiting CPU usage or setting memory limits for running processes.</p><p>When you use the resource management capabilities in Kubernetes, such as configuring requests and limits for Pods and containers, Kubernetes uses cgroups to enforce your resource requests and limits.</p><p>The Linux kernel offers two versions of cgroups: cgroup v1 and v2.</p><p>Here is our comparison between the two versions based on our research:</p><table><thead><tr><th></th><th>cgroup v1</th><th>cgroup v2</th></tr></thead><tbody><tr><td>Maintainability</td><td>deprecate and systemd community intend to remove cgroup v1 support from systemd release after the end of 2023</td><td>Many recent releases of Linux distributions have switched over to cgroup v2 by default</td></tr><tr><td>Compatibility</td><td>support</td><td>1. Components such as kubelet need to be adapted for cgroup v2 2. Business applications require JDK version upgrades, which can be achieved by replacing the base image</td></tr><tr><td>hierarchy</td><td>multiple hierarchies，it wasn’t useful and complicated in practice</td><td>single unified hierarchy</td></tr><tr><td>resource allocation management</td><td>basic</td><td>more powerful、dynamic and enhanced resource allocation management，such as the following： • Unified accounting for different types of memory allocations (network and kernel memory, etc) • Accounting for non-immediate resource changes such as page cache write backs • Safer sub-tree delegation to containers</td></tr><tr><td>Performance</td><td>support for multiple hierarchies came at a steep cost</td><td>better</td></tr><tr><td>Scalability</td><td>it seemed to provide a high level of flexibility,  but it wasn’t useful in practice</td><td>provide a high level of flexibility ，new features like PSI</td></tr><tr><td>Security</td><td>the known CVEs, such as cve-2022-0492、cve-2021-4154</td><td>support rootless container</td></tr></tbody></table><p>cgroup v2 has been in development in the Linux Kernel since 2016 and in recent years has matured across the container ecosystem. With Kubernetes 1.25, cgroup v2 support has graduated to general availability.</p><h2 id="What-issues-were-encountered？"><a href="#What-issues-were-encountered？" class="headerlink" title="What issues were encountered？"></a>What issues were encountered？</h2><h3 id="Java-applications"><a href="#Java-applications" class="headerlink" title="Java applications"></a>Java applications</h3><p><a href="https://bugs.openjdk.org/browse/JDK-8146115">JDK-8146115</a> added Hotspot runtime support for JVMs running in Docker containers. At the time Docker used cgroups v1 and, hence, runtime support only includes cgroup v1 controllers.</p><p>JDK-8230305 extended functionality of <a href="https://bugs.openjdk.org/browse/JDK-8146115">JDK-8146115</a> to also detect cgroups v2. That is iff cgroups v2 unified hierarchy is available only, use the cgroups v2 backend. Otherwise fall back to existing cgroups v1 container support.</p><p>require version：jdk8u372, 11.0.16, 15 and later</p><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>Currently,  the version of Kubernetes in production is 1.19 and enabling cgroup v2 support for kubelet is proving to be challenging. While a comprehensive upgrade of Kubernetes is being researched and prepared, we are currently focusing on implementing cgroup v2 support specifically for kubelet. This approach allows for a shorter implementation time while laying the foundation for the subsequent comprehensive upgrade.</p><p>To enable cgroup v2 support, several adjustments need to be made to various components:</p><ol><li>Kernel Version: We are currently using kernel version 5.15, which meets the minimum requirement for cgroup v2 (4.14). However, it is recommended to use kernel version 5.2 or newer due to the lack of freezer support in older versions.</li><li>Systemd and Runc: It is highly recommended to run runc with the systemd cgroup driver (<code>runc --systemd-cgroup</code>), although it is not mandatory. To ensure compatibility, it is recommended to use systemd version 244 or later, as older versions do not support delegation of the <code>cpuset</code> controller.</li><li>Kubelet : The vendor for kubelet needs to upgrade the runc version. Currently, the latest fully supported version of runc for cgroup v2 is rc93. To minimize changes, we have chosen rc94 and modified the kubelet code to internally maintain runc rc94. This allows us to merge the necessary cgroup v2-related pull requests. However, in rc95, there are significant changes to the cgroup.Manager interface, which does not align with the principle of minimal changes.</li></ol><p>metrics-server retrieves resource usage information of nodes and pods using kubelet summary and other interfaces. This data is crucial for Horizontal Pod Autoscaling (HPA) based on resource scaling. To eliminate dockershim, the kubelet should utilize the systemd cgroup driver and configure the runtime accordingly.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><ol start="4"><li>Containerd : starting from version 1.4, containerd supports cgroup v2. We have successfully validated the removal of dockershim and conducted thorough testing of business operations in the testing environment. With the successful testing, we will proceed with the production rollout. Similar to kubelet, containerd also requires the systemd cgroup driver. Use the following configuration for the systemd cgroup driver:</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc]</span><br><span class="line">  ...</span><br><span class="line">  [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc.options]</span><br><span class="line">    SystemdCgroup = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>The configuration of the sandbox image and registry can be customized based on specific requirements.</p><h3 id="Systemd-with-cilium"><a href="#Systemd-with-cilium" class="headerlink" title="Systemd with cilium"></a>Systemd with cilium</h3><p>SystemD versions greater than 245 automatically set the rp_filter value to 1 for all network interfaces. This conflicts with Cilium, which requires rp_filter to be 0 on its interfaces, leading to a disruption in out-of-node IPv4 traffic.</p><p>Therefore, it is crucial to exercise caution before upgrading SystemD, as demonstrated by the failure experienced by Datadog, which served as a significant warning.</p><p>on March 8, 2023, at 06:00 UTC, a security update to systemd was automatically applied to several VMs, which caused a latent adverse interaction in the network stack (on Ubuntu 22.04 via systemd v249) to manifest upon systemd-networkd restarting.Namely, <code>systemd-networkd</code> forcibly deleted the routes managed by the Container Network Interface (CNI) plugin (Cilium) we use for communication between containers. This caused the affected nodes to go offline.</p><p>Additionally, when container runtimes are configured with cgroup v2, the Cilium agent pod is deployed in a separate cgroup namespace. For example, Docker container runtime with cgroupv2 support defaults to private cgroup namespace mode. Due to cgroup namespaces, the Cilium pod’s cgroup filesystem points to a virtualized hierarchy instead of the host cgroup root. Consequently, BPF programs are attached to the nested cgroup root, rendering socket load balancing ineffective for other pods. To address this limitation, work is being done in the Cilium project  to revisit assumptions made around cgroup hierarchies and enable socket load balancing in different environments.</p><p>Don’t worry, these issues have already been fixed by the community.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>During the testing process of Cgroup V2 and removing dockershim in our testing environment, we conducted extensive adaptation and stability tests. Our long-term analysis revealed that the benefits of adopting this technology roadmap far outweigh the initial investment. As part of our plan, we intend to promote the adoption of Cgroup V2-based machines in production. This will involve a meticulous testing and validation process, followed by a gradual rollout in production environments. We will start with offline applications such as logging and big data applications.</p><p>The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in Shanghai, China from 26-28 September, 2023.  We are considering submitting a proposal for a presentation at KubeCon 2023, where we will have the opportunity to share the latest developments and insights with the conference attendees.</p><p><a href="https://bugs.openjdk.org/browse/JDK-8230305">https://bugs.openjdk.org/browse/JDK-8230305</a></p><p><a href="https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/">https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</a></p><p><a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html</a></p><p><a href="https://github.com/cilium/cilium/issues/10645">https://github.com/cilium/cilium/issues/10645</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;With the announcement of CentOS discontinuation by the CentOS community , along with the set dates for service termination, we have put t</summary>
      
    
    
    
    <category term="cgroupv2" scheme="https://zoues.com/categories/cgroupv2/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="almalinux" scheme="https://zoues.com/tags/almalinux/"/>
    
  </entry>
  
  <entry>
    <title>30分钟搞定Ziglang</title>
    <link href="https://zoues.com/posts/5941d835/"/>
    <id>https://zoues.com/posts/5941d835/</id>
    <published>2024-01-14T05:40:08.000Z</published>
    <updated>2024-08-03T10:02:00.553Z</updated>
    
    <content type="html"><![CDATA[<p>这份zig简明教程适合已经有编程基础知识的同学快速了解zig语言，同时也适合没有编程经验但是懂得善用搜索引擎的同学,该文章详细介绍Zig编程语言各种概念，主要包括基础知识、函数、结构体、枚举、数组、切片、控制结构、错误处理、指针、元编程和堆管理等内容。</p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>命令 <code>zig run my_code.zig</code> 将编译并立即运行你的 Zig 程序。每个单元格都包含一个 Zig 程序，你可以尝试运行它们（其中一些包含编译时错误，你可以注释掉后再尝试）。</p><p>首先需要声明一个 <code>main()</code> 函数来运行代码。</p><p>下面的代码什么都不会做，只是简单的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// comments look like this and go to the end of the line</span><br><span class="line">pub fn main() void &#123;&#125;</span><br></pre></td></tr></table></figure><p>可以使用 内置函数<code>@import</code> 导入标准库，并将命名空间赋值给一个 <code>const</code> 值。Zig 中的几乎所有东西都必须明确地被赋予标识符。你也可以通过这种方式导入其他 Zig 文件，类似地，你可以使用 <code>@cImport</code> 导入 C 文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;hello world!\n&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：后续会在结构部分部分解释 <code>print</code> 语句中的第二个参数。</p><p>一般用<code>var</code> 来声明变量，同时在大多数情况下，需要带上声明变量类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 47; // declares &quot;x&quot; of type i32 to be 47.</span><br><span class="line">    std.debug.print(&quot;x: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>const</code> 声明一个变量的值是不可变的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pub fn main() void &#123;</span><br><span class="line">    const x: i32 = 47;</span><br><span class="line">    x = 42; // error: cannot assign to constant</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 非常严苛，不允许你从外部作用域屏蔽标识符，以防止混淆：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const x: i32 = 47;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 42;  // error: redefinition of &#x27;x&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>全局作用域的常量默认为编译时的 “comptime” 值，如果省略了类型，它们就是编译时类型，并且可以在运行时转换为运行时类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const x: i32 = 47;</span><br><span class="line">const y = -47;  // comptime integer.</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var a: i32 = y; // comptime constant coerced into correct type</span><br><span class="line">    var b: i64 = y; // comptime constant coerced into correct type</span><br><span class="line">    var c: u32 = y; // error: cannot cast negative value -47 to unsigned integer</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果希望在后面设置它，也可以明确选择将其保留为未定义。如果你在调试模式下意外使用它引发错误，Zig 将使用 0XAA 字节填充一个虚拟值，以帮助检测错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">  var x: i32 = undefined;</span><br><span class="line">  std.debug.print(&quot;undefined: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在某些情况下，如果 Zig 可以推断出类型信息，才允许你省略类型信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 47;</span><br><span class="line">    var y: i32 = 47;</span><br><span class="line">    var z = x + y; // declares z and sets it to 94.</span><br><span class="line">    std.debug.print(&quot;z: &#123;&#125;\n&quot;, .&#123;z&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是需要注意，整数字面值是编译时类型，所以下面的示例是行不通的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x = 47; // error: variable of type &#x27;comptime_int&#x27; must be const or comptime</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>函数可以带参数和返回值，使用<code>fn</code>关键字声明。<code>pub</code>关键字表示函数可以从当前作用域导出，使其它地方可以调用。下面示例是一个不返回任何值的函数（foo）。<code>pub</code>关键字表示该函数可以从当前作用域导出，这就是为什么<code>main</code>函数必须是<code>pub</code>的。你可以像大多数编程语言中一样调用函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() void &#123;</span><br><span class="line">    std.debug.print(&quot;foo!\n&quot;, .&#123;&#125;);</span><br><span class="line"></span><br><span class="line">    //optional:</span><br><span class="line">    return;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面示例是一个返回整数值的函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var result = foo();</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;result&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig不允许你忽略函数的返回值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo(); // error: expression value is ignored</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是你可以将其赋值给丢弃变量 <code>_</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">  _ = foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以声明函数时带上参数的类型，这样函数调用时可以传入参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(x: i32) void &#123;</span><br><span class="line">    std.debug.print(&quot;foo param: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo(47);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><p>结构体通过使用<code>const</code>关键字分配一个名称来声明，它们的赋值顺序可以是任意的，并且可以使用常规的点语法进行解引用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Vec2 = struct &#123;</span><br><span class="line">    x: f64,</span><br><span class="line">    y: f64</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var v = Vec2&#123;.y = 1.0, .x = 2.0&#125;;</span><br><span class="line">    std.debug.print(&quot;v: &#123;&#125;\n&quot;, .&#123;v&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结构体可以有默认值；结构体也可以是匿名的，并且可以强制转换为另一个结构体，只要所有的值都能确定：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Vec3 = struct&#123;</span><br><span class="line">    x: f64 = 0.0,</span><br><span class="line">    y: f64,</span><br><span class="line">    z: f64</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var v: Vec3 = .&#123;.y = 0.1, .z = 0.2&#125;;  // ok</span><br><span class="line">    var w: Vec3 = .&#123;.y = 0.1&#125;; // error: missing field: &#x27;z&#x27;</span><br><span class="line">    std.debug.print(&quot;v: &#123;&#125;\n&quot;, .&#123;v&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以将函数放入结构体中，使其像面向对象编程中的对象一样工作。这里有一个语法糖，如果你定义的函数的第一个参数为对象的指针，我们称之为”面向对象编程”，类似于Python带self参数的函数。一般约定是通过将变量命名为self来表示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const LikeAnObject = struct&#123;</span><br><span class="line">    value: i32,</span><br><span class="line"></span><br><span class="line">    fn print(self: *LikeAnObject) void &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;self.value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var obj = LikeAnObject&#123;.value = 47&#125;;</span><br><span class="line">    obj.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们一直传递给<code>std.debug.print</code>的第二个参数是一个元组，它是一个带有数字字段的匿名结构体。在编译时，<code>std.debug.print</code>会找出元组中参数的类型，并生成一个针对你提供的参数字符串的版本，这就是为何Zig知道如何将打印的内容变得漂亮的原因。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;1, 2&#125;); #  error: Unused arguments</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h3><p>枚举通过使用<code>const</code>关键字将枚举组以类型方式来声明。</p><p>注意：在某些情况下，可以简化枚举的名称。 其可以将枚举的值设置为整数，但它不会自动强制转换，你必须使用<code>@enumToInt</code>或<code>@intToEnum</code>来进行转换。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const EnumType = enum&#123;</span><br><span class="line">    EnumOne,</span><br><span class="line">    EnumTwo,</span><br><span class="line">    EnumThree = 3</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;One: &#123;&#125;\n&quot;, .&#123;EnumType.EnumOne&#125;);</span><br><span class="line">    std.debug.print(&quot;Two?: &#123;&#125;\n&quot;, .&#123;EnumType.EnumTwo == .EnumTwo&#125;);</span><br><span class="line">    std.debug.print(&quot;Three?: &#123;&#125;\n&quot;, .&#123;@enumToInt(EnumType.EnumThree) == 3&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数组和切片"><a href="#数组和切片" class="headerlink" title="数组和切片"></a>数组和切片</h3><p>Zig有数组概念，它们是具有在编译时已知长度的连续内存。你可以通过在前面声明类型并提供值列表来初始化它们，同时可以通过数组的<code>len</code>字段访问它们的长度。</p><p>注意：Zig中的数组也是从零开始索引的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array: [3]u32 = [3]u32&#123;47, 47, 47&#125;;</span><br><span class="line"></span><br><span class="line">    // also valid:</span><br><span class="line">    // var array = [_]u32&#123;47, 47, 47&#125;;</span><br><span class="line"></span><br><span class="line">    var invalid = array[4]; // error: index 4 outside array of size 3.</span><br><span class="line">    std.debug.print(&quot;array[0]: &#123;&#125;\n&quot;, .&#123;array[0]&#125;);</span><br><span class="line">    std.debug.print(&quot;length: &#123;&#125;\n&quot;, .&#123;array.len&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟golang类似，Zig也有切片（slices），它们的长度在运行时已知。你可以使用切片操作从数组或其他切片构造切片。与数组类似，切片有一个<code>len</code>字段，告诉它的长度。</p><p>注意：切片操作中的间隔参数是开口的（不包含在内）。 尝试访问超出切片范围的元素会引发运行时panic。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array: [3]u32 = [_]u32&#123;47, 47, 47&#125;;</span><br><span class="line">    var slice: []u32 = array[0..2];</span><br><span class="line"></span><br><span class="line">    // also valid:</span><br><span class="line">    // var slice = array[0..2];</span><br><span class="line"></span><br><span class="line">    var invalid = slice[3]; // panic: index out of bounds</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;slice[0]: &#123;&#125;\n&quot;, .&#123;slice[0]&#125;);</span><br><span class="line">    std.debug.print(&quot;length: &#123;&#125;\n&quot;, .&#123;slice.len&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>字符串文字是以null结尾的utf-8编码的const u8字节数组。Unicode字符只允许在字符串文字和注释中使用。</p><p>注意：长度不包括null终止符（官方称为”sentinel termination”）。 访问null终止符是安全的。 索引是按字节而不是Unicode字符。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const string = &quot;hello 世界&quot;;</span><br><span class="line">const world = &quot;world&quot;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var slice: []const u8 = string[0..5];</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;string &#123;&#125;\n&quot;, .&#123;string&#125;);</span><br><span class="line">    std.debug.print(&quot;length &#123;&#125;\n&quot;, .&#123;world.len&#125;);</span><br><span class="line">    std.debug.print(&quot;null &#123;&#125;\n&quot;, .&#123;world[5]&#125;);</span><br><span class="line">    std.debug.print(&quot;slice &#123;&#125;\n&quot;, .&#123;slice&#125;);</span><br><span class="line">    std.debug.print(&quot;huh? &#123;&#125;\n&quot;, .&#123;string[0..7]&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>const数组可以强制转换为const切片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() []const u8 &#123;  // note function returns a slice</span><br><span class="line">    return &quot;foo&quot;;      // but this is a const array.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;foo()&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><p>Zig提供了与其他语言类似的if语句、switch语句、for循环和while循环。示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) []const u8 &#123;</span><br><span class="line">    if (v &lt; 0) &#123;</span><br><span class="line">        return &quot;negative&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        return &quot;non-negative&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;positive &#123;&#125;\n&quot;, .&#123;foo(47)&#125;);</span><br><span class="line">    std.debug.print(&quot;negative &#123;&#125;\n&quot;, .&#123;foo(-47)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>switch方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) []const u8 &#123;</span><br><span class="line">    switch (v) &#123;</span><br><span class="line">        0 =&gt; return &quot;zero&quot;,</span><br><span class="line">        else =&gt; return &quot;nonzero&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;47 &#123;&#125;\n&quot;, .&#123;foo(47)&#125;);</span><br><span class="line">    std.debug.print(&quot;0 &#123;&#125;\n&quot;, .&#123;foo(0)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>for-loop</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array = [_]i32&#123;47, 48, 49&#125;;</span><br><span class="line"></span><br><span class="line">    for (array) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;array &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    for (array) | value, index | &#123;</span><br><span class="line">        std.debug.print(&quot;array &#123;&#125;:&#123;&#125;\n&quot;, .&#123;index, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var slice = array[0..2];</span><br><span class="line"></span><br><span class="line">    for (slice) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;slice &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    for (slice) | value, index | &#123;</span><br><span class="line">        std.debug.print(&quot;slice &#123;&#125;:&#123;&#125;\n&quot;, .&#123;index, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while loop</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array = [_]i32&#123;47, 48, 49&#125;;</span><br><span class="line">    var index: u32 = 0;</span><br><span class="line"></span><br><span class="line">    while (index &lt; 2) &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;array[index]&#125;);</span><br><span class="line">        index += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>错误是特殊的联合类型，你可以在函数前面加上 ! 来表示该函数可能返回错误。你可以通过简单地将错误作为正常返回值返回来抛出错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError,</span><br><span class="line">    OtherError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    return MyError.GenericError;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn wrap_foo(v: i32) void &#123;</span><br><span class="line">    if (foo(v)) |value| &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125; else |err| &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你编写一个可能出错的函数，当它返回时你必须决定如何处理错误。两个常见的选择是 <code>try</code> 和 <code>catch</code>。<code>try</code> 方式很摆烂，它只是简单地将错误转发为函数的错误。而 <code>catch</code> 需要处理错误。</p><p><code>try</code> 其实就是 <code>catch | err | &#123;return err&#125;</code> 的语法糖。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) !i32 &#123;</span><br><span class="line">    if (v == 42) return MyError.GenericError;</span><br><span class="line">    return v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // catch traps and handles errors bubbling up</span><br><span class="line">    _ = foo(42) catch |err| &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    // try won&#x27;t get activated here.</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;try foo(47)&#125;);</span><br><span class="line"></span><br><span class="line">    // this will ultimately cause main to print an error trace and return nonzero</span><br><span class="line">    _ = try foo(42);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们也可以使用 <code>if</code> 来检查错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) !i32 &#123;</span><br><span class="line">    if (v == 42) return MyError.GenericError;</span><br><span class="line">    return v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// note that it is safe for wrap_foo to not have an error ! because</span><br><span class="line">// we handle ALL cases and don&#x27;t return errors.</span><br><span class="line">fn wrap_foo(v: i32) void &#123;    </span><br><span class="line">    if (foo(v)) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125; else | err | &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    wrap_foo(42);</span><br><span class="line">    wrap_foo(47);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><p>Zig使用<code>*</code>表示指针类型，可以通过<code>.*</code>语法访问指针指向的值。示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn printer(value: *i32) void &#123;</span><br><span class="line">    std.debug.print(&quot;pointer: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value.*&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    printer(&amp;value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：在Zig中，指针需要正确对齐到它所指向的值的对齐方式。 对于结构体，类似于Java，您可以解引用指针并一次获取字段，使用 . 运算符。需要注意的是，这仅适用于一层间接引用，因此如果您有指向指针的指针，您必须首先解引用外部指针。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const MyStruct = struct &#123;</span><br><span class="line">    value: i32</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn printer(s: *MyStruct) void &#123;</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;s.value&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value = MyStruct&#123;.value = 47&#125;;</span><br><span class="line">    printer(&amp;value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig允许任何类型（不仅仅是指针）可为空，但请注意它们是基本类型和特殊值 null 的联合体。要访问未包装的可选类型，请使用 .? 字段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    var vptr: ?*i32 = &amp;value;</span><br><span class="line">    var throwaway1: ?*i32 = null;</span><br><span class="line">    var throwaway2: *i32 = null; // error: expected type &#x27;*i32&#x27;, found &#x27;(null)&#x27;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;vptr.*&#125;); // error: attempt to dereference non-pointer type</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;vptr.?.*&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：当我们使用来自C ABI函数的指针时，它们会自动转换为可为空指针。 获得未包装的可选指针的另一种方法是使用 if 语句：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn nullChoice(value: ?*i32) void &#123;</span><br><span class="line">    if (value) | v | &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;v.*&#125;);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        std.debug.print(&quot;null!\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    var vptr1: ?*i32 = &amp;value;</span><br><span class="line">    var vptr2: ?*i32 = null;</span><br><span class="line"></span><br><span class="line">    nullChoice(vptr1);</span><br><span class="line">    nullChoice(vptr2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="元编程"><a href="#元编程" class="headerlink" title="元编程"></a>元编程</h3><p>Zig的元编程受几个基本概念驱动：</p><ul><li><p>类型在编译时是有效的值。</p></li><li><p>大多数运行时代码在编译时也能工作。</p></li><li><p>结构体字段的评估是编译时的鸭子类型（duck-typed）。</p></li><li><p>Zig标准库提供了执行编译时反射的工具。</p><p>下面是元编程的一个示例：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(x : anytype) @TypeOf(x) &#123;</span><br><span class="line">    // note that this if statement happens at compile-time, not runtime.</span><br><span class="line">    if (@TypeOf(x) == i64) &#123;</span><br><span class="line">        return x + 2;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return 2 * x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i64 = 47;</span><br><span class="line">    var y: i32 =  47;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;i64-foo: &#123;&#125;\n&quot;, .&#123;foo(x)&#125;);</span><br><span class="line">    std.debug.print(&quot;i32-foo: &#123;&#125;\n&quot;, .&#123;foo(y)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下是泛型类型的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn Vec2Of(comptime T: type) type &#123;</span><br><span class="line">    return struct&#123;</span><br><span class="line">        x: T,</span><br><span class="line">        y: T</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">const V2i64 = Vec2Of(i64);</span><br><span class="line">const V2f64 = Vec2Of(f64);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var vi = V2i64&#123;.x = 47, .y = 47&#125;;</span><br><span class="line">    var vf = V2f64&#123;.x = 47.0, .y = 47.0&#125;;</span><br><span class="line">    </span><br><span class="line">    std.debug.print(&quot;i64 vector: &#123;&#125;\n&quot;, .&#123;vi&#125;);</span><br><span class="line">    std.debug.print(&quot;f64 vector: &#123;&#125;\n&quot;, .&#123;vf&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这些概念，我们可以构建非常强大的泛型类型！</p><h3 id="堆管理"><a href="#堆管理" class="headerlink" title="堆管理"></a>堆管理</h3><p>Zig为我们提供了与堆交互的多种方式，通常要求您明确选择使用哪种方式。它们都遵循下述相同的模式：</p><ol><li>创建一个分配器工厂结构体。</li><li>检索由分配器工厂创建的<code>std.mem.Allocator</code>结构体。</li><li>使用<code>alloc/free</code>和<code>create/destroy</code>函数来操作堆。</li><li>（可选）销毁分配器工厂。</li></ol><p>这么处理的目的是：</p><ul><li>为了阻止您过度使用堆。</li><li>这使得调用堆的任何东西（基本上是可失败的操作）都是显式的。</li><li>您可以仔细调整权衡，并使用标准数据结构而无需重写标准库。</li><li>您可以在测试中运行非常安全的分配器，并在发布&#x2F;生产环境中切换到不同的分配器。</li></ul><p>好的，但是你也可以偷点懒。你是不是想一直使用jemalloc？ 只需选择一个全局分配器，并在所有地方使用它（请注意，某些分配器是线程安全的，而某些则不是）。</p><p>在这个示例中，我们将使用<code>std.heap.GeneralPurposeAllocator</code>工厂创建一个具有多种特性（包括泄漏检测）的分配器，并看看它是如何组合在一起的。</p><p>最后一件事，这里使用了<code>defer</code>关键字，它非常类似于Go语言中的<code>defer</code>关键字！还有一个<code>errdefer</code>关键字，如果需要了解更多信息，请查阅Zig文档。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">// factory type</span><br><span class="line">const Gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;);</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // instantiates the factory</span><br><span class="line">    var gpa = Gpa&#123;&#125;;</span><br><span class="line">    </span><br><span class="line">    // retrieves the created allocator.</span><br><span class="line">    var galloc = &amp;gpa.allocator;</span><br><span class="line">    </span><br><span class="line">    // scopes the lifetime of the allocator to this function and</span><br><span class="line">    // performs cleanup; </span><br><span class="line">    defer _ = gpa.deinit();</span><br><span class="line"></span><br><span class="line">    var slice = try galloc.alloc(i32, 2);</span><br><span class="line">    // uncomment to remove memory leak warning</span><br><span class="line">    // defer galloc.free(slice);</span><br><span class="line">    </span><br><span class="line">    var single = try galloc.create(i32);</span><br><span class="line">    // defer gallo.destroy(single);</span><br><span class="line"></span><br><span class="line">    slice[0] = 47;</span><br><span class="line">    slice[1] = 48;</span><br><span class="line">    single.* = 49;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;slice: [&#123;&#125;, &#123;&#125;]\n&quot;, .&#123;slice[0], slice[1]&#125;);</span><br><span class="line">    std.debug.print(&quot;single: &#123;&#125;\n&quot;, .&#123;single.*&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>现在我们已经掌握了相当大的Zig基础知识。没有覆盖的一些（非常重要的）内容包括：</p><ul><li>测试（Zig使得编写测试非常容易）</li><li>标准库</li><li>内存模型（Zig在分配器方面没有倾向性）</li><li>异步编程（Zig 的异步特性在编译器中出现了性能退化，在 0.11 版本的 Zig 中已经不存在了，并且在 Zig 0.12 版本中也可能不会出现。）</li><li>交叉编译</li><li><code>build.zig</code> 文件</li></ul><p>如果想要了解更多细节，请查阅最新的文档：</p><ul><li><a href="https://ziglang.org/documentation/master/">https://ziglang.org/documentation/master/</a></li><li><a href="https://gist.github.com/ityonemo/769532c2017ed9143f3571e5ac104e50">https://gist.github.com/ityonemo/769532c2017ed9143f3571e5ac104e50</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这份zig简明教程适合已经有编程基础知识的同学快速了解zig语言，同时也适合没有编程经验但是懂得善用搜索引擎的同学,该文章详细介绍Zig编程语言各种概念，主要包括基础知识、函数、结构体、枚举、数组、切片、控制结构、错误处理、指针、元编程和堆管理等内容。&lt;/p&gt;
&lt;h3 id</summary>
      
    
    
    
    <category term="Ziglang简明教程" scheme="https://zoues.com/categories/Ziglang%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI关于Kubernetes集群近万节点的生产实践</title>
    <link href="https://zoues.com/posts/1df3dc63/"/>
    <id>https://zoues.com/posts/1df3dc63/</id>
    <published>2021-01-28T00:40:08.000Z</published>
    <updated>2024-02-03T06:32:45.373Z</updated>
    
    <content type="html"><![CDATA[<p>OpenAI已经将Kubernetes集群规模扩展至7500个节点，为大型神经网络模型（如GPT-3，CLIP和DALL·E）及小型实验性研究提供了可扩展的基础架构。 很少将单个Kubernetes集群扩展到如此规模，为此进行了一些必要的改进，但好处是单一的基础架构使我们的机器学习研究团队可以在不修改代码的前提下，快速扩展以缩短实验时间、加速研发进度。</p><hr><p>作者：Benjamin Chess、Eric Sigler</p><p>译者：zouyee</p><p>原文：<a href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/">https://openai.com/blog/scaling-kubernetes-to-7500-nodes/</a></p><p><img src="https://s3.ax1x.com/2021/01/28/yS7JHA.png"></p><p>自上一篇有关扩展到2500个节点的文章以来，我们一直在不断扩展基础架构以满足研究人员的需求，并在此过程中学习了许多其他相关知识。 该篇文章总结了相关经验，以便Kubernetes社区中的其他人可以从中受益，接下来介绍，需要解决的问题。</p><h4 id="一、工作负载"><a href="#一、工作负载" class="headerlink" title="一、工作负载"></a>一、工作负载</h4><p>首先需要说明的是，针对工作负载，我们在Kubernetes集群上运行的应用程序和硬件与其他公司中的场景完全不同。我们面临的问题和相应的解决方案可能与读者所处的实际场景不是太一致。</p><p>大型的机器学习作业可以访问多个节点，及每个节点上的所有硬件资源，因此运行效率最高。允许GPU使用NVLink进行交叉通信，或者GPU使用GPUDirect与NIC通信。因此，对于我们的许多工作负载，单个pod占据了整个节点，因此调度不涉及任何NUMA，CPU或PCIE资源抢占。当前的集群具有完整的双向带宽互通，因此无需考虑任何网络拓扑。因此，调度程序的压力相对较低。</p><p>因为一个新的任务可能包含数百个Pod调度的需求，kube-scheduler存在毛刺现象。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS7GBd.png"></p><p>最大的job是运行MPI（并行计算），job中的所有Pod都工作在同一个MPI通信器中。任何Pod的消亡，都会导致整个job暂停，并重新启动。job定期备份相关信息（即checkpoint），在重新启动时从最近的备份信息处恢复。</p><p>我们不完全依赖Kubernetes进行负载平衡。我们的七层流量很少，因为不需要进行A &#x2F; B测试，蓝绿升级或金丝雀发布等。 Pod通过SSH与其他Pod的MPI直接通信(这部分貌似有点疑问)，而不是<code>service endpoint</code>。服务发现功能相对有限，因为我们只执行一次查找，即在工作启动时（pod刚参与MPI时）。</p><p>大多数job都与Blob类型存储进行交互，通常直接向Blob传输一些数据集的分片，或将其缓存到本地盘。我们也使用了一些PersistentVolumes，但是blob类型存储具有更好的伸缩性，并且不需要挂载、卸载操作。</p><p>超级计算团队努力致力于提供生产级别的计算基础架构，当前在该集群上运行的应用寿命较短，开发人员正在快速迭代中。任何时候都有可能出现新的应用场景，这需要我们对趋势进行预判，并做出适当折衷的设想。</p><hr><h4 id="二、网络"><a href="#二、网络" class="headerlink" title="二、网络"></a>二、网络</h4><p>随着集群中节点和Pod数量的增加，我们发现<code>Flannel</code>难以满足需求。转而使用主机pod网络技术进行Azure VMSSes和相关CNI插件的IP配置。这使我们能够在Pod上获得主机级别的网络吞吐量。</p><p>我们改用基于别名的IP寻址的另一个原因是，在我们最大的集群上，我们可能随时有大约200,000个IP地址正在使用。在测试基于路由的Pod网络时，我们发现路由数量存在明显的限制。</p><p>改造SDN或路由引擎虽然麻烦，但它会使我们的网络设置变得简单。无需任何其他适配器即可添加VPN或隧道。同时我们不必担心数据包分片，因为网络的某些部分的MTU较低。网络策略和流量监控非常简单；数据包的来源和目的地没有任何歧义。</p><p>我们在主机上使用iptables来跟踪每个命名空间和pod的网络资源使用情况。这使研究人员可以可视化其网络使用。由于我们的许多实验都具有独特的外部和Pod内部通信模式，因此对于调查可能出现瓶颈的位置很有用。</p><p>iptables mangle规则可用于标记任意符合特定条件的数据包。如下是我们用来检测流量是内部流量还是外部流量的规则。 FORWARD规则涵盖来自Pod的流量，以及来自主机的INPUT和OUTPUT流量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A INPUT ! -s 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-in&quot;</span><br><span class="line">iptables -t mangle -A FORWARD ! -s 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-in&quot;</span><br><span class="line">iptables -t mangle -A OUTPUT ! -d 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-out&quot;</span><br><span class="line">iptables -t mangle -A FORWARD ! -d 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-out&quot;</span><br></pre></td></tr></table></figure><p>一旦标记，iptables将启动计数器以跟踪与此规则匹配的字节和数据包。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% iptables -t mangle -L -v</span><br><span class="line">Chain FORWARD (policy ACCEPT 50M packets, 334G bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">....</span><br><span class="line">1253K  555M            all  --  any    any     anywhere            !10.0.0.0/8           /* iptables-exporter openai traffic=internet-out */</span><br><span class="line">1161K 7937M            all  --  any    any    !10.0.0.0/8           anywhere             /* iptables-exporter openai traffic=internet-in */</span><br></pre></td></tr></table></figure><p>我们使用基于Prometheus的<code>iptables-exporter</code>的方案，然后将其接入到我们的监控系统。 </p><p><img src="https://s3.ax1x.com/2021/01/28/yS7tAI.png"></p><p>我们网络模型的一个特别的地方是，我们向研究人员公开了节点，容器和服务网络CIDR范围。 我们有一个辐射状网络模型，并使用本机节点和Pod CIDR范围来路由该流量。 研究人员连接到中枢节点，从那里可以访问任何单个集群。 但是集群本身无法相互通信。 这样可以确保集群间相互隔离，且没有跨集群的依存关系以破坏隔离（译者表示…）。</p><p>我们使用主机<code> NAT</code>来转换服务网络CIDR，以处理来自集群外部的流量。 这种设置使我们的研究人员在选择实验方式和选择哪种网络配置上具有极大的灵活性。</p><hr><h5 id="三、API-Server"><a href="#三、API-Server" class="headerlink" title="三、API Server"></a>三、API Server</h5><p>Kubernetes的API Server和etcd集群是集群健康运行的关键组件，因此我们特别注意这些系统上的压力。 我们使用kube-prometheus项目提供的Grafana以及其他内部仪表板。 我们发现针对API Server的HTTP（如429、5xx等状态）告警还是很有效的。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS78nH.png"></p><p>尽管大多数人在k8s集群内运行API Server，但我们选择在集群外运行。 etcd和API Server服务都在它们自己的专用节点上运行。 我们最大的集群运行了5个API  Server和5个etcd节点，以分散负载并最大程度地降低影响（如果其中一台发生故障）。 自从我们在上一篇文章中将Kubernetes Events写入到其他etcd集群以来，我们在etcd方面没有遇到任何麻烦。 API Server是无状态的，通常很容易在自愈实例组或规模集中运行。 我们尚未尝试建立etcd集群的任何自愈等自动化功能。</p><p>API  Server会占用相当大的内存，并且会随着集群中节点的数量线性上升。 对于具有7500个节点的集群，我们观察到每个API Server最多使用了70GB。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS71je.png"></p><p>API Server上的另一大压力是API上的WATCH能力，例如<code>kubelet</code>和<code> node-exporter</code>。 当从集群中添加或删除节点时，将触发此WATCH。 并且由于通常每个节点本身都通过<code>kube-proxy</code>监视<code>kubelet</code>服务（译者：可通过本地LB优化，并分配固定几个Master），因此这些响应所需的带宽为节点的二次方，有时甚至达到1GB &#x2F; s或更高。 在Kubernetes 1.17中的EndpointSlices特性带来巨大的优化，使此负载降低了1000倍。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS7NNt.png"></p><p>通常，我们密切关注任何随集群大小扩展的API Server请求。 我们尝试避免让任何DaemonSet与API Server进行交互。 在确实需求更改所有节点的监控组件时，引入中间缓存服务（例如Datadog Cluster Agent）似乎成了一种避免集群范围瓶颈的最佳实践。</p><p>随着集群数量的增长，我们对集群的自动伸缩操作逐步减少。 有时自动伸缩超标时，我们就会遇到麻烦。 当新节点加入集群时，就会产生许多请求，并且一次添加数百个节点可能会使API Server服务过载。</p><hr><h5 id="四、监控"><a href="#四、监控" class="headerlink" title="四、监控"></a>四、监控</h5><p>我们使用Prometheus收集指标，并使用Grafana配置图形界面，管理仪表板和警报。我们从部署<code>kube-prometheus</code>项目开始，该项目收集各种指标，并提供良好的仪表板以完成可视化。随着时间的推移，我们添加了许多自己特有的仪表板，指标和警报。</p><p>随着节点日益增多，我们发现Prometheus收集的大量指标毫无用处。尽管kube-prometheus公开了许多有用的数据，但其中有部分我们从未使用过。我们使用Prometheus接口<code>删除</code>其中的某些指标。</p><p>一段时间以来，我们一直在努力解决一个问题，即Prometheus会消耗越来越多的内存，直到最终OOM。即使在设置了超大内存容量之后，这种情况似乎仍会发生（译者：该问题应该是发生在旧版本）。更糟糕的是，当它崩溃时，启动后需要花费很多时间进行恢复。</p><p>最终，我们找到了这些OOM的来源，是Grafana和Prometheus之间的交互，其中Grafana调用Prometheus接口<code>/api/v1/series</code>查询。<code> /api/v1/series</code>接口获取所有监控指标，这将带来内存的持续增长。我们改进了Prometheus，使其在Context中包含此超时控制。</p><p>虽然Prometheus崩溃的频率降低了很多，但在确实需要重新启动它的时候，WAL恢复仍然是一个问题。在Prometheus收集新指标和为查询提供服务之前，通常需要花费很长时间来恢复所有WAL日志。在Robust Perception的帮助下，我们发现通过配置GOMAXPROCS &#x3D; 24进行优化。 Prometheus会在WAL重放期间尝试使用所有内核，而对于具有大量内核的服务器来说，抢占会削减性能。</p><hr><h5 id="五、健康检查"><a href="#五、健康检查" class="headerlink" title="五、健康检查"></a>五、健康检查</h5><p>对于规模如此大的集群，当然需要依靠自动化来检测和删除集群中行为异常的节点。 随之逐步深入，我们已经建立了一套完善的健康检查系统。</p><h5 id="a-被动检查"><a href="#a-被动检查" class="headerlink" title="a. 被动检查"></a>a. 被动检查</h5><p>（译者：可以将之称为性能监控）某些运行状况检查是被动的，始终在所有节点上运行。它们监视基本的系统资源，例如网络可达性，磁盘损坏或磁盘已满或GPU错误等。 GPU会出现多种不同的问题，但一个比较常见的错误是<code>无法纠正的ECC错误</code>。 Nvidia的数据中心GPU管理器（DCGM）工具使查询此错误和许多其他<code>Xid</code>错误变得容易了许多。我们跟踪这些错误的一种方法是通过<code>dcgm-exporter</code>将指标抓取到我们的监控系统Prometheus中。其为DCGM_FI_DEV_XID_ERRORS指标。此外，NVML设备查询API公开了有关GPU的运行状况和操作的详细信息。</p><p>一旦我们检测到错误，通常可以通过重置GPU或系统来修复它们。</p><p>健康检查的另一种形式是跟踪来自上游云提供商的维护事件。大多数云提供商都提供了一种方法来了解当前虚拟机是否由于即将发生的维护事件而导致的中断。如安装升级补丁、替换硬件等。</p><p>这些被动运行的监控运行在所有节点上。如果健康检查开始失败，该节点将自动建立报警，对于更严重的健康检查故障，我们还将尝试驱逐容器，该操组由Pod本身决定，可以通过Pod Disruption Budget进行配置，以决定是否允许这种驱逐。</p><h5 id="b-GPU动态测试"><a href="#b-GPU动态测试" class="headerlink" title="b. GPU动态测试"></a>b. GPU动态测试</h5><p>不幸的是，并非所有GPU问题都表现为通过DCGM可见的错误代码。我们已经建立了自己的测试库，这些测试库可以利用GPU来捕获其他问题，并确保硬件和驱动程序的运行情况符合预期。这些测试无法在后台运行，它们需要在几秒钟或几分钟内独占GPU。</p><p>所有节点都以<code>preflight</code>污点和标签加入集群。此污点会阻止在节点上调度常规Pod。将DaemonSet配置为在带有此标签的节点上运行预检测试Pod。成功完成测试后，测试本身将去除<code>preflight</code>污点和标签，然后该节点即可用于常规用途。</p><p>随后，我们将在节点的生命周期内定期运行这些测试。我们以CronJob方式运行，使其可以在群集中的任何可用节点上运行。</p><hr><h5 id="五、资源配额及用量"><a href="#五、资源配额及用量" class="headerlink" title="五、资源配额及用量"></a>五、资源配额及用量</h5><p>随着我们集群规模的不断扩大，然而研究人员开始发现自己难以获得分配的所有容量。 传统的调度系统具有许多不同的能力以确保团队之间公平地运行任务，而Kubernetes则没有。我们从这些调度系统中获得了灵感，并以Kubernetes原生的方式构建了一些功能。</p><p><em><strong>污点</strong></em></p><p>我们在每个集群中都有一个服务，即<code>team-resource-manager</code>，它具有多种功能。 它的数据源是ConfigMap，它为在给定集群中具有容量的所有研究团队指定元组（节点选择器，要应用的团队标签，分配数量）。 它使用openai.com&#x2F;team&#x3D;teamname:NoSchedule调整适当数量的节点。</p><p><code>team-resource-manager</code>还配置一个<code>admission webhook</code>(译者：即准入服务插件)服务，以便在提交每个作业时，根据提交者的团队成员身份应用相应的容忍度。 通过使用污点，我们可以灵活地约束Kubernetes Pod Scheduler，例如允许对优先级较低的Pod允许<code>任意</code>容忍，这允许团队在无需强力协调的情况下资源共享。</p><p><em><strong>CPU &amp; GPU balloons</strong></em></p><p>除了使用cluster-autoscaler动态扩展虚拟机集群外，我们还使用它来管理（删除和重新添加）集群中不正常的节点。为此，我们将激情的<code>最小</code>设置为零，并将集群的<code>最大</code>设置为可用容量。但是，如果cluster-autoscaler看到空闲节点，则将尝试缩小到仅所需的容量。由于多种原因（VM启动延迟，预分配的成本，上述API Server的影响），这种空闲扩展并不理想。</p><p>因此，我们为CPU和GPU主机引入了balloons Deployment。该Deployment包含一个具有<code>最大值</code>数量的低优先级容器配置。这些Pod占用了节点内的资源，因此自cluster-autoscaler不会将其视为空闲。但是，由于它们的优先级较低，因此调度程序可以立即将其逐出，以便为实际工作腾出空间。 （我们选择使用Deployment而不是DaemonSet，以避免将DaemonSet视为节点上的空闲工作负载。）</p><p>需要注意的一件事是，我们使用容器抗亲和力来确保容器在节点上均匀分布。自Kubernetes 1.18起已更正了该算法的性能问题。</p><hr><h5 id="六、成组调度-Gang-scheduling"><a href="#六、成组调度-Gang-scheduling" class="headerlink" title="六、成组调度(Gang scheduling)"></a>六、成组调度(Gang scheduling)</h5><p>我们的实验通常涉及一个或多个StatefulSet，每个StatefulSet都在训练工作的不同部分进行。对于优化器，研究人员需要在进行任何训练之前调度完StatefulSet的所有pod（因为我们经常在优化器成员之间使用MPI进行协作，并且MPI对组成员身份更改很敏感）。</p><p>但是，默认情况下，Kubernetes并不一定要优先执行一个StatefulSet的请求。例如，如果两个实验作业各自请求集群容量的100％，但Kubernetes可能只调度每个实验Pod的一半，从而导致调度僵局，这两个实验作业都无法完成。</p><p>我们尝试了实现自定义调度程序，但是遇到了一些极端情况，这些情况导致与常规Pod的调度方式发生冲突。 Kubernetes 1.18引入了Kubernetes framwork plugin架构，这使得在本地添加此类功能变得更加容易。我们最近引入Coscheduling插件解决此问题。</p><hr><h5 id="七、结论"><a href="#七、结论" class="headerlink" title="七、结论"></a>七、结论</h5><p>在扩展Kubernetes集群时，仍有许多问题需要解决。 其中一些包括：</p><p>a. 监控指标</p><p>就我们的规模而言，Prometheus的内置TSDB存储引擎的压缩速度很慢，并且每次重新启动时都需要花费很长的时间来恢复WAL（Write-Ahead-Log），这给我们带来了很大的麻烦。 我们正在迁移到其他与Prometheus兼容的存储和查询引擎。 期待将来有关它如何发展的博客文章！</p><p>b. Pod网络流量整形</p><p>当我们扩展群集时，每个Pod都会被计算为具有一定数量的Internet带宽，那么所有Pod总体流量将非常惊人，因而需要引入流量整形技术，防止网络风暴、流量泛滥等问题。</p><p>我们发现Kubernetes是满足我们研究需求的异常灵活的平台。 它具有扩展能力，可以满足我们要求的最苛刻的工作负载。 尽管还有很多地方需要改进，但OpenAI的超级计算团队将继续探索Kubernetes如何扩展。 </p><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li>[scaling-kubernetes-to-7500-nodes](<a href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/%EF%BC%89">https://openai.com/blog/scaling-kubernetes-to-7500-nodes/）</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;OpenAI已经将Kubernetes集群规模扩展至7500个节点，为大型神经网络模型（如GPT-3，CLIP和DALL·E）及小型实验性研究提供了可扩展的基础架构。 很少将单个Kubernetes集群扩展到如此规模，为此进行了一些必要的改进，但好处是单一的基础架构使我们的</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes废弃PodSecurityPolicy后续</title>
    <link href="https://zoues.com/posts/aa6cf752/"/>
    <id>https://zoues.com/posts/aa6cf752/</id>
    <published>2021-01-21T00:40:08.000Z</published>
    <updated>2024-02-03T06:32:45.373Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes社区将在1.21版本中弃用PSP，并将1.25版本中移除该API。目前CNCF生态圈类似项目：Kyverno与Open Policy Agen(OPA).</p><p>PodSecurityPolicy是集群级别的Pod安全策略，其对Pod的操作进行细粒度的授权。在Kubernetes架构中以Admission Controller（准入控制，类似NamespaceLifecycle、ResourceQuota等），通俗来讲就是一种写入前检查插件</p><hr><h4 id="一、PSP困境"><a href="#一、PSP困境" class="headerlink" title="一、PSP困境"></a>一、PSP困境</h4><p>当前PodSecurityPolicy特性存在以下问题：</p><ol><li>授权模型存在缺陷</li><li>功能易开难关</li><li>API接口缺乏一致性及扩展性,如MustRunAsNonRoot、AllowPrivilegeEscalation此类配置</li><li>无法处理动态注入的side-car（如knative）</li><li>在CI&#x2F;CD场景难以落地</li></ol><hr><h4 id="二、备选方案"><a href="#二、备选方案" class="headerlink" title="二、备选方案"></a>二、备选方案</h4><h5 id="Kyverno简介"><a href="#Kyverno简介" class="headerlink" title="Kyverno简介"></a>Kyverno简介</h5><p>Kyverno是为Kubernetes设计的策略引擎（CNCF sandbox项目）。其具备以下功能：</p><ul><li><p>相关策略类似Kubernetes对象，上手容易</p></li><li><p>配置管理便利</p></li><li><p>为Kubernetes资源的策略进行声明式验证，更改和生成资源配置。</p></li><li><p>在Kubernetes集群中作为动态准入控制器运行。</p></li><li><p>可以使用资源种类，名称和标签选择器来匹配资源。名称中支持通配符等</p></li></ul><p>当前采纳该方案的开源项目：fluxcd v2等</p><h5 id="OPA简介"><a href="#OPA简介" class="headerlink" title="OPA简介"></a>OPA简介</h5><p>Open Policy Agent（即OPA, CNCF孵化项目）, 为策略决策需求提供了一个统一的框架。它将策略决策从软件业务逻辑中解耦剥离，将策略定义、决策过程抽象为通用模型，实现了一个通用策略引擎，</p><p>其可用于微服务、Kubernetes、 CI&#x2F;CD、API网关等应用场景。</p><p>OPA可以通过sidecar、外部服务或是依赖库的方式与已有的软件系统进行集成。OPA 可以接受任何类型的结构化数据，决策流程如下图所示：</p><p><img src="https://s3.ax1x.com/2021/01/20/sRWx7d.png"></p><p>OPA通过数据输入和策略来进行决策，决策过程和数据无关。例如：</p><ul><li>判断某用户可以访问哪些资源</li><li>允许哪些子网对外访问</li><li>工作负载可以部署在哪个集群</li><li>可以使用哪些镜像</li><li>容器可以使用哪些系统功能</li><li>什么时间可以访问等</li></ul><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li>[deprecate PSP](<a href="https://github.com/kubernetes/kubernetes/pull/9717%EF%BC%89">https://github.com/kubernetes/kubernetes/pull/9717）</a></li><li><a href="https://docs.google.com/document/d/1VKqjUlpU888OYtIrBwidL43FOLhbmOD5tesYwmjzO4E/edit#">PodSecurityPolicy Options</a></li><li><a href="https://servicesblog.redhat.com/2019/10/16/open-policy-agent-part-i-the-introduction/">redhat关于OPA系列</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Kubernetes社区将在1.21版本中弃用PSP，并将1.25版本中移除该API。目前CNCF生态圈类似项目：Kyverno与Open Policy Agen(OPA).&lt;/p&gt;
&lt;p&gt;PodSecurityPolicy是集群级别的Pod安全策略，其对Pod的操作进行细</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
</feed>
