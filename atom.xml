<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zouyee</title>
  
  <subtitle>life is short, enjoy it</subtitle>
  <link href="https://zoues.com/atom.xml" rel="self"/>
  
  <link href="https://zoues.com/"/>
  <updated>2024-03-02T13:36:42.964Z</updated>
  <id>https://zoues.com/</id>
  
  <author>
    <name>zouyee</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>解读Kubernetes常见错误码</title>
    <link href="https://zoues.com/posts/b421b57/"/>
    <id>https://zoues.com/posts/b421b57/</id>
    <published>2024-03-02T12:23:48.000Z</published>
    <updated>2024-03-02T13:36:42.964Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我们将深入分析Kubernetes中的退出码127与137，解释它们是什么，K8s和Docker中常见的原因是什么，以及如何修复它们！</p><p>我们将涵盖以下内容：</p><ol><li><p>退出码的历史 </p></li><li><p>退出码 127</p></li><li><p>退出码 137</p></li></ol><h2 id="退出码的历史"><a href="#退出码的历史" class="headerlink" title="退出码的历史"></a>退出码的历史</h2><p>进程退出码的历史可以追溯到Unix操作系统的早期。在Unix系统中，进程退出码是进程终止时向其父进程传递的一个整数值，用于表示进程的终止状态。这个整数值通常在0到255之间，其中0表示进程成功终止，其他值通常用来表示不同的错误或异常情况。</p><p>进程退出码最初被设计用于提供一种简单的机制，使父进程能够了解子进程的执行结果。这使得父进程能够根据子进程的退出码来采取适当的行动，比如处理错误情况或继续执行其他操作。</p><p>在Unix系统中，特定的退出码值通常具有特定的含义，例如：</p><ul><li>0：表示成功执行，没有错误。</li><li>1：通常表示通用的错误。</li><li>2：表示命令的语法错误。</li><li>127：表示命令未找到。</li></ul><p>随着时间的推移，Unix操作系统的发展和不同的实现，进程退出码的含义可能有所不同，但基本的概念保持不变。</p><p>在Linux系统中，进程退出码的使用与Unix系统类似。Linux继承了Unix的进程管理机制，并在其基础上进行了扩展和改进。因此，Linux中的进程退出码仍然是一个重要的概念，用于帮助理解和诊断进程的执行状态。</p><p>进程退出码的历史可以追溯到早期的Unix系统，是Unix和Linux操作系统中的一个重要概念，为进程间通信提供了一种简单而有效的机制。当应用程序或命令因致命错误而终止或执行失败时，将产生 128 系列退出码（<code>128+n</code>），其中 <code>n</code> 为信号编号。<code>n</code> 包括所有类型的终止代码，如 <code>SIGTERM</code>、<code>SIGKILL</code> 等。</p><h2 id="退出码127"><a href="#退出码127" class="headerlink" title="退出码127"></a>退出码127</h2><p>退出码 127 不是特定于 Kubernetes 的错误代码，而是 Linux 和类 Unix 操作系统中使用的标准退出码。当然，我们在Kubernetes中经常看到它，并且通常表示容器内执行的命令或二进制文件找不到。</p><p>一些标准的退出码包括：</p><table><thead><tr><th>退出码</th><th>解释</th></tr></thead><tbody><tr><td>0</td><td>命令成功执行</td></tr><tr><td>1</td><td>通用错误</td></tr><tr><td>2</td><td>命令（参数）使用不当</td></tr><tr><td>126</td><td>权限被拒绝、无法执行</td></tr><tr><td>127</td><td>未找到命令行、PATH错误</td></tr><tr><td>128+n</td><td>命令被信号从外部终止、遇到致命错误</td></tr><tr><td>&gt;255</td><td>退出码超过255范围的，会重新计算（mod 256）</td></tr></tbody></table><p>让我们看一下退出码 127 的一些常见原因：</p><ol><li><p>命令或二进制文件未安装 </p><p>Kubernetes 容器的 command 字段中指定的可执行文件未安装在容器的文件系统中。需要确保所需的二进制文件或命令可用。</p></li><li><p>路径或命令不正确</p><p>Pod 定义中指定的命令不正确或在指定的路径中不存在。这是错误的最常见原因之一，通常是由于 Dockerfile 或 pod  spec中的entrypoint或command输入不正确造成的。</p></li><li><p>缺少依赖</p><p>在容器内运行的应用程序或脚本未安装相关依赖。需要确保所有必需的依赖项包含在容器映像中。</p></li><li><p>shell 解释器</p><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p></li><li><p>shell 脚本语法错误</p><p>如果 shell 脚本退出码是127，请检查脚本是否存有语法错误或可能阻止其执行的问题。</p></li><li><p>权限不足</p><p>在容器内运行命令的用户可能没有执行指定命令所需的必要权限。确保容器以适当的特权运行。</p></li><li><p>镜像兼容性问题</p><p>确保使用的容器镜像与宿主机架构和操作系统兼容。不匹配的映像可能导致命令找不到，比如x86的镜像运行在arm的机器上</p></li><li><p>卷挂载</p><p>如果命令是卷挂载的文件，请检查卷挂载是否配置正确，且所需的文件可以被访问到。</p></li><li><p>环境变量</p><p>一些命令可能依赖于特定的环境变量。确保必需的环境变量设置正确。</p></li><li><p>Kubernetes RBAC 策略</p><p> 如果启用了RBAC，需要确保具有执行指定命令所需的权限。</p></li></ol><h3 id="如何排查"><a href="#如何排查" class="headerlink" title="如何排查"></a>如何排查</h3><p>要排除问题，可以使用以下命令检查 Pod 的日志：</p><p><code>kubectl logs -f &lt;pod-name&gt; </code></p><p>还可以检查 Pod 状态，该状态提供有关 Pod 的详细信息，包括其当前状态、最近事件和任何错误消息。</p><p><code>kubectl describe pod &lt;pod-name&gt; </code></p><p>还可以为把调试容器attach到Pod 中，该容器包括一个 shell（例如 BusyBox）。这允许您进入容器并手动检查环境、路径和命令的可用性。</p><p>使用 BusyBox 进行调试的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/bin/sleep&quot;, &quot;infinity&quot;]</span><br><span class="line">  - name: debug-container</span><br><span class="line">    image: busybox:latest</span><br><span class="line">    command: [&quot;/bin/sh&quot;]</span><br><span class="line">    tty: true</span><br><span class="line">    stdin: true</span><br></pre></td></tr></table></figure><p>如果是高版本K8s，也可以使用Ephemeral Containers，它就是一个临时容器。这是一个自Kubernetes v1.16中作为alpha引入的新功能，启用临时容器的特性也非常简单，在kubernetes v1.16之后的版本中将启动参数<code>--feature-gates=EphemeralContainers=true</code>配置到kube-api和kubelet服务上重启即可。</p><p>通过仔细查看日志并排查上述几个方向，应该能够确定退出码 127 问题的原因。</p><h3 id="如何修复"><a href="#如何修复" class="headerlink" title="如何修复"></a>如何修复</h3><p> 我们知道了退出码 127 的常见原因以及排查方式，现在让我们看看如何修复它们。</p><ol><li>命令或二进制文件未安装</li></ol><p>如果所需的命令或二进制文件丢失，则可能需要在容器镜像中安装。修改 Dockerfile 或构建过程安装所需软件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add &lt;package-name&gt; </span><br></pre></td></tr></table></figure><ol start="2"><li>路径或命令不正确</li></ol><p>在 Pod 定义中指定命令时，考虑使用二进制文件的绝对路径。这有助于确保不受当前工作目录的影响， runtime可以找到二进制文件。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    command: [&quot;/usr/local/bin/my-command&quot;]</span><br></pre></td></tr></table></figure><ol start="3"><li>缺少依赖项</li></ol><p>导致命令无法运行的原因可能是容器镜像需要安装额外的软件。如果命令需要额外的设置或安装步骤，可以使用init容器在主容器启动之前执行这些任务。</p><p>示例（使用init容器安装软件包）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">initContainers:</span><br><span class="line">  - name: install-package</span><br><span class="line">    image: alpine:latest</span><br><span class="line">    command: [&quot;apk&quot;, &quot;--no-cache&quot;, &quot;add&quot;, &quot;&lt;package-name&gt;&quot;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: shared-data</span><br><span class="line">      mountPath: /data</span><br></pre></td></tr></table></figure><ol start="4"><li>shell解释器</li></ol><p>如果指定了脚本作为命令，需要确保脚本有效 （例如#!&#x2F;bin&#x2F;bash），且在容器中可用。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br></pre></td></tr></table></figure><ol start="5"><li>卷挂载</li></ol><p>检查Pod的配置，确保卷已正确挂载。验证卷名称、挂载路径和 subPaths是否正确。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">containers:</span><br><span class="line">  - name: my-container</span><br><span class="line">    image: my-image:latest</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: my-volume</span><br><span class="line">      mountPath: /path/in/container</span><br></pre></td></tr></table></figure><p>同时我们需要确认Pod 定义指定的卷存在且可用。如果是持久卷（PV），需要检查其状态。如果是 emptyDir 或其他类型的卷，需要验证其是否正确创建和挂载。如果在卷挂载中使用了 subPaths，需要确保源目录或文件中存在指定的 subPaths。</p><p>示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumeMounts:</span><br><span class="line">  - name: my-volume</span><br><span class="line">    mountPath: /path/in/container</span><br><span class="line">    subPath: my-file.txt</span><br></pre></td></tr></table></figure><h2 id="退出码137"><a href="#退出码137" class="headerlink" title="退出码137"></a>退出码137</h2><p>在Kubernetes中，137退出码表示进程被强制终止。在Unix和Linux系统中，当进程由于信号而终止时，退出码由信号编号加上128确定。信号编号为9，意味着“SIGKILL”，因此将9加上128，得到137退出码。</p><p>当Kubernetes集群中容器超出其内存限制时，它可能会被Kubernetes系统终止，并显示“OOMKilled”错误，这表示进程因内存不足而被终止。此错误的退出码为137OOM代表“内存耗尽（out-of-memory）”。</p><p>如果Pod状态将显示为“OOMKilled”，你可以使用以下命令查看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br></pre></td></tr></table></figure><h3 id="OOMKiller"><a href="#OOMKiller" class="headerlink" title="OOMKiller"></a>OOMKiller</h3><p>OOMKiller是Linux内核中的一种机制，它负责通过终止消耗过多内存的进程来防止系统耗尽内存。当系统内存耗尽时，内核会调用OOMKiller来选择一个要终止的进程，以释放内存并保持系统运行。</p><p>内核中有两种不同的OOM Killer；一种是全局的OOM Killer，另一种是基于cgroup内存控制器的OOM Killer，可以是cgroup v1或cgroup v2。</p><p>简单来说是，当内核在分配物理内存页面时遇到问题时，全局的OOM Killer 会触发。当内核尝试分配内存页面（无论是用于内核使用还是用于需要页面的进程），并且最初失败时，它将尝试各种方式来回收和整理内存。如果这种尝试成功或者至少取得了一些进展，内核将继续重试分配（从代码中我可以看到）；如果无法释放页面或者取得进展，它将在许多情况下触发OOM Killer。</p><p>一旦OOMKiller选择要终止的进程，它会向该进程发送信号，要求其优雅地终止。如果进程不响应信号，则内核会强制终止该进程并释放其内存。</p><p>注意：由于内存问题而被终止的Pod不一定会被节点驱逐，如果其设置的重启策略设置为“Always”，它将尝试重新启动Pod。</p><p>在系统层面，Linux内核为运行在主机上的每个进程维护一个oom_score。进程被终止的机率取决于分数有多高。</p><p>oom_score_adj值允许用户自定义OOM进程，并定义何时应终止进程。Kubernetes在定义Pod的Quality of Service（QoS）时使用oom_score_adj值。</p><p>K8s针对Pod定义了三种QoS，每个类型具有对应的oom_score_adj值：</p><ul><li>Guaranteed: -997</li><li>BestEffort: 1000</li><li>Burstable: <em>min(max(2, 1000 — (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999)</em></li></ul><p>其中Pod为Guaranteed QoS，则其oom_score_adj的值是-997，因此它们在节点内存不足时最后一个被终止。BestEffort Pod配置的是1000，所以它们第一个被被终止。</p><p>要查看Pod的QoS，可以通过下述命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o jsonpath=&#x27;&#123;.status.qosClass&#125;&#x27;</span><br></pre></td></tr></table></figure><p>下面是定义Pod<code>Guaranteed</code> QoS 类型的计算策略：</p><ul><li>Pod 中的每个容器必须有内存 limit 和内存 request。</li><li>对于 Pod 中的每个容器，内存 limit 必须等于内存 request。</li><li>Pod 中的每个容器必须有 CPU limit 和 CPU request。</li><li>对于 Pod 中的每个容器，CPU limit 必须等于 CPU request。</li></ul><p>退出码137通常有两种情况：</p><ul><li><p>首先，也是最常见的原因是与资源限制相关。在这种情况下，通常情况下，Kubernetes超出了容器的分配内存限制，当发生这种情况时，它将终止容器以确保节点的稳定性。 </p></li><li><p>另一种情况是手动干预 - 用户或脚本可能会向容器进程发送“SIGKILL”信号，导致此退出码。 OOMKilled（退出码137）</p></li></ul><h3 id="如何排查-1"><a href="#如何排查-1" class="headerlink" title="如何排查"></a>如何排查</h3><ol><li>检查Pod日志</li></ol><p>诊断OOMKilled错误的第一步是检查Pod日志，查看是否有任何指示内存问题的错误消息。描述命令的事件部分将提供进一步的确认以及发生错误的时间&#x2F;日期。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;podname&gt;</span><br><span class="line">State:          Running</span><br><span class="line">       Started:      Fri, 12 May 2023 11:14:13 +0200</span><br><span class="line">       Last State:   Terminated</span><br><span class="line">       Reason:       OOMKilled</span><br><span class="line">       Exit Code:    137</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure><p>您还可以查询Pod日志：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/pods/&lt;podname&gt;</span><br></pre></td></tr></table></figure><p>当然也可以通过(标准输出)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f &lt;podname&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>监视内存使用情况</li></ol><p>使用监视系统（如Prometheus或Grafana）监视Pod和容器中的内存使用情况。这可以帮助我们确定哪些容器消耗了过多的内存从而触发了OOMKilled错误，也可以在容器宿主机使用dmesg查看当时oomkiller的现场</p><ol start="3"><li>使用内存分析器</li></ol><p>使用内存分析器（如pprof）来识别可能导致过多内存使用的内存泄漏或低效代码。</p><h3 id="如何修复-1"><a href="#如何修复-1" class="headerlink" title="如何修复"></a>如何修复</h3><p>以下是OOMKilled Kubernetes错误的常见原因及其解决方法。</p><ol><li>容器内存限制已达到</li></ol><p>这可能是由于在容器指定的内存限制值设置不当导致的。解决方法是增加内存限制的值，或者调查导致负载增加的根本原因并进行纠正。导致这种情况的常见原因包括大文件上传，因为上传大文件可能会消耗大量内存资源，特别是当多个容器在一个Pod内运行时，以及突然增加的流量量。</p><ol start="2"><li>因为应用程序内存泄漏,容器内存使用达到上限</li></ol><p>需要调试应用程序来定位内存泄漏的原因，</p><ol start="3"><li>所有Pod使用的总内存大于节点可用内存</li></ol><p>通过增加节点可用内存来增加节点内存，或者将Pod迁移到内存更多的节点。当然也可以调整运行在节点上的Pod的内存限制，使其符合内存限制，注意你还应该注意内存请求设置，它指定了Pod应该使用的最小内存量。如果设置得太高，可能不是有效利用可用内存，关于资源配置相关的建议，可以参看VPA组件</p><p>在调整内存请求和限制时，当节点过载时，Kubernetes按照以下优先级顺序终止Pod：</p><ul><li><p>没有请求或限制的Pod。 </p></li><li><p>具有请求但没有限制的Pod。</p></li><li><p>使用超过其内存请求值的内存 - 指定的最小内存值 - 但低于其内存限制的Pod。 </p></li><li><p>使用超过其内存限制的Pod。</p></li></ul><h3 id="如何预防"><a href="#如何预防" class="headerlink" title="如何预防"></a>如何预防</h3><p>有几种方法可以防止OOMKilled的发生：</p><ol><li>设置适当的内存限制</li></ol><p>通过压测及监控来确定应用程序的内存使用，通过上述方式配置容器允许使用的最大内存量。过度保守可能会导致因资源利用率低效而造成资金的浪费，同时低估会导致频繁出现OOMKilled现象。</p><ol start="2"><li>HPA</li></ol><p>最佳做法是利用K8s提供的HPA机制，当应用程序的内存使用升高时自动增加Pod副本数量。</p><ol start="3"><li>节点资源分配</li></ol><p>确保节点具有足够的资源来处理业务。</p><ol start="4"><li>优化应用程序内存使用</li></ol><p>监视应用程序并进行适当优化，以减少内存消耗。</p><ol start="5"><li>避免应用程序中的内存泄漏</li></ol><p>从应用程序来看，需要长期检查并修复内存泄漏。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><p><a href="https://spacelift.io/blog/oomkilled-exit-code-137">https://spacelift.io/blog/oomkilled-exit-code-137</a></p></li><li><p><a href="https://spacelift.io/blog/exit-code-127">https://spacelift.io/blog/exit-code-127</a></p></li><li><p><a href="https://cloud.tencent.com/developer/news/1152344">https://cloud.tencent.com/developer/news/1152344</a></p></li><li><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen">https://utcc.utoronto.ca/~cks/space/blog/linux/OOMKillerWhen</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在这篇文章中，我们将深入分析Kubernetes中的退出码127与137，解释它们是什么，K8s和Docker中常见的原因是什么，以及如何修复它们！&lt;/p&gt;
&lt;p&gt;我们将涵盖以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;退出码的历史 &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;退出码</summary>
      
    
    
    
    <category term="云原生面试案例50篇" scheme="https://zoues.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F%E9%9D%A2%E8%AF%95%E6%A1%88%E4%BE%8B50%E7%AF%87/"/>
    
    
  </entry>
  
  <entry>
    <title>Exploring the Root Cause of Kubernetes 1.28 Scheduler OOM</title>
    <link href="https://zoues.com/posts/e4e37b07/"/>
    <id>https://zoues.com/posts/e4e37b07/</id>
    <published>2024-03-01T09:13:10.000Z</published>
    <updated>2024-03-01T09:17:38.524Z</updated>
    
    <content type="html"><![CDATA[<p>Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a look together. In the context of fluctuating business tidal changes affecting both pods and nodes in the cluster, memory usage shows a continuous upward trend until reaching Out Of Memory (OOM) conditions.</p><h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><blockquote><p>The following data is all publicly available information from the community.</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>The triggering scenarios include the following two types (there are also other reproduction methods in the community):</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=60</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>Our findings in the community revealed multiple instances of similar memory anomaly scenarios, with various methods of reproduction. The conclusion regarding the above issue is as follows:</p><blockquote><p>The Kubernetes community defaulted to enabling the scheduling feature SchedulerQueueingHints in version 1.28, which led to memory anomalies in the scheduler component. To temporarily address memory-related issues, the community adjusted this feature to default to disabled in version 1.28.5. However, as the problem has not been completely resolved, it is advisable to exercise caution when enabling this feature.</p></blockquote><h2 id="Technical-Background"><a href="#Technical-Background" class="headerlink" title="Technical Background"></a>Technical Background</h2><p>This section introduces the following content:</p><ul><li>Introduction to Kubernetes scheduler related data structures</li><li>Introduction to Kubernetes scheduler QueueingHint</li><li>Doubly linked lists in Golang</li></ul><h3 id="K8s-Scheduler-Introduction"><a href="#K8s-Scheduler-Introduction" class="headerlink" title="K8s-Scheduler Introduction"></a>K8s-Scheduler Introduction</h3><p>The PriorityQueue is an interface implementation of the SchedulingQueue. It holds the highest priority pod ready for scheduling at its head. PriorityQueue contains the following important fields:</p><ol><li>activeQ: This queue holds pods ready for scheduling. Newly added pods are placed into this queue. When the scheduling queue needs to perform scheduling, it fetches pods from this queue. activeQ is implemented using a heap.</li><li>backoffQ: This queue holds pods that have been determined to be unschedulable for various reasons (such as not meeting node requirements). These pods will be moved to activeQ to attempt scheduling again after a certain backoff period. backoffQ is also implemented using a heap.</li><li>unschedulablePods: This map data structure holds pods that cannot be scheduled for various reasons. Instead of directly placing them in backoffQ, they are recorded here. When conditions are met, they will be moved to activeQ or backoffQ. The scheduling queue periodically cleans up pods in unschedulablePods.</li><li>inFlightEvents: This is used to store events received by the scheduling queue (with the entry value as clusterEvent) and pods that are currently being processed (with the entry value as *v1.Pod). It’s based on a doubly linked list implemented in Go.</li><li>inFlightPods: This holds the UIDs of all pods that have been popped but have not yet had Done called on them. In other words, it keeps track of all pods that are currently being processed (i.e., in scheduling, in admit, or in binding phases).</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p> For a comprehensive introduction to Kubernetes, please refer to <a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">An In-depth Introduction to Kubernetes Scheduling: Framework</a>. Updates on the latest Kubernetes scheduler will be provided subsequently.</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>Kubernetes scheduler introduced the <code>QueueingHint</code> feature to provide recommendations for re-queueing pods from each plugin. This aims to reduce unnecessary scheduling retries, thereby enhancing scheduling throughput. Additionally, it skips backoff under appropriate circumstances to further improve pod scheduling efficiency.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Currently, each plugin can define when to retry scheduling pods that have been rejected by the plugin through the <code>EventsToRegister</code> mechanism.</p><p>For example, the <code>NodeAffinity</code> plugin may retry scheduling pods when nodes are added or updated because newly added or updated nodes may have labels that match the node affinity on the pod. However, in practice, a large number of node update events occur in the cluster, which does not guarantee successful scheduling of pods previously rejected by <code>NodeAffinity</code>.</p><p>To address this issue, the scheduler introduced more refined callback functions to filter out irrelevant events, thereby only retrying pods that are likely to be successfully scheduled in the next scheduling cycle.</p><p>Furthermore, the Dynamic Resource Allocation (DRA) scheduling plugin sometimes needs to reject pods to wait for status updates from device drivers. Therefore, certain pods may require several scheduling cycles to be scheduled. For this scenario, the wait time for fallback is longer compared to waiting for device driver status updates. Hence, there is a need to allow plugins to skip fallback in specific cases to improve scheduling performance.</p><h3 id="Implementation-Goals"><a href="#Implementation-Goals" class="headerlink" title="Implementation Goals"></a>Implementation Goals</h3><p>To improve scheduling throughput, the community proposed the following enhancements:</p><ol><li>Introduction of QueueingHint<ul><li>Introducing <code>QueueingHint</code> into the <code>EventsToRegister</code> mechanism, allowing plugins to provide recommendations for re-queuing pods.</li></ul></li><li>Enhancement of Pod Tracking and Re-queueing Mechanism<ul><li>Optimizing the implementation for tracking pods currently being processed in the scheduling queue.</li><li>Implementing a mechanism to re-queue rejected pods to appropriate queues.</li><li>Optimizing the backoff strategy for rejected pods, allowing plugins to skip backoff in specific cases to improve scheduling throughput.</li></ul></li></ol><h3 id="Potential-Risks"><a href="#Potential-Risks" class="headerlink" title="Potential Risks"></a>Potential Risks</h3><blockquote><h4 id="1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods"><a href="#1-Errors-in-Implementation-Leading-to-Pods-Being-Unschedulable-in-unschedulablePods-for-Extended-Periods" class="headerlink" title="1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods"></a>1. Errors in Implementation Leading to Pods Being Unschedulable in unschedulablePods for Extended Periods</h4></blockquote><p>If a plugin is configured with QueueingHint but misses some events that could make pods schedulable, pods rejected by that plugin may remain stuck in unschedulablePods for a long time.</p><p>Although the scheduling queue periodically cleans up pods in unschedulablePods (default is 5 minutes, configurable).</p><blockquote><h4 id="2-Increase-in-Memory-Usage"><a href="#2-Increase-in-Memory-Usage" class="headerlink" title="2. Increase in Memory Usage"></a>2. Increase in Memory Usage</h4></blockquote><p>As the scheduling queue needs to retain events occurring during scheduling, the memory usage of kube-scheduler will increase. Therefore, the busier the cluster, the more memory it may require.</p><p>Although it’s not possible to completely eliminate memory growth, releasing cached events as soon as possible can slow down the rate of memory growth.</p><blockquote><h4 id="3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension"><a href="#3-Significant-Changes-in-EventsToRegister-in-EnqueueExtension" class="headerlink" title="3. Significant Changes in EventsToRegister in EnqueueExtension"></a>3. Significant Changes in <code>EventsToRegister</code> in <code>EnqueueExtension</code></h4></blockquote><p>Developers of custom scheduler plugins need to perform compatibility upgrades, as the return type of <code>EventsToRegister</code> in <code>EnqueueExtension</code> has changed from <code>ClusterEvent</code> to <code>ClusterEventWithHint</code>. <code>ClusterEventWithHint</code> allows each plugin to filter out more useless events through a callback function called <code>QueueingHintFn</code>.</p><p>To simplify the migration work, an empty <code>QueueingHintFn</code> is considered to always return <code>Queue</code>. Thus, if they only want to maintain the existing behavior, they only need to change <code>ClusterEvent</code> to <code>ClusterEventWithHint</code> without registering any <code>QueueingHintFn</code>.</p><h3 id="Design-of-QueueingHints"><a href="#Design-of-QueueingHints" class="headerlink" title="Design of QueueingHints"></a>Design of QueueingHints</h3><p>The return type of the <code>EventsToRegister</code> method has been changed to <code>[]ClusterEventWithHint</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Each <code>ClusterEventWithHint</code> structure consists of a <code>ClusterEvent</code> and a <code>QueueingHintFn</code>. When an event occurs, the <code>QueueingHintFn</code> is executed to determine whether the event can make the pod eligible for scheduling.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>The type <code>QueueingHintFn</code> is a function with a return type of <code>(QueueingHint, error)</code>. Here, <code>QueueingHint</code> is an enumeration type with possible values of <code>QueueSkip</code> and <code>Queue</code>. The invocation of <code>QueueingHintFn</code> occurs before moving pods from <code>unschedulableQ</code> to <code>backoffQ</code> or <code>activeQ</code>. If an error is returned, the <code>QueueingHint</code> provided by the caller will be treated as <code>QueueAfterBackoff</code>, which ensures that the pod does not remain indefinitely in the <code>unschedulableQ</code> queue, regardless of the returned result.</p><h3 id="When-to-Skip-Not-Skip-Backoff"><a href="#When-to-Skip-Not-Skip-Backoff" class="headerlink" title="When to Skip&#x2F;Not Skip Backoff"></a>When to Skip&#x2F;Not Skip Backoff</h3><p>The <code>backoffQ</code> prevents pods that are “long-term unschedulable” from blocking the queue, maintaining a lightweight queue with high throughput.</p><p>The longer a pod is rejected during the scheduling cycle, the longer it waits in the <code>backoffQ</code>.</p><p>For example, when <code>NodeAffinity</code> rejects a pod, and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod needs to wait for backoff before retrying scheduling.</p><p>However, certain plugins are designed to experience some failures during the scheduling cycle itself. For instance, the built-in DRA (Dynamic Resource Allocation) plugin, at the Reserve extension, informs the resource driver of the scheduling result and rejects the pod once to wait for a response from the resource driver. For such rejection scenarios, it should not be considered wasted scheduling cycles. Although a specific scheduling cycle fails, the scheduling result based on that cycle can facilitate pod scheduling. Therefore, pods rejected for this reason do not need to be penalized (backoff).</p><p>To support this scenario, we introduce a new state, <code>Pending</code>. When the DRA plugin rejects a pod using <code>Pending</code> and later returns <code>Queue</code> in its <code>QueueingHintFn</code>, the pod skips backoff and is rescheduled.</p><h3 id="How-QueueingHint-Works"><a href="#How-QueueingHint-Works" class="headerlink" title="How QueueingHint Works"></a>How QueueingHint Works</h3><p>When Kubernetes cluster events occur, the scheduling queue executes the <code>QueueingHintFn</code> of those plugins that rejected pods in the previous scheduling cycle.</p><p>The following scenarios describe how they are executed and how pods are moved:</p><blockquote><p>Pod Rejected by One or More Plugins</p></blockquote><p>Suppose there are three nodes. When a pod enters the scheduling cycle, one node rejects the pod due to insufficient resources, and the other two nodes reject it due to mismatching node affinity.</p><p>In this scenario, the pod is rejected by the <code>NodeResourceFit</code> and <code>NodeAffinity</code> plugins and is eventually placed in <code>unschedulableQ</code>.</p><p>Subsequently, whenever cluster events registered in these plugins occur, the scheduling queue notifies them via <code>QueueingHint</code>. If any <code>QueueingHintFn</code> from <code>NodeResourceFit</code> or <code>NodeAffinity</code> returns <code>Queue</code>, the pod is moved to <code>activeQ</code> or <code>backoffQ</code>. (For example, when a <code>NodeAdded</code> event occurs, <code>QueueingHint</code> from <code>NodeResourceFit</code> returns <code>Queue</code> because the pod may be schedulable to the new node.)</p><p>Whether it moves to <code>activeQ</code> or <code>backoffQ</code> depends on how long the pod has been in <code>unschedulableQ</code>. If the time spent in <code>unschedulableQ</code> exceeds the expected pod backoff delay time, it is moved directly to <code>activeQ</code>. Otherwise, it is moved to <code>backoffQ</code>.</p><blockquote><p>Pod Rejected due to Pending State</p></blockquote><p>When the DRA plugin returns <code>Pending</code> for a pod during the Reserve extension phase, the scheduling queue adds the DRA plugin to the pod’s <code>pendingPlugins</code> dictionary and returns the pod.</p><p>When a call to the <code>QueueingHint</code> of the DRA plugin returns <code>Queue</code> in subsequent invocations, the scheduling queue places the pod directly into <code>activeQ</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Tracking-Pods-Being-Processed-in-the-Scheduling-Queue"><a href="#Tracking-Pods-Being-Processed-in-the-Scheduling-Queue" class="headerlink" title="Tracking Pods Being Processed in the Scheduling Queue"></a>Tracking Pods Being Processed in the Scheduling Queue</h4><p>By introducing <code>QueueingHint</code>, we can only retry scheduling when specific events occur. But what if these events happen during pod scheduling?</p><p>The scheduler takes a snapshot of the cluster data and schedules pods based on the snapshot. The snapshot is updated each time a scheduling cycle is initiated, meaning the same snapshot is used within the same scheduling cycle.</p><p>Consider a scenario where, during the scheduling of a pod, it is rejected due to no nodes meeting the pod’s node affinity, but a new node matching the pod’s node affinity is added during the scheduling process.</p><p>As mentioned earlier, this new node is not considered a candidate node within the current scheduling cycle. Therefore, the pod is still rejected by the node affinity plugin. The issue arises if the scheduling queue puts the pod into <code>unschedulableQ</code>, as the pod would still need to wait for another event even though a node matching the pod’s node affinity requirement is available.</p><p>To prevent scenarios where pods miss events during scheduling, the scheduling queue records events occurring during pod scheduling and determines the pod’s queueing position based on these events and <code>QueueingHint</code>.</p><p>Therefore, the scheduling queue caches events from the time a pod leaves the scheduling queue until it returns to the scheduling queue or is scheduled. When the cached events are no longer needed, they are discarded.</p><h2 id="Golang-Doubly-Linked-List"><a href="#Golang-Doubly-Linked-List" class="headerlink" title="Golang Doubly Linked List"></a>Golang Doubly Linked List</h2><p><code>*list.List</code> is a data structure in Go’s standard library <code>container/list</code> package, representing a doubly linked list. In Go, doubly linked lists are a common data structure used to provide efficient performance for operations like element insertion, deletion, and traversal.</p><p>Here’s a brief overview of the <code>*list.List</code> structure:</p><ul><li><strong>Definition</strong>: <code>*list.List</code> is a pointer to a doubly linked list. It contains pointers to the head and tail of the list, as well as information about the length of the list.</li><li><strong>Features</strong>: Each node in the doubly linked list contains pointers to the previous and next nodes, making operations like inserting and deleting elements in the list efficient.</li><li><strong>Usage</strong>: <code>*list.List</code> is commonly used in scenarios where frequent insertion and deletion operations are required, especially when the number of elements is not fixed or the order may change frequently.</li></ul><p>Here’s a demonstration of how to use <code>*list.List</code> in Go:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">goCopy codepackage main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // Create a new doubly linked list</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // Add elements to the end of the list</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // Iterate over the list and print elements</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>PushBack</code> method adds a new element to the end of the list and returns a pointer to the new element (<code>*list.Element</code>). This pointer can be used for further operations on the element, such as removal or modification.</p><p>The <code>*list.Element</code> structure contains pointers to the previous and next elements in the list, as well as a field for storing the element’s value. By returning a <code>*list.Element</code> pointer, we can conveniently access the newly added element when needed for further operations. To remove an element from the doubly linked list, you can use the <code>list.Remove()</code> method. This method takes a list element as input and removes the element from the list.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>outputs：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>In this example, we remove the second element (with a value of 2) from the linked list.</p><h2 id="A-brief-analysis"><a href="#A-brief-analysis" class="headerlink" title="A brief analysis"></a>A brief analysis</h2><p>Let’s dive straight into analyzing the memory usage using pprof. Here’s a partial list of pprof profiles:</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>Here, we can observe that memory usage is primarily concentrated in protobuf’s <code>Decode</code>. Without delving into specific pprof analysis, we can consider three potential factors:</p><ul><li>Whether grpc-go has memory issues</li><li>Whether there are issues with Go itself</li><li>Whether Kubernetes has memory issues</li></ul><p>Regarding the first assumption, we can check related issues in grpc-go. However, recent reports do not indicate any memory anomalies. As for issues with Go itself, it doesn’t seem likely, although we did find a related Transparent Huge Pages (THP) issue, which we can briefly discuss later. Thus, the most probable cause would be an issue within Kubernetes itself. However, considering that <code>(*FieldsV1).Unmarshal</code> hasn’t been modified for 5 years, it’s highly unlikely to be the source of the problem. Therefore, let’s delve deeper into analyzing pprof.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>In the continuously growing list of pods, I noticed some unreleased data that seemed to align with the results of my previous analysis using pprof. Interestingly, pods are the only continuously changing objects. Therefore, I attempted another troubleshooting method to verify if the community had resolved this issue. I used minikube to launch Kubernetes version 1.18.5 locally for investigation. Fortunately, I couldn’t reproduce the issue, indicating that the problem might have been fixed after version 1.18.5.</p><p>To narrow down the investigation further, I asked my colleagues to inspect the commit history between these three minor versions. Eventually, we found a PR that closed the <code>SchedulerQueueingHints</code> feature. As mentioned in the technical background, the <code>SchedulerQueueingHints</code> feature could potentially lead to memory growth issues.</p><p>By examining the <code>PriorityQueue</code> structure, it was evident that the logic handling the feature was controlled by <code>isSchedulingQueueHintEnabled</code>. If the <code>QueueingHint</code> feature is enabled, when executing the <code>Pop</code> method to schedule pods, the UID of the corresponding pod in <code>inFlightPods</code> needs to be populated with the same linked list as <code>inFlightEvents</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>So, when are the linked list fields removed? We can observe that the only time they are removed is when the pod completes its scheduling cycle, that is, when the <code>Done</code> method is called.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, it can be observed that the later the timing of <code>Done</code>, the more pronounced the memory growth, and if pod events are ignored or missed, abnormal memory growth in the linked list can also occur. Some fixes for the above scenario can be seen:</p><ul><li>A PR, such as #120586, which emphasizes calling <code>Done()</code> as soon as possible.</li><li>The <code>QueueingHint</code> of the NodeAffinity&#x2F;NodeUnschedulable plugins missed relevant Node events, as seen in PR#122284.</li></ul><p>Despite these modifications, such issues are far from over.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://openai.com/">https://openai.com/</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Before the new year, a colleague upgraded the Kubernetes scheduler to version 1.28.3 and observed abnormal memory behavior. Let’s take a </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>探索Kubernetes 1.28调度器OOM的根源</title>
    <link href="https://zoues.com/posts/e46bd846/"/>
    <id>https://zoues.com/posts/e46bd846/</id>
    <published>2024-02-25T12:25:08.000Z</published>
    <updated>2024-02-28T12:18:12.203Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>年前，同事升级K8s调度器至1.28.3，观察到内存异常现象，帮忙一起看看，在集群pod及node随业务潮汐变动的情况下，内存呈现不断上升的趋势，直至OOM</p><blockquote><p>下面数据均为社区公开信息</p></blockquote><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c855.png" alt="K8s 1.28 scheduler OOM" style="zoom:50%;" /><p>触发场景有以下两种(社区还有其他复现方式)：</p><ul><li>case 1</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( ; ; ))</span><br><span class="line">do</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=0 </span><br><span class="line">    sleep 30</span><br><span class="line">    kubectl scale deployment nginx-test --replicas=50</span><br><span class="line">    sleep 30</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>case 2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Create a Pod with NodeAffinity under the situation where no Node can accommodate the Pod.</span><br><span class="line">2. Create a new Node.</span><br></pre></td></tr></table></figure><p>我们在社区的发现多起类似内存异常场景，复现方式不尽相同，关于上述问题的结论是：</p><blockquote><blockquote><p>Kubernetes社区在1.28版本中默认开启了调度特性SchedulerQueueingHints，导致调度组件内存异常。为了临时解决内存等问题，社区在1.28.5中将该特性调整为默认关闭。因为问题并未完全修复，所以建议审慎开启该特性。</p></blockquote></blockquote><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>介绍K8s调度器相关结构体</li><li>介绍K8s调度器QueueingHint</li><li>golang的双向链表</li></ul><h3 id="调度器简介"><a href="#调度器简介" class="headerlink" title="调度器简介"></a>调度器简介</h3><p>PriorityQueue是SchedulingQueue的接口实现。它的头部存放着优先级最高的待调度Pod。PriorityQueue包含以下重要字段：</p><ol><li>activeQ：存放准备好调度的Pod。新添加的Pod会被放入该队列。调度队列需要执行调度时，会从该队列中获取Pod。activeQ由堆来实现。</li><li>backoffQ：存放因各种原因（比如未满足节点要求）而被判定为无法调度的Pod。这些Pod会在一段退避时间后，被移到activeQ以尝试再次调度。backoffQ也由堆来实现。</li><li>unschedulablePods：存放因各种原因无法调度的Pod，是一个map数据结构。这些Pod被认定为无法调度，不会直接放入backoffQ，而是被记录在这里。待条件满足时，它们将被移到activeQ或者backoffQ中，调度队列会定期清理unschedulablePods 中的 Pod。</li><li>inFlightEvents：用于保存调度队列接收到的事件（entry的值是clusterEvent），以及正在处理中的Pod（entry的值是*v1.Pod），基于golang内部实现的双向链表</li><li>inFlightPods：保存了所有已经Pop，但尚未调用Done的Pod的UID，换句话说，所有当前正在处理中的Pod（正在调度、在admit中或在绑定周期中）。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// PriorityQueue implements a scheduling queue.</span><br><span class="line">type PriorityQueue struct &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">inFlightPods map[types.UID]*list.Element</span><br><span class="line"></span><br><span class="line">inFlightEvents *list.List</span><br><span class="line"></span><br><span class="line">activeQ *heap.Heap</span><br><span class="line"></span><br><span class="line">podBackoffQ *heap.Heap</span><br><span class="line">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br><span class="line">unschedulablePods *UnschedulablePods</span><br><span class="line">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="line">// when a pod is popped.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br><span class="line">preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// isSchedulingQueueHintEnabled indicates whether the feature gate for the scheduling queue is enabled.</span><br><span class="line">isSchedulingQueueHintEnabled bool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>关于K8s完整介绍，参看<a href="https://mp.weixin.qq.com/s?__biz=MzI1MzE0NTI0NQ==&mid=2650490396&idx=1&sn=c59b2252a833c7a215a606598f907f5c&chksm=f1d71feec6a096f81f54b2af3830a7e49aaf11ce118c4fda4928bfff5229dbce334610561b3d&token=232089518&lang=zh_CN#rd">kuberneter调度由浅入深：框架</a>，后续会更新最新的K8s调度器梳理</p></blockquote><h3 id="QueueingHint"><a href="#QueueingHint" class="headerlink" title="QueueingHint"></a>QueueingHint</h3><p>K8s调度器引入了<code>QueueingHint</code>特性，通过从每个插件获取有关Pod重新入队的建议，以减少不必要的调度重试，从而提升调度吞吐量。同时，在适当情况下跳过退避，进一步提高Pod调度效率。</p><h4 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h4><p>当前，每个插件可以通过EventsToRegister定义何时重试调度被插件拒绝的Pod。</p><p>比如，NodeAffinity会在节点添加或更新时重试调度Pod，因为新添加或更新的节点可能具有与Pod上的NodeAffinity匹配的标签。然而，实际上，在集群中会发生大量节点更新事件，这并不能保证之前被NodeAffinity拒绝的Pod能够成功调度。</p><p>为了解决这个问题，调度器引入了更精细的回调函数，以过滤掉无关的事件，从而在下一个调度周期中仅重试可能成功调度的Pod。</p><p>另外，DRA（动态资源分配）调度插件有时需要拒绝Pod以等待来自设备驱动程序的状态更新。因此，某些Pod可能需要经过几个调度周期才能完成调度。针对这种情况，与等待设备驱动程序状态更新相比，回退等待的时间更长。因此，希望能够使插件在特定情况下跳过回退以改善调度性能。</p><h4 id="实现目标"><a href="#实现目标" class="headerlink" title="实现目标"></a>实现目标</h4><p>为了提高调度吞吐量，社区提出以下改进：</p><ol><li><strong>引入QueueingHint</strong><ul><li>将 <code>QueueingHint</code> 引入到 <code>EventsToRegister</code> 机制中，允许插件提供针对Pods重新入队的建议</li></ul></li><li><strong>增强 Pod 跟踪和重新入队机制</strong>：<ul><li>优化追踪调度队列内正在处理的 Pods实现</li><li>实现一种机制，将被拒绝的 Pods 重新入队到适当的队列</li><li>优化被拒绝的Pods的退避策略，能够使插件在特定情况下跳过回退，从而提高调度吞吐量。</li></ul></li></ol><h4 id="潜在风险"><a href="#潜在风险" class="headerlink" title="潜在风险"></a>潜在风险</h4><p><em><strong>1. 实现中的错误可能导致 Pod 在 unschedulablePods 中长时间无法被调度</strong></em></p><p>如果一个插件配置了 QueueingHint，但它错过了一些可以让 Pod 可调度的事件， 被该插件拒绝的 Pod 可能会长期困在 unschedulablePods 中。</p><p>虽然调度队列会定期清理unschedulablePods 中的 Pod。（默认为 5 分钟，可配）</p><p><em><strong>2. 内存使用量的增加</strong></em></p><p>因为调度队列需要保留调度过程中发生的事件，kube-scheduler的内存使用量会增加。 所以集群越繁忙，它可能需要的内存就越多。</p><p>虽然无法完全消除内存增长，但如果能够尽快释放缓存的事件，就可以延缓内存增长的速度。</p><p><em><strong>3.<code>EnqueueExtension</code> 中 <code>EventsToRegister</code> 中的重大变更</strong></em></p><p>自定义调度器插件的开发者需要进行兼容性升级， <code>EnqueueExtension</code> 中的 <code>EventsToRegister</code> 将返回值从 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code>。<code>ClusterEventWithHint</code> 允许每个插件通过名为 <code>QueueingHintFn</code> 的回调函数过滤更多无用的事件。</p><p>社区为了简化迁移工作，空的 <code>QueueingHintFn</code> 被视为始终返回 <code>Queue</code>。 因此，如果他们只想保持现有行为，他们只需要将 <code>ClusterEvent</code> 更改为 <code>ClusterEventWithHint</code> 并不需要注册任何 <code>QueueingHintFn</code>。</p><h4 id="QueueingHints设计"><a href="#QueueingHints设计" class="headerlink" title="QueueingHints设计"></a>QueueingHints设计</h4><p>EventsToRegister 方法的返回类型已更改为 []ClusterEventWithHint</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// EnqueueExtensions 是一个可选接口，插件可以实现在内部调度队列中移动无法调度的 Pod。可以导</span><br><span class="line">// 致Pod无法调度（例如，Filter 插件）的插件可以实现此接口。</span><br><span class="line">type EnqueueExtensions interface &#123;</span><br><span class="line">Plugin</span><br><span class="line">...</span><br><span class="line">EventsToRegister() []ClusterEventWithHint</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 ClusterEventWithHint结构体包含一个 ClusterEvent 和一个 QueueingHintFn，当事件发生时执行 QueueingHintFn，并确定事件是否可以让 Pod满足调度。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type ClusterEventWithHint struct &#123;</span><br><span class="line">Event ClusterEvent</span><br><span class="line"></span><br><span class="line">QueueingHintFn QueueingHintFn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueingHintFn func(logger klog.Logger, pod *v1.Pod, oldObj, newObj interface&#123;&#125;) (QueueingHint, error)</span><br><span class="line"></span><br><span class="line">type QueueingHint int</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line">// QueueSkip implies that the cluster event has no impact on</span><br><span class="line">// scheduling of the pod.</span><br><span class="line">QueueSkip QueueingHint = iota</span><br><span class="line"></span><br><span class="line">// Queue implies that the Pod may be schedulable by the event.</span><br><span class="line">Queue</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>类型 QueueingHintFn 是一个函数，其返回类型为 (QueueingHint, error)。其中，QueueingHint 是一个枚举类型，可能的值有 QueueSkip 和 Queue。QueueingHintFn 调用时机位于将 Pod 从 unschedulableQ 移动到 backoffQ 或 activeQ 之前，如果返回错误，将把调用方返回的 QueueingHint 处理为 <code>QueueAfterBackoff</code>，这种处理无论返回的结果是什么，都可以防止 Pod 永远待在unschedulableQ 队列中。</p><p><em><strong>何时跳过&#x2F;不跳过 backoff</strong></em></p><p>BackoffQ 通过防止“长期无法调度”的 Pod 阻塞队列以保持高吞吐量的轻量级队列。</p><p>Pod 在调度周期中被拒绝的次数越多，Pod 需要等待的时间就越长，即在BackoffQ 待得时间就越长。</p><p>例如，当 NodeAffinity 拒绝了 Pod，后来在其 QueueingHintFn 中返回 Queue 时，Pod 需要等待 backoff 后才能重试调度。</p><p>但是，某些插件的设计本身就需要在调度周期中经历一些失败。比如内置插件DRA（动态资源分配），在 Reserve extension处，它告诉资源驱动程序调度结果，并拒绝 Pod 一次以等待资源驱动程序的响应。针对这种拒绝情况，不能将其视作调度周期的浪费，尽管特定调度周期失败了，但基于该周期的调度结果可以促进 Pod 的调度。因此，由于这种原因被拒绝的 Pod 不需要受到惩罚（backoff）。</p><p>为了支持这种情况，我们引入了一个新的状态 Pending。当 DRA 插件使用 Pending 拒绝 Pod，并且后续在其 QueueingHintFn 中返回 Queue 时，Pod 跳过 backoff，Pod 被重新调度。</p><p><em><strong>QueueingHint 如何工作</strong></em></p><p>当K8s集群事件发生时，调度队列将执行在之前调度周期中拒绝 Pod 的那些插件的 QueueingHintFn。</p><p>通过下述几个场景，描述一下它们如何被执行以及如何移动 Pod。</p><p>a. Pod被一个或多个插件拒绝</p><p>假设有三个节点。当 Pod 进入调度周期时，一个节点由于资源不足拒绝了Pod，其他两个节因为Pod 的 NodeAffinity不匹配拒绝了Pod。</p><p>在这种情况下，Pod 被 NodeResourceFit 和 NodeAffinity 插件拒绝，最终被放到 unschedulableQ 中。</p><p>此后，每当注册在这些插件中的集群事件发生时，调度队列通过 QueueingHint 通知它们。如果来自 NodeResourceFit 或 NodeAffinity 的任何一个的 QueueingHintFn 返回 Queue，则将 Pod 移动到 activeQ或者backoffQ中。 （例如，当 NodeAdded 事件发生时，NodeResourceFit 的 QueueingHint 返回 Queue，因为 Pod 可能可调度到该新节点。）</p><p>它是移动到 activeQ 还是 backoffQ，这取决于此 Pod 在unschedulableQ 中停留的时间有多长。如果在unschedulableQ 停留的时间超过了预期的 Pod 的 backoff 延迟时间，则它将直接移动到 activeQ。否则，它将移动到 backoffQ。</p><p>b. Pod因 Pending 状态而被拒绝 </p><p>当 DRA 插件在 Reserve extension 阶段针对Pod返回 Pending时，调度队列将 DRA 插件添加到 Pod 的pendingPlugins 字典中的同时，Pod 返回调度队列。</p><p>当 DRA 插件的 QueueingHint 之后的调用中返回 Queue 时，调度队列将此 Pod 直接放入 activeQ。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// Reserve reserves claims for the pod.</span><br><span class="line">func (pl *dynamicResources) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">if numDelayedAllocationPending == 1 || numClaimsWithStatusInfo == numDelayedAllocationPending &#123;</span><br><span class="line">...</span><br><span class="line">schedulingCtx.Spec.SelectedNode = nodeName</span><br><span class="line">logger.V(5).Info(&quot;start allocation&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to allocate resource&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, klog.ObjectRef&#123;Name: nodeName&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">return statusUnschedulable(logger, &quot;waiting for resource driver to provide information&quot;, &quot;pod&quot;, klog.KObj(pod))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>c. 跟踪调度队列中正在处理的 Pod</p><p>通过引入 QueueingHint，我们只能在特定事件发生时重试调度。但是，如果这些事件发生在Pod 的调度期间呢？</p><p>调度器对集群数据进行快照，并根据快照调度 Pod。每次启动调度周期时都会更新快照，换句话说，相同的快照在相同的调度周期中使用。</p><p>考虑到这样一个情景，比如，在调度一个 Pod 时，由于没有任何节点符合 Pod 的节点亲和性(NodeAffinity)，因此被拒绝，但是在调度过程中加入了一个新的节点，它与 Pod 的节点亲和性匹配。</p><p>如前所述，这个新节点在本次调度周期内不被视为候选节点，因此 Pod 仍然被节点亲和性插件拒绝。问题在于，如果调度队列将 Pod 放入unschedulableQ中，那么即使已经有一个节点匹配了 Pod 的节点亲和性要求，该 Pod 仍需要等待另一个事件。</p><p>为了避免类似Pod 在调度过程中错过事件的场景，调度队列会记录 Pod 调度期间发生的事件，并根据这些事件和QueueingHint来决定Pod 入队的位置。</p><p>因此，调度队列会缓存自 Pod 离开调度队列直到 Pod 返回调度队列或被调度的所有事件。当不再需要缓存的事件时，缓存的事件将被丢弃。</p><h3 id="Golang双向链表"><a href="#Golang双向链表" class="headerlink" title="Golang双向链表"></a>Golang双向链表</h3><p><code>*list.List</code> 是 Go 语言标准库 <code>container/list</code> 包中的一种数据结构，表示一个双向链表。在 Go 中，双向链表是一种常见的数据结构，用于在元素的插入、删除和遍历等操作上提供高效性能。</p><p>以下是 <code>*list.List</code> 结构的简要介绍：</p><ul><li><strong>定义</strong>：<code>*list.List</code> 是一个指向双向链表的指针，它包含了链表的头部和尾部指针，以及链表的长度信息。</li><li><strong>特性</strong>：双向链表中的每个节点都包含指向前一个节点和后一个节点的指针，这使得在链表中插入和删除元素的操作效率很高。</li><li><strong>用途</strong>：<code>*list.List</code> 常用于需要频繁插入和删除操作的场景，尤其是当元素的数量不固定或顺序可能经常变化时。</li></ul><p>演示了如何在 Go 中使用 <code>*list.List</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;container/list&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    // 创建一个新的双向链表</span><br><span class="line">    l := list.New()</span><br><span class="line"></span><br><span class="line">    // 在链表尾部添加元素</span><br><span class="line">    l.PushBack(1)</span><br><span class="line">    l.PushBack(2)</span><br><span class="line">    l.PushBack(3)</span><br><span class="line"></span><br><span class="line">    // 遍历链表并打印元素</span><br><span class="line">    for e := l.Front(); e != nil; e = e.Next() &#123;</span><br><span class="line">        fmt.Println(e.Value)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>PushBack</code> 方法会向链表的尾部添加一个新元素，并返回表示新元素的 <code>*list.Element</code> 指针。这个指针可以用于后续对该元素的操作，例如删除或修改。</p><p><code>*list.Element</code> 结构体包含了指向链表中前一个和后一个元素的指针，以及一个存储元素值的字段。通过返回 <code>*list.Element</code> 指针，我们可以方便地在需要时访问到新添加的元素，以便进行进一步的操作。要从双向链表中删除元素，你可以使用<code>list.Remove()</code>方法。这个方法需要传入一个链表元素，然后会将该元素从链表中移除。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">&quot;container/list&quot;</span><br><span class="line">&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">// 创建一个新的双向链表</span><br><span class="line">myList := list.New()</span><br><span class="line"></span><br><span class="line">// 在链表尾部添加元素</span><br><span class="line">myList.PushBack(1)</span><br><span class="line">myList.PushBack(2)</span><br><span class="line">myList.PushBack(3)</span><br><span class="line"></span><br><span class="line">// 找到要删除的元素</span><br><span class="line">elementToRemove := myList.Front().Next()</span><br><span class="line"></span><br><span class="line">// 从链表中移除该元素</span><br><span class="line">myList.Remove(elementToRemove)</span><br><span class="line"></span><br><span class="line">// 打印剩余的元素</span><br><span class="line">for element := myList.Front(); element != nil; element = element.Next() &#123;</span><br><span class="line">fmt.Println(element.Value)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码会输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>在这个例子中，我们移除了链表中第二个元素（值为2）。</p><h2 id="浅析一番"><a href="#浅析一番" class="headerlink" title="浅析一番"></a>浅析一番</h2><p>直接上pprof来分析一下内存使用情况,部分pprof列表，如下所示：</p><p><img src="https://pic.imgdb.cn/item/65dc98699f345e8d03f7c8e8.png" alt="K8s 1.28 scheduler OOM pprof"></p><p>这里可以发现，内存主要集中在protobuf的Decode，在不具体分析pprof的前提下，我们的思路有三点：</p><ul><li>grpc-go是否有内存问题</li><li>go本身是否问题</li><li>K8s内存问题</li></ul><p>针对第一个的假设，可以捞一下grpc-go的相关issue，可以发现近期未见相关内存异常的报告，go本身的问题，看起来也不太像，但倒是找到一个THP的相关问题，以后可以简单介绍一下，那么只剩一个结果，就是K8s本身存在问题，但其中<code>(*FieldsV1).Unmarshal</code>5年没动了，大概率不会存在问题，那么我们深入分析一下pprof吧</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:      309611     309611 (flat, cum)  2.62%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505       309611     309611           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>过段时间：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/apimachinery/pkg/apis/meta/v1.(*FieldsV1).Unmarshal</span><br><span class="line">vendor/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go</span><br><span class="line"></span><br><span class="line">  Total:     2069705    2069705 (flat, cum)  2.49%</span><br><span class="line">   6502            .          .           if postIndex &gt; l &#123; </span><br><span class="line">   6503            .          .           return io.ErrUnexpectedEOF </span><br><span class="line">   6504            .          .           &#125; </span><br><span class="line">   6505      2069705    2069705           m.Raw = append(m.Raw[:0], dAtA[iNdEx:postIndex]...) </span><br><span class="line">   6506            .          .           if m.Raw == nil &#123; </span><br><span class="line">   6507            .          .           m.Raw = []byte&#123;&#125; </span><br><span class="line">   6508            .          .           &#125; </span><br></pre></td></tr></table></figure><p>在持续增长的 Pod 列表中，发现了一些未释放的数据似乎与先前使用 pprof 分析的结果吻合，仅发现 Pod 是持续变更的对象。因此，我尝试了另一种排查方法，验证社区是否已解决此问题。我使用 minikube 在本地启动了 Kubernetes 1.18.5 版本进行排查。幸运的是，我未能复现这一现象，表明问题可能在 1.18.5 版本后已修复。</p><p>为了进一步缩小排查范围，我让同事检查了这三个小版本之间的提交记录。最终发现了一个关闭了 SchedulerQueueingHints 特性的 PR。正如在技术背景中提到的，SchedulerQueueingHints 特性可能导致内存增长问题。</p><p>通过PriorityQueue结构体可以发现其通过isSchedulingQueueHintEnabled来控制特性的逻辑处理，如果开启了<code>QueueingHint</code> 特性，那么在执行Pop方法来调度Pod时，需要为inFlightPods对应pod的UID填充相同inFlightEvents的链表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">obj, err := p.activeQ.Pop()</span><br><span class="line">...</span><br><span class="line">// In flight, no concurrent events yet.</span><br><span class="line">if p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">p.inFlightPods[pInfo.Pod.UID] = p.inFlightEvents.PushBack(pInfo.Pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">return pInfo, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么链表字段何时移除？我们可以观察到移除的唯一时间点在pod完成调度周期时，也就是调用Done方法时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">func (p *PriorityQueue) Done(pod types.UID) &#123;</span><br><span class="line">p.lock.Lock()</span><br><span class="line">defer p.lock.Unlock()</span><br><span class="line"></span><br><span class="line">p.done(pod)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (p *PriorityQueue) done(pod types.UID) &#123;</span><br><span class="line">if !p.isSchedulingQueueHintEnabled &#123;</span><br><span class="line">// do nothing if schedulingQueueHint is disabled.</span><br><span class="line">// In that case, we don&#x27;t have inFlightPods and inFlightEvents.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">inFlightPod, ok := p.inFlightPods[pod]</span><br><span class="line">if !ok &#123;</span><br><span class="line">// This Pod is already done()ed.</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">delete(p.inFlightPods, pod)</span><br><span class="line"></span><br><span class="line">// Remove the pod from the list.</span><br><span class="line">p.inFlightEvents.Remove(inFlightPod)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for &#123;</span><br><span class="line">...</span><br><span class="line">p.inFlightEvents.Remove(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以发现如何done的时机越晚，内存的增长将越明显，并且如果Pod的事件被忽视或者遗漏，链表的内存同样会出现异常增加的现象，可以看到针对上述场景的一些修复：</p><ul><li>出现了call Done() as soon as possible这样的PR，参看PR#120586</li><li>NodeAffinity&#x2F;NodeUnschedulable插件的QueueingHint 遗漏相关Node事件，参看PR#122284</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ol><li><a href="https://github.com/kubernetes/kubernetes/issues/122725">https://github.com/kubernetes/kubernetes/issues/122725</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122284">https://github.com/kubernetes/kubernetes/issues/122284</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/122289">https://github.com/kubernetes/kubernetes/pull/122289</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118893">https://github.com/kubernetes/kubernetes/issues/118893</a></li><li><a href="https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579">https://github.com/kubernetes/enhancements/blob/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6/keps/sig-scheduling/4247-queueinghint/README.md?plain=1#L579</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/122661">https://github.com/kubernetes/kubernetes/issues/122661</a></li><li><a href="https://github.com/kubernetes/kubernetes/pull/120586">https://github.com/kubernetes/kubernetes/pull/120586</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/118059">https://github.com/kubernetes/kubernetes/issues/118059</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;年前，同事升级K8s调度器至1.28.3，观察到内存异常现象，帮忙一起看看，在集群pod及node随业务潮汐变动的情况下，内</summary>
      
    
    
    
    <category term="云原生面试案例50篇" scheme="https://zoues.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F%E9%9D%A2%E8%AF%95%E6%A1%88%E4%BE%8B50%E7%AF%87/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>揭开K8s适配CgroupV2内存虚高的迷局</title>
    <link href="https://zoues.com/posts/3f237e52/"/>
    <id>https://zoues.com/posts/3f237e52/</id>
    <published>2024-01-27T12:40:08.000Z</published>
    <updated>2024-01-26T23:17:25.813Z</updated>
    
    <content type="html"><![CDATA[<p>在Almalinux替换CentOS的过程中，我们通过kubectl top nodes命令观察到了两个相同规格的节点（只有cgroup版本不同）。在分别调度两个相同的Pod后，我们预期它们的内存使用量应该相近。然而，我们发现使用了cgroupv2的节点的内存使用量比使用了cgroupv1的节点多了约280Mi。</p><p>初步分析表明，可能是cAdvisor在统计cgroupv1和v2的内存使用量时存在逻辑上的不一致。</p><p>理论上，无论使用cgroupv1还是cgroupv2，两个相同配置的节点的内存使用量应该相近。实际上，在比较&#x2F;proc&#x2F;meminfo时，我们发现了总内存使用量近似的情况。那么问题出在哪里呢？</p><p>我们发现，这个问题只影响了节点级别的内存统计数据，而不影响Pod级别的统计数据。</p><p>问题的根本原因是cAdvisor调用了runc的接口，其计算root cgroup的内存数据方面存在差异。在cgroupv2中，root cgroup不存在memory.current这个文件，但在cgroupv1中root cgroup是存在memory.usage_in_bytes文件的。这导致了在统计cgroupv2内存使用量时出现了不一致的情况。</p><p>这个问题可能需要在cAdvisor或runc的逻辑中进行修复，以确保在cgroupv1和cgroupv2中的内存统计一致性。下面我们基于社区issue展开介绍。</p><p>v1.28.3 commit:a8a1abc25cad87333840cd7d54be2efaf31a3177</p><blockquote><p>NOTE: Containerd:1.6.21，K8s:1.28, Kernel:5.15.0<br>(同步以前的文章)</p></blockquote><hr><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为false）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">klet.containerRuntime,</span><br><span class="line">klet.statusManager,</span><br><span class="line">hostStatsProvider)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">klet.cadvisor,</span><br><span class="line">klet.resourceAnalyzer,</span><br><span class="line">klet.podManager,</span><br><span class="line">klet.runtimeCache,</span><br><span class="line">kubeDeps.RemoteRuntimeService,</span><br><span class="line">kubeDeps.RemoteImageService,</span><br><span class="line">hostStatsProvider,</span><br><span class="line">utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在<code>/stats/</code>暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p>图片</p><p>从 Kubernetes 1.12 版本开始，kubelet 直接从 cAdvisor 暴露了多个接口。包括以下接口：</p><ul><li>cAdvisor 的 Prometheus 指标位于 <code>/metrics/cadvisor</code>。</li><li>cAdvisor v1 Json API 位于 <code>/stats/</code>、<code>/stats/container</code>、<code>/stats/&#123;podName&#125;/&#123;containerName&#125;</code> 和 <code>/stats/&#123;namespace&#125;/&#123;podName&#125;/&#123;uid&#125;/&#123;containerName&#125;</code>。</li><li>cAdvisor 的机器信息位于 &#x2F;spec。</li></ul><p>此外，kubelet还暴露了<code>summary API</code>，其中cAdvisor 是该接口指标来源之一。在社区的监控架构文档中描述了“核心”指标和“监控”指标的定义。这个文档中规定了一组核心指标及其用途，并且目标是通过拆分监控架构来实现以下两个目标：</p><ul><li><p>减小核心指标的统计收集性能影响，允许更频繁地收集这些指标。</p></li><li><p>使监控方案可替代且可扩展。</p></li></ul><p>因此移除cadvisor的接口，成了一项长期目标，目前进度如下(进度状态的标记略为滞后)：</p><ul><li><p>[1.13] 引入 Kubelet 的 pod-resources gRPC 端点；KEP: 支持设备监控社区#2454</p></li><li><p>[1.14] 引入 Kubelet 资源指标 API</p></li><li><p>[1.15] 通过添加和弃用 <code>--enable-cadvisor-json-endpoints</code> 标志，废弃“直接” cAdvisor API 端点</p></li><li><p>[1.18] 默认将 –enable-cadvisor-json-endpoints 标志设置为禁用</p></li><li><p>[1.21] 移除 <code>--enable-cadvisor-json-endpoints</code> 标志</p></li><li><p>[1.21] 将监控服务器过渡到 Kubelet 资源指标 API（需要3个版本的差异）</p></li><li><p>[TBD] 为 kubelet 监控端点提出外部替代方案</p></li><li><p>[TBD] 通过添加和废弃 <code>--enable-container-monitoring-endpoints</code> 标志，废弃摘要 API 和 cAdvisor Prometheus 端点</p></li><li><p>[TBD+2] 移除“直接”的 cAdvisor API 端点</p></li><li><p>[TBD+2] 默认将 –enable-container-monitoring-endpoints 标志设置为禁用</p></li><li><p>[TBD+4] 移除摘要 API、cAdvisor Prometheus 指标和移除 –enable-container-monitoring-endpoints 标志。</p></li></ul><p>当前版本的cadvisor接口已经做了部分废弃，例如<code>/spec及/stats/*</code>等<br><a href="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></a></p><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>kubelet 使用 cadvisor 来获取节点级别的统计信息（无论是使用 cri 还是通过cadvisor 来统计提供程序来获取 pod 的统计信息）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/provider.go</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// NewCRIStatsProvider returns a Provider that provides the node stats</span><br><span class="line">// from cAdvisor and the container stats from CRI.</span><br><span class="line">func NewCRIStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">runtimeService internalapi.RuntimeService,</span><br><span class="line">imageService internalapi.ImageManagerService,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">podAndContainerStatsFromCRI bool,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCRIStatsProvider(cadvisor, resourceAnalyzer,</span><br><span class="line">runtimeService, imageService, hostStatsProvider, podAndContainerStatsFromCRI))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// NewCadvisorStatsProvider returns a containerStatsProvider that provides both</span><br><span class="line">// the node and the container stats from cAdvisor.</span><br><span class="line">func NewCadvisorStatsProvider(</span><br><span class="line">cadvisor cadvisor.Interface,</span><br><span class="line">resourceAnalyzer stats.ResourceAnalyzer,</span><br><span class="line">podManager PodManager,</span><br><span class="line">runtimeCache kubecontainer.RuntimeCache,</span><br><span class="line">imageService kubecontainer.ImageService,</span><br><span class="line">statusProvider status.PodStatusProvider,</span><br><span class="line">hostStatsProvider HostStatsProvider,</span><br><span class="line">) *Provider &#123;</span><br><span class="line">return newStatsProvider(cadvisor, podManager, runtimeCache, newCadvisorStatsProvider(cadvisor, resourceAnalyzer, imageService, statusProvider, hostStatsProvider))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下述两种方式获取节点的内存使用情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl top node</span><br><span class="line">kubectl get --raw /api/v1/nodes/foo/proxy/stats/summary | jq -C .node.memory</span><br></pre></td></tr></table></figure><p>结果显示cgroupv2节点的内存使用量比相同节点配置但使用 cgroupv1的高一些。kubectl top node 获取节点信息的逻辑在：<a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40">https://github.com/kubernetes-sigs/metrics-server/blob/master/pkg/storage/node.go#L40</a></p><p>kubelet使用 cadvisor 来获取 cgroup 统计信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/server/stats/summary.go</span><br><span class="line"></span><br><span class="line">rootStats, err := sp.provider.GetCgroupCPUAndMemoryStats(&quot;/&quot;, false)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return nil, fmt.Errorf(&quot;failed to get root cgroup stats: %v&quot;, err)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里GetCgroupCPUAndMemoryStats调用以下cadvisor逻辑</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/kubelet/stats/helper.go</span><br><span class="line"></span><br><span class="line">infoMap, err := cadvisor.ContainerInfoV2(containerName, cadvisorapiv2.RequestOptions&#123;</span><br><span class="line">IdType:    cadvisorapiv2.TypeName,</span><br><span class="line">Count:     2, // 2 samples are needed to compute &quot;instantaneous&quot; CPU</span><br><span class="line">Recursive: false,</span><br><span class="line">MaxAge:    maxAge,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>cadvisor 基于 cgroup v1&#x2F;v2 获取不同 cgroup manager接口实现，然后调用GetStats()获取监控信息。</p><p>这些实现在计算root cgroup 的内存使用方面存在差异。</p><ul><li><p>v1 使用来自 memory.usage_in_bytes 的内存使用情况：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs/memory.go#L204-L224</a></p></li><li><p>v2 使用 &#x2F;proc&#x2F;meminfo 并计算使用情况为总内存 - 空闲内存：<a href="https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217">https://github.com/opencontainers/runc/blob/92c71e725fc6421b6375ff128936a23c340e2d16/libcontainer/cgroups/fs2/memory.go#L217</a></p></li></ul><p>usage_in_bytes 大致等于 RSS + Cache。workingset是 usage - 非活动文件。</p><p>在 cadvisor 中，在workingset中排除了非活动文件：<a href="https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844">https://github.com/google/cadvisor/blob/8164b38067246b36c773204f154604e2a1c962dc/container/libcontainer/handler.go#L835-L844</a>“</p><p>因此可以判断在cgroupv2计算内存使用使用了total-free，这里面包含了inactive_anon，而内核以及cgroupv1计算内存使用量时不会计入 inactive_anon：<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/mm/memcontrol.c#n3720</a></p><p>通过下面的测试中，inactive_anon 解释数据看到了差异。</p><p>下述分别为cgroupv1及cgroupv2的两个集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   98m          2%     1512Mi          12%</span><br><span class="line">node2   99m          2%     1454Mi          11%</span><br><span class="line">node3   94m          2%     1448Mi          11%</span><br></pre></td></tr></table></figure><p>其中cgroupv1节点的root cgroup内存使用如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /sys/fs/cgroup/memory/memory.usage_in_bytes</span><br><span class="line">6236864512</span><br><span class="line">~ # cat /sys/fs/cgroup/memory/memory.stat</span><br><span class="line">cache 44662784</span><br><span class="line">rss 3260416</span><br><span class="line">rss_huge 2097152</span><br><span class="line">shmem 65536</span><br><span class="line">mapped_file 11083776</span><br><span class="line">dirty 135168</span><br><span class="line">writeback 0</span><br><span class="line">pgpgin 114774</span><br><span class="line">pgpgout 103506</span><br><span class="line">pgfault 165891</span><br><span class="line">pgmajfault 99</span><br><span class="line">inactive_anon 135168</span><br><span class="line">active_anon 3645440</span><br><span class="line">inactive_file 5406720</span><br><span class="line">active_file 39333888</span><br><span class="line">unevictable 0</span><br><span class="line">hierarchical_memory_limit 9223372036854771712</span><br><span class="line">total_cache 5471584256</span><br><span class="line">total_rss 767148032</span><br><span class="line">total_rss_huge 559939584</span><br><span class="line">total_shmem 1921024</span><br><span class="line">total_mapped_file 605687808</span><br><span class="line">total_dirty 270336</span><br><span class="line">total_writeback 0</span><br><span class="line">total_pgpgin 51679194</span><br><span class="line">total_pgpgout 50291069</span><br><span class="line">total_pgfault 97383769</span><br><span class="line">total_pgmajfault 5610</span><br><span class="line">total_inactive_anon 1081344</span><br><span class="line">total_active_anon 772235264</span><br><span class="line">total_inactive_file 4648124416</span><br><span class="line">total_active_file 820551680</span><br><span class="line">total_unevictable 0</span><br></pre></td></tr></table></figure><p>meminfo文件如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">~ # cat /proc/meminfo</span><br><span class="line">MemTotal:       16393244 kB</span><br><span class="line">MemFree:         9744148 kB</span><br><span class="line">MemAvailable:   15020900 kB</span><br><span class="line">Buffers:          132344 kB</span><br><span class="line">Cached:          5207356 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:          1557252 kB</span><br><span class="line">Inactive:        4526668 kB</span><br><span class="line">Active(anon):     745916 kB</span><br><span class="line">Inactive(anon):      792 kB</span><br><span class="line">Active(file):     811336 kB</span><br><span class="line">Inactive(file):  4525876 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               636 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        618992 kB</span><br><span class="line">Mapped:           624384 kB</span><br><span class="line">Shmem:              2496 kB</span><br><span class="line">KReclaimable:     285824 kB</span><br><span class="line">Slab:             423600 kB</span><br><span class="line">SReclaimable:     285824 kB</span><br><span class="line">SUnreclaim:       137776 kB</span><br><span class="line">KernelStack:        8400 kB</span><br><span class="line">PageTables:         9060 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8196620 kB</span><br><span class="line">Committed_AS:    2800016 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       40992 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             4432 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    270336 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">CmaTotal:              0 kB</span><br><span class="line">CmaFree:               0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      302344 kB</span><br><span class="line">DirectMap2M:     3891200 kB</span><br><span class="line">DirectMap1G:    14680064 kB</span><br></pre></td></tr></table></figure><p>当前的计算</p><p>memory.current - memory.stat.total_inactive_file &#x3D; 6236864512 - 4648124416 &#x3D; 1515 Mi -&gt; kubelet 报告的结果</p><p>cgroupv2 集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~ # kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">node1   113m         2%     2196Mi          17%</span><br><span class="line">node2   112m         2%     2171Mi          17%</span><br><span class="line">node3   113m         2%     2180Mi          17%</span><br></pre></td></tr></table></figure><p>其中一节点的meminfo文件如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">MemTotal:       16374584 kB</span><br><span class="line">MemFree:         9505980 kB</span><br><span class="line">MemAvailable:   14912544 kB</span><br><span class="line">Buffers:          155164 kB</span><br><span class="line">Cached:          5335576 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:           872420 kB</span><br><span class="line">Inactive:        5399340 kB</span><br><span class="line">Active(anon):       2568 kB</span><br><span class="line">Inactive(anon):   791340 kB</span><br><span class="line">Active(file):     869852 kB</span><br><span class="line">Inactive(file):  4608000 kB</span><br><span class="line">Unevictable:       30740 kB</span><br><span class="line">Mlocked:           27668 kB</span><br><span class="line">SwapTotal:             0 kB</span><br><span class="line">SwapFree:              0 kB</span><br><span class="line">Dirty:               148 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:        716552 kB</span><br><span class="line">Mapped:           608424 kB</span><br><span class="line">Shmem:              6320 kB</span><br><span class="line">KReclaimable:     274360 kB</span><br><span class="line">Slab:             355976 kB</span><br><span class="line">SReclaimable:     274360 kB</span><br><span class="line">SUnreclaim:        81616 kB</span><br><span class="line">KernelStack:        8064 kB</span><br><span class="line">PageTables:         7692 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:     8187292 kB</span><br><span class="line">Committed_AS:    2605012 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:       48092 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:             3472 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:    409600 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">FileHugePages:         0 kB</span><br><span class="line">FilePmdMapped:         0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line">DirectMap4k:      271624 kB</span><br><span class="line">DirectMap2M:     8116224 kB</span><br><span class="line">DirectMap1G:    10485760 kB</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">usage = total - free = 16374584 - 9505980</span><br><span class="line"></span><br><span class="line">workingset = 总内存 - 空闲内存 - 非活动文件 = 16374584 - 9505980 - 4608000 = 2207 Mi（kubelet 报告的结果）</span><br></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如上所述，在Linux kernel及runc cgroupv1计算内存使用为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mem_cgroup_usage =NR_FILE_PAGES + NR_ANON_MAPPED + nr_swap_pages (如果swap启用的话)</span><br><span class="line"></span><br><span class="line">// - rss (NR_ANON_MAPPED)</span><br><span class="line">// - cache (NR_FILE_PAGES)</span><br></pre></td></tr></table></figure><p>但是runc在cgroupv2计算使用了total-free，因此在相似负载下，同一台机器上v1和v2版本的节点级别报告确实会相差约250-750Mi，为了让cgroup v2的内存使用计算更接近 cgroupv1，  cgroup v2调整计算内存使用量方式为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.MemoryStats.Usage.Usage = stats.MemoryStats.Stats[&quot;anon&quot;] + stats.MemoryStats.Stats[&quot;file&quot;]</span><br></pre></td></tr></table></figure><p>当然，我们同时还需要处理cadvisor的woringset的处理逻辑</p><p>由于笔者时间、视野、认知有限，本文难免出现错误、疏漏等问题，期待各位读者朋友、业界专家指正交流，上述排障信息已修改为社区内容。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680">https://github.com/torvalds/linux/blob/06c2afb862f9da8dc5efa4b6076a0e48c3fbaaa5/mm/memcontrol.c#L3673-L3680</a><br>2.<a href="https://github.com/kubernetes/kubernetes/issues/68522">https://github.com/kubernetes/kubernetes/issues/68522</a><br>3.<a href="https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/">https://kubernetes.io/docs/reference/instrumentation/cri-pod-container-metrics/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在Almalinux替换CentOS的过程中，我们通过kubectl top nodes命令观察到了两个相同规格的节点（只有cgroup版本不同）。在分别调度两个相同的Pod后，我们预期它们的内存使用量应该相近。然而，我们发现使用了cgroupv2的节点的内存使用量比使用了</summary>
      
    
    
    
    <category term="云原生面试案例50篇" scheme="https://zoues.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F%E9%9D%A2%E8%AF%95%E6%A1%88%E4%BE%8B50%E7%AF%87/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一条K8s命令行引发的血案</title>
    <link href="https://zoues.com/posts/5a8a6c8d/"/>
    <id>https://zoues.com/posts/5a8a6c8d/</id>
    <published>2024-01-27T01:25:08.000Z</published>
    <updated>2024-01-27T01:36:39.730Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一条K8s命令行引发的血案"><a href="#一条K8s命令行引发的血案" class="headerlink" title="一条K8s命令行引发的血案"></a>一条K8s命令行引发的血案</h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>因为Centos EOL的缘故，去年内部忙着换OS，打算趁此机会从cgroup v1切到cgroup v2，然而，在低版本K8s适配cgroupv2的过程中，遇到了一些问题，前期kubelet在cgroup v1的环境下，使用<code>-enable_load_reader</code>暴露容器的cpu load等相关监控数据，但在cgroup v2环境下，使用该配置会导致kubelet发生panic</p><p>下述为关键性信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container.go:422] Could not initialize cpu load reader for &quot;/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podXXX.slice&quot;: failed to create a netlink based cpuload reader: failed to get netlink family id for task stats: binary.Read: invalid type int32</span><br></pre></td></tr></table></figure><h2 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h2><p>该章节介绍以下内容：</p><ul><li>容器指标如何生成</li><li>K8s如何集成容器监控</li><li>cpu load如何计算等</li></ul><h3 id="cadvisor"><a href="#cadvisor" class="headerlink" title="cadvisor"></a>cadvisor</h3><p>cAdvisor是一款强大的Docker容器监控工具，专为容器场景设计，方便监控资源使用和性能分析。它用于收集、汇总、处理和输出容器的相关信息。cAdvisor支持Docker容器，同时支持其他类型的容器运行时。</p><p>Kubelet内置了对cAdvisor的支持，用户可以直接通过Kubelet组件获取有关节点上容器的监控指标。</p><blockquote><p>K8s 1.19使用的cAdvisor版本为0.39.3，而这里的简要介绍使用的是版本0.48.1。</p></blockquote><p>以下是主要功能代码，其中包含了一些注释以提高可读性。代码路径为：&#x2F;cadvisor&#x2F;cmd&#x2F;cadvisor.go。</p><p>cAdvisor主要完成以下几项任务：</p><ul><li>对外提供外部使用的API，包括一般的API接口和Prometheus接口。</li><li>支持第三方数据存储，包括BigQuery、Elasticsearch、InfluxDB、Kafka、Prometheus、Redis、StatsD和标准输出。</li><li>收集与容器、进程、机器、Go运行时以及自定义业务相关的监控。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func init() &#123;</span><br><span class="line">optstr := container.AllMetrics.String()</span><br><span class="line">flag.Var(&amp;ignoreMetrics, &quot;disable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be disabled. Options are %s.&quot;, optstr))</span><br><span class="line">flag.Var(&amp;enableMetrics, &quot;enable_metrics&quot;, fmt.Sprintf(&quot;comma-separated list of `metrics` to be enabled. If set, overrides &#x27;disable_metrics&#x27;. Options are %s.&quot;, optstr))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上述代码可以看到，cadvisor支持是否开启相关指标的能力，其中<code>AllMetrics</code>主要是下述指标:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/container/factory.go#L72</span><br><span class="line"></span><br><span class="line">var AllMetrics = MetricSet&#123;</span><br><span class="line">CpuUsageMetrics:                struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessSchedulerMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerCpuUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryUsageMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">MemoryNumaMetrics:              struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CpuLoadMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskIOMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">DiskUsageMetrics:               struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkTcpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkAdvancedTcpUsageMetrics: struct&#123;&#125;&#123;&#125;,</span><br><span class="line">NetworkUdpUsageMetrics:         struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ProcessMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">AppMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">HugetlbUsageMetrics:            struct&#123;&#125;&#123;&#125;,</span><br><span class="line">PerfMetrics:                    struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ReferencedMemoryMetrics:        struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUTopologyMetrics:             struct&#123;&#125;&#123;&#125;,</span><br><span class="line">ResctrlMetrics:                 struct&#123;&#125;&#123;&#125;,</span><br><span class="line">CPUSetMetrics:                  struct&#123;&#125;&#123;&#125;,</span><br><span class="line">OOMMetrics:                     struct&#123;&#125;&#123;&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">var includedMetrics container.MetricSet</span><br><span class="line">if len(enableMetrics) &gt; 0 &#123;</span><br><span class="line">includedMetrics = enableMetrics</span><br><span class="line">&#125; else &#123;</span><br><span class="line">includedMetrics = container.AllMetrics.Difference(ignoreMetrics)</span><br><span class="line">&#125;</span><br><span class="line">// 上述处理需要开启的指标</span><br><span class="line">klog.V(1).Infof(&quot;enabled metrics: %s&quot;, includedMetrics.String())</span><br><span class="line">setMaxProcs()</span><br><span class="line">// 内存方式存在监控指标</span><br><span class="line">memoryStorage, err := NewMemoryStorage()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to initialize storage driver: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 这是cadvisor核心逻辑，kubelet内部就是直接调用的manager.New</span><br><span class="line">resourceManager, err := manager.New(memoryStorage, sysFs, manager.HousekeepingConfigFlags, includedMetrics, &amp;collectorHTTPClient, strings.Split(*rawCgroupPrefixWhiteList, &quot;,&quot;), strings.Split(*envMetadataWhiteList, &quot;,&quot;), *perfEvents, *resctrlInterval)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to create a manager: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 注册对外的HTTP接口.</span><br><span class="line">err = cadvisorhttp.RegisterHandlers(mux, resourceManager, *httpAuthFile, *httpAuthRealm, *httpDigestFile, *httpDigestRealm, *urlBasePrefix)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Fatalf(&quot;Failed to register HTTP handlers: %v&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line">// 这里是容器标签的处理，kubelet 1.28切换到CRI之后需要修改kubelet</span><br><span class="line">containerLabelFunc := metrics.DefaultContainerLabels</span><br><span class="line">if !*storeContainerLabels &#123;</span><br><span class="line">whitelistedLabels := strings.Split(*whitelistedContainerLabels, &quot;,&quot;)</span><br><span class="line">// Trim spacing in labels</span><br><span class="line">for i := range whitelistedLabels &#123;</span><br><span class="line">whitelistedLabels[i] = strings.TrimSpace(whitelistedLabels[i])</span><br><span class="line">&#125;</span><br><span class="line">containerLabelFunc = metrics.BaseContainerLabels(whitelistedLabels)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中cpu load是否生成指标，同时也由命令行<code>enable_load_reader</code>控制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/42bb3d13a0cf9ab80c880a16c4ebb4f36e51b0c9/manager/container.go#L455</span><br><span class="line"></span><br><span class="line">if *enableLoadReader &#123;</span><br><span class="line">// Create cpu load reader.</span><br><span class="line">loadReader, err := cpuload.New()</span><br><span class="line">if err != nil &#123;</span><br><span class="line">klog.Warningf(&quot;Could not initialize cpu load reader for %q: %s&quot;, ref.Name, err)</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cont.loadReader = loadReader</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>在Kubernetes中，Google的cAdvisor项目被用于节点上容器资源和性能指标的收集。在kubelet server中，cAdvisor被集成用于监控该节点上kubepods（默认cgroup名称，systemd模式下会加上.slice后缀） cgroup下的所有容器。从1.29.0-alpha.2版本中可以看到，kubelet目前还是提供了以下两种配置选项（但是现在useLegacyCadvisorStats为<strong>false</strong>）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if kubeDeps.useLegacyCadvisorStats &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCadvisorStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      klet.containerRuntime,</span><br><span class="line">      klet.statusManager,</span><br><span class="line">      hostStatsProvider)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    klet.StatsProvider = stats.NewCRIStatsProvider(</span><br><span class="line">      klet.cadvisor,</span><br><span class="line">      klet.resourceAnalyzer,</span><br><span class="line">      klet.podManager,</span><br><span class="line">      klet.runtimeCache,</span><br><span class="line">      kubeDeps.RemoteRuntimeService,</span><br><span class="line">      kubeDeps.RemoteImageService,</span><br><span class="line">      hostStatsProvider,</span><br><span class="line">      utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>kubelet以Prometheus指标格式在&#x2F;stats&#x2F;暴露所有相关运行时指标，如下图所示，Kubelet内置了cadvisor服务</p><p><img src="https://pic.imgdb.cn/item/65b43ba0871b83018ac66a1f.png"></p><p>最终可以看到cadvisor组件如何在kubelet完成初始化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L80</span><br><span class="line"></span><br><span class="line">func New(imageFsInfoProvider ImageFsInfoProvider, rootPath string, cgroupRoots []string, usingLegacyStats, localStorageCapacityIsolation bool) (Interface, error) &#123;</span><br><span class="line">sysFs := sysfs.NewRealSysFs()</span><br><span class="line">// 这里就是kubelet默认暴露的监控指标类型</span><br><span class="line">includedMetrics := cadvisormetrics.MetricSet&#123;</span><br><span class="line">...</span><br><span class="line">cadvisormetrics.CpuLoadMetrics:      struct&#123;&#125;&#123;&#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">// 创建cAdvisor container manager.</span><br><span class="line">m, err := manager.New(memory.New(statsCacheDuration, nil), sysFs, housekeepingConfig, includedMetrics, http.DefaultClient, cgroupRoots, nil /* containerEnvMetadataWhiteList */, &quot;&quot; /* perfEventsFile */, time.Duration(0) /*resctrlInterval*/)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这里就是直接调用的cadvisor的manager.New的函数接口，更详细的信息可参看：<a href="https://zoues.com/posts/3f237e52/">https://zoues.com/posts/3f237e52/</a></p><h3 id="CPU-Load指标"><a href="#CPU-Load指标" class="headerlink" title="CPU Load指标"></a>CPU Load指标</h3><p>CPU使用率反映的是当前cpu的繁忙程度，CPU平均负载（load average）是指某段时间内占用cpu时间的进程和等待cpu时间的进程数，这里等待cpu时间的进程是指等待被唤醒的进程，不包括处于wait状态进程。</p><p>在对设备做相关诊断时，需要结合cpu使用率、平均负载以及任务状态来进行判断，比如CPU使用率低但负载高，可能是IO瓶颈等，对此不作深入介绍。</p><p>在cadvisor中对外暴露的指标名称为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container_cpu_load_average_10s</span><br></pre></td></tr></table></figure><p>那么我们来看看是如何被计算出来的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L632</span><br><span class="line"></span><br><span class="line">// Calculate new smoothed load average using the new sample of runnable threads.</span><br><span class="line">// The decay used ensures that the load will stabilize on a new constant value within</span><br><span class="line">// 10 seconds.</span><br><span class="line">func (cd *containerData) updateLoad(newLoad uint64) &#123;</span><br><span class="line">if cd.loadAvg &lt; 0 &#123;</span><br><span class="line">cd.loadAvg = float64(newLoad) // initialize to the first seen sample for faster stabilization.</span><br><span class="line">&#125; else &#123;</span><br><span class="line">cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>公式计算：<code>cd.loadAvg = cd.loadAvg*cd.loadDecay + float64(newLoad)*(1.0-cd.loadDecay)</code></p><p>大体意思是取的上一次采集计算出来的值cd.loadAvg乘以计算因子cd.loadDecay，然后加上当前采集</p><p>到的newLoad值乘以(1.0-cd.loadDecay)最后得出当前的cd.loadAvg值</p><p>其中<code>cont.loadDecay</code>计算逻辑如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L453</span><br><span class="line"></span><br><span class="line">cont.loadDecay = math.Exp(float64(-cont.housekeepingInterval.Seconds() / 10))</span><br></pre></td></tr></table></figure><p>这里是跟<code>housekeepingInterval</code>相关的固定值，衰变窗口</p><blockquote><p>关于容器cpu load的详细介绍可以看引用链接</p></blockquote><h2 id="寻根溯源"><a href="#寻根溯源" class="headerlink" title="寻根溯源"></a>寻根溯源</h2><p>cpu load的cd.loadAvg前值通过如下方式获取：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/google/cadvisor/blob/master/manager/container.go#L650</span><br><span class="line"></span><br><span class="line">if cd.loadReader != nil &#123;</span><br><span class="line">// TODO(vmarmol): Cache this path.</span><br><span class="line">path, err := cd.handler.GetCgroupPath(&quot;cpu&quot;)</span><br><span class="line">if err == nil &#123;</span><br><span class="line">loadStats, err := cd.loadReader.GetCpuLoad(cd.info.Name, path)</span><br><span class="line">if err != nil &#123;</span><br><span class="line">return fmt.Errorf(&quot;failed to get load stat for %q - path %q, error %s&quot;, cd.info.Name, path, err)</span><br><span class="line">&#125;</span><br><span class="line">stats.TaskStats = loadStats</span><br><span class="line">cd.updateLoad(loadStats.NrRunning)</span><br><span class="line">// convert to &#x27;milliLoad&#x27; to avoid floats and preserve precision.</span><br><span class="line">stats.Cpu.LoadAverage = int32(cd.loadAvg * 1000)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>深入探究可以发现使用了netlink来获取系统指标，关键调用路径:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">updateStats-&gt;GetCpuLoad-&gt;getLoadStats-&gt;prepareCmdMessage-&gt;prepareMessage</span><br></pre></td></tr></table></figure><p>经过上述分析可知， cAdvisor通过发送CGROUPSTATS_CMD_GET请求来获取CPU负载信息，通过netlink消息进行通信：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cadvisor/utils/cpuload/netlink/netlink.go</span><br></pre></td></tr></table></figure><p>在<code>v0.48.1</code>分支的第128到132行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">func prepareCmdMessage(id uint16, cfd uintptr) (msg netlinkMessage) &#123; </span><br><span class="line">buf := bytes.NewBuffer([]byte&#123;&#125;) </span><br><span class="line">addAttribute(buf, unix.CGROUPSTATS_CMD_ATTR_FD, uint32(cfd), 4) </span><br><span class="line">return prepareMessage(id, unix.CGROUPSTATS_CMD_GET, buf.Bytes()) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终内核在<code>cgroupstats_user_cmd</code>中处理获取请求：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/* user-&gt;kernel request/get-response */</span><br></pre></td></tr></table></figure><p><a href="https://github.com/torvalds/linux/blob/master/kernel/taskstats.c#L407"><code>kernel/taskstats.c#L407</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">static int cgroupstats_user_cmd(struct sk_buff *skb, struct genl_info *info)</span><br><span class="line">&#123;</span><br><span class="line">int rc = 0;</span><br><span class="line">struct sk_buff *rep_skb;</span><br><span class="line">struct cgroupstats *stats;</span><br><span class="line">struct nlattr *na;</span><br><span class="line">size_t size;</span><br><span class="line">u32 fd;</span><br><span class="line">struct fd f;</span><br><span class="line"></span><br><span class="line">na = info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD];</span><br><span class="line">if (!na)</span><br><span class="line">return -EINVAL;</span><br><span class="line"></span><br><span class="line">fd = nla_get_u32(info-&gt;attrs[CGROUPSTATS_CMD_ATTR_FD]);</span><br><span class="line">f = fdget(fd);</span><br><span class="line">if (!f.file)</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">size = nla_total_size(sizeof(struct cgroupstats));</span><br><span class="line"></span><br><span class="line">rc = prepare_reply(info, CGROUPSTATS_CMD_NEW, &amp;rep_skb,</span><br><span class="line">size);</span><br><span class="line">if (rc &lt; 0)</span><br><span class="line">goto err;</span><br><span class="line"></span><br><span class="line">na = nla_reserve(rep_skb, CGROUPSTATS_TYPE_CGROUP_STATS,</span><br><span class="line">sizeof(struct cgroupstats));</span><br><span class="line">if (na == NULL) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">rc = -EMSGSIZE;</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stats = nla_data(na);</span><br><span class="line">memset(stats, 0, sizeof(*stats));</span><br><span class="line"></span><br><span class="line">rc = cgroupstats_build(stats, f.file-&gt;f_path.dentry);</span><br><span class="line">if (rc &lt; 0) &#123;</span><br><span class="line">nlmsg_free(rep_skb);</span><br><span class="line">goto err;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rc = send_reply(rep_skb, info);</span><br><span class="line"></span><br><span class="line">err:</span><br><span class="line">fdput(f);</span><br><span class="line">return rc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>并在<code>cgroupstats_build</code>函数中构建cgroup stats结果：</p><p><a href="https://github.com/torvalds/linux/blob/5c1ee569660d4a205dced9cb4d0306b907fb7599/kernel/cgroup/cgroup-v1.c#L699"><code>kernel/cgroup/cgroup-v1.c#L699</code></a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * cgroupstats_build - build and fill cgroupstats</span><br><span class="line"> * @stats: cgroupstats to fill information into</span><br><span class="line"> * @dentry: A dentry entry belonging to the cgroup for which stats have</span><br><span class="line"> * been requested.</span><br><span class="line"> *</span><br><span class="line"> * Build and fill cgroupstats so that taskstats can export it to user</span><br><span class="line"> * space.</span><br><span class="line"> *</span><br><span class="line"> * Return: %0 on success or a negative errno code on failure</span><br><span class="line"> */</span><br><span class="line">int cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)</span><br><span class="line">&#123;</span><br><span class="line">……</span><br><span class="line">/* it should be kernfs_node belonging to cgroupfs and is a directory */</span><br><span class="line">if (dentry-&gt;d_sb-&gt;s_type != &amp;cgroup_fs_type || !kn ||</span><br><span class="line">    kernfs_type(kn) != KERNFS_DIR)</span><br><span class="line">return -EINVAL;  // 导致返回EINVAL错误码</span><br></pre></td></tr></table></figure><p>这里可以发现<code>cgroup_fs_type</code>是cgroup v1的类型，而没有处理cgroup v2。因此，<code>cgroupstats_build</code>函数在路径类型判断语句上返回EINVAL。</p><p>在内核社区也有相关问题的说明：<a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">kernel community issue</a></p><p>那么我们看看tejun(meta，cgroupv2 owner)如何解释的：</p><blockquote><p>The exclusion of cgroupstats from v2 interface was intentional due to the duplication and inconsistencies with other statistics. If you need these numbers, please justify and add them to the appropriate cgroupfs stat file.</p></blockquote><p>简单翻译：对v2接口中排除cgroupstats的操作是有意的，因为它与其他统计数据存在重复和不一致之处。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>那么他的建议是什么？</p><p>他建议我们使用psi，而不是通过CGROUPSTATS_CMD_GET netlink api获取CPU统计信息，直接从<code>cpu.pressure</code>、<code>memory.pressure</code>以及<code>io.pressure</code>文件中获取，后续我们会介绍psi在容器领域的相关进展，当前Containerd已经支持PSI相关监控.</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://github.com/containerd/cgroups/pull/308">https://github.com/containerd/cgroups/pull/308</a></li><li><a href="https://cloud.tencent.com/developer/article/2329489">https://cloud.tencent.com/developer/article/2329489</a></li><li><a href="https://github.com/google/cadvisor/issues/3137">https://github.com/google/cadvisor/issues/3137</a></li><li><a href="https://www.cnblogs.com/vinsent/p/15830271.html">https://www.cnblogs.com/vinsent/p/15830271.html</a></li><li><a href="https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d">https://lore.kernel.org/all/20200910055207.87702-1-zhouchengming@bytedance.com/T/#r50c826a171045e42d0b40a552e0d4d1b2a2bab4d</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一条K8s命令行引发的血案&quot;&gt;&lt;a href=&quot;#一条K8s命令行引发的血案&quot; class=&quot;headerlink&quot; title=&quot;一条K8s命令行引发的血案&quot;&gt;&lt;/a&gt;一条K8s命令行引发的血案&lt;/h2&gt;&lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot;</summary>
      
    
    
    
    <category term="云原生面试案例50篇" scheme="https://zoues.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F%E9%9D%A2%E8%AF%95%E6%A1%88%E4%BE%8B50%E7%AF%87/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Rust vs. Zig：究竟谁更胜一筹？性能、安全性等全面对决！</title>
    <link href="https://zoues.com/posts/423bdf96/"/>
    <id>https://zoues.com/posts/423bdf96/</id>
    <published>2024-01-21T12:23:48.000Z</published>
    <updated>2024-01-21T12:43:12.377Z</updated>
    
    <content type="html"><![CDATA[<p>Rust和Zig，这两种语言都旨在编写高效、性能优异的代码，然而它们在实现这一目标时采用了不同的方式。</p><p>值得注意的是，Rust和Zig根植于截然不同的理念，这可能影响开发者选择时的取舍。为了更深入地了解它们在相互比较中的表现，我们将进一步探讨它们各自的特点。</p><h2 id="什么是Rust？"><a href="#什么是Rust？" class="headerlink" title="什么是Rust？"></a>什么是Rust？</h2><p>Rust是一种以效率、性能和内存安全著称的通用型编程语言。它引入了一种新的编程方式，使开发者仍然能够使用面向对象以及函数式编程。</p><p>使用Rust进行编码需要一种不同往常的思维方式，这部分主要围绕着语言规则中的所有权和借用展开。</p><p>虽然这种思维方式能够让开发者更容易编写出安全高效的代码，但与C和C++等语言相比，特别是对于新手来说，充满挑战性。</p><p>Rust消除了C和C++跨平台的限制，允许将代码编译为目标系统运行的可执行文件。这意味着可以在不做重大修改的情况下将代码编译为多系统版本。</p><p>让我们看一个Rust版的Hello world：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fn main() &#123;</span><br><span class="line">    let text: &amp;str = &quot;World&quot;;</span><br><span class="line">    println!(&quot;Hello, &#123;&#125;!&quot;, text);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类似于其他编译型编程语言，在Rust中，每个可执行程序同样也都从main函数开始。如果运行上述示例，将在你的终端上输出“Hello, World!”。</p><h3 id="Rust优势与短板"><a href="#Rust优势与短板" class="headerlink" title="Rust优势与短板"></a>Rust优势与短板</h3><p>在Rust中，一些特性对开发者是有益的，而另一些则让开发变得更具挑战性。在这一章节，我们将分别介绍一下Rust的优势与劣势。</p><p>Rust的一些<strong>优势</strong>包括以下几点：</p><ul><li><p>并发和并行：Rust内置对并行编程的支持，以及安全高效的多线程特性</p></li><li><p>性能：由于Rust代码不需要运行时，同时它不需要额外的垃圾回收器功耗，从而可以使用更少的资源并提高性能，</p></li><li><p>内存安全且无垃圾回收：由于所有权和借用等规则，Rust在没有垃圾回收器的情况下管理内存，从而实现更高效和可预测的性能</p></li><li><p>跨平台兼容性：Rust支持跨平台开发，意味着可以在多个系统上编译代码而不需要太多的修改代码</p></li><li><p>强大的生态系统：Rust拥有强大的工具和库生态系统。它的包管理器Cargo显著简化了依赖管理和与外部库集成的难度</p></li></ul><p>Rust的一些<strong>劣势</strong>包括以下几点：</p><ul><li><p>学习曲线：Rust的语法对新开发者可能有些棘手。其语法融合了函数式和系统编程，受所有权和借用规则的影响很大。此外，新开发者还必须学习所有权系统、生命周期和借用规则等概念，需要付出一定的努力 ，下图是流传甚广的一张学习曲线图(来源于极客邦)</p><p><img src="https://pic.imgdb.cn/item/65ad10c7871b83018ac90c50.png" alt="Rust 编程第一课，实战驱动，快速上手 Rust"></p></li><li><p>编译耗时：Rust的安全需求导致较长的编译时间。Rust会彻底检查你的代码以防止运行时可能出现的问题，这意味着它的编译时间会比大多数语言更长</p></li><li><p>有限的资源：尽管Cargo是一个有用的包管理器，提供了许多可用的工具和库，但从整体来看，Rust的生态系统相较大多数语言来说都不够成熟。在一些专业领域，Rust的资源可能较少，迫使开发者更多地从零开始编写代码</p></li><li><p>繁琐的开发过程：由于强调安全和准确性，使用严格的规则和明确性，开发者通常在Rust中需要编写更多的代码，虽然可能会有高质量的输出，但往往会使开发过程变得更长，对小项目影响显著</p></li><li><p>互操作性：将Rust代码整合到其他语言编写的代码中可能有些困难。</p></li></ul><p>虽然Rust有其劣势，但它仍然是开发者的热门选择。在2023年Stack Overflow开发者调查中，Rust荣获最受喜爱的语言的桂冠，超过80%的受访者表示明年仍然想要使用它。</p><h3 id="Rust的常见使用场景"><a href="#Rust的常见使用场景" class="headerlink" title="Rust的常见使用场景"></a>Rust的常见使用场景</h3><p>既然你已经了解了Rust的功能，让我们看看它已经在哪些场景落地。</p><ul><li><p>在系统编程中，Rust对于构建操作系统、数据库系统、设备驱动程序和嵌入式系统等场景非常有用。</p></li><li><p>前后端Web开发者也使用Rust，与像Rocket或Actix这样的流行框架一起进行后端开发，以及使用WebAssembly或Tauri进行前端开发。</p></li></ul><p>Rust还被用于网络服务，如网络协议、代理、负载均衡器、VPN软件等。</p><p>一些Rust的更专业用例包括：</p><ul><li>游戏开发，使用像Amethyst和Bevy这样的游戏引擎</li><li>在区块链和加密货币领域，用于开发智能合约和项目中的区块链网络，如Solana 在物联网（IoT）中，用于编程微控制器和传感器等设备</li></ul><h2 id="什么是Zig？"><a href="#什么是Zig？" class="headerlink" title="什么是Zig？"></a>什么是Zig？</h2><p>虽然Zig更类似于传统的编程语言，如C和C++，但它像Rust一样注重内存安全和效率。然而，与Rust不同的是，Zig与现有的C和C++代码整合良好，无需像FFI这样的外部机制来简化互操作性。</p><p>与Rust、C和C++一样，Zig不使用垃圾收集器。为了实现类似Rust的内存安全性，Zig提供了促进内存安全的机制，例如：</p><ul><li>严格的编译时检查</li><li>用于处理潜在空值的可选类型</li><li>带有Error类型的明确错误处理</li><li>内置分配器的增强内存分配</li></ul><p>这些机制不会像Rust中那样严重影响编码习惯。让我们看一个Zig中的Hello world例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zigCopy codeconst std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;Hello, world&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对开发者来说，许多编程语言背后都有一种理念和设计哲学。例如，Rust注重内存安全性、效率、无垃圾收集和性能。</p><p>那么Zig呢？它的哲学包括：</p><ul><li>与C和C++代码轻松整合</li><li>生成不依赖系统依赖项的独立二进制文件</li><li>轻松的跨平台开发</li><li>快速的编译时间</li></ul><p>接下来，我们将看一看Zig的优势和劣势，之后再看它的用例。</p><h3 id="Zig的优势与短板"><a href="#Zig的优势与短板" class="headerlink" title="Zig的优势与短板"></a>Zig的优势与短板</h3><p>与我们在Rust中所做的一样，让我们从优势开始，然后再看劣势。</p><p>Zig为开发者提供的一些好处包括：</p><ul><li><p>控制和低级能力：Zig非常适合系统编程和需要直接管理系统资源的情场景</p></li><li><p>安全功能：内置分配器等功能使开发者能够轻松防止错误，提高代码可靠性，并减少错误和漏洞</p></li><li><p>性能优化：Zig是一个为高效执行和性能调优而优化代码的工具。它提供手动内存管理、编译时检查以及直接访问CPU指令的功能，以实现更高性能的应用程序</p></li><li><p>简单和可读性：Zig具有与C类似的简单语法和语言设计。这使得阅读、编写和维护代码变得简单</p></li><li><p>最小的外部依赖：Zig最小化了构建和运行程序所需的外部依赖，简化了开发，增强了可移植性，并减轻了跨平台依赖管理的负担</p></li><li><p>元编程能力：Zig的编译时元编程通过减少样板代码的需求和启用代码优化来提高代码的灵活性和生产力</p></li></ul><p>Zig的一些劣势包括：</p><ul><li>有限的生态系统：因为它仍处于早期阶段，Zig语言的生态系统比成熟语言更小</li><li>学习曲线：对于不熟悉低级编程概念的开发者来说，理解Zig可能需要一些时间 （相较Rust来说，所需的时间很短）</li><li>成熟度和工具：Zig是一种新语言，还有改进的空间。但请注意，仍然有一个强大而活跃的社区支持它</li><li>互操作性挑战：Zig提供了用于兼容性的C接口，但与其他语言集成可能需要额外的工作，比如管理数据转换和语言之间的通信</li><li>文档可用性：Zig是一种相对较新的语言，因此文档有限，社区正在努力提高文档的可用性</li></ul><h3 id="Zig的常见使用场景"><a href="#Zig的常见使用场景" class="headerlink" title="Zig的常见使用场景"></a>Zig的常见使用场景</h3><p>让我们深入一些Zig的实际用例，看看它在实际场景中是如何落地的！</p><p>开发者可以在系统编程中使用Zig来构建操作系统、设备驱动程序和嵌入式系统。其还在命令行工具中也有很多应用场景，可用于创建高效和快速的命令行界面，构建系统脚本，或优化现有工具的性能。</p><p>在编译器和语言开发中，Zig以其元编程能力和对简易性的追求而闻名。比较著名的开源项目是Bun，其是一个使用Zig开发的JavaScript运行时。</p><p>与Rust一样，Zig也有一些更为专业的使用场景：</p><ul><li>游戏开发，因支持高性能游戏引擎、能够实时模拟</li><li>在嵌入式系统和物联网中，用于编程微控制器、传感器和其他资源受限设备</li><li>在密码应用中，用于实现加密算法、数字签名、安全通信协议和其他安全敏感组件</li></ul><h2 id="Rust-vs-Zig-相似之处与差异"><a href="#Rust-vs-Zig-相似之处与差异" class="headerlink" title="Rust vs. Zig: 相似之处与差异"></a>Rust vs. Zig: 相似之处与差异</h2><p>前面我们已经分别看过Rust和Zig，现在是时候将它们放在一起进行比较了。比较不同的编程语言总是很有趣，特别是当它们有着相似的目标时。</p><p>让我们从它们的共同之处开始：</p><ul><li>内存安全性：Rust和Zig都优先考虑内存安全性，并通过严格的编译器检查、静态类型和适用于每种语言的特殊规则来防止常见的编程错误。</li><li>低级控制：两者都提供对系统资源更多的控制，使它们非常适合低级任务和系统编程。</li><li>性能优化：这两种编程语言都以高度优化的代码而闻名，具有手动内存管理、直接CPU访问和编译时评估的特性。</li><li>社区和可用性：Rust和Zig都是开源项目，拥有积极的社区、文档和工具支持。</li><li>无未定义行为：这两种编程语言都有严格的编译器检查和其他功能，可以防止未定义的行为。通过在编译时捕获问题，提高了程序的稳定性和安全性。</li></ul><p>与此同时，您可以使用下面的比较了解Rust和Zig之间的差异：</p><table><thead><tr><th>特征</th><th>Rust 使用其严格的所有权和借用规则来确保开发者编写的任何代码都是安全的。</th><th>Zig 使用跟踪和控制内存分配和释放的机制来防止开发者编写的任何代码都是不安全的。</th></tr></thead><tbody><tr><td>语法</td><td>Rust 通过显式注解强调所有权和生命周期，可能导致代码更长。</td><td>Zig 遵循类似于C的语法。</td></tr><tr><td>生态系统</td><td>Rust 提供了强大的生态系统，包括库、工具和社区支持。</td><td>Zig 是一个较年轻的语言，生态系统相对较小。</td></tr><tr><td>互操作性</td><td>Rust 具有良好的FFI兼容性。它在从C调用Rust函数方面表现良好，但从Rust调用C函数可能会有难度。</td><td>Zig 具有更出色的FFI。它在从C调用Zig函数和从C调用Zig函数方面表现良好。</td></tr><tr><td>错误处理</td><td>Rust 使用Result和Option类型进行显式错误处理。</td><td>Zig 使用错误类型、错误联合和延迟语句进行错误处理。</td></tr><tr><td>包管理器</td><td>Rust 使用Cargo包管理器处理包和依赖关系。</td><td>Zig 使用其内置的包管理器处理包和依赖关系。</td></tr></tbody></table><p>除了它们的相似之处和差异之外，我们还可以通过性能、流行度以及它们的程序员薪酬来比较Rust和Zig。让我们更仔细地看一看。</p><h3 id="Rust-vs-Zig-性能"><a href="#Rust-vs-Zig-性能" class="headerlink" title="Rust vs. Zig: 性能"></a>Rust vs. Zig: 性能</h3><p>客观来看，在Rust和Zig之间，并没有绝对性能更好的语言。Rust在特定应用中可能会胜过Zig，而Zig在其他方面可能会超越Rust。</p><p>让我们通过从编程语言和编译器基准测试中进行比较，仔细研究每种语言的性能：</p><p><img src="https://pic.imgdb.cn/item/65ad10df871b83018ac973e8.png" alt="Screenshot Taken From Programming Languages And Compiler Benchmark Project Showing Rust Vs Zig Performance For Two Example Programs"></p><p>这个基准测试项目包含用多种编程语言编写，并同时运行的程序。以表格形式呈现它们的运行结果，可以看到每种编程语言在任务中的表现到底如何。</p><p>在上面的图片中，我们使用Rust和Zig编写的mandelbrot和nbody程序，从性能<strong>由好到差</strong>进行排列。</p><p>你会注意到在某些情况下，Zig的性能优于Rust，而在其他情况下，Rust的性能优于Zig。两者都是高性能的语言，因此在项目中选择任一选项都应该能够满足你的需求。</p><h3 id="Rust-vs-Zig：流行度"><a href="#Rust-vs-Zig：流行度" class="headerlink" title="Rust vs. Zig：流行度"></a>Rust vs. Zig：流行度</h3><p>在选择要学习的编程语言时，流行度可能是一个重要因素。选择一种流行的语言不仅增加了你找到资源和支持的机会，还意味着你更有可能找到合作的开发者。</p><p>StackOverflow最新的开发者调查提供了一些有趣的观察视角。正如前面提到的，Rust是今年最受钦佩的语言，有84.66％的受访者表示他们明年想再次使用它，而Zig只有71.33％。</p><p>Rust在受欢迎语言列表中排名第14位，而Zig在总共列出的51种语言中排名第41位。</p><p>可能是因为它仍处于早期阶段，因此Zig在这两种情况下才获得较低的流行度。无论如何，考虑你选择工作的语言的流行度是至关重要的。</p><h3 id="Rust-vs-Zig：薪酬"><a href="#Rust-vs-Zig：薪酬" class="headerlink" title="Rust vs. Zig：薪酬"></a>Rust vs. Zig：薪酬</h3><p>StackOverflow的开发者调查还包含了受访者报告的最高薪酬的信息。如果你对进入软件开发市场感兴趣，这张图表可能对你很有帮助。</p><p>有趣的是，尽管Zig是一个新的选择，但实际上是今年最高薪酬的语言，而Rust在列表中排名第14位。如果你出于专业原因想要学习Rust或Zig，这些信息可能会有所帮助：</p><p><img src="https://pic.imgdb.cn/item/65ad10f2871b83018ac9cd1e.png" alt="Red Bar Chart With Dark Grey Background And White Labels Comparing Reported Pay For Developers By Language Ordered From Highest Pay To Lowest Pay"></p><p>尽管这张图表非常有帮助，但它只提供了局部的一些信息。当确定一个开发者的薪酬时，还有其他因素需要考虑，比如他们的经验水平和他们所在公司。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>关于Rust和Zig，很难说哪一个是明显的赢家。每种语言都有其优点和缺点。在深入使用任何一种语言之前，进行研究是至关重要的。这就是为什么我希望这篇文章能帮助你找到正确的选择。</p><p><a href="https://blog.logrocket.com/comparing-rust-vs-zig-performance-safety-more/">https://blog.logrocket.com/comparing-rust-vs-zig-performance-safety-more/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Rust和Zig，这两种语言都旨在编写高效、性能优异的代码，然而它们在实现这一目标时采用了不同的方式。&lt;/p&gt;
&lt;p&gt;值得注意的是，Rust和Zig根植于截然不同的理念，这可能影响开发者选择时的取舍。为了更深入地了解它们在相互比较中的表现，我们将进一步探讨它们各自的特点。&lt;</summary>
      
    
    
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>Optimizing the construction of the VM ecosystem with KubeVirt</title>
    <link href="https://zoues.com/posts/a78d5062/"/>
    <id>https://zoues.com/posts/a78d5062/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-01-21T02:32:23.551Z</updated>
    
    <content type="html"><![CDATA[<p>Two months ago, we were thrilled to share insights in the article “Best Practices for Migrating VM Clusters to KubeVirt 1.0.” As previously mentioned, we have selected AlmaLinux and Kubernetes 1.28 as the foundation for virtualization, employing cgroup v2 for resource isolation. Before moving to the production phase, we encountered additional challenges, particularly related to Kubernetes, containerd, and specific issues within KubeVirt. Therefore, in this second article, our goal is to share practical experiences and insights gained before the deployment of KubeVirt into a production environment.</p><h3 id="Latest-Developments"><a href="#Latest-Developments" class="headerlink" title="Latest Developments"></a>Latest Developments</h3><p>KubeVirt containerizes the trusted virtualization layer of QEMU and libvirt, enabling the management of VMs as standard Kubernetes resources. This approach offers users a more flexible, scalable, and contemporary solution for virtual machine management. As the project progresses, we’ve identified specific misconceptions, configuration errors, and opportunities to enhance KubeVirt functionality, especially in the context of utilizing Kubernetes 1.28 and containerd. The details are outlined below:</p><h4 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h4><ul><li>kubelet ready-only port</li></ul><p>To address security concerns, we have taken measures to mitigate potential malicious attacks on pods and containers. Specifically, we have discontinued the default opening of the insecure read-only port 10255 for the kubelet in K8s clusters running Kubernetes 1.26 or later. Instead, the authentication port 10250 is now opened and utilized by the kubelet.</p><ul><li>service account token expiration</li></ul><p>To enhance data security, Kubernetes 1.21 defaults to enabling the BoundServiceAccountTokenVolume feature. This feature specifies the validity period of service account tokens, automatically renews them before expiration, and invalidates tokens after associated pods are deleted. If using client-go version 11.0.0 or later, or 0.15.0 or later, the kubelet automatically reloads service account tokens from disks to facilitate token renewal.</p><ul><li>securing controller-manager and scheduler metrics</li></ul><p>Secure serving on port 10257 to kube-controller-manager (configurable via –secure-port) is now enabled. Delegated authentication and authorization are to be configured using the same flags as for aggregated API servers. Without configuration, the secure port will only allow access to &#x2F;healthz. (#64149, @sttts) Courtesy of SIG API Machinery, SIG Auth, SIG Cloud Provider, SIG Scheduling, and SIG Testing</p><p>Added secure port 10259 to the kube-scheduler (enabled by default) and deprecate old insecure port 10251. Without further flags self-signed certs are created on startup in memory. (#69663, @sttts)</p><h4 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h4><ul><li>private registry</li></ul><p>Modify your config.toml file (usually located at &#x2F;etc&#x2F;containerd&#x2F;config.toml) as shown below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">version = 2</span><br><span class="line"></span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">config_path = &quot;/etc/containerd/certs.d&quot;</span><br></pre></td></tr></table></figure><p>In containerd registry configuration, a registry host namespace refers to the path of the hosts.toml file specified by the registry host name or IP address, along with an optional port identifier. When submitting a pull request for an image, the typical format is as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pull [registry_host_name|IP address][:port][/v2][/org_path]&lt;image_name&gt;[:tag|@DIGEST]</span><br></pre></td></tr></table></figure><p>The registry host namespace part is <code>[registry_host_name|IP address][:port]</code>. For example, the directory structure for docker.io looks like this:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plaintextCopy code$ tree /etc/containerd/certs.d</span><br><span class="line">/etc/containerd/certs.d</span><br><span class="line">└── docker.io</span><br><span class="line">└── hosts.toml</span><br></pre></td></tr></table></figure><p>Alternatively, you can use the _default registry host namespace as a fallback if no other namespace matches.</p><ul><li>systemd cgroup</li></ul><p>While containerd and Kubernetes default to using the legacy cgroupfs driver for managing cgroups, it is recommended to utilize the systemd driver on systemd-based hosts to adhere to the “single-writer” rule of cgroups.</p><p>To configure containerd to use the systemd driver, add the following option in <code>/etc/containerd/config.toml</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">version = 2</span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</span><br><span class="line">SystemdCgroup = true</span><br></pre></td></tr></table></figure><p>Additionally, apart from configuring containerd, you need to set the KubeletConfiguration to use the “systemd” cgroup driver. The KubeletConfiguration is typically found at <code>/var/lib/kubelet/config.yaml</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">cgroupDriver: &quot;systemd&quot;</span><br></pre></td></tr></table></figure><ul><li>[community issue]containerd startup hangs when &#x2F;etc is ready-only</li></ul><p>We observed that, following the update from containerd v1.6.21 to v1.6.22, the systemd service failed to start successfully. Upon closer inspection during debugging, it was revealed that containerd did not fully initialize (lacking the “containerd successfully booted in …” message) and did not send the sd notification READY&#x3D;1 event.</p><ul><li>migration docker to containerd</li></ul><p>you have to configure the KubeletConfiguration to use the “containerd” endpoint. The KubeletConfiguration is typically located at &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">containerRuntimeEndpoint: &quot;unix:///run/containerd/containerd.sock&quot;</span><br></pre></td></tr></table></figure><p>Because <code>/var/lib/docker</code> is mounted on a separate disk, switching to containerd requires navigating to the root directory of containerd.</p><h3 id="kubevirt"><a href="#kubevirt" class="headerlink" title="kubevirt"></a>kubevirt</h3><ul><li>containerDisk data persistent<br>The containerDisk feature provides the ability to store and distribute VM disks in the container image registry. containerDisks can be assigned to VMs in the disks section of the VirtualMachineInstance spec.containerDisks are ephemeral storage devices that can be assigned to any number of active VirtualMachineInstances.We can persist data locally through incremental backups.</li><li>hostdisk support qcow2 format</li><li>hostdisk support hostpath capacity expansion</li></ul><h3 id="Storage-Solution"><a href="#Storage-Solution" class="headerlink" title="Storage Solution"></a>Storage Solution</h3><h4 id="VM-Image-storage-Soultion"><a href="#VM-Image-storage-Soultion" class="headerlink" title="VM Image storage Soultion"></a>VM Image storage Soultion</h4><p>In KubeVirt, the original virtual machine image file is incorporated into the &#x2F;disk path of the Docker base image and subsequently pushed to the image repository for utilization in virtual machine creatio.</p><p>Example: we could Inject a local VirtualMachineInstance disk into a container image</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; END &gt; Dockerfile</span><br><span class="line">FROM scratch</span><br><span class="line">ADD --chown=107:107 almalinux.qcow2 /disk/</span><br><span class="line">END</span><br><span class="line"></span><br><span class="line">docker build -t kubevirt/alamlinux:latest .</span><br></pre></td></tr></table></figure><p>When initiating a virtual machine, a Virtual Machine Instance (VMI) Custom Resource Definition (CRD) is created, capturing the specified virtual machine image’s name. Subsequent to VMI creation, the virt-controller generates a corresponding virt-launcher pod for the VMI. This pod comprises threee containers: compute container hosting the compute process for virt-launcher, named container-disk, responsible for managing the storage of the virtual machine image and guest-console-log container. The imageName of the container-disk container corresponds to the virtual machine image name recorded in the VMI. Once the virt-launcher pod is created, kubelet retrieves the container-disk image and initiates the container-disk container. During startup, the container-disk consistently monitors the disk_0.sock file under the -copy-path, with the sock file mapped to the path &#x2F;var&#x2F;run&#x2F;kubevirt&#x2F;container-disk&#x2F;{vmi-uuid}&#x2F; on the host machine through hostPath.</p><p>To facilitate the retrieval of necessary information during virtual machine creation, the virt-handler pod utilizes HostPid, enabling visibility of the host machine’s pid and mount details within the virt-handler container. During the virtual machine creation process, virt-handler identifies the pid of the container-disk process by referencing the disk_0.sock file of the VMI. It proceeds to determine the disk number of the container-disk container’s root disk using &#x2F;proc&#x2F;{pid}&#x2F;mountInfo. Subsequently, by cross-referencing the disk number of the container-disk root disk with the mount information of the host machine , it pinpoints the physical location of the container-disk root disk. Finally, it constructs the path for the virtual machine image file (&#x2F;disk&#x2F;disk.qcow2), retrieves the actual storage location (sourceFile) of the original virtual machine image on the host machine, and mounts the sourceFile to the targetFile for subsequent use as a backingFile during virtual machine creation.</p><h4 id="Host-Disk-Storage"><a href="#Host-Disk-Storage" class="headerlink" title="Host Disk Storage"></a>Host Disk Storage</h4><p>A hostDisk volume type provides the ability to create or use a disk image located somewhere on a node. It works similar to a hostPath in Kubernetes and provides two usage types:</p><p>DiskOrCreate if a disk image does not exist at a given location then create one<br>Disk a disk image must exist at a given location<br>need to enable the HostDisk feature gate.</p><p>Currently, hostdisk feature has some limitations. The expansion of hostdisk is only supported in the manner of using Persistent Volume Claims (PVC), and the disk format is limited to raw files.</p><p>Details regarding the above will be elaborated in the Feature Expansion section.</p><h3 id="Feature-Expansion"><a href="#Feature-Expansion" class="headerlink" title="Feature Expansion"></a>Feature Expansion</h3><h4 id="Support-VM-static-expansion"><a href="#Support-VM-static-expansion" class="headerlink" title="Support VM static expansion"></a>Support VM static expansion</h4><p>The CPU&#x2F;Mem is also provided with a synchronous interface when the CPU&#x2F;Mem disk is stopped and expanded. The CPU hotplug feature was introduced in KubeVirt v1. 0, making it possible to configure the VM workload to allow for adding or removing virtual CPUs while the VM is running,While the current version supports online expansion, we still opt for static expansion, primarily due to the temporary nature of VMs. The challenge here is that when resources are insufficient, the VM will not start.</p><h4 id="hostdisk-support-qcow2-and-online-expand"><a href="#hostdisk-support-qcow2-and-online-expand" class="headerlink" title="hostdisk support qcow2 and online expand"></a>hostdisk support qcow2 and online expand</h4><p>The current hostdisk has some limitations. The expansion of hostdisk is only supported in the manner of using Persistent Volume Claims (PVC), and the disk is limited to raw format,To implement this feature, we made minor adjustments to all components.</p><h4 id="cold-migration"><a href="#cold-migration" class="headerlink" title="cold migration"></a>cold migration</h4><p>We refrain from employing live migration capabilities due to their complexity and several limitations in our specific scenario. Instead, with data locally persisted and VMs scheduled in a fixed manner, we utilize cold migration through the rsync command.</p><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><p>In addition to the enhanced features mentioned earlier, we have integrated support for both static and dynamic addition or removal of host disks for virtual machines, password reset capabilities, pass-through of physical machine disks, and addressed various user requirements to deliver a more versatile and comprehensive usage experience.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>KubeVirt simplifies running virtual machines on Kubernetes, making it as easy as managing containers. It provides a cloud-native approach to managing virtual machines. KubeVirt addresses the challenge of unifying the management of virtual machines and containers, effectively harnessing the strengths of both. However, there is still a long way to go in practice.</p><p><a href="https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132">https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132</a></p><p><a href="https://segmentfault.com/a/1190000040926384/en">https://segmentfault.com/a/1190000040926384/en</a></p><p><a href="https://www.alibabacloud.com/help/en/ack/product-overview/solution-to-serviceaccount-token-expiration-after-upgrading-122-version">https://www.alibabacloud.com/help/en/ack/product-overview/solution-to-serviceaccount-token-expiration-after-upgrading-122-version</a></p><p><a href="https://github.com/containerd/containerd/issues/9139">https://github.com/containerd/containerd/issues/9139</a></p><p><a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md">https://github.com/containerd/containerd/blob/main/docs/cri/config.md</a></p><p><a href="https://www.cncf.io/blog/2023/09/22/best-practices-for-transitioning-vm-clusters-to-kubevirt-1-0/https://kubevirt.io/user-guide/virtualmachines/disksand_volumes/#hostdisk">https://www.cncf.io/blog/2023/09/22/best-practices-for-transitioning-vm-clusters-to-kubevirt-1-0/https://kubevirt.io/user-guide/virtualmachines/disksand_volumes/#hostdisk</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Two months ago, we were thrilled to share insights in the article “Best Practices for Migrating VM Clusters to KubeVirt 1.0.” As previous</summary>
      
    
    
    
    <category term="kubevirt" scheme="https://zoues.com/categories/kubevirt/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="kubevirt" scheme="https://zoues.com/tags/kubevirt/"/>
    
  </entry>
  
  <entry>
    <title>Best practices for transitioning VM clusters to KubeVirt 1.0</title>
    <link href="https://zoues.com/posts/912f3650/"/>
    <id>https://zoues.com/posts/912f3650/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-01-21T02:27:17.870Z</updated>
    
    <content type="html"><![CDATA[<p>The KubeVirt community is thrilled to announce the highly-anticipated release of KubeVirt v1.0! This momentous release signifies the remarkable achievements and widespread adoption within the community, marking a significant milestone for all stakeholders involved. This project became part of CNCF as a sandbox project in September 2019 and attained incubation status in April 2022. KubeVirt has evolved into a production-ready virtual machine management project that seamlessly operates as a native Kubernetes API. We have also chosen KubeVirt as our ultimate solution for virtual machine orchestration. Currently, we are utilizing AlmaLinux as the virtualization foundation and cgroup v2 as the resource isolation mechanism. Throughout the process of implementing KubeVirt, we encountered certain challenges. Therefore, we aim to share some of the practical experiences and insights we’ve gained from working with KubeVirt in this article.</p><h3 id="Why-KubeVirt"><a href="#Why-KubeVirt" class="headerlink" title="Why KubeVirt?"></a>Why KubeVirt?</h3><p>While OpenStack has seen widespread adoption, its architecture is relatively complex. By utilizing KubeVirt, virtual machine management is streamlined, offering an improved integration experience. With KubeVirt’s inclusion in the CNCF sandbox project and its integration with the CNCF ecosystem, Kubernetes API has been extended with custom resource definitions (CRDs) to enable native VM operation within Kubernetes.</p><p>KubeVirt containerizes the trusted virtualization layer of QEMU and libvirt, allowing VMs to be handled just like any other Kubernetes resource. This approach provides users with a more flexible, scalable, and modern virtual machine management solution, offering the following key advantages:</p><ul><li>Simplified Architecture and Management: Compared to OpenStack, KubeVirt offers a simplified architecture and management requirements. OpenStack can be unwieldy and costly to maintain, while KubeVirt leverages Kubernetes for the automated lifecycle management of VMs. It eliminates separate processes for VMs and containers, facilitating the integration of workflows for both virtualization and containerization. This simplifies the underlying infrastructure stack and reduces management costs.</li><li>Modern, Scalable, Kubernetes-Based Solution: KubeVirt is a modern, scalable, Kubernetes-based virtual machine management solution. By standardizing automated testing and deployment of all applications using Kubernetes, and unifying metadata within Kubernetes, it reduces the risk of deployment errors and enables faster iteration. This minimizes the operational workload for DevOps teams and accelerates day-to-day operations.</li><li>Tight Integration with the Kubernetes Ecosystem: KubeVirt seamlessly integrates with the Kubernetes ecosystem, offering improved scalability and performance. When VMs are migrated to Kubernetes, it can lead to cost reductions for software and application use and minimize performance overhead at the virtualization layer.</li><li>Ideal for Lightweight, Flexible, and Modern VM Management: KubeVirt is well-suited for scenarios requiring lightweight, flexible, and modern virtual machine management. Users can run their virtual workloads alongside container workloads, managing them in the same manner. They can also leverage familiar cloud-native tools such as Tekton, Istio, ArgoCD, and more, which are already favored by cloud-native users.</li></ul><h3 id="What‘s-new-in-kubeVirt-1-0-？"><a href="#What‘s-new-in-kubeVirt-1-0-？" class="headerlink" title="What‘s new in kubeVirt 1.0 ？"></a>What‘s new in kubeVirt 1.0 ？</h3><p>In POC phase, two RPM-based packages and six container images were used, providing an extension for virtual machine management within Kubernetes:</p><ol><li>kubevirt-virtctl: This package can be installed on any machine with administrator access to the cluster. It contains the virtctl tool, which simplifies virtual machine management using kubectl. While kubectl can be used for this purpose, managing VMs can be complex due to their stateful nature. The virtctl tool abstracts this complexity, enabling operations like starting, stopping, pausing, unpausing, and migrating VMs. It also provides access to the virtual machine’s serial console and graphics server.</li><li>kubevirt-manifests: This package contains manifests for installing KubeVirt. Key files include kubevirt-cr.yaml, representing the KubeVirt Custom Resource definition, and kubevirt-operator.yaml, which deploys the KubeVirt operator responsible for managing the KubeVirt service within the cluster.<br>The container images are as follows:</li></ol><ul><li>virt-api: Provides a Kubernetes API extension for virtual machine resources.</li><li>virt-controller: Watches for new or updated objects created via virt-api and ensures object states match the requested state.</li><li>virt-handler: A DaemonSet and node component that keeps cluster-level virtual machine objects in sync with libvirtd domains running in virt-launcher. It can also perform node-centric operations like configuring networking and storage.</li><li>virt-launcher: A node component that runs libvirt and QEMU to provide the virtual machine environment.</li><li>virt-operator: Implements the Kubernetes operator pattern for managing the KubeVirt application.</li><li>libguestfs-tools: Provides utilities for accessing and modifying VM disk images.<br>The v1.0 release signifies significant growth for the KubeVirt community, progressing from an idea to a production-ready Virtual Machine Management solution over the past six years. This release emphasizes maintaining APIs while expanding the project. The release cadence has shifted to align with Kubernetes practices, enabling better stability, compatibility, and support.</li></ul><p>The project has embraced Kubernetes community practices, including SIGs for test and review responsibilities, a SIG release repo for release-related tasks, and regular SIG meetings covering areas like scale, performance, and storage.</p><p>Notable features in v1.0 include memory over-commit support, persistent vTPM for easier BitLocker usage on Windows, initial CPU Hotplug support, hot plug, and hot unplug (in alpha), and further developments in API stabilization and SR-IOV interface support.</p><p>The focus is on aligning KubeVirt with Kubernetes and fostering community collaboration to enhance virtual machine management within the Kubernetes ecosystem.</p><h3 id="What-issues-in-kubeVirt-1-0？"><a href="#What-issues-in-kubeVirt-1-0？" class="headerlink" title="What issues in kubeVirt 1.0？"></a>What issues in kubeVirt 1.0？</h3><h4 id="CgroupV2-support"><a href="#CgroupV2-support" class="headerlink" title="CgroupV2 support"></a>CgroupV2 support</h4><p>When using cgroup v2, starting a VM with a non-hotpluggable volume can be problematic because cgroup v2 doesn’t provide information about the currently allowed devices for a container. KubeVirt addresses this issue by tracking device rules internally using a global variable:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/kubevirt/7/pkg/virt-handler/cgroup/cgroup_v2_manager.go#L10</span><br><span class="line"></span><br><span class="line">var rulesPerPid = make(map[string][]*devices.Rule)</span><br></pre></td></tr></table></figure><p>However, this approach has some drawbacks:</p><p>The variable won’t survive a crash or restart of the virt-handler pod, resulting in data loss.<br>The state is stored in a dynamic structure (a map), and stale data is not removed, causing memory consumption to continuously increase.<br>A potential solution is to store the state in a file, for example, <code>/var/run/kubevirt-private/devices.list</code>. This file should be updated each time a device is added or removed. Additionally, it should be removed when the corresponding VM is destroyed, or periodic cleanup can be performed. The file can follow the same data format as devices.list on cgroup v1 hosts, allowing the same code to parse the current state for both v1 and v2.</p><p>However, managing the file introduces the challenge of performing transactions, i.e., applying actual device rules and writing the state to the file atomically.</p><p>You can find more details and discussions about this issue in GitHub issue #7710.</p><h4 id="Cilium-Support"><a href="#Cilium-Support" class="headerlink" title="Cilium Support"></a>Cilium Support</h4><ul><li>cilium multi-homing<br>In Kubernetes, each pod typically has only one network interface (aside from a loopback interface). Cilium-native multi-homing aims to enable the attachment of additional network interfaces to pods. This functionality is similar to what the Multus CNI offers, which allows the attachment of multiple network interfaces to pods. However, Cilium-native multi-homing distinguishes itself by relying exclusively on the Cilium CNI as the sole CNI installed.</li></ul><p>This feature should provide robust support for all existing Cilium datapath capabilities, including network policies, observability, datapath acceleration, and service discovery. Furthermore, it aims to offer a straightforward developer experience that aligns with the simplicity and usability that Cilium already provides today.</p><ul><li>multus<br>When utilizing Cilium version 1.14.0 alongside multus-cni, there seems to be an issue where the secondary interface does not become visible. Here’s a list of files you can find under the &#x2F;etc&#x2F;cni directory after installing multus in a Cilium 1.14 environment:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l</span><br><span class="line">/etc/cni/net.d/05-cilium.conflist</span><br><span class="line">/etc/cni/net.d/00-multus.conf.cilium_bak</span><br><span class="line">/etc/cni/net.d/100-crio-bridge.conflist.cilium_bak</span><br><span class="line">/etc/cni/net.d/200-loopback.conflist.cilium_bak</span><br><span class="line">/etc/cni/net.d/multus.d/multus.kubeconfig</span><br></pre></td></tr></table></figure>The issue with multus installation in Cilium 1.14 has been resolved by setting cni.exclusive to false.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Make Cilium take ownership over the `/etc/cni/net.d` directory on the</span><br><span class="line"># node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.</span><br><span class="line"># This ensures no Pods can be scheduled using other CNI plugins during Cilium</span><br><span class="line"># agent downtime.</span><br><span class="line">exclusive: false</span><br></pre></td></tr></table></figure></li></ul><h4 id="Harbor-limit"><a href="#Harbor-limit" class="headerlink" title="Harbor limit"></a>Harbor limit</h4><p>We encountered an issue when attempting to push a container with a single layer size which contain win.qcow2 image exceeding 10.25GB to our Harbor instance hosted on an EC2 instance. Our Harbor version is v2.1.2, and we are using S3 as the storage backend.</p><p>Our system has successfully handled containers with total sizes exceeding 15GB in the past. However, this specific container with a single layer size of 13.5GB repeatedly fails to push. On the client side, we receive limited feedback:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sep 10 22:29:19 backend-ci dockerd[934]:</span><br><span class="line">time=&quot;2023-09-10T22:29:19.628869277+02:00&quot; level=error msg=&quot;Upload failed, retrying: blob upload unknown&quot;</span><br><span class="line">Although the push activity completes successfully, the client-side error only appears afterward. In the registry.log, we’ve noticed the following error:</span><br><span class="line"></span><br><span class="line">registry[885]: time=&quot;2023-09-10T08:47:25.330317861Z&quot; level=error msg=&quot;upload resumed at wrong offest: 10485760000 != 12341008872&quot;</span><br></pre></td></tr></table></figure><p>We would greatly appreciate any insights or advice on this matter. Perhaps others have encountered similar issues with very large layers, especially when using S3 as a storage backend, where pushing layers larger than 10GB is not supported. We’ve also come across potential fixes proposed in this GitHub pull request:</p><p><a href="https://github.com/goharbor/harbor/pull/16322">https://github.com/goharbor/harbor/pull/16322</a></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>KubeVirt simplifies running virtual machines on Kubernetes, making it as easy as managing containers. It provides a cloud-native approach to managing virtual machines. KubeVirt addresses the challenge of unifying the management of virtual machines and containers, effectively harnessing the strengths of both. However, there is still a long way to go in practice. Nevertheless, the release of version 1.0 is significant for the community and users. We look forward to the widespread adoption of KubeVirt and its full support for cgroupv2.</p><ul><li><p><a href="https://documentation.suse.com/container/kubevirt/html/SLE-kubevirt/index.html">https://documentation.suse.com/container/kubevirt/html/SLE-kubevirt/index.html</a></p></li><li><p><a href="https://kubevirt.io/2023/KubeVirt-v1-has-landed.html">https://kubevirt.io/2023/KubeVirt-v1-has-landed.html</a></p></li><li><p>CFP: Cilium-native multi-homing · Issue #20129</p></li><li><p><a href="https://github.com/goharbor/harbor/issues/15719">https://github.com/goharbor/harbor/issues/15719</a></p></li><li><p><a href="https://github.com/kubevirt/kubevirt/issues/398">https://github.com/kubevirt/kubevirt/issues/398</a></p></li><li><p><a href="https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132">https://github.com/k8snetworkplumbingwg/multus-cni/issues/1132</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The KubeVirt community is thrilled to announce the highly-anticipated release of KubeVirt v1.0! This momentous release signifies the rema</summary>
      
    
    
    
    <category term="kubevirt" scheme="https://zoues.com/categories/kubevirt/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="kubevirt" scheme="https://zoues.com/tags/kubevirt/"/>
    
  </entry>
  
  <entry>
    <title>Embracing Cgroup V2:Best Practices for Migrating Kubernetes Clusters to AlmaLinux</title>
    <link href="https://zoues.com/posts/58fc8d19/"/>
    <id>https://zoues.com/posts/58fc8d19/</id>
    <published>2024-01-20T05:40:08.000Z</published>
    <updated>2024-01-21T02:29:47.248Z</updated>
    
    <content type="html"><![CDATA[<p>With the announcement of CentOS discontinuation by the CentOS community , along with the set dates for service termination, we have put the switch to a new container operating system on our agenda. Based on factors such as migration cost, smoothness of transition, and maintenance difficulty, we have chosen AlmaLinux from the RHEL series as an alternative solution.</p><p>NOTE: AlmaLinux is just one of the replacement options available within the RHEL ecosystem. Our choice is based on our specific production needs and does not necessarily apply to everyone.</p><p>AlmaLinux 9 defaults to using cgroup v2, and this configuration affects some underlying components. Therefore, certain adaptations and compatibility work need to be done. This article presents the best practices for migrating Kubernetes cluster nodes from CentOS to AlmaLinux which involves removing dockershim and utilizing cgroup v2 for node resource management.</p><h2 id="Why-Cgroup-v2"><a href="#Why-Cgroup-v2" class="headerlink" title="Why Cgroup v2?"></a>Why Cgroup v2?</h2><p>Effective resource management is a critical aspect of Kubernetes. This involves managing the finite resources in your nodes, such as CPU, memory, and storage.</p><p><em>cgroups</em> are a Linux kernel capability that establish resource management functionality like limiting CPU usage or setting memory limits for running processes.</p><p>When you use the resource management capabilities in Kubernetes, such as configuring requests and limits for Pods and containers, Kubernetes uses cgroups to enforce your resource requests and limits.</p><p>The Linux kernel offers two versions of cgroups: cgroup v1 and v2.</p><p>Here is our comparison between the two versions based on our research:</p><table><thead><tr><th></th><th>cgroup v1</th><th>cgroup v2</th></tr></thead><tbody><tr><td>Maintainability</td><td>deprecate and systemd community intend to remove cgroup v1 support from systemd release after the end of 2023</td><td>Many recent releases of Linux distributions have switched over to cgroup v2 by default</td></tr><tr><td>Compatibility</td><td>support</td><td>1. Components such as kubelet need to be adapted for cgroup v2 2. Business applications require JDK version upgrades, which can be achieved by replacing the base image</td></tr><tr><td>hierarchy</td><td>multiple hierarchies，it wasn’t useful and complicated in practice</td><td>single unified hierarchy</td></tr><tr><td>resource allocation management</td><td>basic</td><td>more powerful、dynamic and enhanced resource allocation management，such as the following： • Unified accounting for different types of memory allocations (network and kernel memory, etc) • Accounting for non-immediate resource changes such as page cache write backs • Safer sub-tree delegation to containers</td></tr><tr><td>Performance</td><td>support for multiple hierarchies came at a steep cost</td><td>better</td></tr><tr><td>Scalability</td><td>it seemed to provide a high level of flexibility,  but it wasn’t useful in practice</td><td>provide a high level of flexibility ，new features like PSI</td></tr><tr><td>Security</td><td>the known CVEs, such as cve-2022-0492、cve-2021-4154</td><td>support rootless container</td></tr></tbody></table><p>cgroup v2 has been in development in the Linux Kernel since 2016 and in recent years has matured across the container ecosystem. With Kubernetes 1.25, cgroup v2 support has graduated to general availability.</p><h2 id="What-issues-were-encountered？"><a href="#What-issues-were-encountered？" class="headerlink" title="What issues were encountered？"></a>What issues were encountered？</h2><h3 id="Java-applications"><a href="#Java-applications" class="headerlink" title="Java applications"></a>Java applications</h3><p><a href="https://bugs.openjdk.org/browse/JDK-8146115">JDK-8146115</a> added Hotspot runtime support for JVMs running in Docker containers. At the time Docker used cgroups v1 and, hence, runtime support only includes cgroup v1 controllers.</p><p>JDK-8230305 extended functionality of <a href="https://bugs.openjdk.org/browse/JDK-8146115">JDK-8146115</a> to also detect cgroups v2. That is iff cgroups v2 unified hierarchy is available only, use the cgroups v2 backend. Otherwise fall back to existing cgroups v1 container support.</p><p>require version：jdk8u372, 11.0.16, 15 and later</p><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>Currently,  the version of Kubernetes in production is 1.19 and enabling cgroup v2 support for kubelet is proving to be challenging. While a comprehensive upgrade of Kubernetes is being researched and prepared, we are currently focusing on implementing cgroup v2 support specifically for kubelet. This approach allows for a shorter implementation time while laying the foundation for the subsequent comprehensive upgrade.</p><p>To enable cgroup v2 support, several adjustments need to be made to various components:</p><ol><li>Kernel Version: We are currently using kernel version 5.15, which meets the minimum requirement for cgroup v2 (4.14). However, it is recommended to use kernel version 5.2 or newer due to the lack of freezer support in older versions.</li><li>Systemd and Runc: It is highly recommended to run runc with the systemd cgroup driver (<code>runc --systemd-cgroup</code>), although it is not mandatory. To ensure compatibility, it is recommended to use systemd version 244 or later, as older versions do not support delegation of the <code>cpuset</code> controller.</li><li>Kubelet : The vendor for kubelet needs to upgrade the runc version. Currently, the latest fully supported version of runc for cgroup v2 is rc93. To minimize changes, we have chosen rc94 and modified the kubelet code to internally maintain runc rc94. This allows us to merge the necessary cgroup v2-related pull requests. However, in rc95, there are significant changes to the cgroup.Manager interface, which does not align with the principle of minimal changes.</li></ol><p>metrics-server retrieves resource usage information of nodes and pods using kubelet summary and other interfaces. This data is crucial for Horizontal Pod Autoscaling (HPA) based on resource scaling. To eliminate dockershim, the kubelet should utilize the systemd cgroup driver and configure the runtime accordingly.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><ol start="4"><li>Containerd : starting from version 1.4, containerd supports cgroup v2. We have successfully validated the removal of dockershim and conducted thorough testing of business operations in the testing environment. With the successful testing, we will proceed with the production rollout. Similar to kubelet, containerd also requires the systemd cgroup driver. Use the following configuration for the systemd cgroup driver:</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc]</span><br><span class="line">  ...</span><br><span class="line">  [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc.options]</span><br><span class="line">    SystemdCgroup = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>The configuration of the sandbox image and registry can be customized based on specific requirements.</p><h3 id="Systemd-with-cilium"><a href="#Systemd-with-cilium" class="headerlink" title="Systemd with cilium"></a>Systemd with cilium</h3><p>SystemD versions greater than 245 automatically set the rp_filter value to 1 for all network interfaces. This conflicts with Cilium, which requires rp_filter to be 0 on its interfaces, leading to a disruption in out-of-node IPv4 traffic.</p><p>Therefore, it is crucial to exercise caution before upgrading SystemD, as demonstrated by the failure experienced by Datadog, which served as a significant warning.</p><p>on March 8, 2023, at 06:00 UTC, a security update to systemd was automatically applied to several VMs, which caused a latent adverse interaction in the network stack (on Ubuntu 22.04 via systemd v249) to manifest upon systemd-networkd restarting.Namely, <code>systemd-networkd</code> forcibly deleted the routes managed by the Container Network Interface (CNI) plugin (Cilium) we use for communication between containers. This caused the affected nodes to go offline.</p><p>Additionally, when container runtimes are configured with cgroup v2, the Cilium agent pod is deployed in a separate cgroup namespace. For example, Docker container runtime with cgroupv2 support defaults to private cgroup namespace mode. Due to cgroup namespaces, the Cilium pod’s cgroup filesystem points to a virtualized hierarchy instead of the host cgroup root. Consequently, BPF programs are attached to the nested cgroup root, rendering socket load balancing ineffective for other pods. To address this limitation, work is being done in the Cilium project  to revisit assumptions made around cgroup hierarchies and enable socket load balancing in different environments.</p><p>Don’t worry, these issues have already been fixed by the community.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>During the testing process of Cgroup V2 and removing dockershim in our testing environment, we conducted extensive adaptation and stability tests. Our long-term analysis revealed that the benefits of adopting this technology roadmap far outweigh the initial investment. As part of our plan, we intend to promote the adoption of Cgroup V2-based machines in production. This will involve a meticulous testing and validation process, followed by a gradual rollout in production environments. We will start with offline applications such as logging and big data applications.</p><p>The Cloud Native Computing Foundation’s flagship conference gathers adopters and technologists from leading open source and cloud native communities in Shanghai, China from 26-28 September, 2023.  We are considering submitting a proposal for a presentation at KubeCon 2023, where we will have the opportunity to share the latest developments and insights with the conference attendees.</p><p><a href="https://bugs.openjdk.org/browse/JDK-8230305">https://bugs.openjdk.org/browse/JDK-8230305</a></p><p><a href="https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/">https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</a></p><p><a href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html</a></p><p><a href="https://github.com/cilium/cilium/issues/10645">https://github.com/cilium/cilium/issues/10645</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;With the announcement of CentOS discontinuation by the CentOS community , along with the set dates for service termination, we have put t</summary>
      
    
    
    
    <category term="cgroupv2" scheme="https://zoues.com/categories/cgroupv2/"/>
    
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
    <category term="cncf" scheme="https://zoues.com/tags/cncf/"/>
    
    <category term="almalinux" scheme="https://zoues.com/tags/almalinux/"/>
    
  </entry>
  
  <entry>
    <title>Ziglang简明教程</title>
    <link href="https://zoues.com/posts/5941d835/"/>
    <id>https://zoues.com/posts/5941d835/</id>
    <published>2024-01-14T05:40:08.000Z</published>
    <updated>2024-01-21T02:30:15.984Z</updated>
    
    <content type="html"><![CDATA[<p>这份zig简明教程适合已经有编程基础知识的同学快速了解zig语言，同时也适合没有编程经验但是懂得善用搜索引擎的同学,该文章详细介绍Zig编程语言各种概念，主要包括基础知识、函数、结构体、枚举、数组、切片、控制结构、错误处理、指针、元编程和堆管理等内容。</p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>命令 <code>zig run my_code.zig</code> 将编译并立即运行你的 Zig 程序。每个单元格都包含一个 Zig 程序，你可以尝试运行它们（其中一些包含编译时错误，你可以注释掉后再尝试）。</p><p>首先需要声明一个 <code>main()</code> 函数来运行代码。</p><p>下面的代码什么都不会做，只是简单的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// comments look like this and go to the end of the line</span><br><span class="line">pub fn main() void &#123;&#125;</span><br></pre></td></tr></table></figure><p>可以使用 内置函数<code>@import</code> 导入标准库，并将命名空间赋值给一个 <code>const</code> 值。Zig 中的几乎所有东西都必须明确地被赋予标识符。你也可以通过这种方式导入其他 Zig 文件，类似地，你可以使用 <code>@cImport</code> 导入 C 文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;hello world!\n&quot;, .&#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：后续会在结构部分部分解释 <code>print</code> 语句中的第二个参数。</p><p>一般用<code>var</code> 来声明变量，同时在大多数情况下，需要带上声明变量类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 47; // declares &quot;x&quot; of type i32 to be 47.</span><br><span class="line">    std.debug.print(&quot;x: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>const</code> 声明一个变量的值是不可变的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pub fn main() void &#123;</span><br><span class="line">    const x: i32 = 47;</span><br><span class="line">    x = 42; // error: cannot assign to constant</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig 非常严苛，不允许你从外部作用域屏蔽标识符，以防止混淆：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const x: i32 = 47;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 42;  // error: redefinition of &#x27;x&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>全局作用域的常量默认为编译时的 “comptime” 值，如果省略了类型，它们就是编译时类型，并且可以在运行时转换为运行时类型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const x: i32 = 47;</span><br><span class="line">const y = -47;  // comptime integer.</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var a: i32 = y; // comptime constant coerced into correct type</span><br><span class="line">    var b: i64 = y; // comptime constant coerced into correct type</span><br><span class="line">    var c: u32 = y; // error: cannot cast negative value -47 to unsigned integer</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果希望在后面设置它，也可以明确选择将其保留为未定义。如果你在调试模式下意外使用它引发错误，Zig 将使用 0XAA 字节填充一个虚拟值，以帮助检测错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">  var x: i32 = undefined;</span><br><span class="line">  std.debug.print(&quot;undefined: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在某些情况下，如果 Zig 可以推断出类型信息，才允许你省略类型信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i32 = 47;</span><br><span class="line">    var y: i32 = 47;</span><br><span class="line">    var z = x + y; // declares z and sets it to 94.</span><br><span class="line">    std.debug.print(&quot;z: &#123;&#125;\n&quot;, .&#123;z&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是需要注意，整数字面值是编译时类型，所以下面的示例是行不通的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x = 47; // error: variable of type &#x27;comptime_int&#x27; must be const or comptime</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>函数可以带参数和返回值，使用<code>fn</code>关键字声明。<code>pub</code>关键字表示函数可以从当前作用域导出，使其它地方可以调用。下面示例是一个不返回任何值的函数（foo）。<code>pub</code>关键字表示该函数可以从当前作用域导出，这就是为什么<code>main</code>函数必须是<code>pub</code>的。你可以像大多数编程语言中一样调用函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() void &#123;</span><br><span class="line">    std.debug.print(&quot;foo!\n&quot;, .&#123;&#125;);</span><br><span class="line"></span><br><span class="line">    //optional:</span><br><span class="line">    return;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面示例是一个返回整数值的函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var result = foo();</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;result&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig不允许你忽略函数的返回值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo(); // error: expression value is ignored</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是你可以将其赋值给丢弃变量 <code>_</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fn foo() i32 &#123;</span><br><span class="line">    return 47;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">  _ = foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以声明函数时带上参数的类型，这样函数调用时可以传入参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(x: i32) void &#123;</span><br><span class="line">    std.debug.print(&quot;foo param: &#123;&#125;\n&quot;, .&#123;x&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    foo(47);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><p>结构体通过使用<code>const</code>关键字分配一个名称来声明，它们的赋值顺序可以是任意的，并且可以使用常规的点语法进行解引用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Vec2 = struct &#123;</span><br><span class="line">    x: f64,</span><br><span class="line">    y: f64</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var v = Vec2&#123;.y = 1.0, .x = 2.0&#125;;</span><br><span class="line">    std.debug.print(&quot;v: &#123;&#125;\n&quot;, .&#123;v&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结构体可以有默认值；结构体也可以是匿名的，并且可以强制转换为另一个结构体，只要所有的值都能确定：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const Vec3 = struct&#123;</span><br><span class="line">    x: f64 = 0.0,</span><br><span class="line">    y: f64,</span><br><span class="line">    z: f64</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var v: Vec3 = .&#123;.y = 0.1, .z = 0.2&#125;;  // ok</span><br><span class="line">    var w: Vec3 = .&#123;.y = 0.1&#125;; // error: missing field: &#x27;z&#x27;</span><br><span class="line">    std.debug.print(&quot;v: &#123;&#125;\n&quot;, .&#123;v&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以将函数放入结构体中，使其像面向对象编程中的对象一样工作。这里有一个语法糖，如果你定义的函数的第一个参数为对象的指针，我们称之为”面向对象编程”，类似于Python带self参数的函数。一般约定是通过将变量命名为self来表示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const LikeAnObject = struct&#123;</span><br><span class="line">    value: i32,</span><br><span class="line"></span><br><span class="line">    fn print(self: *LikeAnObject) void &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;self.value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var obj = LikeAnObject&#123;.value = 47&#125;;</span><br><span class="line">    obj.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们一直传递给<code>std.debug.print</code>的第二个参数是一个元组，它是一个带有数字字段的匿名结构体。在编译时，<code>std.debug.print</code>会找出元组中参数的类型，并生成一个针对你提供的参数字符串的版本，这就是为何Zig知道如何将打印的内容变得漂亮的原因。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;&#123;&#125;\n&quot;, .&#123;1, 2&#125;); #  error: Unused arguments</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h3><p>枚举通过使用<code>const</code>关键字将枚举组以类型方式来声明。</p><p>注意：在某些情况下，可以简化枚举的名称。 其可以将枚举的值设置为整数，但它不会自动强制转换，你必须使用<code>@enumToInt</code>或<code>@intToEnum</code>来进行转换。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const EnumType = enum&#123;</span><br><span class="line">    EnumOne,</span><br><span class="line">    EnumTwo,</span><br><span class="line">    EnumThree = 3</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;One: &#123;&#125;\n&quot;, .&#123;EnumType.EnumOne&#125;);</span><br><span class="line">    std.debug.print(&quot;Two?: &#123;&#125;\n&quot;, .&#123;EnumType.EnumTwo == .EnumTwo&#125;);</span><br><span class="line">    std.debug.print(&quot;Three?: &#123;&#125;\n&quot;, .&#123;@enumToInt(EnumType.EnumThree) == 3&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数组和切片"><a href="#数组和切片" class="headerlink" title="数组和切片"></a>数组和切片</h3><p>Zig有数组概念，它们是具有在编译时已知长度的连续内存。你可以通过在前面声明类型并提供值列表来初始化它们，同时可以通过数组的<code>len</code>字段访问它们的长度。</p><p>注意：Zig中的数组也是从零开始索引的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array: [3]u32 = [3]u32&#123;47, 47, 47&#125;;</span><br><span class="line"></span><br><span class="line">    // also valid:</span><br><span class="line">    // var array = [_]u32&#123;47, 47, 47&#125;;</span><br><span class="line"></span><br><span class="line">    var invalid = array[4]; // error: index 4 outside array of size 3.</span><br><span class="line">    std.debug.print(&quot;array[0]: &#123;&#125;\n&quot;, .&#123;array[0]&#125;);</span><br><span class="line">    std.debug.print(&quot;length: &#123;&#125;\n&quot;, .&#123;array.len&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟golang类似，Zig也有切片（slices），它们的长度在运行时已知。你可以使用切片操作从数组或其他切片构造切片。与数组类似，切片有一个<code>len</code>字段，告诉它的长度。</p><p>注意：切片操作中的间隔参数是开口的（不包含在内）。 尝试访问超出切片范围的元素会引发运行时panic。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array: [3]u32 = [_]u32&#123;47, 47, 47&#125;;</span><br><span class="line">    var slice: []u32 = array[0..2];</span><br><span class="line"></span><br><span class="line">    // also valid:</span><br><span class="line">    // var slice = array[0..2];</span><br><span class="line"></span><br><span class="line">    var invalid = slice[3]; // panic: index out of bounds</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;slice[0]: &#123;&#125;\n&quot;, .&#123;slice[0]&#125;);</span><br><span class="line">    std.debug.print(&quot;length: &#123;&#125;\n&quot;, .&#123;slice.len&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>字符串文字是以null结尾的utf-8编码的const u8字节数组。Unicode字符只允许在字符串文字和注释中使用。</p><p>注意：长度不包括null终止符（官方称为”sentinel termination”）。 访问null终止符是安全的。 索引是按字节而不是Unicode字符。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const string = &quot;hello 世界&quot;;</span><br><span class="line">const world = &quot;world&quot;;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var slice: []const u8 = string[0..5];</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;string &#123;&#125;\n&quot;, .&#123;string&#125;);</span><br><span class="line">    std.debug.print(&quot;length &#123;&#125;\n&quot;, .&#123;world.len&#125;);</span><br><span class="line">    std.debug.print(&quot;null &#123;&#125;\n&quot;, .&#123;world[5]&#125;);</span><br><span class="line">    std.debug.print(&quot;slice &#123;&#125;\n&quot;, .&#123;slice&#125;);</span><br><span class="line">    std.debug.print(&quot;huh? &#123;&#125;\n&quot;, .&#123;string[0..7]&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>const数组可以强制转换为const切片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo() []const u8 &#123;  // note function returns a slice</span><br><span class="line">    return &quot;foo&quot;;      // but this is a const array.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;foo()&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><p>Zig提供了与其他语言类似的if语句、switch语句、for循环和while循环。示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) []const u8 &#123;</span><br><span class="line">    if (v &lt; 0) &#123;</span><br><span class="line">        return &quot;negative&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        return &quot;non-negative&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;positive &#123;&#125;\n&quot;, .&#123;foo(47)&#125;);</span><br><span class="line">    std.debug.print(&quot;negative &#123;&#125;\n&quot;, .&#123;foo(-47)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>switch方式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) []const u8 &#123;</span><br><span class="line">    switch (v) &#123;</span><br><span class="line">        0 =&gt; return &quot;zero&quot;,</span><br><span class="line">        else =&gt; return &quot;nonzero&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    std.debug.print(&quot;47 &#123;&#125;\n&quot;, .&#123;foo(47)&#125;);</span><br><span class="line">    std.debug.print(&quot;0 &#123;&#125;\n&quot;, .&#123;foo(0)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>for-loop</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array = [_]i32&#123;47, 48, 49&#125;;</span><br><span class="line"></span><br><span class="line">    for (array) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;array &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    for (array) | value, index | &#123;</span><br><span class="line">        std.debug.print(&quot;array &#123;&#125;:&#123;&#125;\n&quot;, .&#123;index, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var slice = array[0..2];</span><br><span class="line"></span><br><span class="line">    for (slice) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;slice &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    for (slice) | value, index | &#123;</span><br><span class="line">        std.debug.print(&quot;slice &#123;&#125;:&#123;&#125;\n&quot;, .&#123;index, value&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while loop</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var array = [_]i32&#123;47, 48, 49&#125;;</span><br><span class="line">    var index: u32 = 0;</span><br><span class="line"></span><br><span class="line">    while (index &lt; 2) &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;array[index]&#125;);</span><br><span class="line">        index += 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>错误是特殊的联合类型，你可以在函数前面加上 ! 来表示该函数可能返回错误。你可以通过简单地将错误作为正常返回值返回来抛出错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError,</span><br><span class="line">    OtherError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    return MyError.GenericError;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn wrap_foo(v: i32) void &#123;</span><br><span class="line">    if (foo(v)) |value| &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125; else |err| &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你编写一个可能出错的函数，当它返回时你必须决定如何处理错误。两个常见的选择是 <code>try</code> 和 <code>catch</code>。<code>try</code> 方式很摆烂，它只是简单地将错误转发为函数的错误。而 <code>catch</code> 需要处理错误。</p><p><code>try</code> 其实就是 <code>catch | err | &#123;return err&#125;</code> 的语法糖。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) !i32 &#123;</span><br><span class="line">    if (v == 42) return MyError.GenericError;</span><br><span class="line">    return v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // catch traps and handles errors bubbling up</span><br><span class="line">    _ = foo(42) catch |err| &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    // try won&#x27;t get activated here.</span><br><span class="line">    std.debug.print(&quot;foo: &#123;&#125;\n&quot;, .&#123;try foo(47)&#125;);</span><br><span class="line"></span><br><span class="line">    // this will ultimately cause main to print an error trace and return nonzero</span><br><span class="line">    _ = try foo(42);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们也可以使用 <code>if</code> 来检查错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line">const MyError = error&#123;</span><br><span class="line">    GenericError</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">fn foo(v: i32) !i32 &#123;</span><br><span class="line">    if (v == 42) return MyError.GenericError;</span><br><span class="line">    return v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// note that it is safe for wrap_foo to not have an error ! because</span><br><span class="line">// we handle ALL cases and don&#x27;t return errors.</span><br><span class="line">fn wrap_foo(v: i32) void &#123;    </span><br><span class="line">    if (foo(v)) | value | &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    &#125; else | err | &#123;</span><br><span class="line">        std.debug.print(&quot;error: &#123;&#125;\n&quot;, .&#123;err&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    wrap_foo(42);</span><br><span class="line">    wrap_foo(47);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><p>Zig使用<code>*</code>表示指针类型，可以通过<code>.*</code>语法访问指针指向的值。示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn printer(value: *i32) void &#123;</span><br><span class="line">    std.debug.print(&quot;pointer: &#123;&#125;\n&quot;, .&#123;value&#125;);</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;value.*&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    printer(&amp;value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：在Zig中，指针需要正确对齐到它所指向的值的对齐方式。 对于结构体，类似于Java，您可以解引用指针并一次获取字段，使用 . 运算符。需要注意的是，这仅适用于一层间接引用，因此如果您有指向指针的指针，您必须首先解引用外部指针。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">const MyStruct = struct &#123;</span><br><span class="line">    value: i32</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">pub fn printer(s: *MyStruct) void &#123;</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;s.value&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value = MyStruct&#123;.value = 47&#125;;</span><br><span class="line">    printer(&amp;value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Zig允许任何类型（不仅仅是指针）可为空，但请注意它们是基本类型和特殊值 null 的联合体。要访问未包装的可选类型，请使用 .? 字段：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    var vptr: ?*i32 = &amp;value;</span><br><span class="line">    var throwaway1: ?*i32 = null;</span><br><span class="line">    var throwaway2: *i32 = null; // error: expected type &#x27;*i32&#x27;, found &#x27;(null)&#x27;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;vptr.*&#125;); // error: attempt to dereference non-pointer type</span><br><span class="line">    std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;vptr.?.*&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：当我们使用来自C ABI函数的指针时，它们会自动转换为可为空指针。 获得未包装的可选指针的另一种方法是使用 if 语句：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn nullChoice(value: ?*i32) void &#123;</span><br><span class="line">    if (value) | v | &#123;</span><br><span class="line">        std.debug.print(&quot;value: &#123;&#125;\n&quot;, .&#123;v.*&#125;);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        std.debug.print(&quot;null!\n&quot;, .&#123;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var value: i32 = 47;</span><br><span class="line">    var vptr1: ?*i32 = &amp;value;</span><br><span class="line">    var vptr2: ?*i32 = null;</span><br><span class="line"></span><br><span class="line">    nullChoice(vptr1);</span><br><span class="line">    nullChoice(vptr2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="元编程"><a href="#元编程" class="headerlink" title="元编程"></a>元编程</h3><p>Zig的元编程受几个基本概念驱动：</p><ul><li><p>类型在编译时是有效的值。</p></li><li><p>大多数运行时代码在编译时也能工作。</p></li><li><p>结构体字段的评估是编译时的鸭子类型（duck-typed）。</p></li><li><p>Zig标准库提供了执行编译时反射的工具。</p><p>下面是元编程的一个示例：</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn foo(x : anytype) @TypeOf(x) &#123;</span><br><span class="line">    // note that this if statement happens at compile-time, not runtime.</span><br><span class="line">    if (@TypeOf(x) == i64) &#123;</span><br><span class="line">        return x + 2;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return 2 * x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var x: i64 = 47;</span><br><span class="line">    var y: i32 =  47;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;i64-foo: &#123;&#125;\n&quot;, .&#123;foo(x)&#125;);</span><br><span class="line">    std.debug.print(&quot;i32-foo: &#123;&#125;\n&quot;, .&#123;foo(y)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下是泛型类型的一个示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">fn Vec2Of(comptime T: type) type &#123;</span><br><span class="line">    return struct&#123;</span><br><span class="line">        x: T,</span><br><span class="line">        y: T</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">const V2i64 = Vec2Of(i64);</span><br><span class="line">const V2f64 = Vec2Of(f64);</span><br><span class="line"></span><br><span class="line">pub fn main() void &#123;</span><br><span class="line">    var vi = V2i64&#123;.x = 47, .y = 47&#125;;</span><br><span class="line">    var vf = V2f64&#123;.x = 47.0, .y = 47.0&#125;;</span><br><span class="line">    </span><br><span class="line">    std.debug.print(&quot;i64 vector: &#123;&#125;\n&quot;, .&#123;vi&#125;);</span><br><span class="line">    std.debug.print(&quot;f64 vector: &#123;&#125;\n&quot;, .&#123;vf&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过这些概念，我们可以构建非常强大的泛型类型！</p><h3 id="堆管理"><a href="#堆管理" class="headerlink" title="堆管理"></a>堆管理</h3><p>Zig为我们提供了与堆交互的多种方式，通常要求您明确选择使用哪种方式。它们都遵循下述相同的模式：</p><ol><li>创建一个分配器工厂结构体。</li><li>检索由分配器工厂创建的<code>std.mem.Allocator</code>结构体。</li><li>使用<code>alloc/free</code>和<code>create/destroy</code>函数来操作堆。</li><li>（可选）销毁分配器工厂。</li></ol><p>这么处理的目的是：</p><ul><li>为了阻止您过度使用堆。</li><li>这使得调用堆的任何东西（基本上是可失败的操作）都是显式的。</li><li>您可以仔细调整权衡，并使用标准数据结构而无需重写标准库。</li><li>您可以在测试中运行非常安全的分配器，并在发布&#x2F;生产环境中切换到不同的分配器。</li></ul><p>好的，但是你也可以偷点懒。你是不是想一直使用jemalloc？ 只需选择一个全局分配器，并在所有地方使用它（请注意，某些分配器是线程安全的，而某些则不是）。</p><p>在这个示例中，我们将使用<code>std.heap.GeneralPurposeAllocator</code>工厂创建一个具有多种特性（包括泄漏检测）的分配器，并看看它是如何组合在一起的。</p><p>最后一件事，这里使用了<code>defer</code>关键字，它非常类似于Go语言中的<code>defer</code>关键字！还有一个<code>errdefer</code>关键字，如果需要了解更多信息，请查阅Zig文档。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">const std = @import(&quot;std&quot;);</span><br><span class="line"></span><br><span class="line">// factory type</span><br><span class="line">const Gpa = std.heap.GeneralPurposeAllocator(.&#123;&#125;);</span><br><span class="line"></span><br><span class="line">pub fn main() !void &#123;</span><br><span class="line">    // instantiates the factory</span><br><span class="line">    var gpa = Gpa&#123;&#125;;</span><br><span class="line">    </span><br><span class="line">    // retrieves the created allocator.</span><br><span class="line">    var galloc = &amp;gpa.allocator;</span><br><span class="line">    </span><br><span class="line">    // scopes the lifetime of the allocator to this function and</span><br><span class="line">    // performs cleanup; </span><br><span class="line">    defer _ = gpa.deinit();</span><br><span class="line"></span><br><span class="line">    var slice = try galloc.alloc(i32, 2);</span><br><span class="line">    // uncomment to remove memory leak warning</span><br><span class="line">    // defer galloc.free(slice);</span><br><span class="line">    </span><br><span class="line">    var single = try galloc.create(i32);</span><br><span class="line">    // defer gallo.destroy(single);</span><br><span class="line"></span><br><span class="line">    slice[0] = 47;</span><br><span class="line">    slice[1] = 48;</span><br><span class="line">    single.* = 49;</span><br><span class="line"></span><br><span class="line">    std.debug.print(&quot;slice: [&#123;&#125;, &#123;&#125;]\n&quot;, .&#123;slice[0], slice[1]&#125;);</span><br><span class="line">    std.debug.print(&quot;single: &#123;&#125;\n&quot;, .&#123;single.*&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>现在我们已经掌握了相当大的Zig基础知识。没有覆盖的一些（非常重要的）内容包括：</p><ul><li>测试（Zig使得编写测试非常容易）</li><li>标准库</li><li>内存模型（Zig在分配器方面没有倾向性）</li><li>异步编程（Zig 的异步特性在编译器中出现了性能退化，在 0.11 版本的 Zig 中已经不存在了，并且在 Zig 0.12 版本中也可能不会出现。）</li><li>交叉编译</li><li><code>build.zig</code> 文件</li></ul><p>如果想要了解更多细节，请查阅最新的文档：</p><ul><li><a href="https://ziglang.org/documentation/master/">https://ziglang.org/documentation/master/</a></li><li><a href="https://gist.github.com/ityonemo/769532c2017ed9143f3571e5ac104e50">https://gist.github.com/ityonemo/769532c2017ed9143f3571e5ac104e50</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这份zig简明教程适合已经有编程基础知识的同学快速了解zig语言，同时也适合没有编程经验但是懂得善用搜索引擎的同学,该文章详细介绍Zig编程语言各种概念，主要包括基础知识、函数、结构体、枚举、数组、切片、控制结构、错误处理、指针、元编程和堆管理等内容。&lt;/p&gt;
&lt;h3 id</summary>
      
    
    
    
    <category term="Ziglang简明教程" scheme="https://zoues.com/categories/Ziglang%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="zig" scheme="https://zoues.com/tags/zig/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI关于Kubernetes集群近万节点的生产实践</title>
    <link href="https://zoues.com/posts/1df3dc63/"/>
    <id>https://zoues.com/posts/1df3dc63/</id>
    <published>2021-01-28T00:40:08.000Z</published>
    <updated>2024-01-20T12:01:27.441Z</updated>
    
    <content type="html"><![CDATA[<p>OpenAI已经将Kubernetes集群规模扩展至7500个节点，为大型神经网络模型（如GPT-3，CLIP和DALL·E）及小型实验性研究提供了可扩展的基础架构。 很少将单个Kubernetes集群扩展到如此规模，为此进行了一些必要的改进，但好处是单一的基础架构使我们的机器学习研究团队可以在不修改代码的前提下，快速扩展以缩短实验时间、加速研发进度。</p><hr><p>作者：Benjamin Chess、Eric Sigler</p><p>译者：zouyee</p><p>原文：<a href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/">https://openai.com/blog/scaling-kubernetes-to-7500-nodes/</a></p><p><img src="https://s3.ax1x.com/2021/01/28/yS7JHA.png"></p><p>自上一篇有关扩展到2500个节点的文章以来，我们一直在不断扩展基础架构以满足研究人员的需求，并在此过程中学习了许多其他相关知识。 该篇文章总结了相关经验，以便Kubernetes社区中的其他人可以从中受益，接下来介绍，需要解决的问题。</p><h4 id="一、工作负载"><a href="#一、工作负载" class="headerlink" title="一、工作负载"></a>一、工作负载</h4><p>首先需要说明的是，针对工作负载，我们在Kubernetes集群上运行的应用程序和硬件与其他公司中的场景完全不同。我们面临的问题和相应的解决方案可能与读者所处的实际场景不是太一致。</p><p>大型的机器学习作业可以访问多个节点，及每个节点上的所有硬件资源，因此运行效率最高。允许GPU使用NVLink进行交叉通信，或者GPU使用GPUDirect与NIC通信。因此，对于我们的许多工作负载，单个pod占据了整个节点，因此调度不涉及任何NUMA，CPU或PCIE资源抢占。当前的集群具有完整的双向带宽互通，因此无需考虑任何网络拓扑。因此，调度程序的压力相对较低。</p><p>因为一个新的任务可能包含数百个Pod调度的需求，kube-scheduler存在毛刺现象。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS7GBd.png"></p><p>最大的job是运行MPI（并行计算），job中的所有Pod都工作在同一个MPI通信器中。任何Pod的消亡，都会导致整个job暂停，并重新启动。job定期备份相关信息（即checkpoint），在重新启动时从最近的备份信息处恢复。</p><p>我们不完全依赖Kubernetes进行负载平衡。我们的七层流量很少，因为不需要进行A &#x2F; B测试，蓝绿升级或金丝雀发布等。 Pod通过SSH与其他Pod的MPI直接通信(这部分貌似有点疑问)，而不是<code>service endpoint</code>。服务发现功能相对有限，因为我们只执行一次查找，即在工作启动时（pod刚参与MPI时）。</p><p>大多数job都与Blob类型存储进行交互，通常直接向Blob传输一些数据集的分片，或将其缓存到本地盘。我们也使用了一些PersistentVolumes，但是blob类型存储具有更好的伸缩性，并且不需要挂载、卸载操作。</p><p>超级计算团队努力致力于提供生产级别的计算基础架构，当前在该集群上运行的应用寿命较短，开发人员正在快速迭代中。任何时候都有可能出现新的应用场景，这需要我们对趋势进行预判，并做出适当折衷的设想。</p><hr><h4 id="二、网络"><a href="#二、网络" class="headerlink" title="二、网络"></a>二、网络</h4><p>随着集群中节点和Pod数量的增加，我们发现<code>Flannel</code>难以满足需求。转而使用主机pod网络技术进行Azure VMSSes和相关CNI插件的IP配置。这使我们能够在Pod上获得主机级别的网络吞吐量。</p><p>我们改用基于别名的IP寻址的另一个原因是，在我们最大的集群上，我们可能随时有大约200,000个IP地址正在使用。在测试基于路由的Pod网络时，我们发现路由数量存在明显的限制。</p><p>改造SDN或路由引擎虽然麻烦，但它会使我们的网络设置变得简单。无需任何其他适配器即可添加VPN或隧道。同时我们不必担心数据包分片，因为网络的某些部分的MTU较低。网络策略和流量监控非常简单；数据包的来源和目的地没有任何歧义。</p><p>我们在主机上使用iptables来跟踪每个命名空间和pod的网络资源使用情况。这使研究人员可以可视化其网络使用。由于我们的许多实验都具有独特的外部和Pod内部通信模式，因此对于调查可能出现瓶颈的位置很有用。</p><p>iptables mangle规则可用于标记任意符合特定条件的数据包。如下是我们用来检测流量是内部流量还是外部流量的规则。 FORWARD规则涵盖来自Pod的流量，以及来自主机的INPUT和OUTPUT流量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A INPUT ! -s 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-in&quot;</span><br><span class="line">iptables -t mangle -A FORWARD ! -s 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-in&quot;</span><br><span class="line">iptables -t mangle -A OUTPUT ! -d 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-out&quot;</span><br><span class="line">iptables -t mangle -A FORWARD ! -d 10.0.0.0/8 -m comment --comment &quot;iptables-exporter openai traffic=internet-out&quot;</span><br></pre></td></tr></table></figure><p>一旦标记，iptables将启动计数器以跟踪与此规则匹配的字节和数据包。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% iptables -t mangle -L -v</span><br><span class="line">Chain FORWARD (policy ACCEPT 50M packets, 334G bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">....</span><br><span class="line">1253K  555M            all  --  any    any     anywhere            !10.0.0.0/8           /* iptables-exporter openai traffic=internet-out */</span><br><span class="line">1161K 7937M            all  --  any    any    !10.0.0.0/8           anywhere             /* iptables-exporter openai traffic=internet-in */</span><br></pre></td></tr></table></figure><p>我们使用基于Prometheus的<code>iptables-exporter</code>的方案，然后将其接入到我们的监控系统。 </p><p><img src="https://s3.ax1x.com/2021/01/28/yS7tAI.png"></p><p>我们网络模型的一个特别的地方是，我们向研究人员公开了节点，容器和服务网络CIDR范围。 我们有一个辐射状网络模型，并使用本机节点和Pod CIDR范围来路由该流量。 研究人员连接到中枢节点，从那里可以访问任何单个集群。 但是集群本身无法相互通信。 这样可以确保集群间相互隔离，且没有跨集群的依存关系以破坏隔离（译者表示…）。</p><p>我们使用主机<code> NAT</code>来转换服务网络CIDR，以处理来自集群外部的流量。 这种设置使我们的研究人员在选择实验方式和选择哪种网络配置上具有极大的灵活性。</p><hr><h5 id="三、API-Server"><a href="#三、API-Server" class="headerlink" title="三、API Server"></a>三、API Server</h5><p>Kubernetes的API Server和etcd集群是集群健康运行的关键组件，因此我们特别注意这些系统上的压力。 我们使用kube-prometheus项目提供的Grafana以及其他内部仪表板。 我们发现针对API Server的HTTP（如429、5xx等状态）告警还是很有效的。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS78nH.png"></p><p>尽管大多数人在k8s集群内运行API Server，但我们选择在集群外运行。 etcd和API Server服务都在它们自己的专用节点上运行。 我们最大的集群运行了5个API  Server和5个etcd节点，以分散负载并最大程度地降低影响（如果其中一台发生故障）。 自从我们在上一篇文章中将Kubernetes Events写入到其他etcd集群以来，我们在etcd方面没有遇到任何麻烦。 API Server是无状态的，通常很容易在自愈实例组或规模集中运行。 我们尚未尝试建立etcd集群的任何自愈等自动化功能。</p><p>API  Server会占用相当大的内存，并且会随着集群中节点的数量线性上升。 对于具有7500个节点的集群，我们观察到每个API Server最多使用了70GB。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS71je.png"></p><p>API Server上的另一大压力是API上的WATCH能力，例如<code>kubelet</code>和<code> node-exporter</code>。 当从集群中添加或删除节点时，将触发此WATCH。 并且由于通常每个节点本身都通过<code>kube-proxy</code>监视<code>kubelet</code>服务（译者：可通过本地LB优化，并分配固定几个Master），因此这些响应所需的带宽为节点的二次方，有时甚至达到1GB &#x2F; s或更高。 在Kubernetes 1.17中的EndpointSlices特性带来巨大的优化，使此负载降低了1000倍。</p><p><img src="https://s3.ax1x.com/2021/01/28/yS7NNt.png"></p><p>通常，我们密切关注任何随集群大小扩展的API Server请求。 我们尝试避免让任何DaemonSet与API Server进行交互。 在确实需求更改所有节点的监控组件时，引入中间缓存服务（例如Datadog Cluster Agent）似乎成了一种避免集群范围瓶颈的最佳实践。</p><p>随着集群数量的增长，我们对集群的自动伸缩操作逐步减少。 有时自动伸缩超标时，我们就会遇到麻烦。 当新节点加入集群时，就会产生许多请求，并且一次添加数百个节点可能会使API Server服务过载。</p><hr><h5 id="四、监控"><a href="#四、监控" class="headerlink" title="四、监控"></a>四、监控</h5><p>我们使用Prometheus收集指标，并使用Grafana配置图形界面，管理仪表板和警报。我们从部署<code>kube-prometheus</code>项目开始，该项目收集各种指标，并提供良好的仪表板以完成可视化。随着时间的推移，我们添加了许多自己特有的仪表板，指标和警报。</p><p>随着节点日益增多，我们发现Prometheus收集的大量指标毫无用处。尽管kube-prometheus公开了许多有用的数据，但其中有部分我们从未使用过。我们使用Prometheus接口<code>删除</code>其中的某些指标。</p><p>一段时间以来，我们一直在努力解决一个问题，即Prometheus会消耗越来越多的内存，直到最终OOM。即使在设置了超大内存容量之后，这种情况似乎仍会发生（译者：该问题应该是发生在旧版本）。更糟糕的是，当它崩溃时，启动后需要花费很多时间进行恢复。</p><p>最终，我们找到了这些OOM的来源，是Grafana和Prometheus之间的交互，其中Grafana调用Prometheus接口<code>/api/v1/series</code>查询。<code> /api/v1/series</code>接口获取所有监控指标，这将带来内存的持续增长。我们改进了Prometheus，使其在Context中包含此超时控制。</p><p>虽然Prometheus崩溃的频率降低了很多，但在确实需要重新启动它的时候，WAL恢复仍然是一个问题。在Prometheus收集新指标和为查询提供服务之前，通常需要花费很长时间来恢复所有WAL日志。在Robust Perception的帮助下，我们发现通过配置GOMAXPROCS &#x3D; 24进行优化。 Prometheus会在WAL重放期间尝试使用所有内核，而对于具有大量内核的服务器来说，抢占会削减性能。</p><hr><h5 id="五、健康检查"><a href="#五、健康检查" class="headerlink" title="五、健康检查"></a>五、健康检查</h5><p>对于规模如此大的集群，当然需要依靠自动化来检测和删除集群中行为异常的节点。 随之逐步深入，我们已经建立了一套完善的健康检查系统。</p><h5 id="a-被动检查"><a href="#a-被动检查" class="headerlink" title="a. 被动检查"></a>a. 被动检查</h5><p>（译者：可以将之称为性能监控）某些运行状况检查是被动的，始终在所有节点上运行。它们监视基本的系统资源，例如网络可达性，磁盘损坏或磁盘已满或GPU错误等。 GPU会出现多种不同的问题，但一个比较常见的错误是<code>无法纠正的ECC错误</code>。 Nvidia的数据中心GPU管理器（DCGM）工具使查询此错误和许多其他<code>Xid</code>错误变得容易了许多。我们跟踪这些错误的一种方法是通过<code>dcgm-exporter</code>将指标抓取到我们的监控系统Prometheus中。其为DCGM_FI_DEV_XID_ERRORS指标。此外，NVML设备查询API公开了有关GPU的运行状况和操作的详细信息。</p><p>一旦我们检测到错误，通常可以通过重置GPU或系统来修复它们。</p><p>健康检查的另一种形式是跟踪来自上游云提供商的维护事件。大多数云提供商都提供了一种方法来了解当前虚拟机是否由于即将发生的维护事件而导致的中断。如安装升级补丁、替换硬件等。</p><p>这些被动运行的监控运行在所有节点上。如果健康检查开始失败，该节点将自动建立报警，对于更严重的健康检查故障，我们还将尝试驱逐容器，该操组由Pod本身决定，可以通过Pod Disruption Budget进行配置，以决定是否允许这种驱逐。</p><h5 id="b-GPU动态测试"><a href="#b-GPU动态测试" class="headerlink" title="b. GPU动态测试"></a>b. GPU动态测试</h5><p>不幸的是，并非所有GPU问题都表现为通过DCGM可见的错误代码。我们已经建立了自己的测试库，这些测试库可以利用GPU来捕获其他问题，并确保硬件和驱动程序的运行情况符合预期。这些测试无法在后台运行，它们需要在几秒钟或几分钟内独占GPU。</p><p>所有节点都以<code>preflight</code>污点和标签加入集群。此污点会阻止在节点上调度常规Pod。将DaemonSet配置为在带有此标签的节点上运行预检测试Pod。成功完成测试后，测试本身将去除<code>preflight</code>污点和标签，然后该节点即可用于常规用途。</p><p>随后，我们将在节点的生命周期内定期运行这些测试。我们以CronJob方式运行，使其可以在群集中的任何可用节点上运行。</p><hr><h5 id="五、资源配额及用量"><a href="#五、资源配额及用量" class="headerlink" title="五、资源配额及用量"></a>五、资源配额及用量</h5><p>随着我们集群规模的不断扩大，然而研究人员开始发现自己难以获得分配的所有容量。 传统的调度系统具有许多不同的能力以确保团队之间公平地运行任务，而Kubernetes则没有。我们从这些调度系统中获得了灵感，并以Kubernetes原生的方式构建了一些功能。</p><p><em><strong>污点</strong></em></p><p>我们在每个集群中都有一个服务，即<code>team-resource-manager</code>，它具有多种功能。 它的数据源是ConfigMap，它为在给定集群中具有容量的所有研究团队指定元组（节点选择器，要应用的团队标签，分配数量）。 它使用openai.com&#x2F;team&#x3D;teamname:NoSchedule调整适当数量的节点。</p><p><code>team-resource-manager</code>还配置一个<code>admission webhook</code>(译者：即准入服务插件)服务，以便在提交每个作业时，根据提交者的团队成员身份应用相应的容忍度。 通过使用污点，我们可以灵活地约束Kubernetes Pod Scheduler，例如允许对优先级较低的Pod允许<code>任意</code>容忍，这允许团队在无需强力协调的情况下资源共享。</p><p><em><strong>CPU &amp; GPU balloons</strong></em></p><p>除了使用cluster-autoscaler动态扩展虚拟机集群外，我们还使用它来管理（删除和重新添加）集群中不正常的节点。为此，我们将激情的<code>最小</code>设置为零，并将集群的<code>最大</code>设置为可用容量。但是，如果cluster-autoscaler看到空闲节点，则将尝试缩小到仅所需的容量。由于多种原因（VM启动延迟，预分配的成本，上述API Server的影响），这种空闲扩展并不理想。</p><p>因此，我们为CPU和GPU主机引入了balloons Deployment。该Deployment包含一个具有<code>最大值</code>数量的低优先级容器配置。这些Pod占用了节点内的资源，因此自cluster-autoscaler不会将其视为空闲。但是，由于它们的优先级较低，因此调度程序可以立即将其逐出，以便为实际工作腾出空间。 （我们选择使用Deployment而不是DaemonSet，以避免将DaemonSet视为节点上的空闲工作负载。）</p><p>需要注意的一件事是，我们使用容器抗亲和力来确保容器在节点上均匀分布。自Kubernetes 1.18起已更正了该算法的性能问题。</p><hr><h5 id="六、成组调度-Gang-scheduling"><a href="#六、成组调度-Gang-scheduling" class="headerlink" title="六、成组调度(Gang scheduling)"></a>六、成组调度(Gang scheduling)</h5><p>我们的实验通常涉及一个或多个StatefulSet，每个StatefulSet都在训练工作的不同部分进行。对于优化器，研究人员需要在进行任何训练之前调度完StatefulSet的所有pod（因为我们经常在优化器成员之间使用MPI进行协作，并且MPI对组成员身份更改很敏感）。</p><p>但是，默认情况下，Kubernetes并不一定要优先执行一个StatefulSet的请求。例如，如果两个实验作业各自请求集群容量的100％，但Kubernetes可能只调度每个实验Pod的一半，从而导致调度僵局，这两个实验作业都无法完成。</p><p>我们尝试了实现自定义调度程序，但是遇到了一些极端情况，这些情况导致与常规Pod的调度方式发生冲突。 Kubernetes 1.18引入了Kubernetes framwork plugin架构，这使得在本地添加此类功能变得更加容易。我们最近引入Coscheduling插件解决此问题。</p><hr><h5 id="七、结论"><a href="#七、结论" class="headerlink" title="七、结论"></a>七、结论</h5><p>在扩展Kubernetes集群时，仍有许多问题需要解决。 其中一些包括：</p><p>a. 监控指标</p><p>就我们的规模而言，Prometheus的内置TSDB存储引擎的压缩速度很慢，并且每次重新启动时都需要花费很长的时间来恢复WAL（Write-Ahead-Log），这给我们带来了很大的麻烦。 我们正在迁移到其他与Prometheus兼容的存储和查询引擎。 期待将来有关它如何发展的博客文章！</p><p>b. Pod网络流量整形</p><p>当我们扩展群集时，每个Pod都会被计算为具有一定数量的Internet带宽，那么所有Pod总体流量将非常惊人，因而需要引入流量整形技术，防止网络风暴、流量泛滥等问题。</p><p>我们发现Kubernetes是满足我们研究需求的异常灵活的平台。 它具有扩展能力，可以满足我们要求的最苛刻的工作负载。 尽管还有很多地方需要改进，但OpenAI的超级计算团队将继续探索Kubernetes如何扩展。 </p><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li>[scaling-kubernetes-to-7500-nodes](<a href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/%EF%BC%89">https://openai.com/blog/scaling-kubernetes-to-7500-nodes/）</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;OpenAI已经将Kubernetes集群规模扩展至7500个节点，为大型神经网络模型（如GPT-3，CLIP和DALL·E）及小型实验性研究提供了可扩展的基础架构。 很少将单个Kubernetes集群扩展到如此规模，为此进行了一些必要的改进，但好处是单一的基础架构使我们的</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes废弃PodSecurityPolicy后续</title>
    <link href="https://zoues.com/posts/aa6cf752/"/>
    <id>https://zoues.com/posts/aa6cf752/</id>
    <published>2021-01-21T00:40:08.000Z</published>
    <updated>2024-01-20T12:01:27.443Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes社区将在1.21版本中弃用PSP，并将1.25版本中移除该API。目前CNCF生态圈类似项目：Kyverno与Open Policy Agen(OPA).</p><p>PodSecurityPolicy是集群级别的Pod安全策略，其对Pod的操作进行细粒度的授权。在Kubernetes架构中以Admission Controller（准入控制，类似NamespaceLifecycle、ResourceQuota等），通俗来讲就是一种写入前检查插件</p><hr><h4 id="一、PSP困境"><a href="#一、PSP困境" class="headerlink" title="一、PSP困境"></a>一、PSP困境</h4><p>当前PodSecurityPolicy特性存在以下问题：</p><ol><li>授权模型存在缺陷</li><li>功能易开难关</li><li>API接口缺乏一致性及扩展性,如MustRunAsNonRoot、AllowPrivilegeEscalation此类配置</li><li>无法处理动态注入的side-car（如knative）</li><li>在CI&#x2F;CD场景难以落地</li></ol><hr><h4 id="二、备选方案"><a href="#二、备选方案" class="headerlink" title="二、备选方案"></a>二、备选方案</h4><h5 id="Kyverno简介"><a href="#Kyverno简介" class="headerlink" title="Kyverno简介"></a>Kyverno简介</h5><p>Kyverno是为Kubernetes设计的策略引擎（CNCF sandbox项目）。其具备以下功能：</p><ul><li><p>相关策略类似Kubernetes对象，上手容易</p></li><li><p>配置管理便利</p></li><li><p>为Kubernetes资源的策略进行声明式验证，更改和生成资源配置。</p></li><li><p>在Kubernetes集群中作为动态准入控制器运行。</p></li><li><p>可以使用资源种类，名称和标签选择器来匹配资源。名称中支持通配符等</p></li></ul><p>当前采纳该方案的开源项目：fluxcd v2等</p><h5 id="OPA简介"><a href="#OPA简介" class="headerlink" title="OPA简介"></a>OPA简介</h5><p>Open Policy Agent（即OPA, CNCF孵化项目）, 为策略决策需求提供了一个统一的框架。它将策略决策从软件业务逻辑中解耦剥离，将策略定义、决策过程抽象为通用模型，实现了一个通用策略引擎，</p><p>其可用于微服务、Kubernetes、 CI&#x2F;CD、API网关等应用场景。</p><p>OPA可以通过sidecar、外部服务或是依赖库的方式与已有的软件系统进行集成。OPA 可以接受任何类型的结构化数据，决策流程如下图所示：</p><p><img src="https://s3.ax1x.com/2021/01/20/sRWx7d.png"></p><p>OPA通过数据输入和策略来进行决策，决策过程和数据无关。例如：</p><ul><li>判断某用户可以访问哪些资源</li><li>允许哪些子网对外访问</li><li>工作负载可以部署在哪个集群</li><li>可以使用哪些镜像</li><li>容器可以使用哪些系统功能</li><li>什么时间可以访问等</li></ul><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li>[deprecate PSP](<a href="https://github.com/kubernetes/kubernetes/pull/9717%EF%BC%89">https://github.com/kubernetes/kubernetes/pull/9717）</a></li><li><a href="https://docs.google.com/document/d/1VKqjUlpU888OYtIrBwidL43FOLhbmOD5tesYwmjzO4E/edit#">PodSecurityPolicy Options</a></li><li><a href="https://servicesblog.redhat.com/2019/10/16/open-policy-agent-part-i-the-introduction/">redhat关于OPA系列</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Kubernetes社区将在1.21版本中弃用PSP，并将1.25版本中移除该API。目前CNCF生态圈类似项目：Kyverno与Open Policy Agen(OPA).&lt;/p&gt;
&lt;p&gt;PodSecurityPolicy是集群级别的Pod安全策略，其对Pod的操作进行细</summary>
      
    
    
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes调度由浅入深：框架</title>
    <link href="https://zoues.com/posts/27bb006d/"/>
    <id>https://zoues.com/posts/27bb006d/</id>
    <published>2021-01-19T13:40:08.000Z</published>
    <updated>2024-01-20T12:01:27.437Z</updated>
    
    <content type="html"><![CDATA[<p>今天zouyee先带各位盘点CNCF上周的一些有趣的事情：</p><ol><li><p>Kubernetes社区GB代表选举结束 Paris Pittman当选</p></li><li><p>CNCF孵化项目OPA进入毕业流程</p></li><li><p>上周<code>helm</code>项目发布v3.5.0功能性版本</p></li><li><p>CoreDNS项目通过Docker镜像仓库放开拉取限制的申请</p></li></ol><p>书接上文《Kubernetes调度系统由浅入深系列：初探》，今天zouyee为大家带来《kuberneter调度由浅入深：框架》，该系列对应版本为<code>1.20.+</code>.</p><hr><h4 id="一、前文回顾"><a href="#一、前文回顾" class="headerlink" title="一、前文回顾"></a>一、前文回顾</h4><p>在《Kubernetes调度系统由浅入深系列：初探》中，给出整体的交互图，来构建Pod调度的直观感受，我们拓展了一下交互图，如下所示。</p><p>注：该交互图非专业UML，还请谅解。</p><p><img src="https://s3.ax1x.com/2021/01/19/sgNXVJ.png" alt="图1 交互流程图"></p><p>上述以创建一个Pod为例，简要介绍调度流程：</p><ol><li><p>用户通过命令行创建Pod(选择直接创建Pod而不是其他workload，是为了省略kube-controller-manager)</p></li><li><p>kube-apiserver经过对象校验、admission、quota等准入操作，写入etcd</p></li><li><p>kube-apiserver将结果返回给用户</p></li><li><p>同时kube-scheduler一直监听节点、Pod事件等（流程1）</p></li><li><p><strong>kube-scheduler将spec.nodeName的pod加入到调度队列中，调度系统选择pod，进入调度周期（本文介绍内容）（流程2-3）</strong></p></li><li><p>kube-scheduler将pod与得分最高的节点进行binding操作（流程4）</p></li><li><p>kube-apiserver将binding信息写入etcd</p></li><li><p>kubelet监听分配给自己的Pod，调用CRI接口进行Pod创建（该部分内容后续出系列，进行介绍）</p></li><li><p>kubelet创建Pod后，更新Pod状态等信息，并向kube-apiserver上报</p></li><li><p>kube-apiserver写入数据</p></li></ol><h5 id="二、框架背景"><a href="#二、框架背景" class="headerlink" title="二、框架背景"></a>二、框架背景</h5><p>​Kubernetes 随着功能的增多，代码与逻辑也日益复杂。代码体量及复杂度的提升必然带来维护成本的增加，隐形的增加错误定位和修复的难度。旧版本的Kubernetes调度程序（<strong>1.16前</strong>）提供了webhooks进行扩展。但有以下缺陷：</p><ul><li><p>用户可以扩展的点比较有限，位置比较固定，无法支持灵活的扩展与调配，例如只能在执行完默认的 Filter 策略后才能调用。</p></li><li><p>调用扩展接口使用 HTTP 请求，其受到网络影响，性能远低于本地的函数调用。同时每次调用都需要将 Pod 和 Node 的信息进行 序列化与反序列化 操作，会进一步降低性能。</p></li><li><p>Pod当前的相关信息，无法及时传递（利用调度Cache）。</p></li></ul><p>为了解决上述问题，使调度系统代码精简、扩展性更好，社区从<code> Kubernetes 1.16</code> 版本开始, 引入了一种新的调度框架- Scheduling Framework 。</p><p>Scheduling Framework 在原有调度流程的基础之上, 定义了丰富的扩展点接口，开发者可以通过实现扩展点所定义的接口来实现插件，将插件注册到扩展点。Scheduling Framework 在执行调度流程时，运行到相应的扩展点时，执行用户注册的插件，生成当前阶段的结果。通过这种方式来将用户的调度逻辑集成到 Scheduling Framework 中。Scheduling Framework明确了以下目标：</p><ul><li>扩展性：调度更具扩展性</li><li>维护性：将调度器的一些特性移到插件中</li><li>功能性<ul><li>框架提供扩展</li><li>提供一种机制来接收插件结果并根据接收到的结果继续或终止</li><li>提供一种机制处理错误与插件通信</li></ul></li></ul><h5 id="三、框架原理"><a href="#三、框架原理" class="headerlink" title="三、框架原理"></a>三、框架原理</h5><p>​Framework 的调度流程是分为两个阶段：</p><ul><li>调度阶段是同步执行的，同一个周期内只有一个 scheduling cycle，线程安全</li><li>绑定阶段(gouroutine)是异步执行的，同一个周期内可能会有多个 binding cycle在运行，线程不安全</li></ul><p>在介绍Framework 的调度流程之前，先介绍上图的调度流程，即schedulerOne的处理逻辑：</p><p><strong>a. 调度阶段</strong></p><pre><code> 1. **过滤**操作即findNodesThatFitPod函数 - 执行PreFilterPlugins - 执行FilterPlugins - 执行扩展 Filter - 若出现FitError，执行PostFilter 2. **评分**操作即prioritizeNodes函数     - 执行PreScorePlugins     - 执行ScorePlugins     - 执行扩展Prioritize 3. 挑选节点即select函数（符合条件节点，按照评分排序及采样选择） 4. 节点预分配即assume（只是预先分配，可收回） 5. 相关调度数据缓存即RunReservePlugins，从该节点开始，后续阶段发生错误，需要调用UnReserve，进行回滚（类似事务） 6.  执行准入操作即RunPermitPlugins</code></pre><p><strong>b. 绑定阶段</strong></p><pre><code>1. 执行WaitOnPermit，失败时调用RunReservePluginsUnreserve 2. 执行预绑定即RunPreBindPlugins，失败时调用RunReservePluginsUnreserve 3. 执行扩展bingding即extendersBinding，失败时调用RunReservePluginsUnreserve 4. 执行绑定收尾工作即RunPostBindPlugins</code></pre><h5 id="扩展点介绍"><a href="#扩展点介绍" class="headerlink" title="扩展点介绍"></a>扩展点介绍</h5><p>上述涉及到的各类Plugins（图中紫色部分），针对下图，各位应该看了很多篇了，需要注意的是Unreserve的时机，各插件功能说明如下：</p><p><code>pkg/scheduler/framework/interface.go</code></p><p><img src="https://s3.ax1x.com/2021/01/19/sg2Jns.png"></p><table><thead><tr><th>扩展点</th><th>用途说明</th></tr></thead><tbody><tr><td>QueueSort</td><td>用来支持自定义 Pod 的排序。如果指定 QueueSort 的排序算法，在调度队列里面就会按照指定的排序算法来进行排序，只能enable一个</td></tr><tr><td>Prefilter</td><td>对 Pod 信息的预处理，比如 Pod 的缓存等</td></tr><tr><td>Filter</td><td>对应旧式的Predicate ，过滤不满足要求的节点</td></tr><tr><td>PostFilter</td><td>用于处理当 Pod 在 Filter 阶段失败后的操作，例如抢占等行为</td></tr><tr><td>PreScore</td><td>用于在 Score 之前进行一些信息生成，也可以在此处生成一些日志或者监控信息</td></tr><tr><td>Score</td><td>对应旧式的Priority，根据 扩展点定义的评分策略挑选出最优的节点(打分与归一化处理)</td></tr><tr><td>Reserver</td><td>调度阶段的最后一个插件, 防止调度成功后资源的竞争, 确保集群的资源信息的准确性</td></tr><tr><td>Permit</td><td>主要提供了Pod绑定的拦截功能，根据条件对pod进行allow、reject或者wait。</td></tr><tr><td>PreBind</td><td>在真正 bind node 之前，执行一些操作</td></tr><tr><td>Bind</td><td>一个 Pod 只会被一个 BindPlugin 处理，创建Bind对象</td></tr><tr><td>PostBind</td><td>bind 成功之后执行的逻辑</td></tr><tr><td>Unreserve</td><td>在 Permit 到 Bind 这几个阶段只要报错就回滚数据至初始状态，类似事务。</td></tr><tr><td></td><td></td></tr></tbody></table><h5 id="四、使用场景"><a href="#四、使用场景" class="headerlink" title="四、使用场景"></a>四、使用场景</h5><p>​下述为一些关于如何使用调度框架来解决常见调度场景的示例。</p><ol><li><p>联合调度</p><p>类似<code>kube-batch</code>，允许调度以一定数量的Pod为整体的任务。其能够将一个训练任务的多个worker当做一个整体进行调度，只有当任务所有worker的资源都满足，才会将容器在节点上启动。</p></li><li><p>集群资源的动态绑定</p><p> Volume topology-aware调度可以通过filter和prebind方式实现。</p></li><li><p>调度拓展</p><p>该框架允许自定义插件，以main函数封装scheduler方式运行。</p></li></ol><p>关于框架部分，该文就介绍到此处，接下里将进入源码阶段，后续内容为调度配置及第三方调度集成的相关内容，敬请关注。</p><h5 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h5><pre><code>1. https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/ 2. https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/ 3. https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天zouyee先带各位盘点CNCF上周的一些有趣的事情：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kubernetes社区GB代表选举结束 Paris Pittman当选&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CNCF孵化项目OPA进入毕业流程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;上周</summary>
      
    
    
    
    <category term="Kubernetes GO" scheme="https://zoues.com/categories/Kubernetes-GO/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一文搞懂Kubernetes网络策略</title>
    <link href="https://zoues.com/posts/d6be75a2/"/>
    <id>https://zoues.com/posts/d6be75a2/</id>
    <published>2021-01-13T04:40:08.000Z</published>
    <updated>2024-01-20T12:01:27.471Z</updated>
    
    <content type="html"><![CDATA[<p>从CNCF基金会的成立，到Kubernetes社区蓬勃发展，历经6载，17年异军突起，在mesos、swarm等项目角逐中，拔得头筹，继而一统容器编排,其成功的关键原因可概括为以下几点：</p><ul><li>项目领导者们的坚守与远见</li><li>社区的良好的运作与社区文化</li><li>社区与企业落地的正反馈</li></ul><p>今天zouyee为大家带来《一文搞懂Kubernetes网络策略》，其中《kuberneter调度由浅入深：框架》正在编写中，敬请期待，当前涉及版本均为<code>1.20.+</code>。</p><hr><h4 id="一、Network-Policy简介"><a href="#一、Network-Policy简介" class="headerlink" title="一、Network Policy简介"></a>一、Network Policy简介</h4><p>​随着微服务架构的日渐盛行，Serverless框架的逐步落地，应用上云后带来了模块间网络调用需求的大规模增长，Kubernetes 自 1.3 引入了 Network Policy，其提供以应用为中心， 基于策略的网络控制，用于隔离应用以减少攻击面。</p><p>​Pod之间能否通信可通过如下三种组合进行确认：</p><ol><li>其他被允许的 Pods（例如：Pod 无法限制对自身的访问）</li><li>被允许访问的namespace</li><li>IP  CIDR（例如：与 Pod 运行所在节点的通信总是被允许的）</li></ol><p>在定义基于 Pod 或namespace的 NetworkPolicy 时，可以使用<code>标签选择器</code>来设定哪些流量可以进入或离开 Pod。同时，当创建基于 IP 的 NetworkPolicy 时，可以基于 IP CIDR 来定义策略。</p><p>以下结构体示意图辅助理解，后面章节有具体说明：</p><p><img src="https://s3.ax1x.com/2021/01/12/sYphbq.png"></p><h5 id="版本变迁"><a href="#版本变迁" class="headerlink" title="版本变迁"></a>版本变迁</h5><table><thead><tr><th>Kubernetes 版本</th><th>Networking API 版本</th><th>说明</th></tr></thead><tbody><tr><td>v1.5-v1.6</td><td>extensions&#x2F;v1beta1</td><td>需要在kube-apiserver开启 <code>extensions/v1beta1/networkpolicies</code></td></tr><tr><td>v1.7</td><td>networking.k8s.io&#x2F;v1</td><td></td></tr><tr><td>v1.8</td><td>networking.k8s.io&#x2F;v1</td><td>新增 <strong>Egress</strong> 和 <strong>IPBlock</strong> 的支持</td></tr></tbody></table><h4 id="二、简要介绍"><a href="#二、简要介绍" class="headerlink" title="二、简要介绍"></a>二、简要介绍</h4><p>默认情况下，Pod 是非隔离的，它们接受任何流量。</p><p>Pod 在被某 NetworkPolicy 选中时进入隔离状态。 一旦名字空间中有 NetworkPolicy 选择了特定的 Pod，该 Pod 会拒绝该 NetworkPolicy 所不允许的连接。 （名字空间下其他未被 NetworkPolicy 所选择的 Pod 会继续接受所有的流量）</p><p><em><strong>网络策略不会冲突</strong></em>。 如果任何一个或多个策略选择了一个 Pod, 则该 Pod 受限于这些策略的 入站（Ingress）&#x2F;出站（Egress）规则的并集。</p><p>⚠️在使用 Network Policy 时，网络插件需要支持 Network Policy，如 Calico、Romana、Weave Net 和 Trireme 等，其中Engress为 出口流量，Ingress为 入口流量。</p><h5 id="2-1-结构体说明"><a href="#2-1-结构体说明" class="headerlink" title="2.1 结构体说明"></a>2.1 结构体说明</h5><p><code>staging/src/k8s.io/api/networking/v1/types.go</code></p><p>下面是 NetworkPolicy 的一个示例，如需完整说明，可参看结构定义文档:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">network-policy-sample</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">db</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">172.17</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">        <span class="attr">except:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">172.17</span><span class="number">.1</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">project:</span> <span class="string">myproject</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">role:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">5978</span></span><br></pre></td></tr></table></figure><p><strong>必需字段</strong>：与所有其他的 Kubernetes 对象一样，NetworkPolicy 需要 <code>apiVersion</code>、 <code>kind</code> 和 <code>metadata</code> 字段。</p><p><strong>spec</strong>：NetworkPolicy <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">规约</a>中包含了在名字空间中定义特定网络策略所需的所有信息。</p><p><strong>podSelector</strong>：每个 NetworkPolicy 都包括一个 <code>podSelector</code>，它选择适用该该策略的 Pod。示例中的策略选择带有 “role&#x3D;db” 标签的 Pod。 若<code>podSelector</code>为空的，则选择名字空间下所有 Pod。</p><p><strong>policyTypes</strong>: 每个 NetworkPolicy 都包含一个 <code>policyTypes</code> 列表，其中包含 <code>Ingress</code> 或 <code>Egress</code> 或（两者亦可）。<code>policyTypes</code> 字段表示给定的策略是应用于 所选 Pod 的入口流量还是来出口流量（两者亦可）。 如果 NetworkPolicy 未指定 <code>policyTypes</code> 则默认情况下始终设置 <code>Ingress</code>； 如果 NetworkPolicy 有任何出口规则的话则设置 <code>Egress</code>。</p><p><strong>ingress</strong>: 每个 NetworkPolicy 可包含一个 <code>ingress</code> 规则的白名单列表。 每个规则都允许同时匹配 <code>from</code> 和 <code>ports</code> 部分的流量。示例策略中包含一条 简单的规则： 它匹配某个特定端口，第一个通过 <code>ipBlock</code> 指定，第二个通过 <code>namespaceSelector</code> 指定，第三个通过 <code>podSelector</code> 指定。</p><p><strong>egress</strong>: 每个 NetworkPolicy 可包含一个 <code>egress</code> 规则的白名单列表。 每个规则都允许匹配 <code>to</code> 和 <code>port</code> 部分的流量。该示例策略包含一条规则， 该规则指定端口上的流量匹配到 <code>10.0.0.0/24</code> 中的任何目的地。</p><p>该网络策略总结如下:</p><ol><li>隔离 <code>default</code>名字空间下 <code>role=db</code> 的 Pod 。</li><li>出口限制:允许符合以下条件的 Pod 连接到 <code>default</code>名字空间下标签为 <code>role=db</code>的所有 Pod 的 6379 TCP 端口：<ul><li><code>default</code>名字空间下带有 <code>role=frontend</code> 标签的所有 Pod</li><li>带有 <code>project=myproject</code> 标签的所有名字空间中的 Pod</li><li>IP 地址范围为<code> 172.17.0.0–172.17.0.255</code> 和 <code>172.17.2.0–172.17.255.255 </code>（即除了 172.17.1.0&#x2F;24 之外的所有 172.17.0.0&#x2F;16）</li></ul></li><li>入口限制：允许从带有 <code>role=db</code>标签的名字空间下的任何 Pod 到 CIDR 10.0.0.0&#x2F;24 下 5978 TCP 端口。</li></ol><h5 id="2-2-简单示例"><a href="#2-2-简单示例" class="headerlink" title="2.2 简单示例"></a>2.2 简单示例</h5><p>以 calico 为例看一下 Network Policy 的具体用法。</p><ol><li>配置 kubelet 使用 CNI 网络插件(默认已经配置，无需更改)</li></ol><p><code>kubelet --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin ...</code></p><ol start="2"><li>安装 calio 网络插件</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 注意修改 CIDR，需要跟 k8s pod-network-cidr 一致，默认为 192.168.0.0/16</span><br><span class="line"># 当前选择的是小于50节点的安装方式，具体安装可查看</span><br><span class="line"># https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</span><br><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br></pre></td></tr></table></figure><ol start="3"><li>部署应用</li></ol><p>部署 nginx 服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create deployment nginx --image=nginx</span><br><span class="line">deployment &quot;nginx&quot; created</span><br><span class="line">$ kubectl expose deployment nginx --port=80</span><br><span class="line">service &quot;nginx&quot; exposed</span><br></pre></td></tr></table></figure><p>测试网络</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc,pod</span><br><span class="line">NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/kubernetes   ClusterIP   10.233.0.1      &lt;none&gt;        443/TCP   186d</span><br><span class="line">service/nginx        ClusterIP   10.233.27.142   &lt;none&gt;        80/TCP    2s</span><br><span class="line"></span><br><span class="line">NAME                            READY   STATUS              RESTARTS   AGE</span><br><span class="line">pod/nginx-f89759699-kfmbj       1/1     Running   0          62s</span><br><span class="line"></span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"></span><br><span class="line">/ # wget --spider --timeout=1 nginx</span><br><span class="line">Connecting to nginx (10.233.27.142:80)</span><br><span class="line">remote file exists</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><p>4）测试网络策略</p><p>如果只让那些拥有标签 <code>access: true</code> 的 Pod 访问 <code>nginx</code> 服务， 那么可以创建一个如下所示的 NetworkPolicy 对象：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ cat nginx-policy.yaml</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: access-nginx</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          access: &quot;true&quot;</span><br><span class="line"></span><br><span class="line">$ kubectl create -f nginx-policy.yaml</span><br><span class="line">networkpolicy &quot;access-nginx&quot; created</span><br><span class="line"></span><br><span class="line"># 不带 access=true 标签的 Pod 还是无法访问 nginx 服务</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"></span><br><span class="line">/ # wget --spider --timeout=1 nginx</span><br><span class="line">Connecting to nginx (10.233.27.142:80)</span><br><span class="line">wget: download timed out</span><br><span class="line">/ #</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 而带有 access=true 标签的 Pod 可以访问 nginx 服务</span><br><span class="line">$ kubectl run busybox --rm -ti --labels=&quot;access=true&quot; --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"></span><br><span class="line">/ # wget --spider --timeout=1 nginx</span><br><span class="line">Connecting to nginx (10.233.27.142:80)</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><h5 id="三、应用场景"><a href="#三、应用场景" class="headerlink" title="三、应用场景"></a>三、应用场景</h5><h5 id="3-1-一般场景"><a href="#3-1-一般场景" class="headerlink" title="3.1 一般场景"></a>3.1 一般场景</h5><h5 id="a-禁止访问指定服务"><a href="#a-禁止访问指定服务" class="headerlink" title="a. 禁止访问指定服务"></a>a. 禁止访问指定服务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run web --image=nginx --labels app=web --expose --port 80</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 未有策略限制时，可以访问</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- http://web</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br></pre></td></tr></table></figure><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># cat web-deny-all.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-deny-all</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress: []</span><br><span class="line">  </span><br><span class="line">$ kubectl apply -f web-deny-all.yaml</span><br><span class="line">networkpolicy &quot;web-deny-all&quot; created</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web</span><br><span class="line">wget: download timed out</span><br></pre></td></tr></table></figure><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022447799137.jpg" alt="img"></p><h5 id="b-限制访问指定服务"><a href="#b-限制访问指定服务" class="headerlink" title="b. 限制访问指定服务"></a>b. 限制访问指定服务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80</span><br></pre></td></tr></table></figure><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022448622429.jpg" alt="img"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># cat api-allow.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: api-allow</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: bookstore</span><br><span class="line">      role: api</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">      - podSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            app: bookstore</span><br><span class="line"># kubectl apply -f api-allow.yaml</span><br><span class="line">networkpolicy &quot;api-allow&quot; created</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">创建不加label的pod，预期结果，访问被限制</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver</span><br><span class="line">wget: download timed out</span><br><span class="line">/ # exit</span><br><span class="line">创建带app=bookstore标签的pod，预期结果，访问被限制</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox --labels app=bookstore,role=frontend /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;&lt;head&gt;</span><br><span class="line">/ # exit</span><br></pre></td></tr></table></figure><h5 id="c-放通访问限制"><a href="#c-放通访问限制" class="headerlink" title="c. 放通访问限制"></a>c. 放通访问限制</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80</span><br></pre></td></tr></table></figure><p>应用a中的网络策略，限制所有流量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># cat web-deny-all.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-deny-all</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress: []</span><br><span class="line">  </span><br><span class="line">$ kubectl apply -f web-deny-all.yaml</span><br><span class="line">networkpolicy &quot;web-deny-all&quot; created</span><br></pre></td></tr></table></figure><p>创建放通通网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># cat web-deny-all.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-allow-all</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress:</span><br><span class="line">  - &#123;&#125;</span><br><span class="line">  </span><br><span class="line">$ kubectl apply -f web-allow-all.yaml</span><br><span class="line">networkpolicy &quot;web-allow-all&quot; created</span><br><span class="line"># 需要注意deny跟allow的细微差别就是[]与&#123;&#125;，其中&#123;&#125;代表</span><br><span class="line">- from:</span><br><span class="line">    podSelector: &#123;&#125;</span><br><span class="line">    namespaceSelector: &#123;&#125;</span><br></pre></td></tr></table></figure><h5 id="3-2-namespace限制"><a href="#3-2-namespace限制" class="headerlink" title="3.2 namespace限制"></a>3.2 namespace限制</h5><h5 id="a-禁止-namespace-中非白名单流量"><a href="#a-禁止-namespace-中非白名单流量" class="headerlink" title="a. 禁止 namespace 中非白名单流量"></a>a. 禁止 namespace 中非白名单流量</h5><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022451724392.gif" alt="img"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># cat default-deny-all.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: default-deny-all</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  ingress: []</span><br><span class="line"># kubectl apply -f default-deny-all.yaml</span><br></pre></td></tr></table></figure><p>说明:</p><ul><li><p><code>namespace: default</code> 该策略部署于default</p></li><li><p><code>podSelector</code>为<code>&#123;&#125;</code>指匹配所有pod，因而该策略对default命名空间的所有pod都有效</p></li><li><p><code>ingress</code>未指定，因而对于所有进入流量都禁止</p></li></ul><h5 id="b-禁止其他-namespace-流量"><a href="#b-禁止其他-namespace-流量" class="headerlink" title="b. 禁止其他 namespace 流量"></a>b. 禁止其他 namespace 流量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create namespace secondary</span><br><span class="line">kubectl run web --namespace secondary --image=nginx \</span><br><span class="line">    --labels=app=web --expose --port 80</span><br></pre></td></tr></table></figure><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022452203435.gif" alt="img"></p><p>创建网络配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: secondary</span><br><span class="line">  name: web-deny-other-namespaces</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - podSelector: &#123;&#125;</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># default命名空间访问</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.secondary</span><br><span class="line">wget: download timed out</span><br><span class="line">/ # exit</span><br><span class="line"># secondary命名空间访问</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox --namespace=secondary /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.secondary</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br></pre></td></tr></table></figure><h5 id="c-运行所有namespace流量"><a href="#c-运行所有namespace流量" class="headerlink" title="c. 运行所有namespace流量"></a>c. 运行所有namespace流量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run web --image=nginx --labels app=web --expose --port 80</span><br></pre></td></tr></table></figure><p><img src="https://github.com/ahmetb/kubernetes-network-policy-recipes/raw/master/img/5.gif" alt="Diagram of  ALLOW traffic to an application from all namespaces policy"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># cat web-allow-all-namespaces.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: web-allow-all-namespaces</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector: &#123;&#125;</span><br><span class="line"># kubectl apply -f web-allow-all-namespaces.yaml</span><br><span class="line"># kubectl create namespace secondary</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>app: web</code>网络策略应用到该标签pod</li><li><code>namespaceSelector: &#123;&#125;</code>匹配所有命名空间</li></ul><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl run busybox --rm -ti --image=busybox --namespace=secondary /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.secondary</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br></pre></td></tr></table></figure><h5 id="c-指定-namespace-访问服务"><a href="#c-指定-namespace-访问服务" class="headerlink" title="c. 指定 namespace 访问服务"></a>c. 指定 namespace 访问服务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># kubectl run web --image=nginx \</span><br><span class="line">    --labels=app=web --expose --port 80</span><br><span class="line"># kubectl create namespace dev</span><br><span class="line"># kubectl label namespace/dev purpose=testing</span><br><span class="line"># kubectl create namespace prod</span><br><span class="line"># kubectl label namespace/prod purpose=production</span><br></pre></td></tr></table></figure><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022453441751.gif" alt="img"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># cat web-allow-prod.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-allow-prod</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          purpose: production</span><br><span class="line"># kubectl apply -f web-allow-prod.yaml</span><br></pre></td></tr></table></figure><h5 id="d-允许其他namespace的指定pod的流量"><a href="#d-允许其他namespace的指定pod的流量" class="headerlink" title="d. 允许其他namespace的指定pod的流量"></a>d. 允许其他namespace的指定pod的流量</h5><p>⚠️ Kubernetes 1.11后支持<code>podSelector</code> 与<code>namespaceSelector</code>的运算符操作，同时需要网络插件支持</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># kubectl run web --image=nginx \</span><br><span class="line">    --labels=app=web --expose --port 80</span><br><span class="line"># kubectl create namespace other</span><br><span class="line"># kubectl create namespace other</span><br></pre></td></tr></table></figure><p> 创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># cat web-allow-all-ns-monitoring.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-allow-all-ns-monitoring</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress:</span><br><span class="line">    - from:</span><br><span class="line">      - namespaceSelector:     # 选择namespaces中带有team=operations标签的pod</span><br><span class="line">          matchLabels:</span><br><span class="line">            team: operations  </span><br><span class="line">        podSelector:           # 选择带有type=monitoring标签的pod</span><br><span class="line">          matchLabels:</span><br><span class="line">            type: monitoring</span><br><span class="line"># kubectl apply -f web-allow-all-ns-monitoring.yaml</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">kubectl run busybox --rm -ti --image=busybox  /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.default</span><br><span class="line">wget: download timed out</span><br><span class="line"></span><br><span class="line">(访问限制)</span><br><span class="line">/ # exit</span><br><span class="line"></span><br><span class="line"># kubectl run busybox --rm -ti --image=busybox --labels type=monitoring /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.default</span><br><span class="line">wget: download timed out</span><br><span class="line"></span><br><span class="line">(访问限制)</span><br><span class="line"></span><br><span class="line"># kubectl run busybox --rm -ti --image=busybox --namespace=other /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.default</span><br><span class="line">wget: download timed out</span><br><span class="line"></span><br><span class="line">(访问限制)</span><br><span class="line"></span><br><span class="line"># kubectl run busybox --rm -ti --image=busybox --namespace=other  --labels type=monitoring /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://web.default</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">...</span><br><span class="line">(允许访问)</span><br></pre></td></tr></table></figure><h5 id="3-3-允许外网访问服务"><a href="#3-3-允许外网访问服务" class="headerlink" title="3.3 允许外网访问服务"></a>3.3 允许外网访问服务</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl run web --image=nginx --labels=app=web --port 80</span><br><span class="line">kubectl expose pod/web --type=LoadBalancer</span><br><span class="line">kubectl get svc web</span><br><span class="line">NAME   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">web    LoadBalancer   10.233.54.206   &lt;pending&gt;     80:32548/TCP   40s</span><br><span class="line">直至EXTERNAL-IP分配IP为止</span><br></pre></td></tr></table></figure><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/concepts/images/15022454444461.gif" alt="img"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># cat web-allow-external.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: web-allow-external</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  ingress:</span><br><span class="line">  - ports:</span><br><span class="line">    - port: 80</span><br><span class="line">    from: []</span><br><span class="line"># kubectl apply -f web-allow-external.yaml</span><br></pre></td></tr></table></figure><h5 id="3-5-高级功能"><a href="#3-5-高级功能" class="headerlink" title="3.5 高级功能"></a>3.5 高级功能</h5><h5 id="a-允许应用固定端口流量"><a href="#a-允许应用固定端口流量" class="headerlink" title="a. 允许应用固定端口流量"></a>a. 允许应用固定端口流量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># kubectl run busybox -ti --image=busybox --labels=app=apiserver /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"># nohup python3 -m http.server 8001 &amp;</span><br><span class="line"># nohup python3 -m http.server 5001 &amp;</span><br><span class="line"># exit</span><br><span class="line"># kubectl create service clusterip apiserver \</span><br><span class="line">    --tcp 8001:8000 \</span><br><span class="line">    --tcp 5001:5000</span><br></pre></td></tr></table></figure><p><img src="https://github.com/ahmetb/kubernetes-network-policy-recipes/raw/master/img/9.gif" alt="Diagram of ALLOW traffic only to a port of an application policy"></p><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># cat api-allow-5000.yml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: api-allow-5000</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: apiserver</span><br><span class="line">  ingress:</span><br><span class="line">  - ports:</span><br><span class="line">    - port: 5000</span><br><span class="line">    from:</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: monitoring</span><br><span class="line"># kubectl apply -f api-allow-5000.yml</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 启动pod未携带指定label时，访问受限</span><br><span class="line"># kubectl run busybox --rm -ti --image=busybox /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver:8001</span><br><span class="line">wget: download timed out</span><br><span class="line"></span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver:5001/metrics</span><br><span class="line">wget: download timed out</span><br><span class="line"></span><br><span class="line"># 启动pod携带指定label时，访问不受限</span><br><span class="line">$ kubectl run busybox --rm -ti --image=busybox --labels=role=monitoring /bin/sh</span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver:8001</span><br><span class="line">&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">/ # wget -qO- --timeout=2 http://apiserver:5001/</span><br><span class="line">&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h5 id="b-多标签限制"><a href="#b-多标签限制" class="headerlink" title="b. 多标签限制"></a>b. 多标签限制</h5><p>说明：Network Policy定义一组微服务访问某一应用,如下述示例中，一组微服务共享redis服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl run db --image=redis:4 --port 6379 --expose \</span><br><span class="line">    --labels app=bookstore,role=db</span><br></pre></td></tr></table></figure><p>以下服务共享redis服务</p><table><thead><tr><th>service</th><th>labels</th></tr></thead><tbody><tr><td><code>search</code></td><td><code>app=bookstore</code> <code>role=search</code></td></tr><tr><td><code>api</code></td><td><code>app=bookstore</code> <code>role=api</code></td></tr><tr><td><code>catalog</code></td><td><code>app=inventory</code> <code>role=web</code></td></tr></tbody></table><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># cat redis-allow-services.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-allow-services</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: bookstore</span><br><span class="line">      role: db</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: bookstore</span><br><span class="line">          role: search</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: bookstore</span><br><span class="line">          role: api</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: inventory</span><br><span class="line">          role: web</span><br><span class="line"># kubectl apply -f redis-allow-services.yaml</span><br></pre></td></tr></table></figure><p>访问测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run busybox --rm -ti --image=curl --labels=app=inventory,role=web /bin/sh</span><br><span class="line"></span><br><span class="line">/ # nc -v -w 2 db 6379</span><br><span class="line">db (10.233.27.143:6379) open</span><br><span class="line"></span><br><span class="line">(works)</span><br><span class="line"></span><br><span class="line">$ kubectl run busybox --rm -ti --image=curl --labels=app=other /bin/sh</span><br><span class="line"></span><br><span class="line">/ # nc -v -w 2 db 6379</span><br><span class="line">nc: db (10.233.27.143:6379): Operation timed out</span><br><span class="line"></span><br><span class="line">(访问受限)</span><br></pre></td></tr></table></figure><h5 id="3-6-控制出口流量"><a href="#3-6-控制出口流量" class="headerlink" title="3.6 控制出口流量"></a>3.6 控制出口流量</h5><h5 id="a-禁止应用的出口流量"><a href="#a-禁止应用的出口流量" class="headerlink" title="a. 禁止应用的出口流量"></a>a. 禁止应用的出口流量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run web --image=nginx --labels app=web --expose --port 80</span><br></pre></td></tr></table></figure><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># cat foo-deny-egress.yaml</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: foo-deny-egress</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: foo</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br><span class="line">  egress: []</span><br><span class="line"># kubectl apply -f foo-deny-egress.yaml</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>policyTypes: [&quot;egress&quot;]</code> 该策略类型为出口流量</li><li><code>egress: []</code> 策略为空说明出口流量全部禁止</li></ul><h5 id="b-禁止命名空间非白名单流量"><a href="#b-禁止命名空间非白名单流量" class="headerlink" title="b. 禁止命名空间非白名单流量"></a>b. 禁止命名空间非白名单流量</h5><p>创建网络策略</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># cat default-deny-all-egress.yaml</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: default-deny-all-egress</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  egress: []</span><br><span class="line"># kubectl apply -f default-deny-all-egress.yaml</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li><code>podSelector</code>为空，说明匹配所有pod</li><li><code>egress</code>为空数组，说明禁止所有符合<code>podSelector</code>的出口流量</li></ul><h5 id="四、NetworkPolicy-开发"><a href="#四、NetworkPolicy-开发" class="headerlink" title="四、NetworkPolicy 开发"></a>四、NetworkPolicy 开发</h5><p>​实现一个支持 Network Policy 的网络扩展需要至少包含两个组件</p><ul><li>CNI 网络插件：负责给 Pod 配置网络接口</li><li>Policy controller：监听 Network Policy 的变化，并将 Policy 应用到相应的网络接口</li></ul><p><img src="https://github.com/feiskyer/kubernetes-handbook/raw/master/plugins/images/policy-controller.jpg" alt="img"></p><h5 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h5><p>下图基于Kubernetes 1.19版本测试了以下特性：</p><p>1）MTU auto config</p><p>2） 带宽性能: Pod to Pod、Pod to Service（TCP、UDP）</p><p>3）资源消耗: Pod to Pod、Pod to Service（TCP、UDP）</p><p>4）安全特性：Network Policies、 Encryption等</p><p><img src="https://s3.ax1x.com/2021/01/12/sJzrLt.png" alt="Image for post"></p><p>calico其他详细的能力说明，可参看官网。</p><h5 id="五、未来展望"><a href="#五、未来展望" class="headerlink" title="五、未来展望"></a>五、未来展望</h5><h5 id="a-SCTP特性"><a href="#a-SCTP特性" class="headerlink" title="a. SCTP特性"></a>a. SCTP特性</h5><p><strong>支持版本：</strong> <code>Kubernetes v1.19 [beta]</code></p><p>作为一个 Beta 特性，SCTP 默认是被启用的。 要在集群层面禁用 SCTP，需要为 <code>kube-apiserver</code>关闭特性<code>--feature-gates=SCTPSupport=false,...</code> 以禁用 <code>SCTP</code> 。 启用该特性后，用户可以将 NetworkPolicy 的 <code>protocol</code> 字段设置为 <code>SCTP</code>。</p><p>⚠️ CNI插件需要支持SCTP协议</p><h5 id="b-待开发"><a href="#b-待开发" class="headerlink" title="b. 待开发"></a>b. 待开发</h5><p>截止Kubernetes v1.20 ，NetworkPolicy API 还不支持下述功能。</p><ul><li>强制集群内部流量经过某公用网关（可通过服务网格或其他代理来实现）</li><li>与 TLS 相关的场景（可使用服务网格或者 Ingress 控制器）</li><li>实现适用于所有名字空间或 Pods 的默认策略（如calico）</li><li>高级的策略查询或者策略验证相关工具（如calico）</li><li>在同一策略声明中选择目标端口范围的能力</li><li>生成网络安全事件日志的能力（例如，被阻塞或接收的连接请求）</li><li>禁止本地回路或指向宿主的网络流量（Pod 目前无法阻塞 localhost 访问， 它们也无法禁止来自所在节点的访问请求）。</li></ul><p>上述需求可以通过操作系统组件（如 SELinux、OpenVSwitch、IPTables 等） 或者七层技术（Ingress 控制器、服务网格实现）及准入控制器进行功能增强，当然有兴趣的可以参考calico及OPA项目。</p><h5 id="六、参考文档"><a href="#六、参考文档" class="headerlink" title="六、参考文档"></a>六、参考文档</h5><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes network policies</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy</a></li><li><a href="https://ahmet.im/blog/kubernetes-network-policy/">Securing Kubernetes Cluster Networking</a></li><li><a href="https://github.com/ahmetb/kubernetes-networkpolicy-tutorial">Kubernetes Network Policy Recipes</a></li><li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/concepts/network-policy.md">fesikyer network policy</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从CNCF基金会的成立，到Kubernetes社区蓬勃发展，历经6载，17年异军突起，在mesos、swarm等项目角逐中，拔得头筹，继而一统容器编排,其成功的关键原因可概括为以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目领导者们的坚守与远见&lt;/li&gt;
&lt;li&gt;社区的良好的运作</summary>
      
    
    
    
    <category term="Kubernetes GO" scheme="https://zoues.com/categories/Kubernetes-GO/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes调度由浅入深：初探</title>
    <link href="https://zoues.com/posts/bafe2904/"/>
    <id>https://zoues.com/posts/bafe2904/</id>
    <published>2021-01-08T04:40:08.000Z</published>
    <updated>2024-01-20T12:01:27.435Z</updated>
    
    <content type="html"><![CDATA[<p>从CNCF基金会的成立，到Kubernetes社区蓬勃发展，历经6载，17年异军突起，在mesos、swarm等项目角逐中，拔得头筹，继而一统容器编排,其成功的关键原因可概括为以下几点：</p><ul><li>项目领导者们的坚守与远见</li><li>社区的良好的运作与社区文化</li><li>社区与企业落地的正反馈</li></ul><p>虽然针对kubernetes的介绍已经比较多了，但是云原生还是Kubernetes项目的发展都已经迈入深水区，因而今天zouyee为大家带来《kuberneter调度由浅入深》，希望通过接下来的五篇文章，让各位能够系统深入的了解kubernetes调度系统，该系列对应版本为<code>1.20.+</code>，今天带来《Kubernetes调度系统由浅入深系列：初探》</p><h4 id="一、调度简介"><a href="#一、调度简介" class="headerlink" title="一、调度简介"></a>一、调度简介</h4><p>在开始前，先来看看Kubernetes的架构示意图，其中控制平面包含以下三大组件：kube-scheduler、kube-apiserver、kube-controller-manager。kubelet及kube-proxy组件的分析我们后续单独成章进行讲解，现在我们可以简单给理解上述组件的难易程度排个序，kube-apiserver、kubelet、kube-scheduler、kube-controller-manager、kube-proxy。</p><p><img src="https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg" alt="Components of Kubernetes"></p><p>如上所述，<code>kube-scheduler</code>是K8S系统的核心组件之一，其主要负责Pod的调度，其监听kube-apiserver，查询未分配 Node的Pod（未分配、分配失败及尝试多次无法分配），根据配置的调度策略，将Pod调度到最优的工作节点上，从而高效、合理的利用集群的资源，该特性是用户选择K8S系统的关键因素之一，帮助用户提升效率、降低能耗。</p><p><code>kube-scheduler</code> 负责将Pod 调度到集群内的<em>最佳节点</em>(基于相应策略计算出的最佳值)上，它监听<code> kube-apiserver</code>，查询还未分配节点 的 Pod，然后根据调度策略为这些 Pod 分配节点，执行绑定节点的操作(更新Pod的<strong>nodeName</strong>字段)。</p><p>在上述过程中，需要考虑以下问题：</p><ul><li>如何确保节点分配的公平性</li><li>如何确保节点资源分配的高效性</li><li>如何确保Pod调度的公平性</li><li>如何确保Pod调度的高效性</li><li>如何扩展Pod调度策略</li></ul><p>为解决上述的问题，<code>kube-scheduler</code>通过汇集节点资源、节点地域、节点镜像、Pod调度等信息综合决策，确保Pod分配到最佳节点，以下为<code>kube-scheduler</code>的主要目标：</p><ul><li>公平性：在调度Pod时需要公平的决策，每个节点都有被分配的机会，调度器需要针对不同节点作出平衡决策。</li><li>资源高效：最大化提升所有可调度资源的利用率，使有限的CPU、内存等资源服务尽可能更多的Pod。</li><li>性能：能快速的完成对大规模Pod的调度工作，在集群规模扩增的情况下，依然能确保调度的性能。</li><li>灵活性：在实际生产中，用户希望Pod的调度策略是可扩展的，从而可以定制化调度算法以处理复杂的实际问题。因此平台要允许多种调度器并行工作，并支持自定义调度器。</li></ul><h4 id="二、调度流程"><a href="#二、调度流程" class="headerlink" title="二、调度流程"></a>二、调度流程</h4><p>首先我们通过下面的整体的交互图，来构建Pod调度的直观感受。</p><p><img src="https://s3.ax1x.com/2021/01/06/sZ9Aun.png"></p><p>上述以创建一个Pod为例，简要介绍调度流程：</p><ol><li><p>用户通过命令行创建Pod(选择直接创建Pod而不是其他workload，是为了省略kube-controller-manager)</p></li><li><p>kube-apiserver经过对象校验、admission、quota等准入操作，写入etcd</p></li><li><p>kube-apiserver将结果返回给用户</p></li><li><p>同时kube-scheduler一直监听节点、Pod事件等（流程1）</p></li><li><p>kube-scheduler将spec.nodeName的pod加入到调度队列中，进行调度周期（该周期即位后续介绍内容）（流程2-3）</p></li><li><p>kube-scheduler将pod与得分最高的节点进行binding操作（流程4）</p></li><li><p>kube-apiserver将binding信息写入etcd</p></li><li><p>kubelet监听分配给自己的Pod，调用CRI接口进行Pod创建（该部分内容后续出系列，进行介绍）</p></li><li><p>kubelet创建Pod后，更新Pod状态等信息，并向kube-apiserver上报</p></li><li><p>kube-apiserver写入数据</p></li></ol><h5 id="调度周期"><a href="#调度周期" class="headerlink" title="调度周期"></a>调度周期</h5><p><code>kube-scheduler</code>的工作任务是根据各种调度算法将Pod绑定（bind）到最合适的工作节点，整个调度流程分为两个阶段：过滤和评分。流程示意图如下所示。</p><p><img src="https://s3.ax1x.com/2021/01/06/sZ9lv9.png"></p><p>​注：以前称之为predicate与priorities，当前统称为过滤与评分，实际效果一致</p><ul><li><p><strong>过滤</strong>：输入是所有节点，输出是满足预选条件的节点。<code>kube-scheduler</code>根据过滤策略过滤掉不满足的节点。例如，如果某节点的资源不足或者不满足预选策略的条件如“节点的标签必须与Pod的Selector一致”时则无法通过过滤。</p></li><li><p><strong>评分：</strong>输入是通过过滤阶段的节点，评分时会根据评分算法为通过过滤的节点进行打分，选择得分最高的节点。例如，资源越充足、负载越小的节点可能具有越高的排名。</p></li></ul><p>​通俗点说，调度的过程就是在回答两个问题：1. 候选节点有哪些？2. 其中最适合的是哪一个？</p><p>​如果在过滤阶段没有节点满足条件，Pod会一直处在Pending状态直到出现满足的节点，在此期间调度器会不断的进行重试。如果有多个节点评分一致，那么<code>kube-scheduler</code>任意选择其一。</p><p>​注：Pod首先进入调度队列，失败后进入backoff，多次失败后进入unschedule，该部分内容后续介绍。</p><h5 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h5><p>当前支持两种方式配置过滤、评分算法：</p><pre><code>1. Scheduling Policies：通过文件或者configmap配置Predicate算法(过滤)和 Priorities算法(评分)的算法 2. Scheduling Profiles：当前调度系统为插拔架构，其将调度周期分为 `QueueSort`、`PreFilter`、`Filter`、`PreScore`、`Score`、`Reserve`、`Permit`、`PreBind`、`Bind`、`PostBind`等阶段，通过定制调度周期中不同阶段的插件行为来实现自定义。</code></pre><p>下面简单介绍一下通过Scheduling Policies方式配置</p><p><code>pkg/scheduler/framework/plugins/legacy_registry.go</code></p><p> <em><strong>预选(Precates)</strong></em></p><table><thead><tr><th>算法名称</th><th>算法含义</th></tr></thead><tbody><tr><td>MatchInterPodAffinity</td><td>检查pod资源对象与其他pod资源对象是否符合亲和性规则</td></tr><tr><td>CheckVolumeBinding</td><td>检查节点是否满足pod资源对象的pvc挂载需求</td></tr><tr><td>GeneralPredicates</td><td>检查节点上pod资源对象数量的上线，以及CPU 内存 GPU等资源是否符合要求</td></tr><tr><td>HostName</td><td>检查Pod指定的NodeName是否匹配当前节点</td></tr><tr><td>PodFitsHostPorts</td><td>检查Pod容器所需的HostPort是否已被节点上其它容器或服务占用。如果已被占用，则禁止Pod调度到该节点</td></tr><tr><td>MatchNodeSelector</td><td>检查Pod的节点选择器是否与节点的标签匹配</td></tr><tr><td>PodFitsResources</td><td>检查节点是否有足够空闲资源（例如CPU和内存）来满足Pod的要求</td></tr><tr><td>NoDiskConflict</td><td>检查当前pod资源对象使用的卷是否与节点上其他的pod资源对象使用的卷冲突</td></tr><tr><td>PodToleratesNodeTaints</td><td>如果当前节点被标记为taints，检查pod资源对象是否能容忍node taints</td></tr><tr><td>CheckNodeUnschedulable</td><td>检查节点是否可调度</td></tr><tr><td>CheckNodeLabelPresence</td><td>检查节点标签是否存在</td></tr><tr><td>CheckServiceAffinity</td><td>检查服务亲和性</td></tr><tr><td>MaxCSIVolumeCountPred</td><td>如果设置了featuregate （attachvolumelimit）功能，检查pod资源对象挂载的csi卷是否超出了节点上卷的最大挂载数量</td></tr><tr><td>NoVolumeZoneConflict</td><td>检查pod资源对象挂载pvc是否属于跨区域挂载，因为gce的pd存储或aws的ebs卷都不支持跨区域挂载</td></tr><tr><td>EvenPodsSpreadPred</td><td>一组 Pod 在某个 TopologyKey 上的均衡打散需求</td></tr></tbody></table><p>注：其中 MaxEBSVolumeCountPred、 MaxGCEPDVolumeCountPred  MaxAzureDiskVolumeCountPred、MaxCinderVolumeCountPred 等云厂商相关预选算法已经废弃</p><p><em><strong>优选(Priorities)</strong></em></p><table><thead><tr><th>算法名称</th><th>算法含义</th></tr></thead><tbody><tr><td>EqualPriority</td><td>节点权重相等</td></tr><tr><td>MostRequestedPriority</td><td>偏向具有最多请求资源的节点。这个策略将把计划的Pods放到整个工作负载集所需的最小节点上运行。</td></tr><tr><td>RequestedToCapacityRatioPriority</td><td>使用默认的资源评分函数模型创建基于ResourceAllocationPriority的requestedToCapacity。</td></tr><tr><td>SelectorSpreadPriority</td><td>将属于相同service rcs rss sts 的pod尽量调度在不同的节点上</td></tr><tr><td>ServiceSpreadingPriority</td><td>对于给定的服务，此策略旨在确保Service的Pods运行在不同的节点上。总的结果是，Service对单个节点故障变得更有弹性。</td></tr><tr><td>InterPodAffinityPriority</td><td>基于亲和性（affinity）和反亲和性（anti-affinity）计算分数</td></tr><tr><td>LeastRequestdPriority</td><td>偏向使用较少请求资源的节点。换句话说，放置在节点上的Pod越多，这些Pod使用的资源越多，此策略给出的排名就越低。</td></tr><tr><td>BalancedRequestdPriority</td><td>计算节点上cpu和内存的使用率，使用率最均衡的节点最优</td></tr><tr><td>NodePreferAvoidPodsPriority</td><td>基于节点上定义的注释（annotaion）记分，注释中如果定义了alpha.kubernetes.io&#x2F;preferAvoidPods则会禁用ReplicationController或者将ReplicaSet的pod资源对象调度在该节点上</td></tr><tr><td>NodeAffinityPriority</td><td>基于节点亲和性计算分数</td></tr><tr><td>TaintTolerationPriority</td><td>基于污点（taint）和容忍度（toleration）是否匹配计算分数</td></tr><tr><td>ImageLocalityPriority</td><td>基于节点上是否已经下拉了运行pod资源对象的镜像计算分数</td></tr><tr><td>EvenPodsSpreadPriority</td><td>用来指定一组符合条件的 Pod 在某个拓扑结构上的打散需求</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从CNCF基金会的成立，到Kubernetes社区蓬勃发展，历经6载，17年异军突起，在mesos、swarm等项目角逐中，拔得头筹，继而一统容器编排,其成功的关键原因可概括为以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;项目领导者们的坚守与远见&lt;/li&gt;
&lt;li&gt;社区的良好的运作</summary>
      
    
    
    
    <category term="Kubernetes GO" scheme="https://zoues.com/categories/Kubernetes-GO/"/>
    
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CloudEvents三部曲:实践篇</title>
    <link href="https://zoues.com/posts/c8b5de4b/"/>
    <id>https://zoues.com/posts/c8b5de4b/</id>
    <published>2020-12-20T13:40:08.000Z</published>
    <updated>2024-01-21T02:27:48.223Z</updated>
    
    <content type="html"><![CDATA[<p>随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。<br>对于CloudEvent的介绍、规范说明及实践落地，将以三篇系列文章进行说明，本文为《CloudEvent三部曲:实践篇》</p><hr><h4 id="一、产品接入"><a href="#一、产品接入" class="headerlink" title="一、产品接入"></a>一、产品接入</h4><h5 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h5><p>Serverless是一项基于事件驱动的函数计算服务，通过使用函数计算产品，函数以弹性、免运维、高可靠的方式运行，用户可以无需采购和维护服务器等基础设施，可以更加专注于函数代码的编写。<br>目前 CloudEvents 在函数计算中已有广泛的应用，第三方服务接入函数计算服务，需要使用符合 CloudEvents 规范的消息传递数据，方便函数计算平台方对第三方服务的消息进行分发过滤，同时由于规范的通用性，第三方服务一次改造后可以无缝适配到各类符合 CloudEvents 规范的平台上。<br>此外消息类产品（例如：消息队列，消息服务，事件总线等）也可通过支持 CloudEvents 规范，统一云的事件标准，加速云原生生态集成。</p><h5 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h5><p>通常情况下，要构建一个CloudEvent，需要使用CloudEvents的软件开发工具包（SDK），利用SDK可以极大方便开发人员进行集成开发，截至 CloudEvents v1.0 规范的发布，CloudEvents 团队支持和维护以下6种SDK：</p><ul><li>CSharp</li><li>Go SDK</li><li>Java SDK</li><li>JavaScript SDK</li><li>Python SDK</li><li>Ruby SDK</li></ul><p>以下使用 Go，Python SDK构造符合CloudEvent 1.0 规范的消息接收发送，HTTP&#x2F;JSON请求转化等功能的范例。</p><p><em><strong>Golang</strong></em></p><ol><li><p>获取依赖<br><code>go get github.com/cloudevents/sdk-go/v2@v2.0.0</code></p></li><li><p>依赖引用<br><code>import cloudevents &quot;github.com/cloudevents/sdk-go/v2&quot;</code></p></li><li><p>发送事件</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;log&quot;</span></span><br><span class="line"></span><br><span class="line">cloudevents <span class="string">&quot;github.com/cloudevents/sdk-go/v2&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// The default client is HTTP.</span></span><br><span class="line">c, err := cloudevents.NewDefaultClient()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to create client, %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an Event.</span></span><br><span class="line">event :=  cloudevents.NewEvent()</span><br><span class="line">event.SetSource(<span class="string">&quot;example/uri&quot;</span>)</span><br><span class="line">event.SetType(<span class="string">&quot;example.type&quot;</span>)</span><br><span class="line">event.SetData(cloudevents.ApplicationJSON, <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;<span class="string">&quot;hello&quot;</span>: <span class="string">&quot;world&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set a target.</span></span><br><span class="line">ctx := cloudevents.ContextWithTarget(context.Background(), <span class="string">&quot;http://localhost:8080/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Send that Event.</span></span><br><span class="line"><span class="keyword">if</span> result := c.Send(ctx, event); !cloudevents.IsACK(result) &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to send, %v&quot;</span>, result)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>接受事件</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;log&quot;</span></span><br><span class="line"></span><br><span class="line">cloudevents <span class="string">&quot;github.com/cloudevents/sdk-go/v2&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">receive</span><span class="params">(event cloudevents.Event)</span></span> &#123;</span><br><span class="line"><span class="comment">// do something with event.</span></span><br><span class="line">    fmt.Printf(<span class="string">&quot;%s&quot;</span>, event)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// The default client is HTTP.</span></span><br><span class="line">c, err := cloudevents.NewDefaultClient()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to create client, %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">log.Fatal(c.StartReceiver(context.Background(), receive));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>序列化</p></li></ol><p><em><strong>序列化为JSON</strong></em></p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">event := cloudevents.NewEvent()</span><br><span class="line">event.SetSource(&quot;example/uri&quot;)</span><br><span class="line">event.SetType(&quot;example.type&quot;)</span><br><span class="line">event.SetData(cloudevents.ApplicationJSON, map[string]string&#123;&quot;hello&quot;: &quot;world&quot;&#125;)</span><br><span class="line"></span><br><span class="line">bytes, err := json.Marshal(event)</span><br></pre></td></tr></table></figure><p><em><strong>反序列化</strong></em></p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">event :=  cloudevents.NewEvent()</span><br><span class="line">err := json.Marshal(bytes, &amp;event)</span><br></pre></td></tr></table></figure><p><em><strong>Python</strong></em></p><ol><li><p>依赖包安装<br><code>pip install cloudevents</code></p></li><li><p>事件发送者<br>通过 Python SDK 中的 CloudEvent 类型构造 CloudEvents 事件，再利用 to_binary函数将其序列化为 JSON 格式的数据，使用 requests框架发送该 JSON 请求。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cloudevents.http <span class="keyword">import</span> CloudEvent, to_binary</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建CloudEvent结构体</span></span><br><span class="line"><span class="comment"># - The CloudEvent &quot;id&quot; is generated if omitted. &quot;specversion&quot; defaults to &quot;1.0&quot;.</span></span><br><span class="line">attributes = &#123;</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;com.example.sampletype1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;source&quot;</span>: <span class="string">&quot;https://example.com/event-producer&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">data = &#123;<span class="string">&quot;message&quot;</span>: <span class="string">&quot;Hello World!&quot;</span>&#125;</span><br><span class="line">event = CloudEvent(attributes, data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用to_binary函数将其序列化为 JSON 格式的数据</span></span><br><span class="line">headers, body = to_binary(event)</span><br><span class="line"></span><br><span class="line"><span class="comment"># POST</span></span><br><span class="line">requests.post(<span class="string">&quot;&lt;some-url&gt;&quot;</span>, data=body, headers=headers)</span><br></pre></td></tr></table></figure><ol start="3"><li>接受事件处理</li></ol><p>通过 Python SDK 中的 from_http 函数解析出 CloudEvents 事件，并打印事件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cloudevents.http <span class="keyword">import</span> from_http</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create an endpoint at http://localhost:/3000/</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">home</span>():</span><br><span class="line">    <span class="comment"># create a CloudEvent</span></span><br><span class="line">    event = from_http(request.headers, request.get_data())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># you can access cloudevent fields as seen below</span></span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Found <span class="subst">&#123;event[<span class="string">&#x27;id&#x27;</span>]&#125;</span> from <span class="subst">&#123;event[<span class="string">&#x27;source&#x27;</span>]&#125;</span> with type &quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;event[<span class="string">&#x27;type&#x27;</span>]&#125;</span> and specversion <span class="subst">&#123;event[<span class="string">&#x27;specversion&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>, <span class="number">204</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(port=<span class="number">3000</span>)</span><br></pre></td></tr></table></figure><hr><h4 id="二、接入方式"><a href="#二、接入方式" class="headerlink" title="二、接入方式"></a>二、接入方式</h4><h5 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h5><p>基于事件驱动服务是函数计算的核心功能之一，平台使用 Knative Eventing 的Broker&#x2F;Trigger 事件处理模型对事件进行过滤分发，此外为了确保跨平台和互操作性，将采用CNCF定义的标准数据格式CloudEvents 进行事件传输。</p><p><img src="https://s3.ax1x.com/2020/12/20/rafMi4.png" alt="图 1 产品接入架构"></p><p>如上图所示，架构分为三块内容，从左到右分别为事件源，事件接收&#x2F;转发者，事件消费者。</p><ol><li>事件源</li></ol><p>事件源是一种 Kubernetes 定制资源，它提供了一种机制，用于注册来自特定服务系统的一类事件。例如：对象存储事件源，Github事件源等等，因此不同的事件源需要的不同的自定义资源进行描述。</p><p>事件源负责获取特定服务系统的事件，并将事件转化为CloudEvents格式事件发送给 Knative Eventing 平台（即 Broker&#x2F;Trigger事件处理模型）。</p><ol start="2"><li>事件接受&#x2F;转发者</li></ol><p>引入Broker&#x2F;Trigger事件处理模型的目的，是为了搭建一些黑盒子，将具体的实现隐藏起来，用户无需关心底层实现细节。</p><ul><li>Broker如同事件桶，接收各种不同的事件，这些事件可以通过属性来过滤。</li><li>Trigger描述了一个过滤器，只有通过了过滤器选择的事件，可以被传送给事件消费者。</li></ul><p>如图1所示，用户通过 filter指定感兴趣红色小球的事件，最终只有该类事件被传送给事件消费者（这里是Knative Service，即 KSvc函数）。</p><ol start="3"><li>事件消费者</li></ol><p>事件消费者可以是某个服务或系统，这里的事件消费者是用户编写的KSvc函数（即处理事件的逻辑代码）。</p><h5 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h5><ol><li>第三方接入</li></ol><p>第三方服务接入基于knative实现的serverless平台需要提供特定的事件源，Knative社区已维护部分事件源，具体列表请查看：<a href="https://github.com/knative/eventing-contrib">https://github.com/knative/eventing-contrib</a></p><p>如果第三方服务不在社区提供的支持列表中，就需要自定义事件源，有如下常用的几种方式：</p><p><img src="https://s3.ax1x.com/2020/12/20/rafGsx.png"></p><p>ContainerSource 实现简单，是目前大部分自定义事件源的实现方式，也是KNative平台推荐的方式。</p><p>ContainerSource 是 Kubernetes 中自定义的 CRD（Custom Resource Definition）资源类型，具体定义如下</p><p><img src="https://s3.ax1x.com/2020/12/20/rafteK.png" alt="CRD"></p><p>主要看以下几个部分：</p><ol><li>sink：事件转发的目标对象，这里即图1中介绍的Borker</li><li>image：需要开发的镜像，包括了监听具体数据源的事件和转发事件到sink的实现</li><li>arg和env：开发者自定义的一些数据通过 arg 和 env 传入镜像</li></ol><p>ContainerSource 中 image 镜像部分即需要自定义实现的部分，实现方式根据获取第三方服务事件的不同分为以下两种：</p><ol><li>消息队列方式<br>如下图 2所示，如果第三方服务已适配消息队列，可以将产生的事件发往消息队列，此时 ContainerSource 可以直接从消息队列中消费第三方服务的事件。</li></ol><p><img src="https://s3.ax1x.com/2020/12/20/rafdFe.png" alt="图 2 消息队列方式"></p><ol start="2"><li>直连方式<br>如下图 3所示，如果第三方服务未适配消息队列，但服务本身提供事件订阅能力（如 Redis 的键空间特性，Keyspace Notifications future），此时 ContainerSource 可以直接订阅第三方服务的事件，监听服务变化。</li></ol><p><img src="https://s3.ax1x.com/2020/12/20/raf0Wd.png" alt="图 3 直连方式"></p><p>注意：无论采用以上哪种方式，ContainerSource 在生成 CloudEvents 事件时，都需要携带 KSVC 目标函数的唯一标识，以供平台侧分发事件时使用。例如：1. 消息队列方式，由于所有事件都从同一个消息队列中获取，此时需要在第三方服务生产事件时就携带目标函数的标识（对象存储产品接入时采用该方式）；2. 直连方式，由于 ContainerSource 与第三方服务是一对一关系，所以可以在 ContainerSource 生成 CloudEvents 事件时添加目标函数的标识。</p><p>利用 Broker&#x2F;Trigger 事件处理模型，极大简化了第三方服务接入函数计算的流程。无论使用消息队列方式还是直连方式，产品侧只需要提供适配第三方服务的 ContainerSource 镜像，以供平台侧使用。</p><ol start="2"><li>平台侧纳管</li></ol><p>平台侧的工作主要是纳管产品侧提供的 ContainerSource，并利用 Trigger 提供事件过滤的能力。</p><p>针对 ContainerSource 不同的实现方式，平台侧工作内容也有所区别：</p><p><em><strong>消息队列实现方式</strong></em></p><p>平台侧会创建如下内容：</p><ol><li>一组相同的ContainerSource（用于高可用）</li><li>一个 Broker 类型的资源，用于分发事件</li><li>多个 Trigger 类型资源，用于事件过滤</li></ol><p>平台侧会预先创建好 ContainerSource 和 Broker 资源，并提供纳管 Trigger 的增删改查接口用于事件过滤，此时 ContainerSource，Broker，Trigger 对应关系如下图所示：</p><p><img src="https://s3.ax1x.com/2020/12/20/rafsyt.png"></p><p><em><strong>直连方式</strong></em></p><p>平台侧会创建如下内容：</p><ol><li>多个 ContainerSource 订阅监听不同的服务实例</li><li>一个 Broker 类型的资源，用于分发事件</li><li>多个 Trigger 类型资源，用于事件过滤</li></ol><p>平台侧会预先提供好 Broker 资源，并提供纳管 ContainerSource 和 Trigger 的增删改查接口，此时 ContainerSource，Broker，Trigger 对应关系如下图所示：</p><p><img src="https://s3.ax1x.com/2020/12/20/rafcef.png"></p><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li><a href="https://www.bookstack.cn/read/serverless-handbook/core-function-code.md">function</a></li><li><a href="https://cloudevents.github.io/sdk-go/">go sdk</a></li><li><a href="https://github.com/cloudevents/sdk-go">sdk-go</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。&lt;br&gt;对于CloudEvent的介绍</summary>
      
    
    
    
    <category term="serverless" scheme="https://zoues.com/categories/serverless/"/>
    
    
    <category term="serverless" scheme="https://zoues.com/tags/serverless/"/>
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
  </entry>
  
  <entry>
    <title>CloudEvents三部曲:规范篇</title>
    <link href="https://zoues.com/posts/e6aacf5d/"/>
    <id>https://zoues.com/posts/e6aacf5d/</id>
    <published>2020-12-20T13:40:08.000Z</published>
    <updated>2024-01-21T02:27:48.214Z</updated>
    
    <content type="html"><![CDATA[<p>随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。<br>对于CloudEvent的介绍、规范说明及实践落地，将以三篇系列文章进行说明，本文为《CloudEvent三部曲:规范篇》.</p><hr><h4 id="一、规范概述"><a href="#一、规范概述" class="headerlink" title="一、规范概述"></a>一、规范概述</h4><p>事件虽无处不在，但事件生产者往往会以不同的方式来描述事件。</p><p>关于事件的行业规范的缺失意味着开发人员必须重复对接事件处理流程。这增加了跨系统（多系统）发送事件的难度，降低事件处理的可移植性。</p><p>CloudEvents是一种用通用格式描述事件数据的规范，以提供跨服务、平台和系统的互操作性。事件格式指定了如何用特定的编码格式对 CloudEvents 进行序列化。支持这些编码的CloudEvents实现必须遵守相应事件格式中指定的编码规则，所有实现都必须支持JSON格式，<em>protobuf</em>的格式也在有序推进中。</p><h6 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h6><p>CloudEvents通常用于分布式系统，以允许服务在开发期间低耦合，并独立部署，便于连接以构建新应用程序。</p><p>CloudEvents的目标是定义事件系统的互操作性，该系统允许服务生成或使用事件，其中包括独立开发和部署生产者和使用者。生产者可以在消费者收听之前生成事件，并且消费者可以进行相关订阅操作。需要注意的是，此工作产生的规范集中于事件格式的互操作性以及在各种协议（例如HTTP、protobuf）上发送事件格式的显示方式。规范不关注事件产生者或事件消费者的处理模型。</p><p>CloudEvents的核心是定义了一组元数据，称为属性，以及有关在系统之间传输的事件和这些元数据应如何出现在消息中。该元数据是定义了将请求路由到适当组件并促进该组件对事件进行适当处理所需的最少数据集。尽管这可能意味着事件本身的某些应用程序数据可能会作为CloudEvents属性集的一部分，但这也是为了正确传递和处理消息而进行的必要操作。</p><p><strong><strong>非规范范畴</strong></strong></p><p>以下内容不属于规范考虑范畴：</p><ul><li>函数构建和调用过程</li><li>特定语言的运行时API</li><li>单一身份&#x2F;访问控制系统</li></ul><hr><h4 id="二、符号和术语"><a href="#二、符号和术语" class="headerlink" title="二、符号和术语"></a>二、符号和术语</h4><h5 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h5><p>为了清楚起见，当一个功能被标记为”可选”时，这表明消息的生产者和消费者都可以选择支持该功能。换句话说，如果生产者愿意，可以选择将该功能包含在消息中，如果消费者愿意，可以选择支持该功能。那么不支持该功能的消费者就会默默地忽略消息中的那部分内容。生产者需要为消费者忽略该功能的情况做好准备。中间人应该转发选择性属性。</p><p><em><strong>触发</strong></em></p><p>“触发”是指在软件系统运行过程中获取到的事件。这可能是由于系统引发的信号或系统观察到的信号、状态变化、计时器过期或任何其他值得注意的活动而发生。例如，设备可能因为电池电量不足而进入警报状态，或者虚拟机即将执行预定的重启。</p><p><em><strong>事件</strong></em></p><p>“事件”是表示事件发生及其上下文的数据记录。事件被从事件生产者（源）分发到感兴趣的事件消费者。可以根据事件中包含的信息进行分发，但事件不会确定具体的分发目的地。事件将包含两类信息：事件数据代表触发后的数据，而上下文元数据提供关于事件触发的上下文信息。一个事件触发可能会产生多个事件。</p><p><em><strong>生产者</strong></em></p><p>“生产者”是特定实例、过程或设备，用于创建描述CloudEvent的数据结构。</p><p><em><strong>事件源</strong></em></p><p>“来源”是指事件触发时的上下文。在一个分布式系统中，它可能由多个生产者组成。</p><p><em><strong>消费者</strong></em></p><p>“消费者”接收事件，并对其采取行动。它使用上下文和数据来执行一些逻辑，这可能导致新事件的发生。</p><p><em><strong>中介机构</strong></em></p><p>“中介机构”接收包含事件的消息（如中间件），目的是将其转发给下一个接收者，而接收者可能是另一个中间人或消费者。中间人的一个典型任务是根据上下文中的信息将事件转发到接收者。</p><p><em><strong>上下文</strong></em></p><p>上下文元数据将被封装在上下文属性中。工具和应用程序代码可以使用这些信息来识别事件与系统的各个方面或与其他事件的关系。</p><p><em><strong>数据</strong></em></p><p>事件的特定信息。这可能包括有关触发事件的信息、修改后的数据等。</p><p><em><strong>事件格式</strong></em></p><p>事件格式指定了如何将CloudEvent序列化为字节序列。独立的事件格式（如JSON、protobuf格式）可指定独立于任何协议或存储介质的序列化。</p><p><em><strong>消息</strong></em></p><p>事件通过消息从事件源传输到目的地。</p><p>“结构化报文”是指使用独立的事件格式对事件进行完全编码并存储在消息主体中。</p><p>“二进制报文”是指事件数据存储在消息体中，事件属性作为消息元数据的一部分进行存储。</p><p><em><strong>协议</strong></em></p><p>消息可以通过各种行业标准协议（例如HTTP，AMQP，MQTT，SMTP），开源协议（例如Kafka，NATS）或平台&#x2F;供应商特定协议（AWS Kinesis，Azure Event Grid）进行传递。</p><h5 id="上下文属性"><a href="#上下文属性" class="headerlink" title="上下文属性"></a>上下文属性</h5><p>符合本规范的每个CloudEvent必须包含上下文等属性。这些属性在描述事件的同时，被设计为可以独立于事件数据进行序列化。这使得它们可以在目的地被检查，而不需要对事件数据进行反序列化。</p><p><em><strong>属性命名约定</strong></em></p><p>CloudEvents规范定义了对各种协议和编码的映射，随附的CloudEvents SDK针对各种运行时和语言。其中有些将元数据元素视为大小写敏感，而另一些则不敏感，而且单个CloudEvent可能会通过多个跳转来实现，中间涉及协议、编码和运行时。因此，本规范限制了所有属性的可用字符集，以防止大小写敏感问题或与通用语言中标识符的允许字符集冲突。</p><p>CloudEvents属性名称必须由ASCII字符集中的小写字母或数字组成。属性名称应具有描述性和简洁性，长度不得超过20个字符。</p><p><em><strong>类型系统</strong></em></p><p>以下是可用于属性中的抽象数据类型。这些类型中的每个类型都可以由不同的事件格式和协议元数据字段来表示。本规范为每个类型定义了一个标准的字符串编码，所有的实现都必须支持。</p><ul><li>布尔型 - 值为<code>true</code>或<code>false</code>的布尔值。<br>字符串编码：大小写敏感的<code>true</code>或<code>false</code>值。</li><li>整型 - <code>2,147,483,648</code>到<code>+2,147,483,647</code>之间的整数。这是一个有符号的、32位的、二进制编码的范围。事件格式不一定要使用这个编码，但它们必须只<br>使用这个范围内的整数值。<br>字符串编码: 根据RFC 7159，第6节，JSON号码的整数部分。</li><li>字符串 - 允许的Unicode字符序列。以下字符不允许使用。</li></ul><ul><li>U+0000-U+001F 和 U+007F-U+009F (两个范围都包括在内)中的”控制字符”，因为大多数字符没有约定的含义，有些字符，如 U+000A (换行符)，在HTTP头等上下文中不能使用。</li><li>代码点被Unicode识别为非字符。</li><li>U+D800-U+DBFF和U+DC00-U+DFFF，这两个范围都包括在内，除非正确地成对使用。</li></ul><p>而”\uD800\uDEAD”是合法的。</p><ul><li>字节 - 字节序列。</li><li>URI–绝对统一的资源标识符。</li></ul><ul><li>字符串编码：RFC4648中定义的绝对统一资源标识符。</li></ul><ul><li>URI-reference - 统一资源标识符引用。</li><li>时间戳 -使用Gregorian Calendar的日期和时间表达式。</li></ul><p>所有的上下文属性值必须是上面列出的类型之一。属性值可以以本地类型或标准字符串的形式呈现。表示 CloudEvent 或任何扩展的强类型编程模型必须能够将常规字符串编码转换为最适合抽象类型的运行时&#x2F;语言原生类型。</p><p>例如，在给定的实现中，时间属性可以用语言的本机日期时间类型来表示，但必须提供RFC3339字符串，并且在映射到HTTP消息的报文头时，必须可转换为RFC3339字符串。</p><p>同样，CloudEvents协议绑定或事件格式实现也必须能够在编码或协议元数据字段中将标准字符串编码转换为相应的数据类型。时间戳类型的属性值确实可以作为一个字符串通过多次跳转，并且只在生产者和最终消费者那里以本地运行时&#x2F;语言类型的形式实现。时间戳也可能被路由为本地协议类型，并可能在生产者和消费者端被映射到&#x2F;从各自的语言&#x2F;运行时类型，而永远不会以字符串的形式实现。</p><p>序列化机制的选择将决定上下文属性和事件数据的序列化方式。例如，在JSON序列化的情况下，上下文属性和事件数据可能同时出现在同一个JSON对象中。</p><h5 id="必要属性"><a href="#必要属性" class="headerlink" title="必要属性"></a>必要属性</h5><p>以下属性必须在所有CloudEvents中出现。</p><p><em><strong>id</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">id</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">字符串</td></tr><tr><td align="left">描述</td><td align="left">标示事件。生产者必须确保 source + id 对于每个独立的事件都是唯一的。如果一个重复的事件被重新发送（例如，由于网络错误），它可能有相同的id。消费者可能会认为 source 和 id 相同的事件是重复的。｜</td></tr><tr><td align="left">约束</td><td align="left">1. 必须 <br>2. 必须是一个非空字符串<br>3.必须在生产者范围内是唯一的</td></tr><tr><td align="left">范例</td><td align="left">1.由生产者维护的事件计数器<br>2.UUID</td></tr></tbody></table><p><em><strong>source</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">source</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">URI-reference</td></tr><tr><td align="left">描述</td><td align="left">标示事件发生的上下文。通常包括事件源的类型、发布事件的组织或产生事件的过程等信息。URI中编码的数据背后的确切语法和语义由事件生产者定义。生产者必须确保 source + id 对于每个独立的事件都是唯一的。应用程序可以为每个独立的生产者分配一个唯一的source，这样就很容易产生唯一的ID，因为没有其他生产者会有相同的source。应用程序可以使用 UUIDs、URNs、DNS 权限或特定于应用程序的方案来创建唯一的source标识符。一个源也可以包括多个的生产者。在这种情况下，生产者必须合作，确保 source + id 对于每个独立的事件都是唯一的。</td></tr><tr><td align="left">约束</td><td align="left">1. 必须<br>2.必须为非空URI-reference<br>3.推荐使用绝对URI</td></tr><tr><td align="left">范例</td><td align="left">全网唯一的URI，具有DNS权限。<br>1.<a href="https://github.com/cloudevents">https://github.com/cloudevents</a> <br>2.mailto:<a href="mailto:&#99;&#x6e;&#99;&#102;&#45;&#x77;&#x67;&#x2d;&#115;&#x65;&#114;&#x76;&#x65;&#x72;&#108;&#x65;&#x73;&#115;&#64;&#108;&#105;&#115;&#x74;&#x73;&#x2e;&#x63;&#110;&#x63;&#x66;&#x2e;&#105;&#x6f;">&#99;&#x6e;&#99;&#102;&#45;&#x77;&#x67;&#x2d;&#115;&#x65;&#114;&#x76;&#x65;&#x72;&#108;&#x65;&#x73;&#115;&#64;&#108;&#105;&#115;&#x74;&#x73;&#x2e;&#x63;&#110;&#x63;&#x66;&#x2e;&#105;&#x6f;</a><br><br>通用唯一的URN与UUID<br>1.urn:uuid:6e8bc430-9c3a-11d9-9669-0800200c9a66<br><br>应用特定的标识符<br>1.&#x2F;cloudevents&#x2F;spec&#x2F;pull&#x2F;123<br>2.&#x2F;sensors&#x2F;tn-1234567&#x2F;alertsd.1-555-123-4567</td></tr></tbody></table><p><em><strong>specversion</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">specversion</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">字符串</td></tr><tr><td align="left">描述</td><td align="left">事件所使用的CloudEvents规范的版本。该版本可用于解释上下文。</td></tr><tr><td align="left">约束</td><td align="left">1. 必须<br>2.必须是一个非空字符串<br>3.必须在生产者范围内是唯一的</td></tr></tbody></table><p><em><strong>type</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">type</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">字符串</td></tr><tr><td align="left">描述</td><td align="left">该属性包含一个描述事件类型的值，描述与起源事件相关的事件类型。该属性通常用于路由、监控、策略执行等。该属性的格式是由生产者定义的。</td></tr><tr><td align="left">约束</td><td align="left">1. 必须<br>2.必须是一个非空字符串<br>3.应该以一个反转的DNS名称为前缀。前缀域决定了定义这个事件类型的语义的组织。</td></tr><tr><td align="left">范例</td><td align="left">a.com.github.pull.create<br>b.com.example.object.delete.v2</td></tr></tbody></table><h5 id="可选属性"><a href="#可选属性" class="headerlink" title="可选属性"></a>可选属性</h5><p>以下为在CloudEvents中出现的可选属性。</p><p><em><strong>datacontenttype</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">datacontenttype</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">字符串</td></tr><tr><td align="left">描述</td><td align="left">数据的内容类型。该属性使数据可以携带任何类型的内容，其格式和编码可能与所选事件格式不同。例如，使用JSON信封格式渲染的事件可能会携带一个XM的数据，这个属性被设置为”application&#x2F;xml”就会通知消费者。不同的数据内容如何渲染不同的数据内容类型值的规则在事件格式规范中定义了，对于一些二进制模式的协议绑定，该字段直接映射到各自协议的内容类型元数据属性。二进制模式和内容类型元数据映射的规范性规则可以在相应的协议中找到。在某些事件格式中，datacontententtype属性可能会被省略。例如，如果一个JSON格式的事件没有datacontententtype属性，那么就意味着数据是符合”application&#x2F;json”类型的JSON值。换句话说：一个没有datacontententtype的JSON格式事件与一个datacontenttype&#x3D;”application&#x2F;json”的事件完全等同。当将一个没有datacontenttype属性的事件消息翻译成不同的格式或协议绑定时，目标datacontenttype应该明确地设置为源的隐含datacontenttype。</td></tr><tr><td align="left">约束</td><td align="left">1. 可选<br>2.如果存在，必须遵守RFC 2046中规定的格式。</td></tr></tbody></table><p><em><strong>dataschema</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">dataschema</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">URI</td></tr><tr><td align="left">描述</td><td align="left">指明数据所遵循的 schema。对模式不兼容的更改应该通过不同的URI来反映。</td></tr><tr><td align="left">约束</td><td align="left">1. 可选 <br>2.必须是一个非空字符串</td></tr></tbody></table><p><em><strong>subject</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">subject</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">字符串</td></tr><tr><td align="left">描述</td><td align="left">描述了事件生产者（通过 source 标识）上下文中的事件主题。在发布-订阅场景中，订阅者通常会订阅由源发出的事件，但如果源上下文有内部子结构，仅有源标识符可能不足以作为任何特定事件的限定符。</td></tr><tr><td align="left">约束</td><td align="left">1.可选<br>2.必须是一个非空字符串</td></tr><tr><td align="left">范例</td><td align="left">当在blob-存储容器内创建新的blob时，订阅者可能会对此进行订阅。在这种情况下，事件 source 标识了订阅范围（存储容器），type 标识了”blob创建”事件，id唯一标识了事件实例，以区分已创建的相同名称的blob；新创建的blob的名称携带在subject中。<br>source：<a href="https://example.com/storage/tenant/container">https://example.com/storage/tenant/container</a><br>subject: mynewfile.jpg</td></tr></tbody></table><p><em><strong>time</strong></em></p><table><thead><tr><th align="left">属性名</th><th align="left">time</th></tr></thead><tbody><tr><td align="left">类型</td><td align="left">Timestamp</td></tr><tr><td align="left">描述</td><td align="left">事件发生的时间戳。如果无法确定事件发生的时间，则该属性可以由 CloudEvents 生产者设置为其他时间（如当前时间），但同一源的所有生产者在这方面必须保持一致。换句话说，它们要么都使用实际发生的时间，要么都使用相同的算法来确定所使用的值。</td></tr><tr><td align="left">约束</td><td align="left">1.必须<br>2.如果存在，必须遵守RFC 3339中规定的格式。</td></tr></tbody></table><h5 id="扩展上下文属性"><a href="#扩展上下文属性" class="headerlink" title="扩展上下文属性"></a>扩展上下文属性</h5><p>CloudEvents可包含任意数量的具有不同名称的附加上下文属性，称为”扩展属性”。扩展属性必须遵循与标准属性相同的命名惯例，并使用与标准属性相同的类型系统。扩展属性在本规范中没有定义的含义，它们允许外部系统将元数据附加到事件中，就像HTTP自定义头一样。扩展属性总是按照与标准属性一样的绑定规则进行序列化。然而，本规范并不妨碍扩展将事件属性值复制到消息的其他部分，以便与同样处理消息的非CloudEvents系统进行交互。这样做的扩展规范应该指定如果复制的值与 cloud event 序列化的值不同，接收者应该如何解释消息。</p><p>扩展的定义应该定义属性的所有方面，例如，其名称、类型、语义和可能的值。新的扩展定义应该使用一个描述性足够强的名称，以减少与其他扩展名称同名的可能性。</p><p>许多协议支持发送者附加元数据的能力，例如作为 HTTP 头文件。虽然 CloudEvents 接收方没有被强制要求传递和处理这些元数据，但建议通过某种机制来解决这些元数据，以表明它们是非CloudEvents元数据。</p><hr><h4 id="事件数据"><a href="#事件数据" class="headerlink" title="事件数据"></a>事件数据</h4><p>CloudEvents可包括与事件发生相关的特定信息。如果存在，该类信息将被封装在数据中。</p><h5 id="长度限制"><a href="#长度限制" class="headerlink" title="长度限制"></a>长度限制</h5><p>在许多场景中，CloudEvents 将通过一个或多个中间件转发，每个中间件都可能会对转发事件的大小进行限制。CloudEvents也可能会被转发到消费者，比如嵌入式设备，这些设备受存储或内存限制。</p><p>事件的”大小”是指事件的线上大小，包括事件的线上传输的每一个比特：协议帧元数据、事件元数据和事件数据，基于所选的事件格式及所选的绑定协议。</p><p>如果应用配置要求在不同的协议之间转发事件，或要求对事件进行重新编码，则应考虑应用所使用的最有效的协议和编码，以符合这些大小限制。</p><ul><li>a.中间件必须转发大小为64KByte或以下的事件。</li><li>b.消费者应接受至少64 KByte大小的事件。</li></ul><p>实际上，这些规则将允许生产者安全地发布大小不超过64KB的事件。</p><p>一般来说，CloudEvents 发布者应该通过避免在事件有效载荷中嵌入大型数据项来保持事件的紧凑性，而是使用事件有效载荷链接到这些数据项。从访问控制的角度来看，这种方法还可以让事件的分布范围更广，因为通过解析链接访问事件相关的细节，可以实现差异化的访问控制和选择性的披露，而不是直接将敏感细节嵌入事件中。</p><h5 id="隐私与安全"><a href="#隐私与安全" class="headerlink" title="隐私与安全"></a>隐私与安全</h5><p>互操作性是本规范背后的主要驱动力，要实现这样的行为，需要将一些信息公开，导致信息泄露的可能性。</p><p>请考虑以下几点，以防止不经意间的数据泄漏，特别是在利用第三方平台及通信网络时。</p><ol><li><p>上下文属性<br>敏感信息不应在上下文属性中携带或表示。<br>CloudEvents生产者、消费者和中间人可以审查并记录上下文属性。</p></li><li><p>数据<br>业务数据应进行加密，以限制受信任方的可见性。数据加密是生产者和消费者之间的协议，不属于本规范的范围。</p></li><li><p>协议绑定<br>应采用工业级的安全方案，以确保CloudEvents的可信及安全的信息交换。</p></li></ol><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.bookstack.cn/read/serverless-handbook/core-function-code.md">function</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。&lt;br&gt;对于CloudEvent的介绍</summary>
      
    
    
    
    <category term="serverless" scheme="https://zoues.com/categories/serverless/"/>
    
    
    <category term="serverless" scheme="https://zoues.com/tags/serverless/"/>
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
  </entry>
  
  <entry>
    <title>CloudEvents三部曲:初识篇</title>
    <link href="https://zoues.com/posts/744a240b/"/>
    <id>https://zoues.com/posts/744a240b/</id>
    <published>2020-12-20T12:40:08.000Z</published>
    <updated>2024-01-21T02:44:16.933Z</updated>
    
    <content type="html"><![CDATA[<p>随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。<br>对于CloudEvent的介绍、规范说明及实践落地，将以三篇系列文章进行说明，本文为《CloudEvent三部曲:初识篇》.</p><hr><h4 id="一、规范背景"><a href="#一、规范背景" class="headerlink" title="一、规范背景"></a>一、规范背景</h4><h5 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h5><p>事件在系统设计中已经变的无处不在，但各类事件的提供方倾向于以不同的方式来描述事件，缺乏一种对事件的统一描述，事件使用方和提供方往往要花费大量的时间沟通字段定义，设计事件属性，并在将来的使用过程中疲于新增或修改事件的属性。这也限制了类库、工具和基础设施在跨环境时发送事件数据的潜力，如SDK、事件路由器或跟踪系统等。</p><p>随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。</p><h5 id="历史背景"><a href="#历史背景" class="headerlink" title="历史背景"></a>历史背景</h5><p>CloudEvents 是一种以通用方式描述事件数据的供应商中立的规范，该事件数据定义规范旨在简化跨服务，平台及其他方面的事件声明和发送。</p><p> CloudEvents 的相关工作最初是作为云原生计算基金会（Cloud Native Computing Foundation，简称 CNCF）Serverless工作组的一部分开展的，当规范达到v0.1的里程碑之后，于2018年5月15日获得技术监督委员会（Technical Oversight Committee，简称 TOC）批准作为一个全新的独立的CNCF沙盒项目。</p><p>2019年10月24日，CloudEvents项目取得了两项重大成果。第一，CNCF的技术监督委员会批准该项目成为 “孵化器“项目（从而使其从CNCF的“沙盒”毕业）。第二，CloudEvents规范发布了1.0版本，这是该规范的第一个主要版本！</p><h6 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h6><p>目前各大第三方云提供商都在大力推广落地CloudEvents规范，例如：</p><ul><li>2018年，微软宣布将通过Event Grid服务（一项由Azure集中管理的事件服务，支持用户通过“发布-订阅”机制发送和接收事件）提供对CloudEvents的支持</li><li>2019年，阿里云开始推广 OpenMessaging 标准协议，希望让 Apache RocketMQ 兼容 Cloudevent 体系，成为 Serverless 的桥梁</li><li>2020年，字节跳动在其函数计算产品中利用CloudEvents规范统一事件源标准，极大方便之后的能力扩展。</li></ul><p>但大多数云目前缺乏CloudEvents规范的相应支持，导致各能力服务之间的联动效率较差，该问题丞待解决。</p><hr><h4 id="二、规范优势"><a href="#二、规范优势" class="headerlink" title="二、规范优势"></a>二、规范优势</h4><p>从历史上看，行业缺乏描述无服务器的事件元数据的标准，这意味着开发人员需要重新学习如何跨系统使用各种类型的事件数据，从而难以构建可移植的工具。 CloudEvents 定义了一套一致的元数据，且首次将业界的云供应商、企业软件巨头和初创企业的无服务器社区聚集在一起，实现并支持该规范。</p><p>CloudEvents 具有如下一些优势：</p><h5 id="消费优势"><a href="#消费优势" class="headerlink" title="消费优势"></a>消费优势</h5><p>生产者生产事件供消费者使用时，由于使用了 CloudEvents 规范，消费者不再需要为平台或服务的差异性编写特定的消费逻辑，改而使用通用逻辑处理事件数据，方便事件消费者提高开发效率，并降低系统复杂度。例如：阿里云的事件总线 EventBridge，微软Azure的Event Grid服务都极大方便了产品的端到端集成。</p><h5 id="路由优势"><a href="#路由优势" class="headerlink" title="路由优势"></a>路由优势</h5><p>中间件将事件从生产者路由到消费者，或者转发到其他中间件的时，CloudEvents会保留事件的身份和语义完整性。 用于事件的分类过滤或元数据的鉴别。例如：消费者利用过滤功能只关注特定用户；或者利用元数据鉴别只接收后缀为 .doc 的新建文件等。</p><p>利用 CloudEvents中间件可以在改变事件的语义含义时承担生成器的角色，在基于事件采取行动时承担消费者的角色，或在路由事件不进行语义更改时承担中间件的角色。例如： Apache RocketMQ 社区发布的OpenMessaging 标准协议兼容CloudEvents规范，从而成为 Serverless 的桥梁，更加高效广泛的进行消息路由。</p><h5 id="交互优势"><a href="#交互优势" class="headerlink" title="交互优势"></a>交互优势</h5><p>利用 CloudEvents系统框架对内解耦各模块的通信能力，提高可维护性；对外与其他事件平台基础设施的交互将更简单，并且方便为其他平台设施提供通用 API，降低交互的复杂性，提高系统平台的扩展性。</p><hr><h4 id="三、落地场景"><a href="#三、落地场景" class="headerlink" title="三、落地场景"></a>三、落地场景</h4><p>CloudEvents 在如下场景中有广泛的应用价值：</p><h5 id="跨平台和服务的事件规范化"><a href="#跨平台和服务的事件规范化" class="headerlink" title="跨平台和服务的事件规范化"></a>跨平台和服务的事件规范化</h5><p>不同的云提供商都在各自的平台上以不同的格式发布事件，此外同一云提供商上的不同服务也可能以不同的格式发布事件。这就迫使事件消费者不得不针对各类平台或各类服务编写针对性逻辑来消费事件数据。而CloudEvents 可以为处理跨平台和跨服务的事件的消费者提供统一的体验。</p><h5 id="事件追踪"><a href="#事件追踪" class="headerlink" title="事件追踪"></a>事件追踪</h5><p>从事件源发送的事件可能会导致各种中间件设备（如事件代理和网关）产生的附加事件序列。利用CloudEvents 包含事件的元数据这一特性（例如：事件经过多个代理时，代理会将自身IP追加到CloudEvents扩展属性中，最终事件消费者可以通过扩展属性中的 IP 信息，知道事件经过哪些代理），可以将这些附加的事件信息作为源事件序列的一部分进行关联，从而进行事件追踪和故障排除。</p><h5 id="提高Serverless的可移植性"><a href="#提高Serverless的可移植性" class="headerlink" title="提高Serverless的可移植性"></a>提高Serverless的可移植性</h5><p>Serverless（也称无服务器计算）是IT领域发展最快的趋势之一，它主要是由事件驱动。然而，供应商的锁定是FaaS的一个主要问题，这种锁定部分主要是由于各供应商函数内部接收事件数据的格式差异造成的。CloudEvents 对事件数据的通用描述方式解决了该问题，从而提高了FaaS的可移植性。</p><p>微软Azure的Event Grid事件服务提供了对CloudEvens的支持，用户可以将自己的CloudEvents推送到指定的Azure Grid Event主题（topic）。此后，Grid Event支持将符合CloudEvent模式的事件转换为符合特定Event Grid模式的事件，或者反之。</p><hr><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul><li><a href="https://www.bookstack.cn/read/serverless-handbook/core-function-code.md">function</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着云原生的发展（云原生的下一个五年在哪里？），逐步进入深水区，业界需要一种统一的事件定义和描述规范，以提供跨服务、跨平台的交互能力。CloudEvents事件规范应运而生，并得到了行业的广泛关注，包括主要的云提供商和 SaaS 公司。&lt;br&gt;对于CloudEvent的介绍</summary>
      
    
    
    
    <category term="serverless" scheme="https://zoues.com/categories/serverless/"/>
    
    
    <category term="serverless" scheme="https://zoues.com/tags/serverless/"/>
    
    <category term="cloudnative" scheme="https://zoues.com/tags/cloudnative/"/>
    
  </entry>
  
  <entry>
    <title>关于Kubernetes废弃内置docker CRI功能的说明</title>
    <link href="https://zoues.com/posts/2698d6f3/"/>
    <id>https://zoues.com/posts/2698d6f3/</id>
    <published>2020-12-06T14:56:01.000Z</published>
    <updated>2024-01-20T12:01:27.474Z</updated>
    
    <content type="html"><![CDATA[<p>今天，zouyee带各位看看关于前几天Kubernetes“废弃”docker支持的申明。首先，请各位稍安勿躁，主要还是中英文的翻译差别以及标题所引发的歧义，对Kubernetes开源项目有所了解的朋友，可能知道，该项目成功的原因之一，就在于对于接口及功能的版本管理，社区有一套完整且行之有效的方案，接口的兼容性、版本的多样性管理是驱动Kubernetes社区不断前行的内因。<br>先说结论：</p><ol><li>对于使用者没有任何影响</li><li>对于开发者，若想保持原有docker使用方式只是新增一个已知组件</li></ol><p>Kubernetes 1.20前后，对于docker的支持没有变化，只是将该部分代码（dockershim）独立出来，使用者可独立配置。</p><hr><h4 id="一、改变动因"><a href="#一、改变动因" class="headerlink" title="一、改变动因"></a>一、改变动因</h4><p>维护dockershim已成为Kubernetes维护人员的一种负担。创建CRI标准就是为了减轻这种负担，并提升不同容器运行时的可移植性性。Docker本身目前没有实现CRI，但Containerd实现了CRI接口。Dockershim一直是一种临时解决方案，此外，与dockershim不兼容的特性，如cgroups v2和用户命名空间，实现CRI接口的运行时也在慢慢探索并实现上述特性。</p><p><strong>何时完全废弃dockershim</strong></p><p>考虑到此更改的影响，它在Kubernetes 1.22之前不会被删除。</p><hr><h4 id="二、架构变化"><a href="#二、架构变化" class="headerlink" title="二、架构变化"></a>二、架构变化</h4><p>在Kubernetes架构中，是由Kubelet组件负责与容器运行时交互的。Kubelet调用容器运行时的流程如下图所示。</p><p><img src="https://s3.ax1x.com/2020/12/06/DjNt8x.png"></p><p>CRI shim是实现CRI接口提供的gRPC server服务，负责连接Kubelet和Container runtime，Container runtime是容器运行时工具；具体的流程是Kubelet调用CRI shim接口，CRI shim响应请求，然后调用底层的Container runtime工具运行容器。Kubelet、CRI shim和Container runtime都部署在一个Kubernetes 业务节点上，前两者是以独立的守护进程的方式启动的，而Container runtime不是守护进程，它通常是一个命令行工具。Kubernetes在1.5版本之前没有CRI接口，当时Kubelet内部只集成了两种容器运行时(Docker和rkt)的代码。但这两种容器运行时并不能满足用户的所有使用场景（rkt早已废弃），因为用户对容器的安全隔离性及性能在不同的应用场景有着不同的需求，用户希望Kubernetes能支持更多种的容器运行时。因此，Kubernetes在1.5版本推出了CRI接口，各个容器运行时只要实现了CRI接口规范，就可以接入到Kubernetes平台。在推出CRI后，Kublet为了满足CRI接口，实现了dockershim以支持直接使用docker接口，前期Containerd为了支持CRI接口，实现了CRI-Containerd，但在Containerd 1.1发布后，CRI-Containerd被废弃，转而使用插件方式支持CRI，即CRI插件，当前Kubernetes(1.20之前)关于docker及Containerd的支持如下所示。</p><p><img src="https://s3.ax1x.com/2020/12/06/DjN8a9.png"><br>内置dockershim方式</p><p><img src="https://s3.ax1x.com/2020/12/06/DjN3VJ.png"><br>containerd CRI方式</p><p>那么Kubernetes 1.20之后(1.22 之前)关于docker及Containerd的支持如下所示。<br><img src="https://s3.ax1x.com/2020/12/06/DjNN26.png"></p><p>Kubernetes 1.20之后，若前期使用dockershim内置方式，那么只需要再部署dockershim即可，若使用containerd等runtime，则保持不变即可,当然，官方推荐配置为containerd方式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天，zouyee带各位看看关于前几天Kubernetes“废弃”docker支持的申明。首先，请各位稍安勿躁，主要还是中英文的翻译差别以及标题所引发的歧义，对Kubernetes开源项目有所了解的朋友，可能知道，该项目成功的原因之一，就在于对于接口及功能的版本管理，社区有</summary>
      
    
    
    
    
    <category term="Docker" scheme="https://zoues.com/tags/Docker/"/>
    
    <category term="Kubernetes" scheme="https://zoues.com/tags/Kubernetes/"/>
    
    <category term="Containerd" scheme="https://zoues.com/tags/Containerd/"/>
    
  </entry>
  
  <entry>
    <title>Containerd CVE-2020–15257细节说明</title>
    <link href="https://zoues.com/posts/652176ee/"/>
    <id>https://zoues.com/posts/652176ee/</id>
    <published>2020-12-06T05:01:21.000Z</published>
    <updated>2024-01-21T02:29:07.231Z</updated>
    
    <content type="html"><![CDATA[<p>  Containerd是基于OCI规范实现的一款工业级标准的容器运行时。 Containerd在宿主机中管理容器生命周期，如容器镜像的传输和存储、容器的执行和管理、存储和网络等。 containerd-shim是用作容器运行的载体，实现容器生命周期管理， 其API以抽象命名空间Unix域套接字方式暴露，该套接字可通过根网络名称空间访问。 因此，一旦普通用户获得主机网络访问权限（通过启动主机网络模式的容器），则可以访问任一容器的API，并以此提权。</p><p>  在主机网络名称空间中运行容器是不安全的：</p><ul><li>请勿使用<code>docker run --net = host</code>运行Docker容器</li><li>请勿使用<code>.spec.hostNetwork：true</code>配置运行Kubernetes Pods</li></ul><h4 id="一、Containerd-CVE-2020–15257"><a href="#一、Containerd-CVE-2020–15257" class="headerlink" title="一、Containerd CVE-2020–15257"></a>一、Containerd CVE-2020–15257</h4><ol><li>漏洞级别</li></ol><p>该漏洞社区评分为5.2 分&#x2F;10分（中等）的安全级别，需要具备一定的触发条件、攻击路径较长。</p><ol start="2"><li>提权条件</li></ol><p>如果不受信任的用户在平台上无法创建主机网络模式（hostnetwork）的容器，或者容器内的进程是以非root用户（UID 0）运行，则不会触发该漏洞，具体满足以下多个条件：</p><ul><li>容器使用主机网络<code>hostnetwork</code>部署，此时容器和主机共享网络命名空间;</li><li>容器使用root用户(即UID 0);</li><li><code>containerd</code>版本在 &lt;&#x3D;1.3.7</li></ul><ol start="3"><li>漏洞确认</li></ol><p>对于在易受攻击的系统上运行容器的用户，可以通过禁止主机网络模式，或者通过确保此类容器以非零UID&#x2F;GID运行来缓解此问题。用户可将<code>containerd</code>版本更新到最新版本。 此外，更新前创建并运行的容器仍会受到攻击，因此用户需要确保所有容器完全停止，然后在更新后重新启动。</p><p>对于不确定<code>CVE-2020-15257</code>是否会影响的用户，可以使用以下命令快速确定受影响的<code>containerd</code>版本创建的容器是否仍在运行。 如果有返回结果，则说明存在。</p><p><code>$ cat /proc/net/unix | grep &#39;containerd-shim&#39; | grep &#39;@&#39;</code></p><ol start="4"><li>特别说明</li></ol><p>即使替换了补丁版本的<code>containerd</code>，使用主机网络也是不安全的。</p><hr><h4 id="二、-安全分析"><a href="#二、-安全分析" class="headerlink" title="二、 安全分析"></a>二、 安全分析</h4><h5 id="2-1-代码定位"><a href="#2-1-代码定位" class="headerlink" title="2.1 代码定位"></a>2.1 代码定位</h5><ul><li>containerd&#x2F;containerd<ul><li>runtime&#x2F;v1&#x2F;shim&#x2F;client&#x2F;client.go: WithStart(), newCommand()</li><li>cmd&#x2F;containerd-shim&#x2F;main_unix.go: serve()</li><li>cmd&#x2F;containerd-shim&#x2F;shim_linux.go: newServer()</li></ul></li><li>containerd&#x2F;ttrpc (via vendor&#x2F;github.com&#x2F;containerd&#x2F;ttrpc&#x2F;unixcreds_linux.go)<ul><li>unixcreds_linux.go: UnixSocketRequireSameUser()</li></ul></li></ul><h5 id="2-2-漏洞细节"><a href="#2-2-漏洞细节" class="headerlink" title="2.2 漏洞细节"></a>2.2 漏洞细节</h5><p>containerd是一个容器运行时的核心组件，其管理基于runc的容器，在Kubernetes中可通过Docker（dockershim）方式或CRI方式使用。Docker架构如下图所示。<br><img src="https://s3.ax1x.com/2020/12/06/DXnbsP.png" alt="containerd架构"><br>Docker架构包含docker、containerd、 containerd-shim、runC等组件。</p><ul><li><code>containerd</code>是容器运行时，作为守护进程，<code>containerd</code>通过<code>containerd-shim</code>调用<code>runc</code>管理容器。</li><li><code>containerd</code>作为守护进程，其对外暴露用于容器生命周期管理（如容器运行管理、镜像管理等）的gRPC接口。</li><li><code>containerd</code>生成<code>containerd-shim</code>进程对容器的生命周期进行一对一的管理。</li></ul><p>为了提供自己的gRPC（实际上是ttrpc，一种裁剪版gRPC协议）API，<code>containered-shim</code>监听Unix域套接字。 这些是Linux独有的Unix域套接字，其使用以空字节开头的长度前缀键，并且可以包含任意二进制序列。 它们在抽象Unix域套接字sun_path中嵌入了结尾的空字节，其可阻止常见的Unix工具（例如socat）与其连接。</p><ul><li>@&#x2F;containerd-shim&#x2F;&#x2F;&#x2F;shim.sock\0</li><li>@&#x2F;containerd-shim&#x2F;.sock\0</li></ul><p><code>containered-shim</code>不仅具有绑定和侦听此类套接字的能力，它还支持从其父进程接收任意套接字文件描述符。 <code>containerd</code>通过此方法，先创建抽象的Unix套接字并对其进行监听，在<code>containerd-shim</code>进程启动后，可以使用该句柄进行初始化，接下来<code>containerd-shim</code>启动<code>ttrpc</code>服务。 <code>containerd-shim</code>使用标准的Unix域套接字功能来验证传入的连接是否具有与其相同的UID和EUID（通常为UID：0和EUID：0）。</p><p><code>containerd-shim</code>所使用的抽象的Unix域套接字，是绑定在主机的网络命名空间上的。当一个恶意容器同样处于主机的网络命名空间中，该容器内的<code>root</code>用户，可以通过譬如<code>netstat -xl</code>或者<code>/proc/net/unix</code>来扫描，找到<code>containerd-shim</code>的套接字，然后链接<code>containerd-shim</code>的API以执行命令。</p><p><code>containerd-shim</code>暴露了许多危险的API，可用于逃避容器和执行特权命令。在使用的<code>containerd（-shim）</code>的两个主要版本1.2.x和1.3.x中，暴露以下能力：</p><ul><li>任意文件读取</li><li>任意文件追加</li><li>任意文件写入</li><li>containerd-shim中的任意命令执行</li><li>从runc config.json文件创建容器</li><li>启动创建的容器</li></ul><p>大多数用户实际上不受此CVE的影响。如果在未指定–user的情况下运行<code>docker run --net = host</code>，则会受到影响。如果Kubernetes用户使用<code>containerd</code>作为CRI运行时并使用<code>.spec.hostNetwork：true</code>配置运行pod且未设置<code>.spec.securityContext.runAsUser</code>，则受到影响。</p><p><img src="https://s3.ax1x.com/2020/12/06/DXk01K.png" alt="prefix"></p><p>该CVE修复了<code>containerd</code>的v1.4.3&#x2F;v1.3.9版本，其将抽象套接字修改为<code>/run/containerd</code>下基于文件的普通UNIX套接字。</p><p><img src="https://s3.ax1x.com/2020/12/06/DXEAaj.png" alt="fix"></p><h5 id="2-3-问题容器"><a href="#2-3-问题容器" class="headerlink" title="2.3 问题容器"></a>2.3 问题容器</h5><p>Docker执行以下命令：</p><p><code>$ docker ps -a --filter &#39;network=host&#39;</code></p><p>Kubernetes执行以下命令：</p><p><code>$ kubectl get pods -A -o json |    jq -c &#39;.items[] | select(.spec.hostNetwork==true) |[.metadata.namespace, .metadata.name]&#39;</code></p><h5 id="2-4-是否不使用network就一劳永逸"><a href="#2-4-是否不使用network就一劳永逸" class="headerlink" title="2.4 是否不使用network就一劳永逸"></a>2.4 是否不使用network就一劳永逸</h5><p>并不是的。 因为除了容器外，还有很多程序使用了抽象套接字。 这些程序包括：</p><ul><li>dbus</li><li>ibus</li><li>irqbalance</li><li>iscsid</li><li>iscsiuio</li><li>LXD</li><li>multipathd</li><li>X Window System</li><li>[historical] systemd before v212</li><li>[historical] Unity (desktop environment)</li><li>[historical] upstart</li></ul><p>等等</p><p>要查看主机上是否使用了抽象套接字，可运行<code>grep -ao &#39;@.*&#39; /proc/net/unix</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">grep -ao <span class="string">&#x27;@.*&#x27;</span> /proc/net/unix ⏎</span></span><br><span class="line">@/org/kernel/linux/storage/multipathd</span><br><span class="line">@/tmp/dbus-ihrEYFlKyT</span><br><span class="line">@/containerd-shim/moby/d0f4f5dd326d505f79e20ca891ad35516656353bc7974378237826b3456bff86/shim.sock</span><br><span class="line">@ISCSIADM_ABSTRACT_NAMESPACE</span><br><span class="line">@/containerd-shim/moby/d0f4f5dd326d505f79e20ca891ad35516656353bc7974378237826b3456bff86/shim.sock</span><br></pre></td></tr></table></figure><p>实际上，其实关于<code>containerd</code>的CVE-2020-15257漏洞，一些开发人员和用户早已知晓，但其一直未被视作安全漏洞，因为使用主机网络名称空间并不安全，无论是否存在<code>containerd</code>套接字。 虽然<code>containerd</code>项目考虑到攻击的影响范围而更改了漏洞策略，但上述的软件应该不会将抽象套接字视作漏洞。</p><hr><h4 id="三、安全建议"><a href="#三、安全建议" class="headerlink" title="三、安全建议"></a>三、安全建议</h4><p>在需要使用主机网络时，需要考虑以下安全策略</p><ul><li>以非root用户运行容器</li><li>AppArmor</li><li>SELinux等</li></ul><p><strong>Docker</strong></p><p>可以使用端口映射方式: <code>docker run -p</code><br>通信时执行以下命令：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker inspect -f <span class="string">&#x27;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#x27;</span> nginx ⏎</span></span><br><span class="line">172.17.0.2</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl http://172.17.0.2</span></span><br><span class="line">...</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br></pre></td></tr></table></figure><p>或者修改docker proxy</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF &gt; /etc/docker/daemon.json ⏎</span></span></span><br><span class="line">&#123;&quot;userland-proxy&quot;: false&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">systemctl restart docker</span></span> </span><br></pre></td></tr></table></figure><p>以及其他方案，如</p><ul><li>AppArmor</li><li>SELinux等</li></ul><p><strong>Kubernetes</strong></p><p>对于使用Kubernetes的用户，可以使用以下方式或特性</p><ul><li><code>kubectl get pods -o wide</code>获取IP进行访问</li><li>内部DNS（CoreDNS）</li><li>kubectl port-forward</li><li>AppArmor</li><li>SELinux等</li></ul><h5 id="3-1-以非root用户运行容器"><a href="#3-1-以非root用户运行容器" class="headerlink" title="3.1 以非root用户运行容器"></a>3.1 以非root用户运行容器</h5><p>对于Docker，运行<code>docker run --net=host --user 12345 --security-opt no-new-privileges</code>。 确保选择与主机上现有用户帐户没有冲突的UID号。<br>无需指定<code>no-new-privileges</code>，但是建议禁止使用sudo之类的特权。</p><p>对于Kubernetes,指定Pod相关字段<code>.spec.[]containers.securityContext</code>:</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hostNetwork: true</span><br><span class="line">containers:</span><br><span class="line">- name: foo</span><br><span class="line">  securityContext:</span><br><span class="line">    runAsUser: 12345</span><br><span class="line">    allowPrivilegeEscalation: false</span><br></pre></td></tr></table></figure><p>对于普通用户使用1024以内端口，需要如下配置：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">echo</span> <span class="string">&#x27;net.ipv4.ip_unprivileged_port_start=0&#x27;</span> &gt; /etc/sysctl.d/99-user.conf ⏎</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sysctl --system</span></span><br></pre></td></tr></table></figure><h5 id="3-2-使用AppArmor"><a href="#3-2-使用AppArmor" class="headerlink" title="3.2 使用AppArmor"></a>3.2 使用AppArmor</h5><p>AppArmor是Linux安全模块，供多个发行版使用，包括Ubuntu，Debian，SUSE和Google COS。<br>以下AppArmor配置文件可用于禁止容器使用抽象套接字：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">include &lt;tunables/global&gt;</span></span><br><span class="line">profile docker-no-abstract-socket flags=(attach_disconnected,mediate_deleted) &#123;</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash">include &lt;abstractions/base&gt;</span></span><br><span class="line">  network,</span><br><span class="line">  capability,</span><br><span class="line">  file,</span><br><span class="line">  umount,</span><br><span class="line">  signal (receive) peer=unconfined,</span><br><span class="line">  signal (send,receive) peer=docker-no-abstract-socket,</span><br><span class="line">  deny @&#123;PROC&#125;/* w,</span><br><span class="line">  deny @&#123;PROC&#125;/&#123;[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*&#125;/** w,</span><br><span class="line">  deny @&#123;PROC&#125;/sys/[^k]** w,</span><br><span class="line">  deny @&#123;PROC&#125;/sys/kernel/&#123;?,??,[^s][^h][^m]**&#125; w,</span><br><span class="line">  deny @&#123;PROC&#125;/sysrq-trigger rwklx,</span><br><span class="line">  deny @&#123;PROC&#125;/kcore rwklx,</span><br><span class="line">  deny mount,</span><br><span class="line">  deny /sys/[^f]*/** wklx,</span><br><span class="line">  deny /sys/f[^s]*/** wklx,</span><br><span class="line">  deny /sys/fs/[^c]*/** wklx,</span><br><span class="line">  deny /sys/fs/c[^g]*/** wklx,</span><br><span class="line">  deny /sys/fs/cg[^r]*/** wklx,</span><br><span class="line">  deny /sys/firmware/** rwklx,</span><br><span class="line">  deny /sys/kernel/security/** rwklx,</span><br><span class="line">  ptrace (trace,read,tracedby,readby) peer=docker-no-abstract-socket,</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Only the following line is related to abstract sockets.</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Other lines are from <span class="string">&quot;docker-default&quot;</span> profile</span> </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">(https://github.com/moby/moby/pull/39923)</span></span><br><span class="line">  deny unix addr=@**,</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">To load the profile, run `sudo apparmor_parser -r docker-no-abstract-socket`</span></span><br></pre></td></tr></table></figure><p>可以按如下所示将此配置文件应用于Docker容器：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo apparmor_parser -r docker-no-abstract-socket</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run --net=host --security-opt apparmor=docker-no-abstract-socket ...</span></span><br></pre></td></tr></table></figure><p>关于在Kubernetes中如何使用<a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/">AppArmor特性</a></p><h5 id="3-3-使用SELinux"><a href="#3-3-使用SELinux" class="headerlink" title="3.3 使用SELinux"></a>3.3 使用SELinux</h5><p>RHEL&#x2F;CentOS和Fedora的SELinux策略，用于保护主机上的抽象套接字：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">getenforce</span></span><br><span class="line">Enforcing</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">socat abstract-listen:foo,fork stdio &amp;</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo podman run -it --net=host alpine</span></span><br><span class="line">/ # cat /proc/self/attr/current</span><br><span class="line">system_u:system_r:container_t:s0:c83,c1019</span><br><span class="line">/ # apk add -q socat</span><br><span class="line">/ # echo test | socat stdio abstract-connect:foo</span><br><span class="line">2020/11/27 15:42:08 socat[7] E connect(5, AF=1 &quot;\0foo&quot;, 6): Permission denied</span><br></pre></td></tr></table></figure><p>默认情况下，SELinux已启用Podman和OpenShift。 要为Docker启用SELinux，请按以下方式配置<code>/etc/docker/daemon.json</code>：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF &gt; /etc/docker/daemon.json</span></span></span><br><span class="line">&#123;&quot;selinux-enabled&quot;: true&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">systemctl restart docker</span></span></span><br></pre></td></tr></table></figure><hr><h4 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h4><ul><li><a href="https://research.nccgroup.com/2020/11/30/technical-advisory-containerd-containerd-shim-api-exposed-to-host-network-containers-cve-2020-15257/">technical advisory</a></li><li><a href="https://medium.com/nttlabs/dont-use-host-network-namespace-f548aeeef575">dont-use-host-network-namespace</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  Containerd是基于OCI规范实现的一款工业级标准的容器运行时。 Containerd在宿主机中管理容器生命周期，如容器镜像的传输和存储、容器的执行和管理、存储和网络等。 containerd-shim是用作容器运行的载体，实现容器生命周期管理， 其API以抽象命</summary>
      
    
    
    
    <category term="containerd" scheme="https://zoues.com/categories/containerd/"/>
    
    
    <category term="CloudNative" scheme="https://zoues.com/tags/CloudNative/"/>
    
    <category term="containerd" scheme="https://zoues.com/tags/containerd/"/>
    
    <category term="kubernetes" scheme="https://zoues.com/tags/kubernetes/"/>
    
  </entry>
  
</feed>
