<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="thanks polarathene  DescriptionThe max open files limit NOFILE of dockerd is 1048576, which is defined in dockerd’s systemd unit file. 12$ cat &#x2F;proc&#x2F;$(pidof dockerd)&#x2F;limits | grep &quot;Max open file">
<meta property="og:type" content="article">
<meta property="og:title" content="Why does RLIMIT_NOFILE slow down your containerized application">
<meta property="og:url" content="https://zoues.com/posts/5cdbe8ce/index.html">
<meta property="og:site_name" content="zouyee">
<meta property="og:description" content="thanks polarathene  DescriptionThe max open files limit NOFILE of dockerd is 1048576, which is defined in dockerd’s systemd unit file. 12$ cat &#x2F;proc&#x2F;$(pidof dockerd)&#x2F;limits | grep &quot;Max open file">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-04-30T05:32:59.000Z">
<meta property="article:modified_time" content="2024-04-30T06:11:34.615Z">
<meta property="article:author" content="zouyee">
<meta property="article:tag" content="kubernetes">
<meta property="article:tag" content="container">
<meta property="article:tag" content="cncf">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Why does RLIMIT_NOFILE slow down your containerized application</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
      <link rel="alternate" href="/true" title="zouyee" type="application/atom+xml" />
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/zouyee">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/posts/186b05db/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/posts/45fc99a8/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zoues.com/posts/5cdbe8ce/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zoues.com/posts/5cdbe8ce/&text=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zoues.com/posts/5cdbe8ce/&is_video=false&description=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Why does RLIMIT_NOFILE slow down your containerized application&body=Check out this article: https://zoues.com/posts/5cdbe8ce/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zoues.com/posts/5cdbe8ce/&name=Why does RLIMIT_NOFILE slow down your containerized application&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zoues.com/posts/5cdbe8ce/&t=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Description"><span class="toc-number">1.</span> <span class="toc-text">Description</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#yum-hang"><span class="toc-number">1.1.</span> <span class="toc-text">yum hang</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rpm-slow"><span class="toc-number">1.2.</span> <span class="toc-text">rpm slow</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PtyProcess-spawn-slowdown-in-close-loop"><span class="toc-number">1.3.</span> <span class="toc-text">PtyProcess.spawn slowdown in close() loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL-has-been-known-to-allocate-excessive-memory"><span class="toc-number">1.4.</span> <span class="toc-text">MySQL has been known to allocate excessive memory</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Technical-Background-Introduction"><span class="toc-number">2.</span> <span class="toc-text">Technical Background Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RLIMIT-NOFILE"><span class="toc-number">2.1.</span> <span class="toc-text">1. RLIMIT_NOFILE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-File-Descriptor-Limits"><span class="toc-number">2.2.</span> <span class="toc-text">2.  File Descriptor Limits</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#select"><span class="toc-number">2.3.</span> <span class="toc-text">select</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dig-deeping-into"><span class="toc-number">3.</span> <span class="toc-text">Dig deeping into</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Investigated-limits-impact-and-costs"><span class="toc-number">3.1.</span> <span class="toc-text">Investigated limits impact and costs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#System-details"><span class="toc-number">3.1.1.</span> <span class="toc-text">System details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Observations-in-service-files-for-LimitNOFILE"><span class="toc-number">3.1.2.</span> <span class="toc-text">Observations in .service files for LimitNOFILE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reproduction"><span class="toc-number">3.1.3.</span> <span class="toc-text">Reproduction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limits-impact-across-process-children"><span class="toc-number">3.2.</span> <span class="toc-text">Limits impact across process children</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Errors"><span class="toc-number">3.3.</span> <span class="toc-text">Errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#docker-service-limit-exceeded"><span class="toc-number">3.3.1.</span> <span class="toc-text">docker.service limit exceeded</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#containerd-service-limit-exceeded"><span class="toc-number">3.3.2.</span> <span class="toc-text">containerd.service limit exceeded</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scope-and-Explanation"><span class="toc-number">4.</span> <span class="toc-text">Scope and Explanation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Systemd-240"><span class="toc-number">4.1.</span> <span class="toc-text">Systemd &lt; 240</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">5.</span> <span class="toc-text">References</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Why does RLIMIT_NOFILE slow down your containerized application
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">zouyee</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-04-30T05:32:59.000Z" class="dt-published" itemprop="datePublished">2024-04-30</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/container/">container</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/cncf/" rel="tag">cncf</a>, <a class="p-category" href="/tags/container/" rel="tag">container</a>, <a class="p-category" href="/tags/kubernetes/" rel="tag">kubernetes</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <blockquote>
<p>thanks <strong><a target="_blank" rel="noopener" href="https://github.com/polarathene">polarathene</a></strong></p>
</blockquote>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>The max open files limit <code>NOFILE</code> of dockerd is 1048576, which is defined in dockerd’s systemd unit file.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/$(pidof dockerd)/limits | grep &quot;Max open files&quot;</span><br><span class="line">Max open files            1048576              1048576              files</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl show docker | grep LimitNOFILE</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNOFILESoft=1048576</span><br></pre></td></tr></table></figure>

<p>However, inside the container, the value of the limit is a very large number — 1073741816:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm ubuntu bash -c &quot;cat /proc/self/limits&quot; | grep  &quot;Max open files&quot;</span><br><span class="line">Max open files            1073741816           1073741816           files</span><br></pre></td></tr></table></figure>

<p>It may cause the program iterate all available fds until the limit is reached; for example, the <code>xinetd</code> program sets the number of file descriptors using <code>setrlimit(2)</code> at initialization, which causes unnecessary waste of CPU resources and time on closing 1073741816 fds.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@1b3165886528# strace xinetd</span><br><span class="line">execve(&quot;/usr/sbin/xinetd&quot;, [&quot;xinetd&quot;], 0x7ffd3c2882e0 /* 9 vars */) = 0</span><br><span class="line">brk(NULL)                               = 0x557690d7a000</span><br><span class="line">arch_prctl(0x3001 /* ARCH_??? */, 0x7ffee17ce6f0) = -1 EINVAL (Invalid argument)</span><br><span class="line">mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb14255c000</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">close(12024371)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024372)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024373)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024374)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024375)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024376)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024377)                         = -1 EBADF (Bad file descriptor)</span><br><span class="line">close(12024378)                         = -1 EBADF (Bad file descriptor)</span><br></pre></td></tr></table></figure>

<p>we found similar cases:</p>
<h3 id="yum-hang"><a href="#yum-hang" class="headerlink" title="yum hang"></a>yum hang</h3><p>I noticed that newest version of docker, on rockylinux-9, taken from <a target="_blank" rel="noopener" href="https://download.docker.com/linux/centos/$releasever/$basearch/stable">https://download.docker.com/linux/centos/$releasever/$basearch/stable</a> are a bit slow especially for operations done by yum</p>
<p>On both centos-7 and rocky-9 hosts I did:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure>

<p>On centos7 host it takes ~2 minutes<br>On rocky-9 host after an hour it did not complete the process, I can leave it under tmux to discover how much time it takes</p>
<p>reproduce steps:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --name centos7 quay.io/centos/centos:centos7</span><br><span class="line">docker exec -it centos7 /bin/bash -c &quot;time yum update -y&quot;</span><br></pre></td></tr></table></figure>

<h3 id="rpm-slow"><a href="#rpm-slow" class="headerlink" title="rpm slow"></a>rpm slow</h3><p>run below comamnd on host:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion</span><br></pre></td></tr></table></figure>

<p>spend time：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m11.248s</span><br><span class="line">user    0m7.316s</span><br><span class="line">sys     0m1.932s</span><br></pre></td></tr></table></figure>

<p>when test it in container</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --net=none --log-driver=none -v &quot;/workspace:/workspace&quot; -v &quot;/disks:/disks&quot; opensuse bash -c &quot;time zypper --reposd-dir /workspace/zypper/reposd --cache-dir /workspace/zypper/cache --solv-cache-dir /workspace/zypper/solv --pkg-cache-dir /workspace/zypper/pkg --non-interactive --root /workspace/root install rpm subversion&quot;</span><br></pre></td></tr></table></figure>

<p>spend time：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real    0m31.089s</span><br><span class="line">user    0m14.876s</span><br><span class="line">sys     0m12.524s</span><br></pre></td></tr></table></figure>

<p>Here’s the relevant section of code from RPM. It’s part of the POSIX lua library that’s inside RPM, and was added by <a target="_blank" rel="noopener" href="https://github.com/rpm-software-management/rpm/commit/7a7c31f551ff167f8718aea6d5048f6288d60205">rpm-software-management&#x2F;rpm@<code>7a7c31f</code></a>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">static int Pexec(lua_State *L) /** exec(path,[args]) */</span><br><span class="line">&#123;</span><br><span class="line">	/* ... */</span><br><span class="line">	open_max = sysconf(_SC_OPEN_MAX);</span><br><span class="line">	if (open_max == -1) &#123;</span><br><span class="line">	    open_max = 1024;</span><br><span class="line">	&#125;</span><br><span class="line">	for (fdno = 3; fdno &lt; open_max; fdno++) &#123;</span><br><span class="line">	    flag = fcntl(fdno, F_GETFD);</span><br><span class="line">	    if (flag == -1 || (flag &amp; FD_CLOEXEC))</span><br><span class="line">		continue;</span><br><span class="line">	    fcntl(fdno, F_SETFD, FD_CLOEXEC);</span><br><span class="line">	&#125;</span><br><span class="line">	/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>So the reason for doing <code>F_GETFD</code> is because they are setting all of the FDs to <code>CLOEXEC</code> before doing the requested <code>exec(2)</code>. There’s a <a target="_blank" rel="noopener" href="https://bugzilla.redhat.com/show_bug.cgi?id=919801">redhat bugzilla entry</a> in the commit message, which says that this was an SELinux issue where Fedora (or RHEL) have an SELinux setup where you cannot execute a process if it will inherit FDs it shouldn’t have access to?</p>
<p>I guess if this is an SELinux issue it should be handled by only applying this fix when SELinux is used (though there are arguably security reasons why you might want to <code>CLOEXEC</code> every file descriptor).</p>
<h3 id="PtyProcess-spawn-slowdown-in-close-loop"><a href="#PtyProcess-spawn-slowdown-in-close-loop" class="headerlink" title="PtyProcess.spawn slowdown in close() loop"></a>PtyProcess.spawn slowdown in close() loop</h3><p>The following code in ptyprocess</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Do not allow child to inherit open file descriptors from parent, </span><br><span class="line"> # with the exception of the exec_err_pipe_write of the pipe </span><br><span class="line"> # and pass_fds. </span><br><span class="line"> # Impose ceiling on max_fd: AIX bugfix for users with unlimited </span><br><span class="line"> # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange() </span><br><span class="line"> # occasionally raises out of range error </span><br><span class="line"> max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0]) </span><br><span class="line"> spass_fds = sorted(set(pass_fds) | &#123;exec_err_pipe_write&#125;) </span><br><span class="line"> for pair in zip([2] + spass_fds, spass_fds + [max_fd]): </span><br><span class="line">     os.closerange(pair[0]+1, pair[1]) </span><br></pre></td></tr></table></figure>

<p>is looping through all possible file descriptors in order to close those (note that <code>closerange()</code> implemented as a loop at least on Linux). In case the limit of open fds (aka <code>ulimit -n</code>, aka <code>RLIMIT_NOFILE</code>, aka <code>SC_OPEN_MAX</code>) is set too high (for example, with recent docker it is 1024*1024), this loop takes considerable time (as it results in about a million <code>close()</code> syscalls).</p>
<p>The solution (at least for Linux and Darwin) is to obtain the list of actually opened fds, and only close those. This is implemented in <code>subprocess</code> module in Python3, and there is a backport of it to Python2 called subprocess32.</p>
<h3 id="MySQL-has-been-known-to-allocate-excessive-memory"><a href="#MySQL-has-been-known-to-allocate-excessive-memory" class="headerlink" title="MySQL has been known to allocate excessive memory"></a><a target="_blank" rel="noopener" href="https://github.com/overhangio/tutor/pull/810/files#diff-a7c720c9cc5fa1b93c92c33e3e75a0b0880d80915d867d57d48464ff4f472b0aR115-R117">MySQL has been known to allocate excessive memory</a></h3><p>In idle mode, the “mysql” container should use ~200MB memory; ~200-300MB for the the “lms” and “cms” containers.</p>
<p>On some operating systems, such as RedHat, Arch Linux or Fedora, a very high limit of the number of open files (<code>nofile</code>) per container may cause the “mysql”, “lms” and “cms” containers to use a lot of memory: up to 8-16GB. To check whether you might impacted, run::</p>
<pre><code>cat /proc/$(pgrep dockerd)/limits | grep &quot;Max open files&quot;
</code></pre>
<p>If the output is 1073741816 or higher, then it is likely that you are affected by <code>this mysql issue &lt;https://github.com/docker-library/mysql/issues/579&gt;</code><strong>. To learn more about the root cause, read <code>this containerd issue comment &lt;https://github.com/containerd/containerd/pull/7566#issuecomment-1285417325&gt;</code></strong>. Basically, the OS is hard-coding a very high limit for the allowed number of open files, and this is causing some containers to fail. To resolve the problem, you should configure the Docker daemon to enforce a lower value, as described <code>here &lt;https://github.com/docker-library/mysql/issues/579#issuecomment-1432576518&gt;</code>__. Edit <code>/etc/docker/daemon.json</code> and add the following contents::</p>
<pre><code>&#123;
    &quot;default-ulimits&quot;: &#123;
        &quot;nofile&quot;: &#123;
            &quot;Name&quot;: &quot;nofile&quot;,
            &quot;Hard&quot;: 1048576,
            &quot;Soft&quot;: 1048576
        &#125;
    &#125;
&#125;
</code></pre>
<p>Check your configuration is valid with:</p>
<pre><code>dockerd --validate
</code></pre>
<p>Then restart the Docker service:</p>
<pre><code>sudo systemctl restart docker.service
</code></pre>
<h2 id="Technical-Background-Introduction"><a href="#Technical-Background-Introduction" class="headerlink" title="Technical Background Introduction"></a>Technical Background Introduction</h2><h3 id="1-RLIMIT-NOFILE"><a href="#1-RLIMIT-NOFILE" class="headerlink" title="1. RLIMIT_NOFILE"></a>1. RLIMIT_NOFILE</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></p>
</blockquote>
<p>Don’t use. Be careful when raising the soft limit above 1024, since <a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a> cannot function with file descriptors above 1023 on Linux. Nowadays, the hard limit defaults to 524288, a very high value compared to historical defaults. Typically applications should increase their soft limit to the hard limit on their own, if they are OK with working with file descriptors above 1023, i.e. do not use <a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/select.2.html">select(2)</a>. Note that file descriptors are nowadays accounted like any other form of memory, thus there should not be any need to lower the hard limit. Use <code>MemoryMax=</code> to control overall service memory use, including file descriptor memory.</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94">https://github.com/systemd/systemd/blob/1742aae2aa8cd33897250d6fcfbe10928e43eb2f/NEWS#L60..L94</a></p>
</blockquote>
<p>The Linux kernel’s current default RLIMIT_NOFILE resource limit for userspace processes is set to 1024 (soft) and 4096 (hard). Previously, systemd passed this on unmodified to all processes it forked off. With this systemd release the hard limit systemd passes on is increased to 512K, overriding the kernel’s defaults and substantially increasing the number of simultaneous file descriptors unprivileged userspace processes can allocate. Note that the soft limit remains at 1024 for compatibility reasons: the traditional UNIX select() call cannot deal with file descriptors &gt;&#x3D; 1024 and increasing the soft limit globally might thus result in programs unexpectedly allocating a high file descriptor and thus failing abnormally when attempting to use it with select() (of course, programs shouldn’t use select() anymore, and prefer poll()&#x2F;epoll, but the call unfortunately remains undeservedly popular at this time). This change reflects the fact that file descriptor<br>handling in the Linux kernel has been optimized in more recent kernels and allocating large numbers of them should be much cheaper both in memory and in performance than it used to be. Programs that<br>want to take benefit of the increased limit have to “opt-in” into high file descriptors explicitly by raising their soft limit. Of course, when they do that they must acknowledge that they cannot use select() anymore (and neither can any shared library they use — or any shared library used by any shared library they use and so on). Which default hard limit is most appropriate is of course hard to decide. However, given reports that ~300K file descriptors are used in real-life applications we believe 512K is sufficiently high as new default for now. Note that there are also reports that using very high hard limits (e.g. 1G) is problematic: some software allocates large arrays with one element for each potential file descriptor  (Java, …) — a high hard limit thus triggers excessively large memory allocations in these applications. Hopefully, the new default of 512K is a good middle ground: higher than what real-life applications currently need, and low enough for avoid triggering excessively large allocations in problematic software. (And yes, somebody should fix<br>Java.)</p>
<p>systemd v240 release in 2018Q4. Both Docker and Containerd projects have recently removed the line from their configs to rely on the <code>1024:524288</code> default systemd v240 provides (<em>unless the system has been configured explicitly to some other value, which the system administrator may do so when they know they need higher limits</em>).</p>
<h3 id="2-File-Descriptor-Limits"><a href="#2-File-Descriptor-Limits" class="headerlink" title="2.  File Descriptor Limits"></a>2.  File Descriptor Limits</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This specifies a value one greater than the maximum file descriptor number that can be opened by this process. Attempts (open(2), pipe(2), dup(2), etc.)  to exceed this limit yield the error EMFILE.  (Historically, this limit was named RLIMIT_OFILE on BSD.)</span><br><span class="line">Since Linux 4.5, this limit also defines the maximum number of file descriptors that an unprivileged process (one without the CAP_SYS_RESOURCE capability) may have &quot;in</span><br><span class="line">flight&quot; to other processes, by being passed across UNIX domain sockets.  This limit applies to the sendmsg(2) system call.  For further details, see unix(7).</span><br></pre></td></tr></table></figure>

<p>The primary way to reference, allocate and pin runtime OS resources on Linux today are file descriptors (“fds”). Originally they were used to reference open files and directories and maybe a bit more, but today they may be used to reference almost any kind of runtime resource in Linux userspace, including open devices, memory (<a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd_create(2)</code></a>), timers (<a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/timerfd_create.2.html"><code>timefd_create(2)</code></a>) and even processes (with the new <a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/pidfd_open.2.html"><code>pidfd_open(2)</code></a> system call). In a way, the philosophically skewed UNIX concept of “everything is a file” through the proliferation of fds actually acquires a bit of sensible meaning: “everything <em>has</em> a file <em>descriptor</em>“ is certainly a much better motto to adopt.</p>
<p>Because of this proliferation of fds, non-trivial modern programs tend to have to deal with substantially more fds at the same time than they traditionally did. Today, you’ll often encounter real-life programs that have a few thousand fds open at the same time.</p>
<p>Like on most runtime resources on Linux limits are enforced on file descriptors: once you hit the resource limit configured via <a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/getrlimit.2.html"><code>RLIMIT_NOFILE</code></a> any attempt to allocate more is refused with the <code>EMFILE</code> error — until you close a couple of those you already have open.</p>
<p>Because fds weren’t such a universal concept traditionally, the limit of <code>RLIMIT_NOFILE</code> used to be quite low. Specifically, when the Linux kernel first invokes userspace it still sets <code>RLIMIT_NOFILE</code> to a low value of 1024 (soft) and 4096 (hard). (Quick explanation: the <em>soft</em> limit is what matters and causes the <code>EMFILE</code> issues, the <em>hard</em> limit is a secondary limit that processes may bump their soft limit to — if they like — without requiring further privileges to do so. Bumping the limit further would require privileges however.). A limit of 1024 fds made fds a <em>scarce</em> resource: APIs tried to be careful with using fds, since you simply couldn’t have that many of them at the same time. This resulted in some questionable coding decisions and concepts at various places: often secondary descriptors that are very similar to fds — but were not actually fds — were introduced (e.g. inotify watch descriptors), simply to avoid for them the low limits enforced on true fds. Or code tried to aggressively close fds when not absolutely needing them (e.g. <code>ftw()</code>&#x2F;<code>nftw()</code>), losing the nice + stable “pinning” effect of open fds.</p>
<p>Worse though is that certain OS level APIs were designed having only the low limits in mind. The worst offender being the BSD&#x2F;POSIX <a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man2/select.2.html"><code>select(2)</code></a> system call: it only works with fds in the numeric range of 0…1023 (aka <code>FD_SETSIZE</code>-1). If you have an fd outside of this range, tough luck: select() won’t work, and only if you are lucky you’ll detect that and can handle it somehow.</p>
<p>Linux fds are exposed as simple integers, and for most calls it is guaranteed that the lowest unused integer is allocated for new fds. Thus, as long as the <code>RLIMIT_NOFILE</code> soft limit is set to 1024 everything remains compatible with <code>select()</code>: the resulting fds will also be below 1024. Yay. If we’d bump the soft limit above this threshold though and at some point in time an fd higher than the threshold is allocated, this fd would not be compatible with <code>select()</code> anymore.</p>
<p>Because of that, indiscriminately increasing the soft <code>RLIMIT_NOFILE</code> resource limit today for every userspace process is problematic: as long as there’s userspace code still using <code>select()</code> doing so will risk triggering hard-to-handle, hard-to-debug errors all over the place.</p>
<p>However, given the nowadays ubiquitous use of fds for all kinds of resources (did you know, an eBPF program is an fd? and a cgroup too? and attaching an eBPF program to cgroup is another fd? …), we’d really like to raise the limit anyway.</p>
<p>So before we continue thinking about this problem, let’s make the problem more complex (…uh, I mean… “more exciting”) first. Having just one hard and one soft per-process limit on fds is boring. Let’s add more limits on fds to the mix. Specifically on Linux there are two system-wide sysctls: <code>fs.nr_open</code> and <code>fs.file-max</code>. (Don’t ask me why one uses a dash and the other an underscore, or why there are two of them…) On today’s kernels they kinda lost their relevance. They had some originally, because fds weren’t accounted by any other counter. But today, the kernel tracks fds mostly as small pieces of memory allocated on userspace requests — because that’s ultimately what they are —, and thus charges them to the memory accounting done anyway.</p>
<p>So now, we have four limits (actually: five if you count the memory accounting) on the same kind of resource, and all of them make a resource artificially scarce that we don’t want to be scarce. So what to do?</p>
<p>Back in systemd v240 already (i.e. 2019) we decided to do something about it. Specifically:</p>
<ul>
<li>Automatically at boot we’ll now bump the two sysctls to their maximum, making them effectively ineffective. This one was easy. We got rid of two pretty much redundant knobs. Nice!</li>
<li>The <code>RLIMIT_NOFILE</code> hard limit is bumped substantially to 512K. Yay, cheap fds! <em>You</em> may have an fd, and <em>you</em>, and <em>you</em> as well, <em>everyone</em> may have an fd!</li>
<li>But … we left the soft <code>RLIMIT_NOFILE</code> limit at 1024. We weren’t quite ready to break all programs still using <code>select()</code> in 2019 yet. But it’s not as bad as it might sound I think: given the hard limit is bumped every program can easily opt-in to a larger number of fds, by setting the soft limit to the hard limit early on — without requiring privileges.</li>
</ul>
<p>So effectively, with this approach fds should be much less scarce (at least for programs that opt into that), and the limits should be much easier to configure, since there are only two knobs now one really needs to care about:</p>
<ul>
<li>Configure the <code>RLIMIT_NOFILE</code> hard limit to the maximum number of fds you actually want to allow a process.</li>
<li>In the program code then either bump the soft to the hard limit, or not. If you do, you basically declare “I understood the problem, I promise to not use <code>select()</code>, drown me fds please!”. If you don’t then effectively everything remains as it always was.</li>
</ul>
<p>Apparently this approach worked, since the negative feedback on change was even scarcer than fds traditionally were (ha, fun!). We got reports from pretty much only two projects that were bitten by the change (one being a JVM implementation): they already bumped their soft limit automatically to their hard limit during program initialization, and then allocated an array with one entry per possible fd. With the new high limit this resulted in one massive allocation that traditionally was just a few K, and this caused memory checks to be hit.</p>
<p>Anyway, here’s the take away of this blog story:</p>
<ul>
<li>Don’t use <code>select()</code> anymore in 2021. Use <code>poll()</code>, <code>epoll</code>, <code>iouring</code>, …, but for heaven’s sake don’t use <code>select()</code>. It might have been all the rage in the 1990s but it doesn’t scale and is simply not designed for today’s programs. I wished the man page of <code>select()</code> would make clearer how icky it is and that there are plenty of more preferably APIs.</li>
<li>If you hack on a program that potentially uses a lot of fds, add <a target="_blank" rel="noopener" href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L373">some simple code</a> somewhere to its start-up that bumps the <code>RLIMIT_NOFILE</code> soft limit to the hard limit. But if you do this, you have to make sure your code (and any code that you link to from it) refrains from using <code>select()</code>. (Note: there’s at least one glibc NSS plugin using <code>select()</code> internally. Given that NSS modules can end up being loaded into pretty much <em>any</em> process such modules should probably be considered just buggy.)</li>
<li>If said program you hack on forks off foreign programs, make sure to reset the <code>RLIMIT_NOFILE</code> soft limit <a target="_blank" rel="noopener" href="https://github.com/systemd/systemd/blob/e7901aba1480db21e06e21cef4f6486ad71b2ec5/src/basic/rlimit-util.c#L394">back to 1024</a> for them. Just because your program might be fine with fds &gt;&#x3D; 1024 it doesn’t mean that those foreign programs might. And unfortunately <code>RLIMIT_NOFILE</code> is inherited down the process tree unless explicitly set.</li>
</ul>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Supervisor/supervisor/issues/26"><code>supervisord</code> with <code>select()</code></a> (<em>2011 reported, <strong>2014</strong> fixed</em>).</li>
<li>Nginx will <a target="_blank" rel="noopener" href="https://nginx.org/en/docs/ngx_core_module.html#worker_rlimit_nofile">raise soft limit when you tell it to</a> via config (<em><a target="_blank" rel="noopener" href="https://forum.nginx.org/read.php?2,258259,258259">bug report in <strong>2015</strong>, where <code>select()</code> was used</a> by nginx limiting it to <code>1024</code></em>). Alternatively if you’re using the nginx container, you can raise the soft limit at the container level.</li>
<li>Redis docs advising <code>2^16</code> in example, will be dependent upon your workload.<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/redis/redis-py/issues/419#issuecomment-41134427"><code>redis-py</code> Dec 2013 issue with <code>select()</code></a>, fixed in June <strong>2014</strong></li>
<li><code>redis/hiredis</code> 2015 issue where a <a target="_blank" rel="noopener" href="https://github.com/redis/hiredis/issues/385#issue-123387718">user was relying on <code>select()</code></a>.</li>
<li><a target="_blank" rel="noopener" href="https://blog.pjam.me/posts/select-syscall-in-rust/">Nov 2020 article on <code>select()</code></a>, references Redis still carries <code>select()</code> as a fallback (<a target="_blank" rel="noopener" href="https://github.com/redis/redis/blob/e12f2decc1cf7742878d516d89d38af178119b17/src/ae_select.c"><code>ae_select.c</code></a>)</li>
</ul>
</li>
<li>httpd has <code>select()</code> (see <a target="_blank" rel="noopener" href="https://github.com/apache/httpd/commit/8286cb80fd160a4259ddf10c3ea016777d34d083">this 2002 commit</a>, which is <a target="_blank" rel="noopener" href="https://github.com/apache/httpd/blob/bc0e56cdd3eebbe0fae3f9f5770b09236e8a4a17/modules/http/http_core.c#L241">still present today</a> in 2024_)</li>
<li>Postgres hasthis 2010 response for why they don’t use the hard limit to not negatively impact other software running. Less of an issue in a container, especially when you can set the limits. The limits work a little bit differently now , that the issue shouldn’t be applicable anymore (the global FD limit for a system is notably higher than the hard limit tends to be per process).<ul>
<li><a target="_blank" rel="noopener" href="https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAX-FILES-PER-PROCESS">Docs on <code>max_files_per_process</code></a>. Similar to nginx, there is a specific setting here, and it’s also per process this software manages. Both of these are doing the correct thing by not having a monolithic process sharing the same FD limit. The soft limit applied is per process, if you have 1024 processes with the soft limit of 1024, you also have <code>2^20</code> FDs available across them…not only 1024.</li>
<li>Postgres still has source with usage of <code>select()</code> <a target="_blank" rel="noopener" href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pg_basebackup/receivelog.c#L904">here</a>, <a target="_blank" rel="noopener" href="https://github.com/postgres/postgres/blob/0eb23285a2579591c09a591e5a52829f65665341/src/bin/pgbench/pgbench.c#L7862">here</a> and various other locations if you want to look through it.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://narkive.com/CrxObBlI.2">MongoDB using <code>select()</code> in 2014</a>, in the <a target="_blank" rel="noopener" href="https://github.com/mongodb/mongo/blob/91fc5673cab5d1267fd805f1375577df9072ea1b/src/mongo/util/net/listen.cpp#L252-L270">3.7.5 release it was still using it for <code>listen.cpp</code></a>, but that was <a target="_blank" rel="noopener" href="https://github.com/mongodb/mongo/commit/55aac9ac800531ad021f18f56d69c69ac5619245">dropped in the 3.7.6 release</a> (April <strong>2018</strong>). A <a target="_blank" rel="noopener" href="https://github.com/mongodb/mongo/blob/8de341d0d2011b51eb1d140fb4820424d29fe510/src/mongo/transport/asio/asio_utils.cpp#L131"><code>select()</code> call still exists though</a>.</li>
</ul>
<h2 id="Dig-deeping-into"><a href="#Dig-deeping-into" class="headerlink" title="Dig deeping into"></a>Dig deeping into</h2><p>ulimit, being an archaic resource management mechanism, is not completely obsoleted by cgroup controllers, and it is still an essential part of system administration.</p>
<p>Default ulimits for a new container are derived from those of <del>dockerd</del> containerd itself. They are set in <code>containerd.service</code> systemd unit file to <code>unlimited</code> values:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep ^Limit /lib/systemd/system/containerd.service</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure>

<p>This is required for containerd itself, but is way too generous for containers it runs. For comparison, ulimits for a user (including root) on the host system are pretty modest (this is an example from Ubuntu 18.04):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ulimit -a</span><br><span class="line">core file size          (blocks, -c) 0</span><br><span class="line">data seg size           (kbytes, -d) unlimited</span><br><span class="line">scheduling priority             (-e) 0</span><br><span class="line">file size               (blocks, -f) unlimited</span><br><span class="line">pending signals                 (-i) 62435</span><br><span class="line">max locked memory       (kbytes, -l) 16384</span><br><span class="line">max memory size         (kbytes, -m) unlimited</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">pipe size            (512 bytes, -p) 8</span><br><span class="line">POSIX message queues     (bytes, -q) 819200</span><br><span class="line">real-time priority              (-r) 0</span><br><span class="line">stack size              (kbytes, -s) 8192</span><br><span class="line">cpu time               (seconds, -t) unlimited</span><br><span class="line">max user processes              (-u) 62435</span><br><span class="line">virtual memory          (kbytes, -v) unlimited</span><br><span class="line">file locks                      (-x) unlimited</span><br></pre></td></tr></table></figure>



<p>This can create a number of problems, such as container abusing system resources (e.g. DoS attacks). In general, cgroup limits should be used to prevent those, yet I think ulimits should be set to a saner values.</p>
<p>In particular, <code>RLIMIT_NOFILE</code>, a number of open files limit, which is set to 2^20 (aka 1048576), causes a slowdown in a number of programs, as they use the upper limit value to iterate over all potentially opened file descriptors, closing those (or setting CLOEXEC bit) before every fork&#x2F;exec. I am aware of the following cases:</p>
<ul>
<li>rpm, reported in <a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/23137">Slow performance when installing RPMs to create new Docker images #23137</a>, <a target="_blank" rel="noopener" href="https://bugzilla.redhat.com/show_bug.cgi?id=1537564">https://bugzilla.redhat.com/show_bug.cgi?id=1537564</a>, fix: <a target="_blank" rel="noopener" href="https://github.com/rpm-software-management/rpm/pull/444">Optimize and unite setting CLOEXEC on fds rpm-software-management&#x2F;rpm#444</a> (fixed in Fedora 28).</li>
<li>python2, reported in <a target="_blank" rel="noopener" href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09 docker&#x2F;for-linux#502</a>, proposed fix: [<a target="_blank" rel="noopener" href="https://github.com/python/cpython/pull/11584">2.7] bpo-35757: subprocess.Popen: optimize close_fds for Linux python&#x2F;cpython#11584</a> (WONTFIX as python2 is frozen)</li>
<li>python’s pexpect&#x2F;ptyprocess library, reported in <a target="_blank" rel="noopener" href="https://github.com/pexpect/ptyprocess/issues/50">PtyProcess.spawn (and thus pexpect) slowdown in close() loop pexpect&#x2F;ptyprocess#50</a>.</li>
</ul>
<p>Attacking those one by one proved complicated and not very fruitful, as some software is obsoleted, some is hard to fix, etc. In addition, the above list is not a concise one, so there might be more cases like this we’re not aware of.</p>
<h3 id="Investigated-limits-impact-and-costs"><a href="#Investigated-limits-impact-and-costs" class="headerlink" title="Investigated limits impact and costs"></a>Investigated limits impact and costs</h3><p><code>2^16</code> (65k) <code>busybox</code> containers estimated resource usage:</p>
<ul>
<li>688k tasks + 206 GB (192 GiB) memory in <code>containerd</code> (<em>10.5 tasks + 3MiB per container</em>)</li>
<li>Requiring at minimum <code>LimitNOFILE=262144</code> (<code>containerd.service</code>) + <code>LimitNOFILE=393216</code> (<code>docker.service</code>) - based on <code>4:1</code> + <code>6:1</code> service FDs needed per container ratio.</li>
<li>2.49 million open files (<em><code>fs.file-nr</code> must be below <code>fs.file-max</code> limit</em>) - approx 38 FDs per container.</li>
<li>25 GiB memory for the containers cgroup (<em>approx 400KiB per container</em>).</li>
</ul>
<p><code>LimitNOFILE=524288</code> (<em>systemd default since v240</em>) should be more than enough for most systems as a sane default. This should be more than enough for both <code>docker.service</code> and <code>containerd.service</code> resource needs, capable of supporting 65k containers.</p>
<p>Containers that do need higher limits can explicitly declare that (<em>via <code>--ulimit</code> or equivalent</em>), as the upper bound is not impacted by <code>containerd.service</code>. The same can be done for lowering limits if necessary, both should rarely be necessary for most containers.</p>
<p>While <code>docker.service</code> and <code>containerd.service</code> need the higher soft limit (<em>enforced implicitly since Go 1.19</em>), it would be unlikely required for containers. An upcoming release of Go (with backports to 1.19) will implicitly restore the soft limit to <code>fork</code> &#x2F; <code>exec</code> processes AFAIK. Until then, the Docker daemon can be configured with <code>default-ulimit</code> setting to enforce a <code>1024</code> soft limit on containers.</p>
<h4 id="System-details"><a href="#System-details" class="headerlink" title="System details"></a>System details</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Fedora 37 VM 6.1.9 kernel x86_64 (16 GB memory)</span><br><span class="line">Docker v23, containerd 1.6.18, systemd v251</span><br><span class="line"></span><br><span class="line"># Additionally verified with builds before Go 1.19 to test soft limit lower than the hard limit:</span><br><span class="line">dnf install docker-ce-3:20.10.23 docker-ce-cli-1:20.10.23 containerd.io-1.6.8</span><br></pre></td></tr></table></figure>

<h4 id="Observations-in-service-files-for-LimitNOFILE"><a href="#Observations-in-service-files-for-LimitNOFILE" class="headerlink" title="Observations in .service files for LimitNOFILE"></a>Observations in <code>.service</code> files for <code>LimitNOFILE</code></h4><p>On a fresh install (<em>via VM on Vultr</em>) there was approx 1800 file descriptors open (<code>sysctl fs.file-nr</code>). I used a shell loop to run <code>busybox</code> containers until failure and adjusted the <code>LimitNOFILE</code> for <code>docker.service</code> and <code>containerd.service</code> to collect metrics for insights.</p>
<p>I noticed a consistent ratio of number of FDs needed per container:</p>
<ul>
<li><code>docker.service</code> - <code>6:1</code> ratio (<em><code>5:1</code> with <code>--network=host</code></em>), approx 853 containers with <code>LimitNOFILE=5120</code> (<em>1024 with host network</em>).</li>
<li><code>containerd.service</code> - <code>4:1</code> ratio (<em>I did not verify if <code>--network=host</code> reduced this</em>), <code>LimitNOFILE=1024</code> should be capable of 256 containers, provided <code>docker.service</code> is also high enough (eg: <code>LimitNOFILE=2048</code>_).</li>
</ul>
<p>In <code>containerd.service</code> there was also a clear pattern in resources per container, where the <code>LimitNOFILE</code> value, image used (<em><code>busybox</code>, <code>alpine</code>, <code>debian</code></em>), and number of containers remained constant:</p>
<ul>
<li><p>Each containers systemd <code>.scope</code> has 1 task and approx 400KiB memory (little bit less for <code>alpine</code> and <code>debian</code>).</p>
</li>
<li><p>10.5 tasks + 3MiB memory added per container to <code>systemctl status containerd</code> report.</p>
</li>
<li><p>Approx 38 open files per container running (<em><code>fs.file-nr</code> after, minus the before value, divided by number of containers</em>).</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mailserver/docker-mailserver:edge</span><br></pre></td></tr></table></figure>

<p>  was also tested to compare to the</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sleep 180</span><br></pre></td></tr></table></figure>

<p>  containers:</p>
<pre><code>- 33 tasks per container `.scope` and 85MiB memory reported via `systemd-cgtop` (*10GiB needed min to run 120 of these containers*).
- In `containerd` per container average resources were 11 tasks + 3.4MiB memory (*approx 400MiB usage for 120 of these containers*). Roughly consistent with the lighter images resource usage in `containerd`.
- Files opened per container also increased to 291 (*approx 35k files open for 120 of these containers*).
- If you want to reproduce for this image, `docker run` should include these extra options: `--hostname example.test --env SMTP_ONLY=1` (*hostname required to init, `SMTP_ONLY=1` skips needing an account configured*).
</code></pre>
<p>Operations like <code>docker stats</code> need to open as many file descriptors as total containers running, otherwise it’ll hang waiting. You can observe if the daemon has reached the limit with <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</p>
<h4 id="Reproduction"><a href="#Reproduction" class="headerlink" title="Reproduction"></a>Reproduction</h4><p>Set <code>LimitNOFILE=768</code> in <code>docker.service</code>, then <code>systemctl daemon-reload &amp;&amp; systemctl restart docker</code>. You can confirm the limit is applied to the daemon process with <code>cat /proc/$(pidof dockerd)/limits</code>.</p>
<p>Running the following should list:</p>
<ul>
<li>How many containers are running.</li>
<li>Number of open files.</li>
<li>How many tasks and memory both the <code>containerd</code> and <code>dockerd</code> daemons are using.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Useful to run before the loop to compare against output after the loop is done</span><br><span class="line">(pgrep containerd-shim | wc -l) &amp;&amp; sysctl fs.file-nr \</span><br><span class="line">  &amp;&amp; (echo &#x27;Containerd service:&#x27; &amp;&amp; systemctl status containerd | grep -E &#x27;Tasks|Memory&#x27;) \</span><br><span class="line">  &amp;&amp; (echo &#x27;Docker service:&#x27; &amp;&amp; systemctl status docker | grep -E &#x27;Tasks|Memory&#x27;)</span><br></pre></td></tr></table></figure>

<p>Running this loop below should fail on the last few containers, about 123 should be created:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># When `docker.service` limit is the bottleneck, you may need to `CTRL + C` to exit the loop</span><br><span class="line"># if it stalls while waiting for new FDs once exhausted and outputting errors:</span><br><span class="line">for i in $(seq 1 130); do docker run --rm -d busybox sleep 180; done</span><br></pre></td></tr></table></figure>

<p>You can add additional options:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--network host</span><br></pre></td></tr></table></figure>

<pre><code>- Avoids creating a new veth interface (see `ip link`) to the default Docker bridge each `docker run`.
- Without this `docker run` [may fail after `1023` interfaces are present on a single bridge](https://github.com/moby/moby/issues/44973#issuecomment-1543747718)?
- Creation will be a bit faster, and the FD to container ratio for `dockerd` is lowered to `5:1`.
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit &quot;nofile=1023456789&quot;</span><br></pre></td></tr></table></figure>

<pre><code>- Useful to observe that it does not affect memory usage on it&#39;s own.
- Also shows `dockerd` + `containerd` limits don&#39;t affect how high this can go.
- For Debian based distros this would fail as it&#39;s higher than `fs.nr_open` (`1 048 576`), use that or a lower value.
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--cgroup-parent=LimitTests.slice</span><br></pre></td></tr></table></figure>

<pre><code>- Similar to `docker stats` but isolated from other running containers. `systemd-cgtop` does include disk cache (*file-backed memory pages*) in it&#39;s reported memory usage however (*use `sync &amp;&amp; sysctl vm.drop_caches=3` to clear that*).
- This can be useful if you want a better overview of resource usage across all the containers created:
    - Create a temporary slice for testing with: `mkdir /sys/fs/cgroup/LimitTests.slice`.
    - Run `systemd-cgtop --order=memory LimitTests.slice` to only view the containers running in this cgroup slice sorted by memory usage.
    - Memory usage shown for the entire slice and per container. A `busybox` container uses roughly 400KB per container.
</code></pre>
<h3 id="Limits-impact-across-process-children"><a href="#Limits-impact-across-process-children" class="headerlink" title="Limits impact across process children"></a>Limits impact across process children</h3><p>I had a misconception that child processes contributed to the parents open files limit. However as my notes in this section detail, it’s only inheriting the limits applied, each process seems to have it’s own individual count.</p>
<p>Although, I’m probably missing something here as I have read of processes passing down FDs to children, which is also why daemons have a common hygiene practice to close the FD range available I think? This is lower level than I’m familiar with 😅</p>
<ul>
<li>You can also observe the number of file descriptors open for the <code>dockerd</code> and <code>containerd</code> processes like this: <code>ls -1 /proc/$(pidof dockerd)/fd | wc -l</code>.</li>
<li>This isn’t applicable to the <code>containerd-shim</code> process that is responsible for the container, so <code>ls -1 /proc/$(pgrep --newest --exact containerd-shim)/fd | wc -l</code> won’t be useful there.</li>
</ul>
<p>To confirm this, run a container to test with: <code>docker run --rm -it --ulimit &quot;nofile=1024:1048576&quot; alpine ash</code>. Then try the following:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Create a folder to add many files to:</span><br><span class="line">mkdir /tmp/test; cd /tmp/test</span><br><span class="line"></span><br><span class="line"># Create empty files:</span><br><span class="line">for x in $(seq 3 2048); do touch &quot;$&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Open files to specific file descriptors:</span><br><span class="line">for x in $(seq 1000 1030); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"># Fails after 1024 because soft limit is capped there. Raise it:</span><br><span class="line">ulimit -Sn 2048</span><br><span class="line"></span><br><span class="line"># Now the loop before will be successful.</span><br><span class="line"># You could cover the whole original soft limit range (excluding FDs 0-2: stdin, stdout, stderr):</span><br><span class="line">for x in $(seq 3 1024); do echo &quot;$&#123;x&#125;&quot;; eval &quot;exec $&#123;x&#125;&lt; $&#123;x&#125;.tmp&quot;; done</span><br><span class="line"></span><br><span class="line"># Multiple container processes / children opening as many files:</span><br><span class="line"># You can run the same loop in a new shell process with `ash -c &#x27;for ... done&#x27;`</span><br><span class="line"># Or `docker exec` into the container from another terminal and run the loop at `/tmp/test` again.</span><br><span class="line"># Each can open files up to their current soft limit, it doesn&#x27;t matter what limits are set on `dockerd`, `containerd` or the containers PID 1.</span><br><span class="line"></span><br><span class="line">############</span><br><span class="line">### Tips ###</span><br><span class="line">############</span><br><span class="line"></span><br><span class="line"># You can observe the current limit applied:</span><br><span class="line">cat /proc/self/limits</span><br><span class="line"># And if you have not exhausted your FDs soft limit (due to the pipe),</span><br><span class="line"># this will report how much of the limit is used:</span><br><span class="line">ls -1 /proc/self/fd | wc -l</span><br><span class="line"># Otherwise, outside of the container if this is your only `ash` process running,</span><br><span class="line"># you can query it&#x27;s PID to get this information:</span><br><span class="line">ls -1 /proc/$(pgrep --newest --exact ash)/fd | wc -l</span><br><span class="line"></span><br><span class="line"># Process count in container:</span><br><span class="line"># `docker stats` should list a containers PIDs count for number of processes,</span><br><span class="line"># `systemd-cgtop` should report the same value in it&#x27;s Tasks column.</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line"># (NOTE: Path may differ if you used `--cgroup-parent`)</span><br><span class="line">cat /sys/fs/cgroup/system.slice/docker-&lt;CONTAINER_ID&gt;.scope/pids.current</span><br><span class="line"></span><br><span class="line"># List the processes with their PIDs:</span><br><span class="line"># For a single container, you can visualize the process tree:</span><br><span class="line">pstree --arguments --show-pids $(pgrep --newest --exact containerd-shim)</span><br><span class="line"># Alternatively if you know the cgroup name like `docker-&lt;CONTAINER_ID&gt;.scope`:</span><br><span class="line">systemd-cgls --unit docker-&lt;CONTAINER_ID&gt;.scope</span><br><span class="line"></span><br><span class="line"># You can also observe disk cache in memory monitoring by creating a 1GB file:</span><br><span class="line">dd if=/dev/zero of=bigfile bs=1M count=1000</span><br><span class="line">free -h</span><br><span class="line"># `systemd-cgtop` will include 1GB more in memory usage for the container,</span><br><span class="line"># while `docker stats` appears to account only 30MiB (scales proportionally).</span><br><span class="line"># Now clear the cache outside of the container and observe memory usage again:</span><br><span class="line">sync &amp;&amp; sysctl vm.drop_caches=3</span><br></pre></td></tr></table></figure>



<p>You will notice that:</p>
<ul>
<li><p>Each process adds those FDs to the open file count returned from <code>fs.file-nr</code>, and frees them when that process is closed.</p>
</li>
<li><p>You can re-run the loops for the same process and observe no change, the files are already counted as open for that process.</p>
</li>
<li><p>There is a memory cost involved:</p>
<ul>
<li>Each file <code>touch</code> costs about <code>2048</code> bytes (<em>disk-cache only until opened</em>).</li>
<li>Each file open (<em>1 or more FD references each increment <code>fs.file-nr</code></em>) costs about <code>512</code> bytes per FD open for it.</li>
<li>Creating 512k files this way uses approx 1.1GiB memory (<em>not released with <code>sysctl vm.drop_caches=3</code> while opened by at least one FD</em>), while each process opening the equivalent amount of file descriptors additionally uses 250MiB (262MB).</li>
</ul>
</li>
</ul>
<h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>Nothing useful here, other than depending on which service limit was exhausted first resulted in slightly different errors.</p>
<p>Sometimes this made any <code>docker</code> command like <code>docker ps</code> hang (daemon exhausted limit). I had also observed:</p>
<ul>
<li>Containers not running (<em>no <code>pgrep containerd-shim</code> output, but <code>docker ps</code> listed containers running well beyond when they should have exited</em>).</li>
<li>Containers running with <code>containerd-shim</code> process (using memory), despite <code>systemctl stop docker containerd</code>. Sometimes this needed <code>pkill containerd-shim</code> to cleanup and <code>systemctl start docker containerd</code> would log a bunch of errors in <code>journalctl</code> handling cleanup of dead shims (<em>depending on the number of containers, this may time out and need to start the <code>containerd</code> service again</em>).</li>
<li>Even with all that out of the way, there was some memory usage of several hundred MB from the baseline that lingered. As it didn’t seem to belong to any process, I assume it was kernel memory. I think the largest number of containers I experimented running with was around 1600-ish.</li>
</ul>
<h4 id="docker-service-limit-exceeded"><a href="#docker-service-limit-exceeded" class="headerlink" title="docker.service limit exceeded"></a><code>docker.service</code> limit exceeded</h4><p>This failure output more errors per <code>docker run</code> but the errors varied:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0000] Error waiting for container: container caff476371b6897ef35a95e26429f100d0d929120ff1abecc8a16aa674d692bf: driver &quot;overlay2&quot; failed to remove root filesystem: openfdat /var/lib/docker/overlay2/35f26ec862bb91d7c3214f76f8660938145bbb36eda114f67e711aad2be89578-init/diff/etc: too many open files </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: time=&quot;2023-03-12T02:26:20Z&quot; level=fatal msg=&quot;failed to create a netlink handle: could not get current namespace while creating netlink socket: too many open files&quot;: unknown.</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to initialize logging driver: open /var/lib/docker/containers/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f/b014a19f7eb89bb909dee158d21f35f001cfeb80c01e0078d6f20aac8151573f-json.log: too many open files.</span><br></pre></td></tr></table></figure>

<h4 id="containerd-service-limit-exceeded"><a href="#containerd-service-limit-exceeded" class="headerlink" title="containerd.service limit exceeded"></a><code>containerd.service</code> limit exceeded</h4><p>I think I’ve seen some others, but it’s usually this one:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: failed to start shim: start failed: : pipe2: too many open files: unknown.</span><br></pre></td></tr></table></figure>



<h2 id="Scope-and-Explanation"><a href="#Scope-and-Explanation" class="headerlink" title="Scope and Explanation"></a>Scope and Explanation</h2><ul>
<li><p>Aug 2023: <code>LimitNOFILE=infinity</code>  <a target="_blank" rel="noopener" href="https://github.com/moby/moby/pull/42373">remove from <code>docker.service</code></a></p>
</li>
<li><p>May 2021: <code>LimitNOFILE=infinity</code> + <code>LimitNPROC=infinity</code> <a target="_blank" rel="noopener" href="https://github.com/moby/moby/pull/42373">brought back into <code>docker.service</code></a> to sync with Docker CE’s equivalent config.</p>
<ul>
<li>This PR was a merge commit of <a target="_blank" rel="noopener" href="https://github.com/moby/moby/commit/80039b4699e36ceb0eb81109cd1686aaa805c5ec">this one from Sep 2018</a> (<em>commit history interleaved ranges from 2017-2021</em>).</li>
<li>As the PR main diff shows, <code>LimitNPROC=infinity</code> was already added, and <code>LimitNOFILE=1048576</code> was changed to <code>infinity</code> by the PR merge (<em>initially confused since I’m using <code>git blame</code> on the master branch</em>).</li>
</ul>
</li>
<li><p>July 2016: <a target="_blank" rel="noopener" href="https://github.com/moby/moby/pull/24555"><code>LimitNOFILE=infinity</code> changed to <code>LimitNOFILE=1048576</code></a> (<em>this number is <code>2^20</code></em>).</p>
<ul>
<li>Discussion references a <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible">2009 StackOverflow answer</a> about <code>infinity</code> being capped to <code>2^20</code> in a specific distro release &#x2F; kernel. On some systems today, that ceiling can be 1024 times higher (<em><code>2^30 == 1073741816</code>, over 1 billion</em>).</li>
</ul>
</li>
<li><p>July 2016: <a target="_blank" rel="noopener" href="https://github.com/moby/moby/pull/24307"><code>LimitNOFILE</code> and <code>LimitNPROC</code> changed from <code>1048576</code> to <code>infinity</code></a></p>
<ul>
<li>This PR reverted the <code>LimitNOFILE</code> change shortly after as described above.</li>
</ul>
</li>
<li><p>March 2014: <a target="_blank" rel="noopener" href="https://github.com/moby/moby/pull/4455#issuecomment-36679884">Original <code>LimitNOFILE</code> + <code>LimitNPROC</code> added with <code>1048576</code></a>.</p>
<ul>
<li><p>Linked PR comment mentions that this 2^20 value is already higher than Docker needs:</p>
</li>
<li><p>It appears it was later changed to <code>infinity</code> to improve CI times where a smaller limit was applied (<em>like <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/1212925/on-linux-set-maximum-open-files-to-unlimited-possible#comment76259289_1213069">this comment about Ubuntu 14.04 adjusting any limit exceeding <code>2^20</code> to <code>2^10</code>?</a></em>).</p>
</li>
<li><p>PR also [referenced relevant systemd docs](<a target="_blank" rel="noopener" href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process">https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process</a> Properties) (<em>which may have changed since 2014</em>)</p>
</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Current Status:</strong></p>
<ul>
<li><code>LimitNOFILE=infinity</code> is still the case until Docker v25, unless the team is backporting it to any releases built with Go 1.19+</li>
<li><code>containerd</code> has <a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/pull/8924">merged the equivalent change to remove <code>LimitNOFILE</code></a> from their systemd service file.</li>
</ul>
<h3 id="Systemd-240"><a href="#Systemd-240" class="headerlink" title="Systemd &lt; 240"></a>Systemd &lt; 240</h3><p>Why is LimitNOFILE not set to infinity when configured in the service?</p>
<p>After setting LimitNOFILE to infinity in the service, when checking the limit of the process ID (pid), it is observed that the open file limit is 65536 instead of infinity.</p>
<p>Please review the service configuration.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# ulimit -n -u</span><br><span class="line">open files                      (-n) 1024</span><br><span class="line">max user processes              (-u) 499403</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>containerd systemd configuration:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/lib/systemd/system/containerd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=containerd container runtime</span><br><span class="line">Documentation=https://containerd.io</span><br><span class="line">After=network.target local-fs.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/local/bin/containerd</span><br><span class="line"></span><br><span class="line">Type=notify</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"># Having non-zero Limit*s causes performance problems due to accounting overhead</span><br><span class="line"># in the kernel. We recommend using cgroups to do container-local accounting.</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line"># Comment TasksMax if your systemd version does not supports it.</span><br><span class="line"># Only systemd 226 and above support this version.</span><br><span class="line">TasksMax=infinity</span><br><span class="line">OOMScoreAdjust=-999</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<p>Viewing the configuration effect</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@XXX ~]# cat /proc/$(pidof dockerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us        </span><br><span class="line">[root@XXX ~]# cat /proc/$(pidof containerd)/limits</span><br><span class="line">Limit                     Soft Limit           Hard Limit           Units     </span><br><span class="line">Max cpu time              unlimited            unlimited            seconds   </span><br><span class="line">Max file size             unlimited            unlimited            bytes     </span><br><span class="line">Max data size             unlimited            unlimited            bytes     </span><br><span class="line">Max stack size            8388608              unlimited            bytes     </span><br><span class="line">Max core file size        unlimited            unlimited            bytes     </span><br><span class="line">Max resident set          unlimited            unlimited            bytes     </span><br><span class="line">Max processes             unlimited            unlimited            processes </span><br><span class="line">Max open files            1048576              1048576              files     </span><br><span class="line">Max locked memory         65536                65536                bytes     </span><br><span class="line">Max address space         unlimited            unlimited            bytes     </span><br><span class="line">Max file locks            unlimited            unlimited            locks     </span><br><span class="line">Max pending signals       499403               499403               signals   </span><br><span class="line">Max msgqueue size         819200               819200               bytes     </span><br><span class="line">Max nice priority         0                    0                    </span><br><span class="line">Max realtime priority     0                    0                    </span><br><span class="line">Max realtime timeout      unlimited            unlimited            us</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">This has systemd look at /proc/sys/fs/nr_open to find the current maximum of</span><br><span class="line">open files compiled into the kernel and tries to set the RLIMIT_NOFILE max to</span><br><span class="line">it. This has the advantage the value chosen as limit is less arbitrary and also</span><br><span class="line">improves the behavior of systemd in containers that have an rlimit set: When</span><br><span class="line">systemd currently starts in a container that has RLIMIT_NOFILE set to e.g.</span><br><span class="line">100000 systemd will lower it to 65536. With this patch systemd will try to set</span><br><span class="line">the nofile limit to the allowed kernel maximum. If this fails, it will compute</span><br><span class="line">the minimum of the current set value (the limit that is set on the container)</span><br><span class="line">and the maximum value as soft limit and the currently set maximum value as the</span><br><span class="line">maximum value. This way it retains the limit set on the container.</span><br></pre></td></tr></table></figure>

<p>see: <a target="_blank" rel="noopener" href="https://github.com/systemd/systemd/issues/6559">https://github.com/systemd/systemd/issues/6559</a></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/45838">https://github.com/moby/moby/issues/45838</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/23137">https://github.com/moby/moby/issues/23137</a></li>
<li><a target="_blank" rel="noopener" href="https://0pointer.net/blog/file-descriptor-limits.html">https://0pointer.net/blog/file-descriptor-limits.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.codenong.com/cs105896693/">https://www.codenong.com/cs105896693/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cri-o/cri-o/issues/7703">https://github.com/cri-o/cri-o/issues/7703</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/envoyproxy/envoy/issues/31502">https://github.com/envoyproxy/envoy/issues/31502</a></li>
<li><a target="_blank" rel="noopener" href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties">https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties</a></li>
</ol>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/zouyee">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Description"><span class="toc-number">1.</span> <span class="toc-text">Description</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#yum-hang"><span class="toc-number">1.1.</span> <span class="toc-text">yum hang</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rpm-slow"><span class="toc-number">1.2.</span> <span class="toc-text">rpm slow</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PtyProcess-spawn-slowdown-in-close-loop"><span class="toc-number">1.3.</span> <span class="toc-text">PtyProcess.spawn slowdown in close() loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL-has-been-known-to-allocate-excessive-memory"><span class="toc-number">1.4.</span> <span class="toc-text">MySQL has been known to allocate excessive memory</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Technical-Background-Introduction"><span class="toc-number">2.</span> <span class="toc-text">Technical Background Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RLIMIT-NOFILE"><span class="toc-number">2.1.</span> <span class="toc-text">1. RLIMIT_NOFILE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-File-Descriptor-Limits"><span class="toc-number">2.2.</span> <span class="toc-text">2.  File Descriptor Limits</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#select"><span class="toc-number">2.3.</span> <span class="toc-text">select</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dig-deeping-into"><span class="toc-number">3.</span> <span class="toc-text">Dig deeping into</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Investigated-limits-impact-and-costs"><span class="toc-number">3.1.</span> <span class="toc-text">Investigated limits impact and costs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#System-details"><span class="toc-number">3.1.1.</span> <span class="toc-text">System details</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Observations-in-service-files-for-LimitNOFILE"><span class="toc-number">3.1.2.</span> <span class="toc-text">Observations in .service files for LimitNOFILE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reproduction"><span class="toc-number">3.1.3.</span> <span class="toc-text">Reproduction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limits-impact-across-process-children"><span class="toc-number">3.2.</span> <span class="toc-text">Limits impact across process children</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Errors"><span class="toc-number">3.3.</span> <span class="toc-text">Errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#docker-service-limit-exceeded"><span class="toc-number">3.3.1.</span> <span class="toc-text">docker.service limit exceeded</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#containerd-service-limit-exceeded"><span class="toc-number">3.3.2.</span> <span class="toc-text">containerd.service limit exceeded</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scope-and-Explanation"><span class="toc-number">4.</span> <span class="toc-text">Scope and Explanation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Systemd-240"><span class="toc-number">4.1.</span> <span class="toc-text">Systemd &lt; 240</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">5.</span> <span class="toc-text">References</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zoues.com/posts/5cdbe8ce/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zoues.com/posts/5cdbe8ce/&text=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zoues.com/posts/5cdbe8ce/&is_video=false&description=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Why does RLIMIT_NOFILE slow down your containerized application&body=Check out this article: https://zoues.com/posts/5cdbe8ce/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zoues.com/posts/5cdbe8ce/&title=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zoues.com/posts/5cdbe8ce/&name=Why does RLIMIT_NOFILE slow down your containerized application&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zoues.com/posts/5cdbe8ce/&t=Why does RLIMIT_NOFILE slow down your containerized application"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    zouyee
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/zouyee">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
